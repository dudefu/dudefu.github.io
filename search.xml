<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[多态的学习]]></title>
    <url>%2F%E5%A4%9A%E6%80%81%E7%9A%84%E5%AD%A6%E4%B9%A0.html</url>
    <content type="text"><![CDATA[一、认识多态 1、方法调用 在Java中，方法调用有两类，动态方法调用与静态方法调用。 （1）静态方法调用是指对于类的静态方法的调用方式，是在编译时刻就已经确定好具体调用方法的情况，是静态绑定的。 （2）动态方法调用需要有方法调用所作用的对象，是在调用的时候才确定具体的调用方法，是动态绑定的。 我们这里所讲的多态就是后者—动态方法调用。 2、多态概念 多态有两种：类内部之间的多态和类之间的多态。我们先看一下标准的概念： 多态是面向对象编程语言的重要特性，它允许基类的指针或引用指向派生类的对象，而在具体访问时实现方法的动态绑定 （1）Java的方法重载（类内部之间的多态）：就是在类中可以创建多个方法，它们具有相同的名字，但可具有不同的参数列表、返回值类型。我们举个例子来解释，就是一对夫妇生了多胞胎，多胞胎之间外观相似，其实是不同的孩子。 （2）Java的方法重写（父类与子类之间的多态）：子类可继承父类中的方法，但有时子类并不想原封不动地继承父类的方法，而是想作一定的修改，这就需要采用方法的重写。重写的参数列表和返回类型均不可修改。我们再举个例子，就是子承父业，但是儿子有自己想法，对父亲得产业进行再投资的过程。 二、代码实现多态 1、类内部之间得多态：方法重载 12345678910111213141516171819202122232425public class SingleClass &#123; //孩子1： public String child()&#123; System.out.println("child1"); return "child1"; &#125; //孩子2：与孩子1参数个数不同 public String child(String a)&#123; System.out.println("child2"); return "child2"; &#125; //孩子3：与孩子4参数顺序不同 public String child(int a,String s)&#123; System.out.println("child3"); return "child3"; &#125; //孩子4：与孩子3参数顺序不同 public String child(String s,int a)&#123; System.out.println("child4"); return "child4"; &#125; public static void main(String[] args)&#123; //重载方法调用：略 &#125;&#125; 从上述代码我们可以看到，在类的内部可以有相同的方法名，但是有唯一的参数列表。当然返回类型和修饰符也可以不同。下面我们再看一下类之间的多态。 2、类之间的多态：方法重写 类之间的多态其实是有两种方式：继承和接口。我们对这两种方式一个一个说明。 （1）继承方式实现多态 对于继承方式我们使用一个例子来解释，比如说父亲可以对自己的房子有处理权，儿子继承父业同样也有处理权。 第一步：定义父类 12345public class Father &#123; public void dealHouse()&#123; System.out.println("父亲处置房产"); &#125; &#125; 第二步：定义子类（大儿子和小儿子） 1234567891011121314//大儿子public class SonA extends Father &#123; @Override public void dealHouse() &#123; System.out.println("大儿子处置房产"); &#125;&#125;//小儿子public class SonB extends Father &#123; @Override public void dealHouse() &#123; System.out.println("小儿子处置房产"); &#125;&#125; 第三步：测试 12345678910111213public class Test &#123; public static void main(String[] args) &#123; Father father=new Father(); Father sonA=new SonA(); Father sonB=new SonB(); father.dealHouse(); sonA.dealHouse(); sonB.dealHouse(); &#125;&#125;//父亲处置房产//大儿子处置房产//小儿子处置房产 （2）接口方式实现多态 接口方式实现继承方式其实跟上面一样，只不过把父类变成了接口而已，其他内容只有微笑的变化，这里就不演示了，在这里只给出父接口的形式。 123public interface Father &#123; public void dealHouse();&#125; 到了这基本上就对多态形式的代码实现进行了演示，案例也比较简单，但是这对我们理解多态的思想还不够，我们最主要的还是从虚拟机的角度来分析一下。 三、分析多态 想要深入分析多态，我们需要弄清楚几个问题。 1、jvm内存 在上面的代码中我们其实已经看到了，不管是类内部之间实现的多态，还是类之间实现的多态，这些方法的名字其实都是一样的，那我们的程序在运行的时候，底层虚拟机是如何去区分的呢（java虚拟机实现动态调用）？为此我们还是先从java虚拟机讲起。 其实java虚拟机在执行java程序的时候，并不是直接运行的，他需要一个过程，我们使用一张图来看下： 上面这张图已经很清晰，也就是说，我们的java文件要想运行，需要通过java编译器编译成.class文件，然后通过类装载器讲.class文件装载到JVM中，最后才是执行。而且JVM分了五个区域，那么在代码中定义的那些多态方法存到了哪个地方呢？为此我们还需要对这块内存区域进行一个分析： 我给出了一张java7的运行时数据区划分图，对于每一个区域的基本情况我相信你也能看明白。那么我们的多态方法到底存在了哪呢？没错就是后一个方法区。java堆存的是就是我们建立的一个个实例对象，而方法区存的就是类的类型信息。 而且这个方法区中的类型信息跟在堆中存放的class对象是不同的。在方法区中，这个class的类型信息只有唯一的实例（所以方法区是各个线程共享的内存区域），而在堆中可以有多个该class对象。也就是说方法区的类型信息就是像一个模板，那些class对象就好比通过这些模板创建的一个个实例。 2、通过例子来分析 现在我们拿上面的例子来说明一下多态在java虚拟机中是如何实现的。在测试类中有两行代码： 12Father sonA=new SonA();Father sonB=new SonB(); 当程序运行到Father sonA=new SonA()这里就出现了多态，这是因为编译时看到Father，但是运行时new出来一个SonA类，两种类型还不一样。那么这些代码在运行的时候在内存中是如何保存的呢？ （1）Father sonA是一个引用类型，存在了java栈中的本地方法表中了。 （2）new SonA其实创建了一个实例对象，存储在了java堆中。 （3）SonA的类型数据存在了方法区中 我们在内存中看一下： reference中存储的就是对象在堆中的实际地址，在堆中存储的对象信息中包含了在方法区中的相应类型数据。流程很简单，我们梳理一下： 第一步：虚拟机通过reference（Father的引用）查询java栈中的本地变量表，得到堆中的对象类型数据的指针， 第二步：通过到对象的指针找到方法区中的对象类型数据 第三步：查询方法表定位到实际类（SonA类）的方法运行。 好了，到第三步我们知道，其实是通过方法表来定位到实际运行的方法的。下面我们再来看看这个方法表是什么。 3、方法表 方法表肯定是存在于方法区中的，它是实现多态的关键所在，这里面保存的就是实例方法的引用，而且是直接引用。java虚拟机在执行程序的时候就是通过这个方法表来确定运行哪一个多态方法的。 我们通过上面的例子，来演示一下父子类在方法表中是如何保存的： 很明显每一个类都会有一个方法表，子类中不同的方法指向不同的类型信息。继承自Object的就指向Object，继承自Father的就指向Father（也就是包含了父类的方法dealHouse）。 可能我们到这就迷糊了，既然子类的dealHouse方法其实是父类Father的，那么为什么会执行子类的dealHouse方法呢？别着急往下看。这是java虚拟机区分多态方法（实现动态调用）的精华所在。 当Son类的方法表会有一个指向Father类dealHouse方法的指针，同时也有一个指向自己dealHouse方法的指针，这时候，新的数据会覆盖原有的数据，也就是说原来指向Father.dealHouse的那个引用会被替换成指向Son.dealHouse的引用（占据原来表中的位置） 注意： 上述讲述的其实是对继承实现的多态的一种分析，对接口实现的，会有着不一样的理解。Java虚拟机 对于接口方法的调用是采用搜索方法表的方式，如，要在Father接口的方法表中找到dealHouse()方法，必须搜索Father的整个方法表。从效率上来说，接口方法的调用总是慢于类方法的调用的。 以上就是对java多态的分析与理解，总结一下就是说，类调用和接口调用两种方式区分不同方法是不一样的，类调用是根据多态方法在方法表中的位移量，而接口调用是根据搜索整个方法表来实现的。 原文地址]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>多态</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈数据挖掘中的关联规则挖掘]]></title>
    <url>%2F%E6%B5%85%E8%B0%88%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%AD%E7%9A%84%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98.html</url>
    <content type="text"><![CDATA[数据挖掘是指以某种方式分析数据源，从中发现一些潜在的有用的信息，所以数据挖掘又称作知识发现，而关联规则挖掘则是数据挖掘中的一个很重要的课题，顾名思义，它是从数据背后发现事物之间可能存在的关联或者联系。举个最简单的例子，比如通过调查商场里顾客买的东西发现，30%的顾客会同时购买床单和枕套，而购买床单的人中有80%购买了枕套，这里面就隐藏了一条关联：床单—&gt;枕套，也就是说很大一部分顾客会同时购买床单和枕套，那么对于商场来说，可以把床单和枕套放在同一个购物区，那样就方便顾客进行购物了。下面来讨论一下关联规则中的一些重要概念以及如何从数据中挖掘出关联规则。 一.关联规则挖掘中的几个概念 先看一个简单的例子，假如有下面数据集，每一组数据ti表示不同的顾客一次在商场购买的商品的集合： 1234567t1: 牛肉、鸡肉、牛奶t2: 牛肉、奶酪t3: 奶酪、靴子t4: 牛肉、鸡肉、奶酪t5: 牛肉、鸡肉、衣服、奶酪、牛奶t6: 鸡肉、衣服、牛奶t7: 鸡肉、牛奶、衣服 假如有一条规则：牛肉—&gt;鸡肉，那么同时购买牛肉和鸡肉的顾客比例是3/7，而购买牛肉的顾客当中也购买了鸡肉的顾客比例是3/4。这两个比例参数是很重要的衡量指标，它们在关联规则中称作支持度（support）和置信度（confidence）。对于规则：牛肉—&gt;鸡肉，它的支持度为3/7，表示在所有顾客当中有3/7同时购买牛肉和鸡肉，其反应了同时购买牛肉和鸡肉的顾客在所有顾客当中的覆盖范围；它的置信度为3/4，表示在买了牛肉的顾客当中有3/4的人买了鸡肉，其反应了可预测的程度，即顾客买了牛肉的话有多大可能性买鸡肉。其实可以从统计学和集合的角度去看这个问题， 假如看作是概率问题，则可以把“顾客买了牛肉之后又多大可能性买鸡肉”看作是条件概率事件，而从集合的角度去看，可以看下面这幅图： 上面这副图可以很好地描述这个问题，S表示所有的顾客，而A表示买了牛肉的顾客，B表示买了鸡肉的顾客，C表示既买了牛肉又买了鸡肉的顾客。那么C.count/S.count=3/7，C.count/A.count=3/4。 在数据挖掘中，例如上述例子中的所有商品集合I={牛肉，鸡肉，牛奶，奶酪，靴子，衣服}称作项目集合，每位顾客一次购买的商品集合ti称为一个事务，所有的事务T={t1,t2,….t7}称作事务集合，并且满足ti是I的真子集。一条关联规则是形如下面的蕴含式： X—&gt;Y，X，Y满足：X，Y是I的真子集，并且X和Y的交集为空集 其中X称为前件，Y称为后件。 对于规则X—&gt;Y，根据上面的例子可以知道它的支持度(support)=(X,Y).count/T.count，置信度(confidence)=(X,Y).count/X.count 。其中(X,Y).count表示T中同时包含X和Y的事务的个数，X.count表示T中包含X的事务的个数。 关联规则挖掘则是从事务集合中挖掘出满足支持度和置信度最低阈值要求的所有关联规则，这样的关联规则也称强关联规则。 对于支持度和置信度，我们需要正确地去看待这两个衡量指标。一条规则的支持度表示这条规则的可能性大小，如果一个规则的支持度很小，则表明它在事务集合中覆盖范围很小，很有可能是偶然发生的；如果置信度很低，则表明很难根据X推出Y。根据条件概率公式P(Y|X)=P(X,Y)/P(X)，即P(X,Y)=P(Y|X)*P(X) P(Y|X)代表着置信度，P(X,Y)代表着支持度，所以对于任何一条关联规则置信度总是大于等于支持度的。并且当支持度很高时，此时的置信度肯定很高，它所表达的意义就不是那么有用了。这里要注意的是支持度和置信度只是两个参考值而已，并不是绝对的，也就是说假如一条关联规则的支持度和置信度很高时，不代表这个规则之间就一定存在某种关联。举个最简单的例子，假如X和Y是最近的两个比较热门的商品，大家去商场都要买，比如某款手机和某款衣服，都是最新款的，深受大家的喜爱，那么这条关联规则的支持度和置信度都很高，但是它们之间没有必然的联系。然而当置信度很高时，支持度仍然具有参考价值，因为当P(Y|X)很高时，可能P(X)很低，此时P(X,Y)也许会很低。 二.关联规则挖掘的原理和过程 从上面的分析可知，关联规则挖掘是从事务集合中挖掘出这样的关联规则：它的支持度和置信度大于最低阈值（minsup,minconf），这个阈值是由用户指定的。根据支持度=(X,Y).count/T.count，置信度=(X,Y).count/X.count ，要想找出满足条件的关联规则，首先必须找出这样的集合F=X U Y ，它满足F.count/T.count ≥ minsup，其中F.count是T中包含F的事务的个数，然后再从F中找出这样的蕴含式X—&gt;Y，它满足(X,Y).count/X.count ≥ minconf，并且X=F-Y。我们称像F这样的集合称为频繁项目集，假如F中的元素个数为k，我们称这样的频繁项目集为k-频繁项目集，它是项目集合I的子集。所以关联规则挖掘可以大致分为两步： 1）从事务集合中找出频繁项目集； 2）从频繁项目集合中生成满足最低置信度的关联规则。 最出名的关联规则挖掘算法是Apriori算法，它主要利用了向下封闭属性：如果一个项集是频繁项目集，那么它的非空子集必定是频繁项目集。它先生成1-频繁项目集，再利用1-频繁项目集生成2-频繁项目集。。。然后根据2-频繁项目集生成3-频繁项目集。。。依次类推，直至生成所有的频繁项目集，然后从频繁项目集中找出符合条件的关联规则。 下面来讨论一下频繁项目集的生成过程，它的原理是根据k-频繁项目集生成（k+1）-频繁项目集。因此首先要做的是找出1-频繁项目集，这个很容易得到，只要循环扫描一次事务集合统计出项目集合中每个元素的支持度，然后根据设定的支持度阈值进行筛选，即可得到1-频繁项目集。下面证明一下为何可以通过k-频繁项目集生成（k+1）-频繁项目集： 假设某个项目集S={s1，s2…，sn}是频繁项目集，那么它的（n-1）非空子集{s1，s2，…sn-1}，{s1，s2，…sn-2，sn}…{s2，s3，…sn}必定都是频繁项目集，通过观察，任何一个含有n个元素的集合A={a1，a2，…an}，它的（n-1）非空子集必行包含两项{a1，a2，…an-2，an-1}和 {a1，a2，…an-2，an}，对比这两个子集可以发现，它们的前（n-2）项是相同的，它们的并集就是集合A。对于2-频繁项目集，它的所有1非空子集也必定是频繁项目集，那么根据上面的性质，对于2-频繁项目集中的任一个，在1-频繁项目集中必定存在2个集合的并集与它相同。因此在所有的1-频繁项目集中找出只有最后一项不同的集合，将其合并，即可得到所有的包含2个元素的项目集，得到的这些包含2个元素的项目集不一定都是频繁项目集，所以需要进行剪枝。剪枝的办法是看它的所有1非空子集是否在1-频繁项目集中，如果存在1非空子集不在1-频繁项目集中，则将该2项目集剔除。经过该步骤之后，剩下的则全是频繁项目集，即2-频繁项目集。依次类推，可以生成3-频繁项目集。。直至生成所有的频繁项目集。 得到频繁项目集之后，则需要从频繁项目集中找出符合条件的关联规则。最简单的办法是：遍历所有的频繁项目集，然后从每个项目集中依次取1、2、…k个元素作为后件，该项目集中的其他元素作为前件，计算该规则的置信度进行筛选即可。这样的穷举效率显然很低。假如对于一个频繁项目集f，可以生成下面这样的关联规则： （f-β）—&gt;β 那么这条规则的置信度=f.count/(f-β).count 根据这个置信度计算公式可知，对于一个频繁项目集f.count是不变的，而假设该规则是强关联规则，则（f-βsub）—&gt;βsub也是强关联规则，其中βsub是β的子集，因为(f-βsub).count肯定小于(f-β).count。即给定一个频繁项目集f，如果一条强关联规则的后件为β，那么以β的非空子集为后件的关联规则都是强关联规则。所以可以先生成所有的1-后件（后件只有一项）强关联规则，然后再生成2-后件强关联规则，依次类推，直至生成所有的强关联规则。 下面举例说明Apiori算法的具体流程： 假如有项目集合I={1，2，3，4，5}，有事务集T： 12345671,2,31,2,41,3,41,2,3,51,3,52,4,51,2,3,4 设定minsup=3/7，misconf=5/7。 首先：生成频繁项目集： 1-频繁项目集：{1}，{2}，{3}，{4}，{5} 生成2-频繁项目集： 根据1-频繁项目集生成所有的包含2个元素的项目集：任意取两个只有最后一个元素不同的1-频繁项目集，求其并集，由于每个1-频繁项目集元素只有一个，所以生成的项目集如下： {1，2}，{1，3}，{1，4}，{1，5} {2，3}，{2，4}，{2，5} {3，4}，{3，5} {4，5} 计算它们的支持度，发现只有{1，2}，{1，3}，{1，4}，{2，3}，{2，4}，{2，5}的支持度满足要求，因此求得2-频繁项目集： {1，2}，{1，3}，{1，4}，{2，3}，{2，4} 生成3-频繁项目集： 因为{1，2}，{1，3}，{1，4}除了最后一个元素以外都相同，所以求{1，2}，{1，3}的并集得到{1，2，3}， {1，2}和{1，4}的并集得到{1，2，4}，{1，3}和{1，4}的并集得到{1，3，4}。但是由于{1，3，4}的子集{3，4}不在2-频繁项目集中，所以需要把{1，3，4}剔除掉。然后再来计算{1，2，3}和{1，2，4}的支持度，发现{1，2，3}的支持度为3/7 ，{1，2，4}的支持度为2/7，所以需要把{1，2，4}剔除。同理可以对{2，3}，{2，4}求并集得到{2，3，4} ，但是{2，3，4}的支持度不满足要求，所以需要剔除掉。 因此得到3-频繁项目集：{1，2，3}。 到此频繁项目集生成过程结束。注意生成频繁项目集的时候，频繁项目集中的元素个数最大值为事务集中事务中含有的最大元素个数，即若事务集中事务包含的最大元素个数为k，那么最多能生成k-频繁项目集，这个原因很简单，因为事务集合中的所有事务都不包含（k+1）个元素，所以不可能存在（k+1）-频繁项目集。在生成过程中，若得到的频繁项目集个数小于2，生成过程也可以结束了。 现在需要生成强关联规则： 这里只说明3-频繁项目集生成关联规则的过程： 对于集合{1，2，3} 先生成1-后件的关联规则： （1，2）—&gt;3， 置信度=3/4 （1，3）—&gt;2， 置信度=3/5 （2，3）—&gt;1 置信度=3/3 （1，3）—&gt;2的置信度不满足要求，所以剔除掉。因此得到1后件的集合{1}，{3}，然后再以{1，3}作为后件 ​ 2—&gt;1，3 置信度=3/5不满足要求，所以对于3-频繁项目集生成的强关联规则为：（1，2）—&gt;3和（2，3）—&gt;1。 算法实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360/*Apriori算法 2012.10.31*/ #include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;map&gt;#include &lt;string&gt;#include &lt;algorithm&gt;#include &lt;cmath&gt;using namespace std;vector&lt;string&gt; T; //保存初始输入的事务集 double minSup,minConf; //用户设定的最小支持度和置信度 map&lt;string,int&gt; mp; //保存项目集中每个元素在事务集中出现的次数 vector&lt; vector&lt;string&gt; &gt; F; //存放频繁项目集 vector&lt;string&gt; R; //存放关联规则 void initTransactionSet() //获取事务集 &#123; int n; cout&lt;&lt;"请输入事务集的个数:"&lt;&lt;endl; cin&gt;&gt;n; getchar(); cout&lt;&lt;"请输入事务集:"&lt;&lt;endl; while(n--) &#123; string str; getline(cin,str); //输入的事务集中每个元素以空格隔开,并且只能输入数字 T.push_back(str); &#125; cout&lt;&lt;"请输入最小支持度和置信度:"&lt;&lt;endl; //支持度和置信度为小数表示形式 cin&gt;&gt;minSup&gt;&gt;minConf;&#125;vector&lt;string&gt; split(string str,char ch) &#123; vector&lt;string&gt; v; int i,j; i=0; while(i&lt;str.size()) &#123; if(str[i]==ch) i++; else &#123; j=i; while(j&lt;str.size()) &#123; if(str[j]!=ch) j++; else break; &#125; string temp=str.substr(i,j-i); v.push_back(temp); i=j+1; &#125; &#125; return v;&#125;void genarateOneFrequenceSet() //生成1-频繁项目集 &#123; int i,j; vector&lt;string&gt; f; //存储1-频繁项目集 for(i=0;i&lt;T.size();i++) &#123; string t = T[i]; vector&lt;string&gt; v=split(t,' '); //将输入的事务集进行切分，如输入1 2 3，切分得到"1","2","3" for(j=0;j&lt;v.size();j++) //统计每个元素出现的次数，注意map默认按照key的升序排序 &#123; mp[v[j]]++; &#125; &#125; for(map&lt;string,int&gt;::iterator it=mp.begin();it!=mp.end();it++) //剔除不满足最小支持度要求的项集 &#123; if( (*it).second &gt;= minSup*T.size()) &#123; f.push_back((*it).first); &#125; &#125; F.push_back(T); //方便用F[1]表示1-频繁项目集 if(f.size()!=0) &#123; F.push_back(f); &#125;&#125;bool judgeItem(vector&lt;string&gt; v1,vector&lt;string&gt; v2) //判断v1和v2是否只有最后一项不同 &#123; int i,j; i=0; j=0; while(i&lt;v1.size()-1&amp;&amp;j&lt;v2.size()-1) &#123; if(v1[i]!=v2[j]) return false; i++; j++; &#125; return true;&#125;bool judgeSubset(vector&lt;string&gt; v,vector&lt;string&gt; f) //判断v的所有k-1子集是否在f中 &#123; int i,j; bool flag=true; for(i=0;i&lt;v.size();i++) &#123; string str; for(j=0;j&lt;v.size();j++) &#123; if(j!=i) str+=v[j]+" "; &#125; str=str.substr(0,str.size()-1); vector&lt;string&gt;::iterator it=find(f.begin(),f.end(),str); if(it==f.end()) flag=false; &#125; return flag;&#125;int calculateSupportCount(vector&lt;string&gt; v) //计算支持度计数 &#123; int i,j; int count=0; for(i=0;i&lt;T.size();i++) &#123; vector&lt;string&gt; t=split(T[i],' '); for(j=0;j&lt;v.size();j++) &#123; vector&lt;string&gt;::iterator it=find(t.begin(),t.end(),v[j]); if(it==t.end()) break; &#125; if(j==v.size()) count++; &#125; return count; &#125;bool judgeSupport(vector&lt;string&gt; v) //判断一个项集的支持度是否满足要求 &#123; int count=calculateSupportCount(v); if(count &gt;= ceil(minSup*T.size())) return true; return false;&#125;void generateKFrequenceSet() //生成k-频繁项目集 &#123; int k; for(k=2;k&lt;=mp.size();k++) &#123; if(F.size()&lt; k) //如果Fk-1为空，则退出 break; else //根据Fk-1生成Ck候选项集 &#123; int i,j; vector&lt;string&gt; c; vector&lt;string&gt; f=F[k-1]; for(i=0;i&lt;f.size()-1;i++) &#123; vector&lt;string&gt; v1=split(f[i],' '); for(j=i+1;j&lt;f.size();j++) &#123; vector&lt;string&gt; v2=split(f[j],' '); if(judgeItem(v1,v2)) //如果v1和v2只有最后一项不同，则进行连接 &#123; vector&lt;string&gt; tempVector=v1; tempVector.push_back(v2[v2.size()-1]); sort(tempVector.begin(),tempVector.end()); //对元素排序，方便判断是否进行连接 //剪枝的过程 //判断 v1的(k-1)的子集是否都在Fk-1中以及是否满足最低支持度 if(judgeSubset(tempVector,f)&amp;&amp;judgeSupport(tempVector)) &#123; int p; string tempStr; for(p=0;p&lt;tempVector.size()-1;p++) tempStr+=tempVector[p]+" "; tempStr+=tempVector[p]; c.push_back(tempStr); &#125; &#125; &#125; &#125; if(c.size()!=0) F.push_back(c); &#125; &#125;&#125;vector&lt;string&gt; removeItemFromSet(vector&lt;string&gt; v1,vector&lt;string&gt; v2) //从v1中剔除v2 &#123; int i; vector&lt;string&gt; result=v1; for(i=0;i&lt;v2.size();i++) &#123; vector&lt;string&gt;::iterator it= find(result.begin(),result.end(),v2[i]); if(it!=result.end()) result.erase(it); &#125; return result;&#125;string getStr(vector&lt;string&gt; v1,vector&lt;string&gt; v2) //根据前件和后件得到规则&#123; int i; string rStr; for(i=0;i&lt;v1.size();i++) rStr+=v1[i]+" "; rStr=rStr.substr(0,rStr.size()-1); rStr+="-&gt;"; for(i=0;i&lt;v2.size();i++) rStr+=v2[i]+" "; rStr=rStr.substr(0,rStr.size()-1); return rStr; &#125; void ap_generateRules(string fs) &#123; int i,j,k; vector&lt;string&gt; v=split(fs,' '); vector&lt;string&gt; h; vector&lt; vector&lt;string&gt; &gt; H; //存放所有的后件 int fCount=calculateSupportCount(v); //f的支持度计数 for(i=0;i&lt;v.size();i++) //先生成1-后件关联规则 &#123; vector&lt;string&gt; temp=v; temp.erase(temp.begin()+i); int aCount=calculateSupportCount(temp); if( fCount &gt;= ceil(aCount*minConf)) //如果满足置信度要求 &#123; h.push_back(v[i]); string tempStr; for(j=0;j&lt;v.size();j++) &#123; if(j!=i) tempStr+=v[j]+" "; &#125; tempStr=tempStr.substr(0,tempStr.size()-1); tempStr+="-&gt;"+v[i]; R.push_back((tempStr)); &#125; &#125; H.push_back(v); if(h.size()!=0) H.push_back(h); for(k=2;k&lt;v.size();k++) //生成k-后件关联规则 &#123; h=H[k-1]; vector&lt;string&gt; addH; for(i=0;i&lt;h.size()-1;i++) &#123; vector&lt;string&gt; v1=split(h[i],' '); for(j=i+1;j&lt;h.size();j++) &#123; vector&lt;string&gt; v2=split(h[j],' '); if(judgeItem(v1,v2)) &#123; vector&lt;string&gt; tempVector=v1; tempVector.push_back(v2[v2.size()-1]); //得到后件集合 sort(tempVector.begin(),tempVector.end()); vector&lt;string&gt; filterV=removeItemFromSet(v,tempVector); //得到前件集合 int aCount=calculateSupportCount(filterV); //计算前件支持度计数 if(fCount &gt;= ceil(aCount*minConf)) //如果满足置信度要求 &#123; string rStr=getStr(filterV,tempVector); //根据前件和后件得到规则 string hStr; for(int s=0;s&lt;tempVector.size();s++) hStr+=tempVector[s]+" "; hStr=hStr.substr(0,hStr.size()-1); addH.push_back(hStr); //得到一个新的后件集合 R.push_back(rStr); &#125; &#125; &#125; &#125; if(addH.size()!=0) //将所有的k-后件集合加入到H中 H.push_back(addH); &#125;&#125;void generateRules() //生成关联规则 &#123; int i,j,k; for(k=2;k&lt;F.size();k++) &#123; vector&lt;string&gt; f=F[k]; for(i=0;i&lt;f.size();i++) &#123; string str=f[i]; ap_generateRules(str); &#125; &#125; &#125;void outputFrequenceSet() //输出频繁项目集 &#123; int i,k; if(F.size()==1) &#123; cout&lt;&lt;"无频繁项目集!"&lt;&lt;endl; return; &#125; for(k=1;k&lt;F.size();k++) &#123; cout&lt;&lt;k&lt;&lt;"-频繁项目集:"&lt;&lt;endl; vector&lt;string&gt; f=F[k]; for(i=0;i&lt;f.size();i++) cout&lt;&lt;f[i]&lt;&lt;endl; &#125;&#125;void outputRules() //输出关联规则 &#123; int i; cout&lt;&lt;"关联规则:"&lt;&lt;endl; for(i=0;i&lt;R.size();i++) &#123; cout&lt;&lt;R[i]&lt;&lt;endl; &#125;&#125;void Apriori()&#123; initTransactionSet(); genarateOneFrequenceSet(); generateKFrequenceSet(); outputFrequenceSet(); generateRules(); outputRules();&#125; int main(int argc, char *argv[])&#123; Apriori(); return 0;&#125; ;) 测试数据： 7 1 2 3 1 4 4 5 1 2 4 1 2 6 4 3 2 6 3 2 3 6 0.3 0.8 运行结果： 1-频繁项目集:123462-频繁项目集:1 21 42 32 63 63-频繁项目集:2 3 6关联规则:3-&gt;22-&gt;36-&gt;26-&gt;33 6-&gt;22 6-&gt;36-&gt;2 3]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>Apriori</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apriori算法简介---关联规则的频繁项集算法]]></title>
    <url>%2FApriori%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E7%9A%84%E9%A2%91%E7%B9%81%E9%A1%B9%E9%9B%86%E7%AE%97%E6%B3%95.html</url>
    <content type="text"><![CDATA[### 由啤酒和尿布引出： 在一家超市中，人们发现了一个特别有趣的现象：尿布与啤酒这两种风马牛不相及的商品居然摆在一起。但这一奇怪的举措居然使尿布和啤酒的稍量大幅增加了。这可不是一个笑话，而是一直被商家所津津乐道的发生在美国沃尔玛连锁超市的真实案例。原来，美国的妇女通常在家照顾孩子，所以她们经常会嘱咐丈夫在下班回家的路上为孩子买尿布，而丈夫在买尿布的同时又会顺手购买自己爱喝的啤酒。这个发现为商家带来了大量的利润，但是如何从浩如烟海却又杂乱无章的数据中，发现啤酒和尿布销售之间的联系呢？这又给了我们什么样的启示呢？这是怎么做到的呢，这就是数据的挖掘，需要对数据之间的关联规则进行分析，进行购物篮分析。 Apriori算法是一种挖掘关联规则的频繁项集算法，其核心思想是通过候选集生成和情节的向下封闭检测两个阶段来挖掘频繁项集。而且算法已经被广泛的应用到商业、网络安全等各个领域。 Apriori算法是一种最有影响的挖掘布尔关联规则频繁项集的算法。很多的的挖掘算法是在Apriori算法的基础上进行改进的，比如基于散列（Hash）的方法，基于数据分割（Partition）的方法以及不产生候选项集的FP-GROWTH方法等。因此要了解关联规则算法不得不先要了解Apriori算法。 Apriori算法是一种最有影响的挖掘布尔关联规则频繁项集的算法。其核心是基于两阶段频集思想的递推算法。该关联规则在分类上属于单维、单层、布尔关联规则。在这里，所有支持度大于最小支持度的项集称为频繁项集，简称频集。可能产生大量的候选集,以及可能需要重复扫描数据库，是Apriori算法的两大缺点 ### 频繁项集 项的集合称为项集。包含k个项的项集称为k-项集。集合{computer,ativirus_software}是一个二项集。项集的出项频率是包含项集的事务数，简称为项集的频率，支持度计数或计数。注意，定义项集的支持度有时称为相对支持度，而出现的频率称为绝对支持度。如果项集I的相对支持度满足预定义的最小支持度阈值，则I是频繁项集。 ### 布尔关联规则: 关联规则是形如X→Y的蕴涵式，其中且， X和Y分别称为关联规则的先导(antecedent或left-hand-side, LHS)和后继(consequent或right-hand-side, RHS) 。 ### 定义 根据 韩家炜等观点，关联规则定义为： 假设I是 项的集合。给定一个交易数据库D，其中每个 事务(Transaction)t是I的非空子集，即，每一个交易都与一个唯一的 标识符TID(Transaction ID)对应。关联规则在D中的 支持度(support)是D中事务同时包含X、Y的百分比，即 概率； 置信度(confidence)是D中事物已经包含X的情况下，包含Y的百分比，即 条件概率。如果满足 最小支持度阈值和 最小置信度阈值。这些阈值是根据挖掘需要人为设定。 基本概念表1：关联规则的简单例子 | TID | 网球拍 | 网 球 | 运动鞋 | 羽毛球 | | —- | —— | —– | —— | —— | | 1 | 1 | 1 | 1 | 0 | | 2 | 1 | 1 | 0 | 0 | | 3 | 1 | 0 | 0 | 0 | | 4 | 1 | 0 | 1 | 0 | | 5 | 0 | 1 | 1 | 1 | | 6 | 1 | 1 | 0 | 0 | 用一个简单的例子说明。表1是顾客购买记录的数据库D，包含6个事务。项集I={网球拍,网球,运动鞋,羽毛球}。考虑关联规则（频繁二项集）：网球拍与网球，事务1,2,3,4,6包含网球拍，事务1,2,6同时包含网球拍和网球，X^Y=3, D=6，支持度(X^Y)/D=0.5；X=5, 置信度(X^Y)/X=0.6。若给定最小支持度α = 0.5，最小 置信度β = 0.6，认为购买网球拍和购买网球之间存在关联。 ## 挖掘关联规则 ### 什么是关联规则 一言蔽之，关联规则是形如X→Y的蕴涵式，表示通过X可以推导“得到”Y，其中X和Y分别称为关联规则的先导(antecedent或left-hand-side, LHS)和后继(consequent或right-hand-side, RHS) ### 如何量化关联规则 关联规则挖掘的一个典型例子便是购物车分析。通过关联规则挖掘能够发现顾客放入购物车中的不同商品之间的关联，分析顾客的消费习惯。这种关联规则的方向能够帮助卖家了解哪些商品被顾客频繁购买，从而帮助他们开发更好的营销策略。比如：将经常同时购买的商品摆近一些，以便进一步刺激这些商品一起销售；或者，将两件经常同时购买的商品摆远一点，这样可能诱发买这两件商品的用户一路挑选其他商品。 在数据挖掘当中，通常用“支持度”（support）和“置性度”（confidence）两个概念来量化事物之间的关联规则。它们分别反映所发现规则的有用性和确定性。 对于A-&gt;B \123456789101112131415161718192021222324\&gt; ①***\*支持度\****：P(A ∩ B)，既有A又有B的概率\&gt; ②***\*置信度\****：\&gt; P(B|A)，在A发生的事件中同时发生B的概率 p(AB)/P(A) 例如购物篮分析：牛奶 ⇒ 面包\&gt;\&gt; **例子**：[支持度：3%，置信度：40%]\&gt; &gt; 支持度3%：意味着3%顾客同时购买牛奶和面包\&gt; &gt; 置信度40%：意味着购买牛奶的顾客40%也购买面包\ 比如： Computer =&gt; antivirus_software , 其中 support=2%, confidence=60% 表示的意思是所有的商品交易中有2%的顾客同时买了电脑和杀毒软件，并且购买电脑的顾客中有60%也购买了杀毒软件。在关联规则的挖掘过程中，通常会设定最小支持度阈值和最小置性度阈值，如果某条关联规则满足最小支持度阈值和最小置性度阈值，则认为该规则可以给用户带来感兴趣的信息。 ### 关联规则挖掘过程 1) 几个基本概念： 关联规则A-&gt;B的支持度*support=P(AB)，指的是事件A和事件B*同时发生的概率。 \置信度***confidence=P(B|A)=P(AB)/P(A),指的是发生事件A的基础上发生事件B*的概率。 同时满足最小支持度阈值和最小置信度阈值的规则称为强规则。 如果事件A中包含k个元素，那么称这个事件A为\k\*项集，**并且事件A满足最小支持度阈值的事件称为频繁***k**项集**。 2) 挖掘过程： 第一，找出所有的频繁项集； 第二，由频繁项集产生强规则。 ## Apriori介绍 Apriori算法使用频繁项集的先验知识，使用一种称作逐层搜索的迭代方法，k项集用于探索(k+1)项集。首先，通过扫描事务（交易）记录，找出所有的频繁1项集，该集合记做L1，然后利用L1找频繁2项集的集合L2，L2找L3，如此下去，直到不能再找到任何频繁k项集。最后再在所有的频繁集中找出强规则，即产生用户感兴趣的关联规则。 其中，\Apriori**算法具有这样一条性质：任一频繁项集的所有非空子集也必须是频繁的。因为假如P(I)&lt; 最小支持度阈值，当有元素A添加到I中时，结果项集（A∩I）不可能比I出现次数更多。因此A∩I也不是频繁的。 2.2 连接步和剪枝步 1） 连接步 若有两个k-1项集，每个项集按照“属性-值”（一般按值）的字母顺序进行排序。如果两个k-1项集的前k-2个项相同，而最后一个项不同，则证明它们是可连接的，即这个k-1项集可以联姻，即可连接生成k项集。使如有两个3项集：｛a, b, c｝{a, b, d}，这两个3项集就是可连接的，它们可以连接生成4项集｛a, b, c, d｝。又如两个3项集｛a, b, c｝｛a, d, e｝，这两个3项集显示是不能连接生成3项集的。 剪枝步 若一个项集的子集不是频繁项集，则该项集肯定也不是频繁项集。这个很好理解，举一个例子，若存在3项集｛a, b, c｝，如果它的2项子集｛a, b｝的支持度即同时出现的次数达不到阈值，则｛a, b, c｝同时出现的次数显然也是达不到阈值的。因此，若存在一个项集的子集不是频繁项集，那么该项集就应该被无情的舍弃。 \Apriori算法流程**： \1. 过单趟扫描数据库D；计算出各个1项集的支持度，得 到频繁1项集的集合。 \2. 从2项集开始循环，由频繁k-1项集生成频繁频繁k项集。 2.1 连接步：为了生成，预先生成,由2个只有一个项不同的属于的频集做一 个（k-2）JOIN运算得到的。 2.2 剪枝步：由于是的超集，所以可能有些元素不是频繁的。舍弃掉子集不是频繁项集即不在频繁k-1项集中的项集 2.3 扫描数据库，计算2.3步中过滤后的k项集的支持度，舍弃掉支持度小于阈值的项集，生成频繁k项集。 \3. 当当前生成的频繁k项集中只有一个项集时循环结束 【注意】 在剪枝步中的每个元 素需在交易数据库中进行验证来决定其是否加入，这里的验证过程 是算法性能的一个瓶颈。这个方法要求多次扫描可能很大的交易数据库。可能产生大量的候选集，以及可能需要重复扫描数据库，是Apriori算法的两大缺 点。 Apriori\算法实例 | \交易\*ID** | *商品**ID**列表\ | | ———————- | ——————————– | | T100 | I1，I2，I5 | | T200 | I2，I4 | | T300 | I2，I3 | | T400 | I1，I2，I4 | | T500 | I1，I3 | | T600 | I2，I3 | | T700 | I1，I3 | | T800 | I1，I2，I3，I5 | | T900 | I1，I2，I3 | 上图为某商场的交易记录，共有9个事务，利用Apriori算法寻找所有的频繁项集的过程如下: 1详细介绍下候选3项集的集合**C3**的产生过程：从连接步，首先C3=&#123;&#123;I1,I2,I3&#125;，&#123;I1,I2,I5&#125;，&#123;I1,I3,I5&#125;，&#123;I2,I3,I4&#125;，&#123;I2,I3,I5&#125;，&#123;I2,I4,I5&#125;&#125;（C3是由**L2**与自身连接产生）。根据Apriori性质，频繁项集的所有子集也必须频繁的，可以确定有4个候选集&#123;I1,I3,I5&#125;，&#123;I2,I3,I4&#125;，&#123;I2,I3,I5&#125;，&#123;I2,I4,I5&#125;&#125;不可能时频繁的，因为它们存在子集不属于频繁集，因此将它们从C3中删除。注意，由于**Apriori**算法使用逐层搜索技术，给定候选k项集后，只需检查它们的（k-1）个子集是否频繁。 3、总结与优化 ①Apriori算法的缺点： (1)由频繁k-1项集进行自连接生成的候选频繁k项集数量巨大。 (2)在验证候选频繁k项集的时候需要对整个数据库进行扫描，非常耗时。 ②网上提到的频集算法的几种优化方法： 基于划分的方法。 基于hash的方法。 基于采样的方法。 减少交易的个数。 我重点看了“基于划分的方法”改进算法，现在简单介绍一下实现思想： 基于划分(partition)的算法，这个算法先把数据库从逻辑上分成几个互不相交的块，每次单独考虑一个分块并 对它生成所有的频集，然后把产生的频集合并，用来生成所有可能的频集，最后计算这些项集的支持度。 其中，partition算法要注意的是分片的大小选取，要保证每个分片可以被放入到内存。当每个分片产生频集后，再合并产生产生全局的候选k-项集。若在多个处理器分片，可以通过处理器之间共享一个杂凑树来产生频集。 转自：http://blog.csdn.net/lizhengnanhua/article/details/9061755 http://blog.csdn.net/qustdjx/article/details/12770883 转载地址]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>Apriori</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apriori算法简析]]></title>
    <url>%2FApriori%E7%AE%97%E6%B3%95%E7%AE%80%E6%9E%90.html</url>
    <content type="text"><![CDATA[学习Apriori算法首先要了解几个概念：项集、支持度、置信度、最小支持度、最小置信度、频繁项集。 项集：顾名思义，即项的集合。eg：牛奶、面包组成一个集合{牛奶、面包}，其中牛奶和面包为项，{牛奶、面包}为项集，称之为2项集。（说白了，其实就是集合）支持度：项集A、B同时发生的概率称之为关联规则的支持度。置信度：项集A发生的情况下，则项集B发生的概率为关联规则的置信度。 支持度与置信度的概念有些抽象，具体可以看下面的例子：如图数据为顾客购物情况，每一个id对应的items都是一个项集，现在需要对{milk，diaper}与{beer}关联性进行研究，计算支持度与置信度。计算如下：计算支持度：计算{milk，diaper}{beer}同时发生的概率就相当于计算{milk，diaper，beer}出现的次数所占数据条的比重，即2/5.计算置信度：计算{milk，diaper}发生的情况下，则{beer}发生的概率就相当于计算{milk，diaper，beer}出现的次数所占{milk，diaper}发生次数的比重，即2/3. 最小支持度：最小支持度就是人为按照实际意义规定的阈值，表示项集在统计意义上的最低重要性。最小置信度：最小置信度也是人为按照实际意义规定的阈值，表示关联规则最低可靠性。如果支持度与置信度同时达到最小支持度与最小置信度，则此关联规则为强规则。频繁项集：满足最小支持度的所有项集，称作频繁项集。（频繁项集性质：1、频繁项集的所有非空子集也为频繁项集；2、若A项集不是频繁项集，则其他项集或事务与A项集的并集也不是频繁项集） 了解了以上定义，那么如何从大量的数据中找出不同项的关联规则呢？下面具体看Apriori算法实现过程。Apriori实现过程：首先，找出所有的频繁项集，再从频繁项集中找出符合最小置信度的项集，最终便得到有强规则的项集（即我们所需的项的关联性）。例如：数据如下算法过程如下首先计算出所有的频繁项集，这里最小支持度为0.2得出L1、L2、L3的各个项集均为频繁项集，再进行计算每个频繁项集的置信度，其中L1不必计算。计算结果如下（如果想了解寻找频繁项集的详细过程，可以研读张良均等著《python数据分析与挖掘实战》，里面有详细过程）至此就完成了Apriori算法的全部过程。 接下来python实现Apriori算法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#-*- coding: utf-8 -*-from __future__ import print_functionimport pandas as pd#自定义连接函数，用于实现L_&#123;k-1&#125;到C_k的连接def connect_string(x, ms): x = list(map(lambda i:sorted(i.split(ms)), x)) l = len(x[0]) r = [] for i in range(len(x)): for j in range(i,len(x)): if x[i][:l-1] == x[j][:l-1] and x[i][l-1] != x[j][l-1]: r.append(x[i][:l-1]+sorted([x[j][l-1],x[i][l-1]])) return r#寻找关联规则的函数def find_rule(d, support, confidence, ms = u'--'): result = pd.DataFrame(index=['support', 'confidence']) #定义输出结果 support_series = 1.0*d.sum()/len(d) #支持度序列 column = list(support_series[support_series &gt; support].index) #初步根据支持度筛选 k = 0 while len(column) &gt; 1: k = k+1 print(u'\n正在进行第%s次搜索...' %k) column = connect_string(column, ms) print(u'数目：%s...' %len(column)) sf = lambda i: d[i].prod(axis=1, numeric_only = True) #新一批支持度的计算函数 #创建连接数据，这一步耗时、耗内存最严重。当数据集较大时，可以考虑并行运算优化。 d_2 = pd.DataFrame(list(map(sf,column)), index = [ms.join(i) for i in column]).T support_series_2 = 1.0*d_2[[ms.join(i) for i in column]].sum()/len(d) #计算连接后的支持度 column = list(support_series_2[support_series_2 &gt; support].index) #新一轮支持度筛选 support_series = support_series.append(support_series_2) column2 = [] for i in column: #遍历可能的推理，如&#123;A,B,C&#125;究竟是A+B--&gt;C还是B+C--&gt;A还是C+A--&gt;B？ i = i.split(ms) for j in range(len(i)): column2.append(i[:j]+i[j+1:]+i[j:j+1]) cofidence_series = pd.Series(index=[ms.join(i) for i in column2]) #定义置信度序列 for i in column2: #计算置信度序列 cofidence_series[ms.join(i)] = support_series[ms.join(sorted(i))]/support_series[ms.join(i[:len(i)-1])] for i in cofidence_series[cofidence_series &gt; confidence].index: #置信度筛选 result[i] = 0.0 result[i]['confidence'] = cofidence_series[i] result[i]['support'] = support_series[ms.join(sorted(i.split(ms)))] result = result.T.sort_values(['confidence','support'], ascending = False) #结果整理，输出 print(u'\n结果为：') print(result) return result Apriori算法调用，进行关联性分析数据如下代码如下 12345678910111213141516171819202122#-*- coding: utf-8 -*-#使用Apriori算法挖掘菜品订单关联规则from __future__ import print_functionimport pandas as pdfrom apriori import * #导入自行编写的apriori函数inputfile = '../data/menu_orders.xls'outputfile = '../tmp/apriori_rules.xls' #结果文件data = pd.read_excel(inputfile, header = None)print(u'\n转换原始数据至0-1矩阵...')ct = lambda x : pd.Series(1, index = x[pd.notnull(x)]) #转换0-1矩阵的过渡函数b = map(ct, data.as_matrix()) #用map方式执行data = pd.DataFrame(list(b)).fillna(0) #实现矩阵转换，空值用0填充print(u'\n转换完毕。')del b #删除中间变量b，节省内存support = 0.2 #最小支持度confidence = 0.5 #最小置信度ms = '---' #连接符，默认'--'，用来区分不同元素，如A--B。需要保证原始表格中不含有该字符find_rule(data, support, confidence, ms).to_excel(outputfile) #保存结果 结果如下support confidencee—a 0.3 1.000000e—c 0.3 1.000000c—e—a 0.3 1.000000a—e—c 0.3 1.000000c—a 0.5 0.714286a—c 0.5 0.714286a—b 0.5 0.714286c—b 0.5 0.714286b—a 0.5 0.625000b—c 0.5 0.625000a—c—e 0.3 0.600000b—c—a 0.3 0.600000a—c—b 0.3 0.600000a—b—c 0.3 0.600000 本文主要参考书籍张良均等著《python数据分析与挖掘实战》本文主要参考博客https://blog.csdn.net/baimafujinji/article/details/53456931 转载地址]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>Apriori</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 零基础入门（八）:SQL编程实践]]></title>
    <url>%2FApache%20Flink%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AB%EF%BC%89-SQL%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5.html</url>
    <content type="text"><![CDATA[本文是 Apache Flink 零基础入门系列文章第八篇，将通过五个实例讲解 Flink SQL 的编程实践。 注： 本教程实践基于 Ververica 开源的 sql-training 项目。基于 Flink 1.7.2 。 通过本课你能学到什么？ 本文将通过五个实例来贯穿 Flink SQL 的编程实践，主要会涵盖以下几个方面的内容。 如何使用 SQL CLI 客户端 如何在流上运行 SQL 查询 运行 window aggregate 与 non-window aggregate，理解其区别 如何用 SQL 消费 Kafka 数据 如何用 SQL 将结果写入 Kafka 和 ElasticSearch 本文假定您已具备基础的 SQL 知识。 环境准备本文教程是基于 Docker 进行的，因此你只需要安装了 Docker 即可。不需要依赖 Java、Scala 环境、或是IDE。 注意：Docker 默认配置的资源可能不太够，会导致运行 Flink Job 时卡死。因此推荐配置 Docker 资源到 3-4 GB，3-4 CPUs。 本次教程的环境使用 Docker Compose 来安装，包含了所需的各种服务的容器，包括： Flink SQL Client：用来提交query，以及可视化结果 Flink JobManager 和 TaskManager：用来运行 Flink SQL 任务。 Apache Kafka：用来生成输入流和写入结果流。 Apache Zookeeper：Kafka 的依赖项 ElasticSearch：用来写入结果 我们已经提供好了Docker Compose 配置文件，可以直接下载 docker-compose.yml 文件。 然后打开命令行窗口，进入存放 docker-compose.yml 文件的目录，然后运行以下命令： Linux &amp; MacOS 1docker-compose up -d Windows 12set COMPOSE_CONVERT_WINDOWS_PATHS=1docker-compose up -d docker-compose 命令会启动所有所需的容器。第一次运行的时候，Docker 会自动地从 Docker Hub 下载镜像，这可能会需要一段时间（将近 2.3GB）。之后运行的话，几秒钟就能启动起来了。运行成功的话，会在命令行中看到以下输出，并且也可以在 http://localhost:8081 访问到 Flink Web UI。 运行 Flink SQL CLI 客户端运行下面命令进入 Flink SQL CLI 。 1docker-compose exec sql-client ./sql-client.sh 该命令会在容器中启动 Flink SQL CLI 客户端。然后你会看到如下的欢迎界面。 数据介绍Docker Compose 中已经预先注册了一些表和数据，可以运行 SHOW TABLES; 来查看。本文会用到的数据是 Rides 表，这是一张出租车的行车记录数据流，包含了时间和位置信息，运行 DESCRIBE Rides; 可以查看表结构。 123456789Flink SQL&gt; DESCRIBE Rides;root |-- rideId: Long // 行为ID (包含两条记录，一条入一条出） |-- taxiId: Long // 出租车ID |-- isStart: Boolean // 开始 or 结束 |-- lon: Float // 经度 |-- lat: Float // 纬度 |-- rideTime: TimeIndicatorTypeInfo(rowtime) // 时间 |-- psgCnt: Integer // 乘客数 Rides 表的详细定义见 training-config.yaml。 实例1：过滤例如我们现在只想查看发生在纽约的行车记录。 注：Docker 环境中已经预定义了一些内置函数，如 isInNYC(lon, lat) 可以确定一个经纬度是否在纽约，toAreaId(lon, lat) 可以将经纬度转换成区块。 因此，此处我们可以使用 isInNYC 来快速过滤出纽约的行车记录。在 SQL CLI 中运行如下 Query： 1SELECT * FROM Rides WHERE isInNYC(lon, lat); SQL CLI 便会提交一个 SQL 任务到 Docker 集群中，从数据源（Rides 流存储在Kafka中）不断拉取数据，并通过 isInNYC 过滤出所需的数据。SQL CLI 也会进入可视化模式，并不断刷新展示过滤后的结果： 也可以到 http://localhost:8081 查看 Flink 作业的运行情况。 实例2：Group Aggregate我们的另一个需求是计算搭载每种乘客数量的行车事件数。也就是搭载1个乘客的行车数、搭载2个乘客的行车… 当然，我们仍然只关心纽约的行车事件。 因此，我们可以按照乘客数psgCnt做分组，使用 COUNT(*) 计算出每个分组的事件数，注意在分组前需要先过滤出isInNYC的数据。在 SQL CLI 中运行如下 Query： 1234SELECT psgCnt, COUNT(*) AS cnt FROM Rides WHERE isInNYC(lon, lat)GROUP BY psgCnt; SQL CLI 的可视化结果如下所示，结果每秒都在发生变化。不过最大的乘客数不会超过 6 人。 实例3：Window Aggregate为了持续地监测纽约的交通流量，需要计算出每个区块每5分钟的进入的车辆数。我们只关心至少有5辆车子进入的区块。 此处需要涉及到窗口计算（每5分钟），所以需要用到 Tumbling Window 的语法。“每个区块” 所以还要按照 toAreaId 进行分组计算。“进入的车辆数” 所以在分组前需要根据 isStart 字段过滤出进入的行车记录，并使用 COUNT(*) 统计车辆数。最后还有一个 “至少有5辆车子的区块” 的条件，这是一个基于统计值的过滤条件，所以可以用 SQL HAVING 子句来完成。 最后的 Query 如下所示： 12345678910SELECT toAreaId(lon, lat) AS area, TUMBLE_END(rideTime, INTERVAL &apos;5&apos; MINUTE) AS window_end, COUNT(*) AS cnt FROM Rides WHERE isInNYC(lon, lat) and isStartGROUP BY toAreaId(lon, lat), TUMBLE(rideTime, INTERVAL &apos;5&apos; MINUTE) HAVING COUNT(*) &gt;= 5; 在 SQL CLI 中运行后，其可视化结果如下所示，每个 area + window_end 的结果输出后就不会再发生变化，但是会每隔 5 分钟会输出一批新窗口的结果。因为 Docker 环境中的source我们做了10倍的加速读取（相对于原始速度），所以演示的时候，大概每隔30秒就会输出一批新窗口。 Window Aggregate 与 Group Aggregate 的区别从实例2和实例3的结果显示上，可以体验出来 Window Aggregate 与 Group Aggregate 是有一些明显的区别的。其主要的区别是，Window Aggregate 是当window结束时才输出，其输出的结果是最终值，不会再进行修改，其输出流是一个 Append 流。而 Group Aggregate 是每处理一条数据，就输出最新的结果，其结果是在不断更新的，就好像数据库中的数据一样，其输出流是一个 Update 流。 另外一个区别是，window 由于有 watermark ，可以精确知道哪些窗口已经过期了，所以可以及时清理过期状态，保证状态维持在稳定的大小。而 Group Aggregate 因为不知道哪些数据是过期的，所以状态会无限增长，这对于生产作业来说不是很稳定，所以建议对 Group Aggregate 的作业配上 State TTL 的配置。 例如统计每个店铺每天的实时PV，那么就可以将 TTL 配置成 24+ 小时，因为一天前的状态一般来说就用不到了。 123SELECT DATE_FORMAT(ts, &apos;yyyy-MM-dd&apos;), shop_id, COUNT(*) as pvFROM TGROUP BY DATE_FORMAT(ts, &apos;yyyy-MM-dd&apos;), shop_id 当然，如果 TTL 配置地太小，可能会清除掉一些有用的状态和数据，从而导致数据精确性地问题。这也是用户需要权衡地一个参数。 实例4：将 Append 流写入 Kafka上一小节介绍了 Window Aggregate 和 Group Aggregate 的区别，以及 Append 流和 Update 流的区别。在 Flink 中，目前 Update 流只能写入支持更新的外部存储，如 MySQL, HBase, ElasticSearch。Append 流可以写入任意地存储，不过一般写入日志类型的系统，如 Kafka。 这里我们希望将“每10分钟的搭乘的乘客数”写入Kafka。 我们已经预定义了一张 Kafka 的结果表 Sink_TenMinPsgCnts（training-config.yaml 中有完整的表定义）。 在执行 Query 前，我们先运行如下命令，来监控写入到 TenMinPsgCnts topic 中的数据： 1docker-compose exec sql-client /opt/kafka-client/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic TenMinPsgCnts --from-beginning 每10分钟的搭乘的乘客数可以使用 Tumbling Window 来描述，我们使用 INSERT INTO Sink_TenMinPsgCnts 来直接将 Query 结果写入到结果表。 1234567INSERT INTO Sink_TenMinPsgCnts SELECT TUMBLE_START(rideTime, INTERVAL &apos;10&apos; MINUTE) AS cntStart, TUMBLE_END(rideTime, INTERVAL &apos;10&apos; MINUTE) AS cntEnd, CAST(SUM(psgCnt) AS BIGINT) AS cnt FROM Rides GROUP BY TUMBLE(rideTime, INTERVAL &apos;10&apos; MINUTE); 我们可以监控到 TenMinPsgCnts topic 的数据以 JSON 的形式写入到了 Kafka 中： 实例5：将 Update 流写入 ElasticSearch最后我们实践一下将一个持续更新的 Update 流写入 ElasticSearch 中。我们希望将“每个区域出发的行车数”，写入到 ES 中。 我们也已经预定义好了一张 Sink_AreaCnts 的 ElasticSearch 结果表（training-config.yaml 中有完整的表定义）。该表中只有两个字段 areaId 和 cnt。 同样的，我们也使用 INSERT INTO 将 Query 结果直接写入到 Sink_AreaCnts 表中。 12345INSERT INTO Sink_AreaCnts SELECT toAreaId(lon, lat) AS areaId, COUNT(*) AS cnt FROM Rides WHERE isStartGROUP BY toAreaId(lon, lat); 在 SQL CLI 中执行上述 Query 后，Elasticsearch 会自动地创建 area-cnts 索引。Elasticsearch 提供了一个 REST API 。我们可以访问 查看area-cnts索引的详细信息： http://localhost:9200/area-cnts 查看area-cnts索引的统计信息： http://localhost:9200/area-cnts/_stats 返回area-cnts索引的内容：http://localhost:9200/area-cnts/_search 显示 区块 49791 的行车数：http://localhost:9200/area-cnts/_search?q=areaId:49791 随着 Query 的一直运行，你也可以观察到一些统计值（_all.primaries.docs.count, _all.primaries.docs.deleted）在不断的增长：http://localhost:9200/area-cnts/_stats 总结本文带大家使用 Docker Compose 快速上手 Flink SQL 的编程，并对比 Window Aggregate 和 Group Aggregate 的区别，以及这两种类型的作业如何写入到 外部系统中。感兴趣的同学，可以基于这个 Docker 环境更加深入地去实践，例如运行自己写的 UDF , UDTF, UDAF。查询内置地其他源表等等。 ▼ Apache Flink 社区推荐 ▼ Apache Flink 及大数据领域顶级盛会 Flink Forward Asia 2019 重磅开启，目前正在征集议题，限量早鸟票优惠ing。了解 Flink Forward Asia 2019 的更多信息，请查看： https://developer.aliyun.com/special/ffa2019 首届 Apache Flink 极客挑战赛重磅开启，聚焦机器学习与性能优化两大热门领域，40万奖金等你拿，加入挑战请点击： https://tianchi.aliyun.com/markets/tianchi/flink2019]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 零基础入门（七）:Table API 编程]]></title>
    <url>%2FApache%20Flink%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%83%EF%BC%89-TableAPI%E7%BC%96%E7%A8%8B.html</url>
    <content type="text"><![CDATA[文章概述：本文主要包含三部分：第一部分，主要介绍什么是 Table API，从概念角度进行分析，让大家有一个感性的认识；第二部分，从代码的层面介绍怎么使用 Table API；第三部分，介绍 Table API 近期的动态。文章结构如下： 什么是 Table API Flink API 总览 Table API 的特性 Table API 编程 WordCount 示例 Table API 操作 如何获取一个 Table 如果输出一个 Table 如果查询一个 Table Table API 动态 一、什么是 Table API为了更好地了解 Table API，我们先看下 Flink 都提供了哪些 API 供用户使用。 1.Flink API 总览 如图，Flink 根据使用的便捷性和表达能力的强弱提供了 3 层 API，由上到下，表达能力逐渐增强，比如 processFunction，是最底层的 API，表达能力最强，我们可以用他来操作 state 和 timer 等复杂功能。Datastream API 相对于 processFunction 来说，又进行了进一步封装，提供了很多标准的语义算子给大家使用，比如我们常用的 window 算子（包括 Tumble, slide,session 等）。那么最上面的 SQL 和 Table API 使用最为便捷，具有自身的很多特点，重点归纳如下： 第一，Table API &amp; SQL 是一种声明式的 API。用户只需关心做什么，不用关心怎么做，比如图中的 WordCount 例子，只需要关心按什么维度聚合，做哪种类型的聚合，不需要关心底层的实现。 第二，高性能。Table API &amp; SQL 底层会有优化器对 query 进行优化。举个例子，假如 WordCount 的例子里写了两个 count 操作，优化器会识别并避免重复的计算，计算的时候只保留一个 count 操作，输出的时候再把相同的值输出两遍即可，以达到更好的性能。 第三，流批统一。上图例子可以发现，API 并没有区分流和批，同一套 query 可以流批复用，对业务开发来说，避免开发两套代码。 第四，标准稳定。Table API &amp; SQL 遵循 SQL 标准，不易变动。API 比较稳定的好处是不用考虑 API 兼容性问题。 第五，易理解。语义明确，所见即所得。 2.Table API 特性上一小节介绍了 Table API 和 SQL 一些共有的特性，这个小节重点介绍下 Table API 自身的特性。主要可以归纳为以下两点： 第一，Table API 使得多声明的数据处理写起来比较容易。 怎么理解？比如我们有一个 Table（tab），并且需要执行一些过滤操作然后输出到结果表，对应的实现是：tab.where(“a &lt; 10”).inertInto(“resultTable1”)；此外，我们还需要做另外一些筛选，然后也对结果输出，即 tab.where(“a &gt; 100”).insertInto(“resultTable2”)。你会发现，用 Table API 写起来会非常简洁方便，两行代码就把功能实现了。 第二，Table API 是 Flink 自身的一套 API，这使得我们更容易地去扩展标准的 SQL。当然，在扩展 SQL 的时候并不是随意的去扩展，需要考虑 API 的语义、原子性和正交性，并且当且仅当需要的时候才去添加。 对比 SQL，我们可以认为 Table API 是 SQL 的超集。SQL 有的操作，Table API 可以有，然而我们又可以从易用性和功能性地角度对 SQL 进行扩展和提升。 二、Table API编程第一章介绍了 Table API 相关的概念。这一章我们来看下如何用 Table API 来编程。本章会先从一个 WordCount 的例子出发，让大家对 Table API 编程先有一个大概的认识，然后再具体介绍一下 Table API 的操作，比如，如何获取一个 Table，如何输出一个 Table，以及如何对 Table 执行查询操作。 1.WordCount举例这是一个完整的，用 java 编写的 batch 版本的 WordCount 例子，此外，还有 scala 和 streaming 版本的 WordCount，都统一上传到了 github 上（https://github.com/hequn8128/TableApiDemo），大家可以下载下来尝试运行或者修改。 12345678910111213141516171819202122232425262728import org.apache.flink.api.common.typeinfo.Types;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.table.api.Table;import org.apache.flink.table.api.java.BatchTableEnvironment;import org.apache.flink.table.descriptors.FileSystem;import org.apache.flink.table.descriptors.OldCsv;import org.apache.flink.table.descriptors.Schema;import org.apache.flink.types.Row;public class JavaBatchWordCount &#123; // line:10 public static void main(String[] args) throws Exception &#123; ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment tEnv = BatchTableEnvironment.create(env); String path = JavaBatchWordCount.class.getClassLoader().getResource(&quot;words.txt&quot;).getPath(); tEnv.connect(new FileSystem().path(path)) .withFormat(new OldCsv().field(&quot;word&quot;, Types.STRING).lineDelimiter(&quot;\n&quot;)) .withSchema(new Schema().field(&quot;word&quot;, Types.STRING)) .registerTableSource(&quot;fileSource&quot;); // line:20 Table result = tEnv.scan(&quot;fileSource&quot;) .groupBy(&quot;word&quot;) .select(&quot;word, count(1) as count&quot;); tEnv.toDataSet(result, Row.class).print(); &#125;&#125; 我们具体看下这个 WordCount 的例子。首先，第13、14行，是对 environment 的一些初始化，先通过 ExecutionEnvironment 的 getExecutionEnvironment 方法拿到执行环境，然后再通过 BatchTableEnvironment 的 create 拿到对应的 Table 环境，拿到环境后，我们可以注册 TableSource、TableSink 或执行一些其他操作。 这里需要注意的是，ExecutionEnvironment 跟 BatchTableEnvironment 都是对应 Java 的版本，对于 scala 程序，这里需要是一个对应 scala 版本的 environment。这也是初学者一开始可能会遇到的问题，因为 environent 有很多且容易混淆。为了让大家更好区分这些 environment，下面对 environment 进行了一些归纳。 这里从 batch/stream，还有 Java/scala，对 environment 进行了分类，对于这些 environment 使用时需要特别注意，不要 import 错了。environment 的问题，社区已经进行了一些讨论，如上图下方的链接，这里不再具体展开。 我们再回到刚刚的 WordCount 的例子，拿到 environment 后，需要做的第二件事情是注册对应的TableSource。 1234tEnv.connect(new FileSystem().path(path)) .withFormat(new OldCsv().field(&quot;word&quot;, Types.STRING).lineDelimiter(&quot;\n&quot;)) .withSchema(new Schema().field(&quot;word&quot;, Types.STRING)) .registerTableSource(&quot;fileSource&quot;); 使用起来也非常方便，首先，因为我们要读一个文件，需要指定读取文件的路径，指定了之后，我们需要再描述文件内容的格式，比如他是 csv 的文件并且行分割符是什么。还有就是指定这个文件对应的 Schema 是什么，比如只有一列单词，并且类型是 String。最后，我们需要把 TableSource 注册到 environment 里去。 12345Table result = tEnv.scan(&quot;fileSource&quot;) .groupBy(&quot;word&quot;) .select(&quot;word, count(1) as count&quot;);tEnv.toDataSet(result, Row.class).print(); 通过 scan 刚才注册好的 TableSource，我们可以拿到一个 Table 对象，并执行相应的一些操作，比如 GroupBy，count。最后，可以把 Table 按 DataSet 的方式进行输出。 以上便是一个 Table API 的 WordCount 完整例子。涉及 Table 的获取，Table 的操作，以及 Table 的输出。接下来会具体介绍如何获取 Table、输出 Table 和执行 Table 操作。 2.如何获取一个Table获取 Table 大体可以分为两步，第一步，注册对应的 TableSource；第二步，调用 Table environement 的 scan 方法获取 Table 对象。注册 Table Source 又有3种方法：通过 Table descriptor 来注册，通过自定义 source 来注册，或者通过 DataStream 来注册。具体的注册方式如下图所示： 3.如何输出一个Table对应输出 Table，我们也有类似的3种方法：Table descriptor, 自定义 Table sink 以及输出成一个 DataStream。如下图所示： 4.如何操作一个Table4.1 Table 操作总览第2、3节介绍了如何获取和输出一个 Table，本节主要介绍如何对 Table 进行操作。Table 上有很多操作，比如一些 projection 操作 select、filter、where；聚合操作，如 groupBy、flatAggrgate；还有join操作，等等。我们以一个具体的例子来介绍下 Table 上各操作的转换流程。 如上图，当我们拿到一个 Table 后，调用 groupBy 会返回一个 GroupedTable。GroupedTable 里只有 select 方法，对 GroupedTable 调用 select 方法会返回一个 Table。拿到这个 Table 后，我们可以再调用 Table 上的方法。图中其他 Table，如 OverWindowedTable 也是类似的流程。值得注意的是，引入各个类型的 Table 是为了保证 API 的合法性和便利性，比如 groupBy 之后只有 select 操作是有意义的，在编辑器上可以直接点出来。 前面我们提到，可以将 Table API 看成是 SQL 的超集，因此我们也可以对 Table 里的操作按此进行分类，大致分为三类，如下图所示： 第一类，是跟 SQL 对齐的一些操作，比如 select, filter, join 等。第二类，是一些提升 Table API 易用性的操作。第三类，是增强 Table API 功能的一些操作。第一类操作由于和 SQL 类似，比较容易理解，其次，也可以查看官方的文档，了解具体的方法，所以这里不再展开介绍。下面的章节会重点介绍后两类操作，这些操作也是 Table API 独有的。 4.2 提升易用性相关操作介绍易用性之前，我们先来看一个问题。假设我们有一张很大的表，里面有一百列，此时需要去掉一列，那么SQL怎么写？我们需要 select 剩下的 99 列！显然这会给用户带来不小的代价。为了解决这个问题，我们在Table上引入了一个 dropColumns 方法。利用 dropColumns 方法，我们便可以只写去掉的列。与此对应，还引入了 addColumns, addOrReplaceColumns 和 renameColumns 方法，如下图所示： 解决了刚才的问题后，我们再看下面另一个问题：假设还是一张100列的表，我们需要选第20到第80列，那么我们如何操作呢？为了解决这个问题，我们又引入了 withColumns 和 withoutColumns 方法。对于刚才的问题，我们可以简单地写成 table.select(“withColumns(20 to 80)”)。 4.3 增强功能相关操作该小节会介绍下 TableAggregateFunction 的功能和用法。在引入 TableAggregateFunction 之前，Flink 里有三种自定义函数：ScalarFunction，TableFunction 和 AggregateFunction。我们可以从输入和输出的维度对这些自定义函数进行分类。如下图所示，ScalarFunction 是输入一行，输出一行；TableFunction 是输入一行，输出多行；AggregateFunction 是输入多行输出一行。为了让语义更加完整，Table API 新加了 TableAggregateFunction，它可以接收和输出多行。TableAggregateFunction 添加后，Table API 的功能可以得到很大的扩展，某种程度上可以用它来实现自定义 operator。比如，我们可以用 TableAggregateFunction 来实现 TopN。 TableAggregateFunction 使用也很简单，方法签名和用法如下图所示： 用法上，我们只需要调用 table.flatAggregate()，然后传入一个 TableAggregateFunction 实例即可。用户可以继承 TableAggregateFunction 来实现自定义的函数。继承的时候，需要先定义一个 Accumulator，用来存取状态，此外自定义的 TableAggregateFunction 需要实现 accumulate 和 emitValue 方法。accumulate 方法用来处理输入的数据，而 emitValue 方法负责根据 accumulator 里的状态输出结果。 三、Table API 动态最后介绍下 Table API 近期的动态： 1.Flip-29 主要是 Table API 功能和易用性的增强。比如刚刚介绍的 columns 相关操作，还有 TableAggregateFunction。 社区对应的 jira 是：https://issues.apache.org/jira/browse/FLINK-10972 2.Python Table API 希望在 Table API 上增加 python 语言的支持。这个应该是 Python 用户的福音。 社区对应的 jira 是：https://issues.apache.org/jira/browse/FLINK-12308 3.Interactive Programming(交互式编程) 即 Table 上会提供一个 cache 算子，执行 cache 操作可以缓存 table 的结果，并在这个结果上做其他操作。社区对应 jira 是：https://issues.apache.org/jira/browse/FLINK-11199 4.Iterative Processing(迭代计算) Table 上会支持一个 iterator 的算子，该算子可以用来执行迭代计算。比如迭代 100 次，或者指定一个收敛的条件，在机器学习领域使用比较广泛。社区对应 jira 是：https://issues.apache.org/jira/browse/FLINK-11199 ▼ Apache Flink 社区推荐 ▼ Apache Flink 及大数据领域顶级盛会 Flink Forward Asia 2019 重磅开启，目前正在征集议题，限量早鸟票优惠ing。了解 Flink Forward Asia 2019 的更多信息，请查看： https://developer.aliyun.com/special/ffa2019 首届 Apache Flink 极客挑战赛重磅开启，聚焦机器学习与性能优化两大热门领域，40万奖金等你拿，加入挑战请点击： https://tianchi.aliyun.com/markets/tianchi/flink2019]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 零基础入门教程（六）：状态管理及容错机制]]></title>
    <url>%2FApache%20Flink%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%8F%8A%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6.html</url>
    <content type="text"><![CDATA[本文主要分享内容如下： 状态管理的基本概念； 状态的类型与使用示例； 容错机制与故障恢复； 一.状态管理的基本概念1.什么是状态 首先举一个无状态计算的例子：消费延迟计算。假设现在有一个消息队列，消息队列中有一个生产者持续往消费队列写入消息，多个消费者分别从消息队列中读取消息。从图上可以看出，生产者已经写入 16 条消息，Offset 停留在 15 ；有 3 个消费者，有的消费快，而有的消费慢。消费快的已经消费了 13 条数据，消费者慢的才消费了 7、8 条数据。 如何实时统计每个消费者落后多少条数据，如图给出了输入输出的示例。可以了解到输入的时间点有一个时间戳，生产者将消息写到了某个时间点的位置，每个消费者同一时间点分别读到了什么位置。刚才也提到了生产者写入了 15 条，消费者分别读取了 10、7、12 条。那么问题来了，怎么将生产者、消费者的进度转换为右侧示意图信息呢？ consumer 0 落后了 5 条，consumer 1 落后了 8 条，consumer 2 落后了 3 条，根据 Flink 的原理，此处需进行 Map 操作。Map 首先把消息读取进来，然后分别相减，即可知道每个 consumer 分别落后了几条。Map 一直往下发，则会得出最终结果。 大家会发现，在这种模式的计算中，无论这条输入进来多少次，输出的结果都是一样的，因为单条输入中已经包含了所需的所有信息。消费落后等于生产者减去消费者。生产者的消费在单条数据中可以得到，消费者的数据也可以在单条数据中得到，所以相同输入可以得到相同输出，这就是一个无状态的计算。 相应的什么是有状态的计算？ 以访问日志统计量的例子进行说明，比如当前拿到一个 Nginx 访问日志，一条日志表示一个请求，记录该请求从哪里来，访问的哪个地址，需要实时统计每个地址总共被访问了多少次，也即每个 API 被调用了多少次。可以看到下面简化的输入和输出，输入第一条是在某个时间点请求 GET 了 /api/a；第二条日志记录了某个时间点 Post /api/b ;第三条是在某个时间点 GET了一个 /api/a，总共有 3 个 Nginx 日志。从这 3 条 Nginx 日志可以看出，第一条进来输出 /api/a 被访问了一次，第二条进来输出 /api/b 被访问了一次，紧接着又进来一条访问 api/a，所以 api/a 被访问了 2 次。不同的是，两条 /api/a 的 Nginx 日志进来的数据是一样的，但输出的时候结果可能不同，第一次输出 count=1 ，第二次输出 count=2，说明相同输入可能得到不同输出。输出的结果取决于当前请求的 API 地址之前累计被访问过多少次。第一条过来累计是 0 次，count = 1，第二条过来 API 的访问已经有一次了，所以 /api/a 访问累计次数 count=2。单条数据其实仅包含当前这次访问的信息，而不包含所有的信息。要得到这个结果，还需要依赖 API 累计访问的量，即状态。 这个计算模式是将数据输入算子中，用来进行各种复杂的计算并输出数据。这个过程中算子会去访问之前存储在里面的状态。另外一方面，它还会把现在的数据对状态的影响实时更新，如果输入 200 条数据，最后输出就是 200 条结果。 什么场景会用到状态呢？下面列举了常见的 4 种： 去重：比如上游的系统数据可能会有重复，落到下游系统时希望把重复的数据都去掉。去重需要先了解哪些数据来过，哪些数据还没有来，也就是把所有的主键都记录下来，当一条数据到来后，能够看到在主键当中是否存在。 窗口计算：比如统计每分钟 Nginx 日志 API 被访问了多少次。窗口是一分钟计算一次，在窗口触发前，如 08:00 ~ 08:01 这个窗口，前59秒的数据来了需要先放入内存，即需要把这个窗口之内的数据先保留下来，等到 8:01 时一分钟后，再将整个窗口内触发的数据输出。未触发的窗口数据也是一种状态。 机器学习/深度学习：如训练的模型以及当前模型的参数也是一种状态，机器学习可能每次都用有一个数据集，需要在数据集上进行学习，对模型进行一个反馈。 访问历史数据：比如与昨天的数据进行对比，需要访问一些历史数据。如果每次从外部去读，对资源的消耗可能比较大，所以也希望把这些历史数据也放入状态中做对比。 2.为什么要管理状态 管理状态最直接的方式就是将数据都放到内存中，这也是很常见的做法。比如在做 WordCount 时，Word 作为输入，Count 作为输出。在计算的过程中把输入不断累加到 Count。 但对于流式作业有以下要求： 7*24小时运行，高可靠； 数据不丢不重，恰好计算一次； 数据实时产出，不延迟； 基于以上要求，内存的管理就会出现一些问题。由于内存的容量是有限制的。如果要做 24 小时的窗口计算，将 24 小时的数据都放到内存，可能会出现内存不足；另外，作业是 7*24，需要保障高可用，机器若出现故障或者宕机，需要考虑如何备份及从备份中去恢复，保证运行的作业不受影响；此外，考虑横向扩展，假如网站的访问量不高，统计每个 API 访问次数的程序可以用单线程去运行，但如果网站访问量突然增加，单节点无法处理全部访问数据，此时需要增加几个节点进行横向扩展，这时数据的状态如何平均分配到新增加的节点也问题之一。因此，将数据都放到内存中，并不是最合适的一种状态管理方式。 3.理想的状态管理 最理想的状态管理需要满足易用、高效、可靠三点需求： 易用，Flink 提供了丰富的数据结构、多样的状态组织形式以及简洁的扩展接口，让状态管理更加易用； 高效，实时作业一般需要更低的延迟，一旦出现故障，恢复速度也需要更快；当处理能力不够时，可以横向扩展，同时在处理备份时，不影响作业本身处理性能； 可靠，Flink 提供了状态持久化，包括不丢不重的语义以及具备自动的容错能力，比如 HA，当节点挂掉后会自动拉起，不需要人工介入。 二.Flink 状态的类型与使用示例1.Managed State &amp; Raw State Managed State 是 Flink 自动管理的 State，而 Raw State 是原生态 State，两者的区别如下： 从状态管理方式的方式来说，Managed State 由 Flink Runtime 管理，自动存储，自动恢复，在内存管理上有优化；而 Raw State 需要用户自己管理，需要自己序列化，Flink 不知道 State 中存入的数据是什么结构，只有用户自己知道，需要最终序列化为可存储的数据结构。 从状态数据结构来说，Managed State 支持已知的数据结构，如 Value、List、Map 等。而 Raw State只支持字节数组 ，所有状态都要转换为二进制字节数组才可以。 从推荐使用场景来说，Managed State 大多数情况下均可使用，而 Raw State 是当 Managed State 不够用时，比如需要自定义 Operator 时，推荐使用 Raw State。 2.Keyed State &amp; Operator State Managed State 分为两种，一种是 Keyed State；另外一种是 Operator State。在Flink Stream模型中，Datastream 经过 keyBy 的操作可以变为 KeyedStream 。每个 Key 对应一个 State，即一个 Operator 实例处理多个 Key，访问相应的多个 State，并由此就衍生了 Keyed State。Keyed State 只能用在 KeyedStream 的算子中，即在整个程序中没有 keyBy 的过程就没有办法使用 KeyedStream。 相比较而言，Operator State 可以用于所有算子，相对于数据源有一个更好的匹配方式，常用于 Source，例如 FlinkKafkaConsumer。相比 Keyed State，一个 Operator 实例对应一个 State，随着并发的改变，Keyed State 中，State 随着 Key 在实例间迁移，比如原来有 1 个并发，对应的 API 请求过来，/api/a 和 /api/b 都存放在这个实例当中；如果请求量变大，需要扩容，就会把 /api/a 的状态和 /api/b 的状态分别放在不同的节点。由于 Operator State 没有 Key，并发改变时需要选择状态如何重新分配。其中内置了 2 种分配方式：一种是均匀分配，另外一种是将所有 State 合并为全量 State 再分发给每个实例。 在访问上，Keyed State 通过 RuntimeContext 访问，这需要 Operator 是一个Rich Function。Operator State 需要自己实现 CheckpointedFunction 或 ListCheckpointed 接口。在数据结构上，Keyed State 支持的数据结构，比如 ValueState、ListState、ReducingState、AggregatingState 和 MapState；而 Operator State 支持的数据结构相对较少，如 ListState。 3.Keyed State 使用示例 Keyed State 有很多种，如图为几种 Keyed State 之间的关系。首先 State 的子类中一级子类有 ValueState、MapState、AppendingState。AppendingState 又有一个子类 MergingState。MergingState 又分为 3 个子类分别是ListState、ReducingState、AggregatingState。这个继承关系使它们的访问方式、数据结构也存在差异。 几种 Keyed State 的差异具体体现在： ValueState 存储单个值，比如 Wordcount，用 Word 当 Key，State 就是它的 Count。这里面的单个值可能是数值或者字符串，作为单个值，访问接口可能有两种，get 和 set。在 State 上体现的是 update(T) / T value()。 MapState 的状态数据类型是 Map，在 State 上有 put、remove等。需要注意的是在 MapState 中的 key 和 Keyed state 中的 key 不是同一个。 ListState 状态数据类型是 List，访问接口如 add、update 等。 ReducingState 和 AggregatingState 与 ListState 都是同一个父类，但状态数据类型上是单个值，原因在于其中的 add 方法不是把当前的元素追加到列表中，而是把当前元素直接更新进了 Reducing 的结果中。 AggregatingState 的区别是在访问接口，ReducingState 中 add（T）和 T get() 进去和出来的元素都是同一个类型，但在 AggregatingState 输入的 IN，输出的是 OUT。 下面以 ValueState 为例，来阐述一下具体如何使用，以状态机的案例来讲解 。 源代码地址：https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/StateMachineExample.java 感兴趣的同学可直接查看完整源代码，在此截取部分。如图为 Flink 作业的主方法与主函数中的内容，前面的输入、后面的输出以及一些个性化的配置项都已去掉，仅保留了主干。 首先 events 是一个 DataStream，通过 env.addSource 加载数据进来，接下来有一个 DataStream 叫 alerts，先 keyby 一个 sourceAddress，然后在 flatMap 一个StateMachineMapper。StateMachineMapper 就是一个状态机，状态机指有不同的状态与状态间有不同的转换关系的结合，以买东西的过程简单举例。首先下订单，订单生成后状态为待付款，当再来一个事件状态付款成功，则事件的状态将会从待付款变为已付款，待发货。已付款，待发货的状态再来一个事件发货，订单状态将会变为配送中，配送中的状态再来一个事件签收，则该订单的状态就变为已签收。在整个过程中，随时都可以来一个事件，取消订单，无论哪个状态，一旦触发了取消订单事件最终就会将状态转移到已取消，至此状态就结束了。 Flink 写状态机是如何实现的？首先这是一个 RichFlatMapFunction，要用 Keyed State getRuntimeContext，getRuntimeContext 的过程中需要 RichFunction，所以需要在 open 方法中获取 currentState ，然后 getState，currentState 保存的是当前状态机上的状态。 如果刚下订单，那么 currentState 就是待付款状态，初始化后，currentState 就代表订单完成。订单来了后，就会走 flatMap 这个方法，在 flatMap 方法中，首先定义一个 State，从 currentState 取出，即 Value，Value 取值后先判断值是否为空，如果 sourceAddress state 是空，则说明没有被使用过，那么此状态应该为刚创建订单的初始状态，即待付款。然后赋值 state = State.Initial，注意此处的 State 是本地的变量，而不是 Flink 中管理的状态，将它的值从状态中取出。接下来在本地又会来一个变量，然后 transition，将事件对它的影响加上，刚才待付款的订单收到付款成功的事件，就会变成已付款，待发货，然后 nextState 即可算出。此外，还需要判断 State 是否合法，比如一个已签收的订单，又来一个状态叫取消订单，会发现已签收订单不能被取消，此时这个状态就会下发，订单状态为非法状态。 如果不是非法的状态，还要看该状态是否已经无法转换，比如这个状态变为已取消时，就不会在有其他的状态再发生了，此时就会从 state 中 clear。clear 是所有的 Flink 管理 keyed state 都有的公共方法，意味着将信息删除，如果既不是一个非法状态也不是一个结束状态，后面可能还会有更多的转换，此时需要将订单的当前状态 update ，这样就完成了 ValueState 的初始化、取值、更新以及清零，在整个过程中状态机的作用就是将非法的状态进行下发，方便下游进行处理。其他的状态也是类似的使用方式。 三.容错机制与故障恢复1.状态如何保存及恢复 Flink 状态保存主要依靠 Checkpoint 机制，Checkpoint 会定时制作分布式快照，对程序中的状态进行备份。分布式快照是如何实现的可以参考【第二课时】的内容，这里就不在阐述分布式快照具体是如何实现的。分布式快照 Checkpoint 完成后，当作业发生故障了如何去恢复？假如作业分布跑在 3 台机器上，其中一台挂了。这个时候需要把进程或者线程移到 active 的 2 台机器上，此时还需要将整个作业的所有 Task 都回滚到最后一次成功 Checkpoint 中的状态，然后从该点开始继续处理。 如果要从 Checkpoint 恢复，必要条件是数据源需要支持数据重新发送。Checkpoint恢复后， Flink 提供两种一致性语义，一种是恰好一次，一种是至少一次。在做 Checkpoint时，可根据 Barries 对齐来判断是恰好一次还是至少一次，如果对齐，则为恰好一次，否则没有对齐即为至少一次。如果作业是单线程处理，也就是说 Barries 是不需要对齐的；如果只有一个 Checkpoint 在做，不管什么时候从 Checkpoint 恢复，都会恢复到刚才的状态；如果有多个节点，假如一个数据的 Barries 到了，另一个 Barries 还没有来，内存中的状态如果已经存储。那么这 2 个流是不对齐的，恢复的时候其中一个流可能会有重复。 Checkpoint 通过代码的实现方法如下： 首先从作业的运行环境 env.enableCheckpointing 传入 1000，意思是做 2 个 Checkpoint 的事件间隔为 1 秒。Checkpoint 做的越频繁，恢复时追数据就会相对减少，同时 Checkpoint 相应的也会有一些 IO 消耗。 接下来是设置 Checkpoint 的 model，即设置了 Exactly_Once 语义，并且需要 Barries 对齐，这样可以保证消息不会丢失也不会重复。 setMinPauseBetweenCheckpoints 是 2 个 Checkpoint 之间最少是要等 500ms，也就是刚做完一个 Checkpoint。比如某个 Checkpoint 做了700ms，按照原则过 300ms 应该是做下一个 Checkpoint，因为设置了 1000ms 做一次 Checkpoint 的，但是中间的等待时间比较短，不足 500ms 了，需要多等 200ms，因此以这样的方式防止 Checkpoint 太过于频繁而导致业务处理的速度下降。 setCheckpointTimeout 表示做 Checkpoint 多久超时，如果 Checkpoint 在 1min 之内尚未完成，说明 Checkpoint 超时失败。setMaxConcurrentCheckpoints 表示同时有多少个 Checkpoint 在做快照，这个可以根据具体需求去做设置。 enableExternalizedCheckpoints 表示下 Cancel 时是否需要保留当前的 Checkpoint，默认 Checkpoint 会在整个作业 Cancel 时被删除。Checkpoint 是作业级别的保存点。 上面讲过，除了故障恢复之外，还需要可以手动去调整并发重新分配这些状态。手动调整并发，必须要重启作业并会提示 Checkpoint 已经不存在，那么作业如何恢复数据？ 一方面 Flink 在 Cancel 时允许在外部介质保留 Checkpoint ；另一方面，Flink 还有另外一个机制是 SavePoint。 Savepoint 与 Checkpoint 类似，同样是把状态存储到外部介质。当作业失败时，可以从外部恢复。Savepoint 与 Checkpoint 有什么区别呢？ 从触发管理方式来讲，Checkpoint 由 Flink 自动触发并管理，而 Savepoint 由用户手动触发并人肉管理； 从用途来讲，Checkpoint 在 Task 发生异常时快速恢复，例如网络抖动或超时异常，而 Savepoint 有计划地进行备份，使作业能停止后再恢复，例如修改代码、调整并发； 最后从特点来讲，Checkpoint 比较轻量级，作业出现问题会自动从故障中恢复，在作业停止后默认清除；而 Savepoint 比较持久，以标准格式存储，允许代码或配置发生改变，恢复需要启动作业手动指定一个路径恢复。 2.可选的状态存储方式 Checkpoint 的存储，第一种是内存存储，即 MemoryStateBackend，构造方法是设置最大的StateSize，选择是否做异步快照，这种存储状态本身存储在 TaskManager 节点也就是执行节点内存中的，因为内存有容量限制，所以单个 State maxStateSize 默认 5 M，且需要注意 maxStateSize &lt;= akka.framesize 默认 10 M。Checkpoint 存储在 JobManager 内存中，因此总大小不超过 JobManager 的内存。推荐使用的场景为：本地测试、几乎无状态的作业，比如 ETL、JobManager 不容易挂，或挂掉影响不大的情况。不推荐在生产场景使用。 另一种就是在文件系统上的 FsStateBackend ，构建方法是需要传一个文件路径和是否异步快照。State 依然在 TaskManager 内存中，但不会像 MemoryStateBackend 有 5 M 的设置上限，Checkpoint 存储在外部文件系统（本地或 HDFS），打破了总大小 Jobmanager 内存的限制。容量限制上，单 TaskManager 上 State 总量不超过它的内存，总大小不超过配置的文件系统容量。推荐使用的场景、常规使用状态的作业、例如分钟级窗口聚合或 join、需要开启HA的作业。 还有一种存储为 RocksDBStateBackend ，RocksDB 是一个 key/value 的内存存储系统，和其他的 key/value 一样，先将状态放到内存中，如果内存快满时，则写入到磁盘中，但需要注意 RocksDB 不支持同步的 Checkpoint，构造方法中没有同步快照这个选项。不过 RocksDB 支持增量的 Checkpoint，也是目前唯一增量 Checkpoint 的 Backend，意味着每次用户不需要将所有状态都写进去，将增量的改变的状态写进去即可。它的 Checkpoint 存储在外部文件系统（本地或HDFS），其容量限制只要单个 TaskManager 上 State 总量不超过它的内存+磁盘，单 Key最大 2G，总大小不超过配置的文件系统容量即可。推荐使用的场景为：超大状态的作业，例如天级窗口聚合、需要开启 HA 的作业、最好是对状态读写性能要求不高的作业。 四.总结1.为什么要使用状态？前面提到有状态的作业要有有状态的逻辑，有状态的逻辑是因为数据之间存在关联，单条数据是没有办法把所有的信息给表现出来。所以需要通过状态来满足业务逻辑。 2.为什么要管理状态？使用了状态，为什么要管理状态？因为实时作业需要7*24不间断的运行，需要应对不可靠的因素而带来的影响。 3.如何选择状态的类型和存储方式？那如何选择状态的类型和存储方式？结合前面的内容，可以看到，首先是要分析清楚业务场景；比如想要做什么，状态到底大不大。比较各个方案的利弊，选择根据需求合适的状态类型和存储方式即可。 视频回顾：https://www.bilibili.com/video/av49736102?from=search&amp;seid=5739530486011132468]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 零基础入门（五）：流处理核心组件Time]]></title>
    <url>%2FApache%20Flink%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E6%B5%81%E5%A4%84%E7%90%86%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6Time.html</url>
    <content type="text"><![CDATA[1. Window &amp; Time 介绍Apache Flink（以下简称 Flink） 是一个天然支持无限流数据处理的分布式计算框架，在 Flink 中 Window 可以将无限流切分成有限流，是处理有限流的核心组件，现在 Flink 中 Window 可以是时间驱动的（Time Window），也可以是数据驱动的（Count Window）。 下面的代码是在 Flink 中使用 Window 的两个示例 2. Window API 使用从第一部分我们已经知道 Window 的一些基本概念，以及相关 API，下面我们以一个实际例子来看看怎么使用 Window 相关的 API。 代码来自 flink-examples： 上面的例子中我们首先会对每条数据进行时间抽取，然后进行 keyby，接着依次调用 window()，evictor(), trigger() 以及 maxBy()。下面我们重点来看 window(), evictor() 和 trigger() 这几个方法。 2.1 WindowAssigner, Evictor 以及 TriggerWindow 方法接收的输入是一个WindowAssigner， WindowAssigner 负责将每条输入的数据分发到正确的 Window 中（一条数据可能同时分发到多个 Window 中），Flink 提供了几种通用的 WindowAssigner：tumbling window(窗口间的元素无重复），sliding window（窗口间的元素可能重复），session window 以及 global window。如果需要自己定制数据分发策略，则可以实现一个 class，继承自 WindowAssigner。 Tumbling Window Sliding Window Session Window Global Window Evictor 主要用于做一些数据的自定义操作，可以在执行用户代码之前，也可以在执行用户代码之后，更详细的描述可以参考 org.apache.flink.streaming.api.windowing.evictors.Evictor 的 evicBefore 和 evicAfter 两个方法。Flink 提供了如下三种通用的 evictor： CountEvictor 保留指定数量的元素 DeltaEvictor 通过执行用户给定的 DeltaFunction 以及预设的 threshold，判断是否删除一个元素。 TimeEvictor设定一个阈值 interval，删除所有不再 max_ts - interval 范围内的元素，其中 max_ts 是窗口内时间戳的最大值。 Evictor 是可选的方法，如果用户不选择，则默认没有。 Trigger 用来判断一个窗口是否需要被触发，每个 WindowAssigner 都自带一个默认的 Trigger，如果默认的 Trigger 不能满足你的需求，则可以自定义一个类，继承自 Trigger 即可，我们详细描述下 Trigger 的接口以及含义： onElement() 每次往 window 增加一个元素的时候都会触发 onEventTime() 当 event-time timer 被触发的时候会调用 onProcessingTime() 当 processing-time timer 被触发的时候会调用 onMerge() 对两个 trigger 的 state 进行 merge 操作 clear() window 销毁的时候被调用 上面的接口中前三个会返回一个 TriggerResult，TriggerResult 有如下几种可能的选择： CONTINUE 不做任何事情 FIRE 触发 window PURGE 清空整个 window 的元素并销毁窗口 FIRE_AND_PURGE 触发窗口，然后销毁窗口 2.2 Time &amp; Watermark了解完上面的内容后，对于时间驱动的窗口，我们还有两个概念需要澄清：Time 和 Watermark。 我们知道在分布式环境中 Time 是一个很重要的概念，在 Flink 中 Time 可以分为三种 Event-Time，Processing-Time 以及 Ingestion-Time，三者的关系我们可以从下图中得知： Event Time、Ingestion Time、Processing Time Event-Time 表示事件发生的时间，Processing-Time 则表示处理消息的时间（墙上时间），Ingestion-Time 表示进入到系统的时间。 在 Flink 中我们可以通过下面的方式进行 Time 类型的设置 env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); // 设置使用 ProcessingTime 了解了 Time 之后，我们还需要知道 Watermark 相关的概念。 我们可以考虑一个这样的例子：某 App 会记录用户的所有点击行为，并回传日志（在网络不好的情况下，先保存在本地，延后回传）。A 用户在 11:02 对 App 进行操作，B 用户在 11:03 操作了 App，但是 A 用户的网络不太稳定，回传日志延迟了，导致我们在服务端先接受到 B 用户 11:03 的消息，然后再接受到 A 用户 11:02 的消息，消息乱序了。 那我们怎么保证基于 event-time 的窗口在销毁的时候，已经处理完了所有的数据呢？这就是 watermark 的功能所在。watermark 会携带一个单调递增的时间戳 t，watermark(t) 表示所有时间戳不大于 t 的数据都已经到来了，未来小于等于 t 的数据不会再来，因此可以放心地触发和销毁窗口了。下图中给了一个乱序数据流中的 Watermark 例子 2.3 迟到的数据上面的 Watermark 让我们能够应对乱序的数据，但是真实世界中我们没法得到一个完美的 Watermark 数值 — 要么没法获取到，要么耗费太大，因此实际工作中我们会使用近似 watermark — 生成 watermark(t) 之后，还有较小的概率接受到时间戳 t 之前的数据，在 Flink 中将这些数据定义为 “late elements”, 同样我们可以在 Window 中指定是允许延迟的最大时间（默认为 0），可以使用下面的代码进行设置 设置allowedLateness 之后，迟来的数据同样可以触发窗口，进行输出，利用 Flink 的 side output 机制，我们可以获取到这些迟到的数据，使用方式如下： 需要注意的是，设置了 allowedLateness 之后，迟到的数据也可能触发窗口，对于 Session window 来说，可能会对窗口进行合并，产生预期外的行为。 3. Window 内部实现在讨论 Window 内部实现的时候，我们再通过下图回顾一下 Window 的生命周期 每条数据过来之后，会由 WindowAssigner 分配到对应的 Window，当 Window 被触发之后，会交给 Evictor（如果没有设置 Evictor 则跳过），然后处理 UserFunction。其中 WindowAssigner，Trigger，Evictor 我们都在上面讨论过，而 UserFunction 则是用户编写的代码。 整个流程还有一个问题需要讨论：Window 中的状态存储。我们知道 Flink 是支持 Exactly Once 处理语义的，那么 Window 中的状态存储和普通的状态存储又有什么不一样的地方呢？ 首先给出具体的答案：从接口上可以认为没有区别，但是每个 Window 会属于不同的 namespace，而非 Window 场景下，则都属于 VoidNamespace ，最终由 State/Checkpoint 来保证数据的 Exactly Once 语义，下面我们从 org.apache.flink.streaming.runtime.operators.windowing.WindowOperator 摘取一段代码进行阐述 从上面我们可以知道，Window 中的的元素同样是通过 State 进行维护，然后由 Checkpoint 机制保证 Exactly Once 语义。 至此，Time、Window 相关的所有内容都已经讲解完毕，主要包括为什么要有 Window； Window 中的三个核心组件：WindowAssigner、Trigger 和 Evictor；Window 中怎么处理乱序数据，乱序数据是否允许延迟，以及怎么处理迟到的数据；最后我们梳理了整个 Window 的数据流程，以及 Window 中怎么保证 Exactly Once 语义]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 零基础入门（四）：客户端操作的 5 种模式]]></title>
    <url>%2FApache%20Flink%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C%E7%9A%84%205%20%E7%A7%8D%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[1.环境说明在前面几期的课程里面讲过了 Flink 开发环境的搭建和应用的部署以及运行，今天的课程主要是讲 Flink 的客户端操作。本次讲解以实际操作为主。这次课程是基于社区的 Flink 1.7.2 版本，操作系统是 Mac 系统，浏览器是 Google Chrome 浏览器。有关开发环境的准备和集群的部署，请参考「开发环境搭建和应用的配置、部署及运行」的内容。 2.课程概要如下图所示，Flink 提供了丰富的客户端操作来提交任务和与任务进行交互，包括 Flink 命令行，Scala Shell，SQL Client，Restful API 和 Web。Flink 首先提供的最重要的是命令行，其次是 SQL Client 用于提交 SQL 任务的运行，还有就是 Scala Shell 提交 Table API 的任务。同时，Flink 也提供了Restful 服务，用户可以通过 http 方式进行调用。此外，还有 Web 的方式可以提交任务。 在 Flink 安装目录的 bin 目录下面可以看到有 flink, start-scala-shell.sh 和 sql-client.sh 等文件，这些都是客户端操作的入口。 3.Flink 客户端操作3.1 Flink 命令行Flink 的命令行参数很多，输入 flink - h 能看到完整的说明： 1flink-1.7.2 bin/flink -h 如果想看某一个命令的参数，比如 Run 命令，输入： 1flink-1.7.2 bin/flink run -h 本文主要讲解常见的一些操作，更详细的文档请参考: Flink 命令行官方文档。 3.1.1 Standalone首先启动一个 Standalone 的集群： 1234 flink-1.7.2 bin/start-cluster.shStarting cluster.Starting standalonesession daemon on host zkb-MBP.local.Starting taskexecutor daemon on host zkb-MBP.local. 打开 http://127.0.0.1:8081 能看到 Web 界面。 Run运行任务，以 Flink 自带的例子 TopSpeedWindowing 为例： 123456 flink-1.7.2 bin/flink run -d examples/streaming/TopSpeedWindowing.jarStarting execution of programExecuting TopSpeedWindowing example with default input data set.Use --input to specify file input.Printing result to stdout. Use --output to specify output path.Job has been submitted with JobID 5e20cb6b0f357591171dfcca2eea09de 运行起来后默认是 1 个并发: 点左侧「Task Manager」，然后点「Stdout」能看到输出日志： 或者查看本地 Log 目录下的 *.out 文件： List查看任务列表： 123456 flink-1.7.2 bin/flink list -m 127.0.0.1:8081Waiting for response...------------------ Running/Restarting Jobs -------------------24.03.2019 10:14:06 : 5e20cb6b0f357591171dfcca2eea09de : CarTopSpeedWindowingExample (RUNNING)--------------------------------------------------------------No scheduled jobs. Stop停止任务。通过 -m 来指定要停止的 JobManager 的主机地址和端口。 123456789101112131415161718192021222324252627282930 flink-1.7.2 bin/flink stop -m 127.0.0.1:8081 d67420e52bd051fae2fddbaa79e046bbStopping job d67420e52bd051fae2fddbaa79e046bb.------------------------------------------------------------The program finished with the following exception: org.apache.flink.util.FlinkException: Could not stop the job d67420e52bd051fae2fddbaa79e046bb. at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:554) at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:985) at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:547) at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1062) at org.apache.flink.client.cli.CliFrontend.lambda$main$11(CliFrontend.java:1126) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1126)Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [Job termination (STOP) failed: This job is not stoppable.] at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915) at org.apache.flink.client.program.rest.RestClusterClient.stop(RestClusterClient.java:392) at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:552)... 9 moreCaused by: org.apache.flink.runtime.rest.util.RestClientException: [Job termination (STOP) failed: This job is not stoppable.] at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:380) at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:364) at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 从日志里面能看出 Stop 命令执行失败了。一个 Job 能够被 Stop 要求所有的 Source 都是可以 Stoppable 的，即实现了 StoppableFunction 接口。 12345678910111213/** * 需要能 stoppable 的函数必须实现这个接口，例如流式任务的 source。 * stop() 方法在任务收到 STOP 信号的时候调用。 * source 在接收到这个信号后，必须停止发送新的数据且优雅的停止。 */@PublicEvolvingpublic interface StoppableFunction &#123; /** * 停止 source。与 cancel() 不同的是，这是一个让 source 优雅停止的请求。 * 等待中的数据可以继续发送出去，不需要立即停止。 */ void stop();&#125; Cancel取消任务。如果在 conf/flink-conf.yaml 里面配置了 state.savepoints.dir，会保存 Savepoint，否则不会保存 Savepoint。 1234 flink-1.7.2 bin/flink cancel -m 127.0.0.1:8081 5e20cb6b0f357591171dfcca2eea09de Cancelling job 5e20cb6b0f357591171dfcca2eea09de.Cancelled job 5e20cb6b0f357591171dfcca2eea09de. 也可以在停止的时候显示指定 Savepoint 目录。 12345678 flink-1.7.2 bin/flink cancel -m 127.0.0.1:8081 -s /tmp/savepoint 29da945b99dea6547c3fbafd57ed8759 Cancelling job 29da945b99dea6547c3fbafd57ed8759 with savepoint to /tmp/savepoint.Cancelled job 29da945b99dea6547c3fbafd57ed8759. Savepoint stored in file:/tmp/savepoint/savepoint-29da94-88299bacafb7. flink-1.7.2 ll /tmp/savepoint/savepoint-29da94-88299bacafb7total 32K-rw-r--r-- 1 baoniu 29K Mar 24 10:33 _metadata 取消和停止（流作业）的区别如下： cancel() 调用，立即调用作业算子的 cancel() 方法，以尽快取消它们。如果算子在接到 cancel() 调用后没有停止，Flink 将开始定期中断算子线程的执行，直到所有算子停止为止。 stop() 调用，是更优雅的停止正在运行流作业的方式。stop() 仅适用于 Source 实现了 StoppableFunction 接口的作业。当用户请求停止作业时，作业的所有 Source 都将接收 stop() 方法调用。直到所有 Source 正常关闭时，作业才会正常结束。这种方式，使作业正常处理完所有作业。 Savepoint触发 Savepoint。 12345 flink-1.7.2 bin/flink savepoint -m 127.0.0.1:8081 ec53edcfaeb96b2a5dadbfbe5ff62bbb /tmp/savepointTriggering savepoint for job ec53edcfaeb96b2a5dadbfbe5ff62bbb.Waiting for response...Savepoint completed. Path: file:/tmp/savepoint/savepoint-ec53ed-84b00ce500eeYou can resume your program from this savepoint with the run command. 说明：Savepoint 和 Checkpoint 的区别（详见文档）： Checkpoint 是增量做的，每次的时间较短，数据量较小，只要在程序里面启用后会自动触发，用户无须感知；Checkpoint 是作业 failover 的时候自动使用，不需要用户指定。 Savepoint 是全量做的，每次的时间较长，数据量较大，需要用户主动去触发。Savepoint 一般用于程序的版本更新（详见文档），Bug 修复，A/B Test 等场景，需要用户指定。 通过 -s 参数从指定的 Savepoint 启动： 12345 flink-1.7.2 bin/flink run -d -s /tmp/savepoint/savepoint-f049ff-24ec0d3e0dc7 ./examples/streaming/TopSpeedWindowing.jarStarting execution of programExecuting TopSpeedWindowing example with default input data set.Use --input to specify file input.Printing result to stdout. Use --output to specify output path. 查看 JobManager 的日志，能够看到类似这样的 Log： 1234562019-03-28 10:30:53,957 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Starting job 790d7b98db6f6af55d04aec1d773852d from savepoint /tmp/savepoint/savepoint-f049ff-24ec0d3e0dc7 ()2019-03-28 10:30:53,959 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Reset the checkpoint ID of job 790d7b98db6f6af55d04aec1d773852d to 2.2019-03-28 10:30:53,959 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Restoring job 790d7b98db6f6af55d04aec1d773852d from latest valid checkpoint: Checkpoint 1 @ 0 for 790d7b98db6f6af55d04aec1d773852d. Modify修改任务并行度。 为了方便演示，我们修改 conf/flink-conf.yaml 将 Task Slot 数从默认的 1 改为 4，并配置 Savepoint 目录。（Modify 参数后面接 -s 指定 Savepoint 路径当前版本可能有 Bug，提示无法识别） 12taskmanager.numberOfTaskSlots: 4state.savepoints.dir: file:///tmp/savepoint 修改参数后需要重启集群生效，然后再启动任务： 12345678910111213 flink-1.7.2 bin/stop-cluster.sh &amp;&amp; bin/start-cluster.shStopping taskexecutor daemon (pid: 53139) on host zkb-MBP.local.Stopping standalonesession daemon (pid: 52723) on host zkb-MBP.local.Starting cluster.Starting standalonesession daemon on host zkb-MBP.local.Starting taskexecutor daemon on host zkb-MBP.local. flink-1.7.2 bin/flink run -d examples/streaming/TopSpeedWindowing.jarStarting execution of programExecuting TopSpeedWindowing example with default input data set.Use --input to specify file input.Printing result to stdout. Use --output to specify output path.Job has been submitted with JobID 7752ea7b0e7303c780de9d86a5ded3fa 从页面上能看到 Task Slot 变为了 4，这时候任务的默认并发度是 1。 通过 Modify 命令依次将并发度修改为 4 和 3，可以看到每次 Modify 命令都会触发一次 Savepoint。 123456789101112131415 flink-1.7.2 bin/flink modify -p 4 7752ea7b0e7303c780de9d86a5ded3faModify job 7752ea7b0e7303c780de9d86a5ded3fa.Rescaled job 7752ea7b0e7303c780de9d86a5ded3fa. Its new parallelism is 4. flink-1.7.2 ll /tmp/savepointtotal 0drwxr-xr-x 3 baoniu 96 Jun 17 09:05 savepoint-7752ea-00c05b015836/ flink-1.7.2 bin/flink modify -p 3 7752ea7b0e7303c780de9d86a5ded3faModify job 7752ea7b0e7303c780de9d86a5ded3fa.Rescaled job 7752ea7b0e7303c780de9d86a5ded3fa. Its new parallelism is 3. flink-1.7.2 ll /tmp/savepointtotal 0drwxr-xr-x 3 baoniu 96 Jun 17 09:08 savepoint-7752ea-449b131b2bd4/ 查看 JobManager 的日志，可以看到： 1234562019-06-17 09:05:11,179 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Starting job 7752ea7b0e7303c780de9d86a5ded3fa from savepoint file:/tmp/savepoint/savepoint-790d7b-3581698f007e ()2019-06-17 09:05:11,182 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Reset the checkpoint ID of job 7752ea7b0e7303c780de9d86a5ded3fa to 3.2019-06-17 09:05:11,182 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Restoring job 790d7b98db6f6af55d04aec1d773852d from latest valid checkpoint: Checkpoint 2 @ 0 for 7752ea7b0e7303c780de9d86a5ded3fa.2019-06-17 09:05:11,184 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - No master state to restore2019-06-17 09:05:11,184 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Job CarTopSpeedWindowingExample (7752ea7b0e7303c780de9d86a5ded3fa) switched from state RUNNING to SUSPENDING.org.apache.flink.util.FlinkException: Job is being rescaled. InfoInfo 命令是用来查看 Flink 任务的执行计划（StreamGraph）的。 1234 flink-1.7.2 bin/flink info examples/streaming/TopSpeedWindowing.jar----------------------- Execution Plan -----------------------&#123;&quot;nodes&quot;:[&#123;&quot;id&quot;:1,&quot;type&quot;:&quot;Source: Custom Source&quot;,&quot;pact&quot;:&quot;Data Source&quot;,&quot;contents&quot;:&quot;Source: Custom Source&quot;,&quot;parallelism&quot;:1&#125;,&#123;&quot;id&quot;:2,&quot;type&quot;:&quot;Timestamps/Watermarks&quot;,&quot;pact&quot;:&quot;Operator&quot;,&quot;contents&quot;:&quot;Timestamps/Watermarks&quot;,&quot;parallelism&quot;:1,&quot;predecessors&quot;:[&#123;&quot;id&quot;:1,&quot;ship_strategy&quot;:&quot;FORWARD&quot;,&quot;side&quot;:&quot;second&quot;&#125;]&#125;,&#123;&quot;id&quot;:4,&quot;type&quot;:&quot;Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction)&quot;,&quot;pact&quot;:&quot;Operator&quot;,&quot;contents&quot;:&quot;Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction)&quot;,&quot;parallelism&quot;:1,&quot;predecessors&quot;:[&#123;&quot;id&quot;:2,&quot;ship_strategy&quot;:&quot;HASH&quot;,&quot;side&quot;:&quot;second&quot;&#125;]&#125;,&#123;&quot;id&quot;:5,&quot;type&quot;:&quot;Sink: Print to Std. Out&quot;,&quot;pact&quot;:&quot;Data Sink&quot;,&quot;contents&quot;:&quot;Sink: Print to Std. Out&quot;,&quot;parallelism&quot;:1,&quot;predecessors&quot;:[&#123;&quot;id&quot;:4,&quot;ship_strategy&quot;:&quot;FORWARD&quot;,&quot;side&quot;:&quot;second&quot;&#125;]&#125;]&#125;-------------------------------------------------------------- 拷贝输出的 Json 内容，粘贴到这个网站：http://flink.apache.org/visualizer/ 可以和实际运行的物理执行计划对比： 3.1.2 Yarn per-job单任务 Attach 模式默认是 Attach 模式，即客户端会一直等待直到程序结束才会退出。 通过 -m yarn-cluster 指定 Yarn 模式 Yarn 上显示名字为 Flink session cluster，这个 Batch 的 Wordcount 任务运行完会 FINISHED。 客户端能看到结果输出 123456789101112131415161718192021222324252627282930313233343536[admin@z17.sqa.zth /home/admin/flink/flink-1.7.2]$echo $HADOOP_CONF_DIR/etc/hadoop/conf/ [admin@z17.sqa.zth /home/admin/flink/flink-1.7.2]$./bin/flink run -m yarn-cluster ./examples/batch/WordCount.jar 2019-06-17 09:15:24,511 INFO org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:80502019-06-17 09:15:24,690 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar2019-06-17 09:15:24,690 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar2019-06-17 09:15:24,907 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=4&#125;2019-06-17 09:15:25,430 WARN org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.2019-06-17 09:15:25,438 WARN org.apache.flink.yarn.AbstractYarnClusterDescriptor - The configuration directory (&apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf&apos;) contains both LOG4J and Logback configuration files. Please delete or rename one of them.2019-06-17 09:15:36,239 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Submitting application master application_1532332183347_07242019-06-17 09:15:36,276 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1532332183347_07242019-06-17 09:15:36,276 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Waiting for the cluster to be allocated2019-06-17 09:15:36,281 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Deploying cluster, current state ACCEPTED2019-06-17 09:15:40,426 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - YARN application has been deployed successfully.Starting execution of programExecuting WordCount example with default input data set.Use --input to specify file input.Printing result to stdout. Use --output to specify output path.(a,5)(action,1)(after,1)(against,1)(all,2)... ...(would,2)(wrong,1)(you,1)Program execution finishedJob with JobID 8bfe7568cb5c3254af30cbbd9cd5971e has finished.Job Runtime: 9371 msAccumulator Results:- 2bed2c5506e9237fb85625416a1bc508 (java.util.ArrayList) [170 elements] 如果我们以 Attach 模式运行 Streaming 的任务，客户端会一直等待不退出，可以运行以下的例子试验下： 1./bin/flink run -m yarn-cluster ./examples/streaming/TopSpeedWindowing.jar 单任务 Detached 模式 由于是 Detached 模式，客户端提交完任务就退出了 Yarn 上显示为 Flink per-job cluster 1234567891011121314151617$./bin/flink run -yd -m yarn-cluster ./examples/streaming/TopSpeedWindowing.jar 2019-06-18 09:21:59,247 INFO org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:80502019-06-18 09:21:59,428 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar2019-06-18 09:21:59,428 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar2019-06-18 09:21:59,940 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=4&#125;2019-06-18 09:22:00,427 WARN org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.2019-06-18 09:22:00,436 WARN org.apache.flink.yarn.AbstractYarnClusterDescriptor - The configuration directory (&apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf&apos;) contains both LOG4J and Logback configuration files. Please delete or rename one of them.^@2019-06-18 09:22:12,113 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Submitting application master application_1532332183347_07292019-06-18 09:22:12,151 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1532332183347_07292019-06-18 09:22:12,151 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Waiting for the cluster to be allocated2019-06-18 09:22:12,155 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Deploying cluster, current state ACCEPTED2019-06-18 09:22:16,275 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - YARN application has been deployed successfully.2019-06-18 09:22:16,275 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - The Flink YARN client has been started in detached mode. In order to stop Flink on YARN, use the following command or a YARN web interface to stop it:yarn application -kill application_1532332183347_0729Please also note that the temporary files of the YARN session in the home directory will not be removed.Job has been submitted with JobID e61b9945c33c300906ad50a9a11f36df 3.1.3 Yarn session启动 Session1./bin/yarn-session.sh -tm 2048 -s 3 表示启动一个 Yarn session 集群，每个 TM 的内存是 2 G，每个 TM 有 3 个 Slot。(注意：-n 参数不生效) 1234567891011121314151617181920212223 flink-1.7.2 ./bin/yarn-session.sh -tm 2048 -s 32019-06-17 09:21:50,177 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.rpc.address, localhost2019-06-17 09:21:50,179 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.rpc.port, 61232019-06-17 09:21:50,179 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.heap.size, 1024m2019-06-17 09:21:50,179 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: taskmanager.heap.size, 1024m2019-06-17 09:21:50,179 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: taskmanager.numberOfTaskSlots, 42019-06-17 09:21:50,179 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: state.savepoints.dir, file:///tmp/savepoint2019-06-17 09:21:50,180 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: parallelism.default, 12019-06-17 09:21:50,180 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: rest.port, 80812019-06-17 09:21:50,644 WARN org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable2019-06-17 09:21:50,746 INFO org.apache.flink.runtime.security.modules.HadoopModule - Hadoop user set to baoniu (auth:SIMPLE)2019-06-17 09:21:50,848 INFO org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:80502019-06-17 09:21:51,148 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=2048, numberTaskManagers=1, slotsPerTaskManager=3&#125;2019-06-17 09:21:51,588 WARN org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.2019-06-17 09:21:51,596 WARN org.apache.flink.yarn.AbstractYarnClusterDescriptor - The configuration directory (&apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf&apos;) contains both LOG4J and Logback configuration files. Please delete or rename one of them.^@2019-06-17 09:22:03,304 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Submitting application master application_1532332183347_07262019-06-17 09:22:03,336 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1532332183347_07262019-06-17 09:22:03,336 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Waiting for the cluster to be allocated2019-06-17 09:22:03,340 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Deploying cluster, current state ACCEPTED2019-06-17 09:22:07,722 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - YARN application has been deployed successfully.2019-06-17 09:22:08,050 INFO org.apache.flink.runtime.rest.RestClient - Rest client endpoint started.Flink JobManager is now running on z07.sqa.net:37109 with leader id 00000000-0000-0000-0000-000000000000.JobManager Web Interface: http://z07.sqa.net:37109 客户端默认是 Attach 模式，不会退出： 可以 ctrl + c 退出，然后再通过 ./bin/yarn-session.sh -id application_1532332183347_0726 连上来； 或者启动的时候用 -d 则为 detached 模式Yarn 上显示为 Flink session cluster； 在本机的临时目录（有些机器是 /tmp 目录）下会生成一个文件： 123456 flink-1.7.2 cat /var/folders/2b/r6d49pcs23z43b8fqsyz885c0000gn/T/.yarn-properties-baoniu#Generated YARN properties file#Mon Jun 17 09:22:08 CST 2019parallelism=3dynamicPropertiesString=applicationID=application_1532332183347_0726 提交任务1./bin/flink run ./examples/batch/WordCount.jar 将会根据 /tmp/.yarn-properties-admin 文件内容提交到了刚启动的 Session。 12345678910111213141516171819202122232425262728 flink-1.7.2 ./bin/flink run ./examples/batch/WordCount.jar2019-06-17 09:26:42,767 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - Found Yarn properties file under /var/folders/2b/r6d49pcs23z43b8fqsyz885c0000gn/T/.yarn-properties-baoniu.2019-06-17 09:26:42,767 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - Found Yarn properties file under /var/folders/2b/r6d49pcs23z43b8fqsyz885c0000gn/T/.yarn-properties-baoniu.2019-06-17 09:26:43,058 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - YARN properties set default parallelism to 32019-06-17 09:26:43,058 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - YARN properties set default parallelism to 3YARN properties set default parallelism to 32019-06-17 09:26:43,097 INFO org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:80502019-06-17 09:26:43,229 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar2019-06-17 09:26:43,229 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar2019-06-17 09:26:43,327 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Found application JobManager host name &apos;z05c07216.sqa.zth.tbsite.net&apos; and port &apos;37109&apos; from supplied application id &apos;application_1532332183347_0726&apos;Starting execution of programExecuting WordCount example with default input data set.Use --input to specify file input.Printing result to stdout. Use --output to specify output path.^@(a,5)(action,1)(after,1)(against,1)(all,2)(and,12)... ...(wrong,1)(you,1)Program execution finishedJob with JobID ad9b0f1feed6d0bf6ba4e0f18b1e65ef has finished.Job Runtime: 9152 msAccumulator Results:- fd07c75d503d0d9a99e4f27dd153114c (java.util.ArrayList) [170 elements] 运行结束后 TM 的资源会释放。 提交到指定的 Session通过 -yid 参数来提交到指定的 Session。 1234567891011$./bin/flink run -d -p 30 -m yarn-cluster -yid application_1532332183347_0708 ./examples/streaming/TopSpeedWindowing.jar 2019-03-24 12:36:33,668 INFO org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:80502019-03-24 12:36:33,773 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar2019-03-24 12:36:33,773 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar2019-03-24 12:36:33,837 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Found application JobManager host name &apos;z05c05218.sqa.zth.tbsite.net&apos; and port &apos;60783&apos; from supplied application id &apos;application_1532332183347_0708&apos;Starting execution of programExecuting TopSpeedWindowing example with default input data set.Use --input to specify file input.Printing result to stdout. Use --output to specify output path.Job has been submitted with JobID 58d5049ebbf28d515159f2f88563f5fd 注：Blink版本 的 Session 与 Flink 的 Session 的区别： Flink 的 session -n 参数不生效，而且不会提前启动 TM； Blink 的 session 可以通过 -n 指定启动多少个 TM，而且 TM 会提前起来； 3.2 Scala Shell官方文档：https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/scala_shell.html 3.2.1 DeployLocal123456$bin/start-scala-shell.sh localStarting Flink Shell:Starting local Flink cluster (host: localhost, port: 8081).Connecting to Flink cluster (host: localhost, port: 8081).... ...scala&gt; 任务运行说明： Batch 任务内置了 benv 变量，通过 print() 将结果输出到控制台； Streaming 任务内置了 senv 变量，通过 senv.execute(“job name”) 来提交任务，且 Datastream 的输出只有在 Local 模式下打印到控制台； Remote先启动一个 yarn session cluster： 12345678910111213$./bin/yarn-session.sh -tm 2048 -s 32019-03-25 09:52:16,341 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.rpc.address, localhost2019-03-25 09:52:16,342 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.rpc.port, 61232019-03-25 09:52:16,342 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.heap.size, 1024m2019-03-25 09:52:16,343 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: taskmanager.heap.size, 1024m2019-03-25 09:52:16,343 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: taskmanager.numberOfTaskSlots, 42019-03-25 09:52:16,343 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: parallelism.default, 12019-03-25 09:52:16,343 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: state.savepoints.dir, file:///tmp/savepoint2019-03-25 09:52:16,343 INFO org.apache.flink.configuration.GlobalConfiguration … ...Flink JobManager is now running on z054.sqa.net:28665 with leader id 00000000-0000-0000-0000-000000000000.JobManager Web Interface: http://z054.sqa.net:28665 启动 scala shell，连到 jm： 12345$bin/start-scala-shell.sh remote z054.sqa.net 28665Starting Flink Shell:Connecting to Flink cluster (host: z054.sqa.net, port: 28665).... ...scala&gt; Yarn123456789101112131415161718192021222324252627$./bin/start-scala-shell.sh yarn -n 2 -jm 1024 -s 2 -tm 1024 -nm flink-yarnStarting Flink Shell:2019-03-25 09:47:44,695 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.rpc.address, localhost2019-03-25 09:47:44,697 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.rpc.port, 61232019-03-25 09:47:44,697 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: jobmanager.heap.size, 1024m2019-03-25 09:47:44,697 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: taskmanager.heap.size, 1024m2019-03-25 09:47:44,697 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: taskmanager.numberOfTaskSlots, 42019-03-25 09:47:44,698 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: parallelism.default, 12019-03-25 09:47:44,698 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: state.savepoints.dir, file:///tmp/savepoint2019-03-25 09:47:44,698 INFO org.apache.flink.configuration.GlobalConfiguration - Loading configuration property: rest.port, 80812019-03-25 09:47:44,717 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - Found Yarn properties file under /tmp/.yarn-properties-admin.2019-03-25 09:47:45,041 INFO org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:80502019-03-25 09:47:45,098 WARN org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable2019-03-25 09:47:45,266 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar2019-03-25 09:47:45,275 INFO org.apache.flink.yarn.cli.FlinkYarnSessionCli - The argument yn is deprecated in will be ignored.2019-03-25 09:47:45,357 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=2, slotsPerTaskManager=2&#125;2019-03-25 09:47:45,711 WARN org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.2019-03-25 09:47:45,718 WARN org.apache.flink.yarn.AbstractYarnClusterDescriptor - The configuration directory (&apos;/home/admin/flink/flink-1.7.2/conf&apos;) contains both LOG4J and Logback configuration files. Please delete or rename one of them.2019-03-25 09:47:46,514 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Submitting application master application_1532332183347_07102019-03-25 09:47:46,534 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1532332183347_07102019-03-25 09:47:46,534 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Waiting for the cluster to be allocated2019-03-25 09:47:46,535 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - Deploying cluster, current state ACCEPTED2019-03-25 09:47:51,051 INFO org.apache.flink.yarn.AbstractYarnClusterDescriptor - YARN application has been deployed successfully.2019-03-25 09:47:51,222 INFO org.apache.flink.runtime.rest.RestClient - Rest client endpoint started.Connecting to Flink cluster (host: 10.10.10.10, port: 56942). 按 CTRL + C 退出 Shell 后，这个 Flink cluster 还会继续运行，不会退出。 3.2.2 ExecuteDataSet1234567891011121314151617181920212223 flink-1.7.2 bin/stop-cluster.shNo taskexecutor daemon to stop on host zkb-MBP.local.No standalonesession daemon to stop on host zkb-MBP.local. flink-1.7.2 bin/start-scala-shell.sh localStarting Flink Shell:Starting local Flink cluster (host: localhost, port: 8081).Connecting to Flink cluster (host: localhost, port: 8081).scala&gt; val text = benv.fromElements(&quot;To be, or not to be,--that is the question:--&quot;)text: org.apache.flink.api.scala.DataSet[String] = org.apache.flink.api.scala.DataSet@5b407336scala&gt; val counts = text.flatMap &#123; _.toLowerCase.split(&quot;\\W+&quot;) &#125;.map &#123; (_, 1) &#125;.groupBy(0).sum(1)counts: org.apache.flink.api.scala.AggregateDataSet[(String, Int)] = org.apache.flink.api.scala.AggregateDataSet@6ee34fe4scala&gt; counts.print()(be,2)(is,1)(not,1)(or,1)(question,1)(that,1)(the,1)(to,2) 对 DataSet 任务来说，print() 会触发任务的执行。 也可以将结果输出到文件（先删除 /tmp/out1，不然会报错同名文件已经存在），继续执行以下命令： 12345scala&gt; counts.writeAsText(&quot;/tmp/out1&quot;)res1: org.apache.flink.api.java.operators.DataSink[(String, Int)] = DataSink &apos;&lt;unnamed&gt;&apos; (TextOutputFormat (/tmp/out1) - UTF-8)scala&gt; benv.execute(&quot;batch test&quot;)res2: org.apache.flink.api.common.JobExecutionResult = org.apache.flink.api.common.JobExecutionResult@737652a9 查看 /tmp/out1 文件就能看到输出结果。 123456789 flink-1.7.2 cat /tmp/out1(be,2)(is,1)(not,1)(or,1)(question,1)(that,1)(the,1)(to,2) DataSteam123456789101112131415161718192021scala&gt; val textStreaming = senv.fromElements(&quot;To be, or not to be,--that is the question:--&quot;)textStreaming: org.apache.flink.streaming.api.scala.DataStream[String] = org.apache.flink.streaming.api.scala.DataStream@4970b93dscala&gt; val countsStreaming = textStreaming.flatMap &#123; _.toLowerCase.split(&quot;\\W+&quot;) &#125;.map &#123; (_, 1) &#125;.keyBy(0).sum(1)countsStreaming: org.apache.flink.streaming.api.scala.DataStream[(String, Int)] = org.apache.flink.streaming.api.scala.DataStream@6a478680scala&gt; countsStreaming.print()res3: org.apache.flink.streaming.api.datastream.DataStreamSink[(String, Int)] = org.apache.flink.streaming.api.datastream.DataStreamSink@42bfc11fscala&gt; senv.execute(&quot;Streaming Wordcount&quot;)(to,1)(be,1)(or,1)(not,1)(to,2)(be,2)(that,1)(is,1)(the,1)(question,1)res4: org.apache.flink.api.common.JobExecutionResult = org.apache.flink.api.common.JobExecutionResult@1878815a 对 DataStream 任务，print() 并不会触发任务的执行，需要显示调用 execute(“job name”) 才会执行任务。 TableAPI在 Blink 开源版本里面，支持了 TableAPI 方式提交任务（可以用 btenv.sqlQuery 提交 SQL 查询），社区版本 Flink 1.8 会支持 TableAPI: https://issues.apache.org/jira/browse/FLINK-9555 3.3 SQL Client BetaSQL Client 目前还只是测试版，处于开发阶段，只能用于 SQL 的原型验证，不推荐在生产环境使用。 3.3.1 基本用法1234567891011121314151617181920212223242526272829303132 flink-1.7.2 bin/start-cluster.shStarting cluster.Starting standalonesession daemon on host zkb-MBP.local.Starting taskexecutor daemon on host zkb-MBP.local. flink-1.7.2 ./bin/sql-client.sh embeddedNo default environment specified.Searching for &apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf/sql-client-defaults.yaml&apos;...found.Reading default environment from: file:/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf/sql-client-defaults.yamlNo session environment specified.Validating current environment...done.… …Flink SQL&gt; help;The following commands are available:QUIT Quits the SQL CLI client.CLEAR Clears the current terminal.HELP Prints the available commands.SHOW TABLES Shows all registered tables.SHOW FUNCTIONS Shows all registered user-defined functions.DESCRIBE Describes the schema of a table with the given name.EXPLAIN Describes the execution plan of a query or table with the given name.SELECT Executes a SQL SELECT query on the Flink cluster.INSERT INTO Inserts the results of a SQL SELECT query into a declared table sink.CREATE VIEW Creates a virtual table from a SQL query. Syntax: &apos;CREATE VIEW &lt;name&gt; AS &lt;query&gt;;&apos;DROP VIEW Deletes a previously created virtual table. Syntax: &apos;DROP VIEW &lt;name&gt;;&apos;SOURCE Reads a SQL SELECT query from a file and executes it on the Flink cluster.SET Sets a session configuration property. Syntax: &apos;SET &lt;key&gt;=&lt;value&gt;;&apos;. Use &apos;SET;&apos; for listing all properties.RESET Resets all session configuration properties.Hint: Make sure that a statement ends with &apos;;&apos; for finalizing (multi-line) statements. Select 查询1Flink SQL&gt; SELECT &apos;Hello World&apos;; 按 ”Q” 退出这个界面打开 http://127.0.0.1:8081 能看到这条 Select 语句产生的查询任务已经结束了。这个查询采用的是读取固定数据集的 Custom Source，输出用的是 Stream Collect Sink，且只输出一条结果。 注意：如果本机的临时目录存在类似 .yarn-properties-baoniu 的文件，任务会提交到 Yarn 上。 ExplainExplain 命令可以查看 SQL 的执行计划。 1234567891011121314151617Flink SQL&gt; explain SELECT name, COUNT(*) AS cnt FROM (VALUES (&apos;Bob&apos;), (&apos;Alice&apos;), (&apos;Greg&apos;), (&apos;Bob&apos;)) AS NameTable(name) GROUP BY name;== Abstract Syntax Tree == // 抽象语法树LogicalAggregate(group=[&#123;0&#125;], cnt=[COUNT()]) LogicalValues(tuples=[[&#123; _UTF-16LE&apos;Bob &apos; &#125;, &#123; _UTF-16LE&apos;Alice&apos; &#125;, &#123; _UTF-16LE&apos;Greg &apos; &#125;, &#123; _UTF-16LE&apos;Bob &apos; &#125;]])== Optimized Logical Plan == // 优化后的逻辑执行计划DataStreamGroupAggregate(groupBy=[name], select=[name, COUNT(*) AS cnt]) DataStreamValues(tuples=[[&#123; _UTF-16LE&apos;Bob &apos; &#125;, &#123; _UTF-16LE&apos;Alice&apos; &#125;, &#123; _UTF-16LE&apos;Greg &apos; &#125;, &#123; _UTF-16LE&apos;Bob &apos; &#125;]])== Physical Execution Plan == // 物理执行计划Stage 3 : Data Source content : collect elements with CollectionInputFormat Stage 5 : Operator content : groupBy: (name), select: (name, COUNT(*) AS cnt) ship_strategy : HASH 3.3.2 结果展示SQL Client 支持两种模式来维护并展示查询结果： table mode: 在内存中物化查询结果，并以分页 table 形式展示。用户可以通过以下命令启用 table mode; 1SET execution.result-mode=table changlog mode: 不会物化查询结果，而是直接对 continuous query 产生的添加和撤回（retractions）结果进行展示。 1SET execution.result-mode=changelog 接下来通过实际的例子进行演示。 Table mode1234Flink SQL&gt; SET execution.result-mode=table;[INFO] Session property has been set.Flink SQL&gt; SELECT name, COUNT(*) AS cnt FROM (VALUES (&apos;Bob&apos;), (&apos;Alice&apos;), (&apos;Greg&apos;), (&apos;Bob&apos;)) AS NameTable(name) GROUP BY name; 运行结果如下图所示： Changlog mode1234Flink SQL&gt; SET execution.result-mode=changelog;[INFO] Session property has been set.Flink SQL&gt; SELECT name, COUNT(*) AS cnt FROM (VALUES (&apos;Bob&apos;), (&apos;Alice&apos;), (&apos;Greg&apos;), (&apos;Bob&apos;)) AS NameTable(name) GROUP BY name; 运行结果如下图所示： 其中 ‘-’ 代表的就是撤回消息。 3.3.3 Environment Files目前的 SQL Client 还不支持 DDL 语句，只能通过 yaml 文件的方式来定义 SQL 查询需要的表，UDF 和运行参数等信息。 首先，准备 env.yaml 和 input.csv 两个文件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 flink-1.7.2 cat /tmp/env.yamltables: - name: MyTableSource type: source-table update-mode: append connector: type: filesystem path: &quot;/tmp/input.csv&quot; format: type: csv fields: - name: MyField1 type: INT - name: MyField2 type: VARCHAR line-delimiter: &quot;\n&quot; comment-prefix: &quot;#&quot; schema: - name: MyField1 type: INT - name: MyField2 type: VARCHAR - name: MyCustomView type: view query: &quot;SELECT MyField2 FROM MyTableSource&quot; - name: MyTableSink type: sink-table update-mode: append connector: type: filesystem path: &quot;/tmp/output.csv&quot; format: type: csv fields: - name: MyField1 type: INT - name: MyField2 type: VARCHAR schema: - name: MyField1 type: INT - name: MyField2 type: VARCHAR # Execution properties allow for changing the behavior of a table program.execution: type: streaming # required: execution mode either &apos;batch&apos; or &apos;streaming&apos; result-mode: table # required: either &apos;table&apos; or &apos;changelog&apos; max-table-result-rows: 1000000 # optional: maximum number of maintained rows in # &apos;table&apos; mode (1000000 by default, smaller 1 means unlimited) time-characteristic: event-time # optional: &apos;processing-time&apos; or &apos;event-time&apos; (default) parallelism: 1 # optional: Flink&apos;s parallelism (1 by default) periodic-watermarks-interval: 200 # optional: interval for periodic watermarks (200 ms by default) max-parallelism: 16 # optional: Flink&apos;s maximum parallelism (128 by default) min-idle-state-retention: 0 # optional: table program&apos;s minimum idle state time max-idle-state-retention: 0 # optional: table program&apos;s maximum idle state time restart-strategy: # optional: restart strategy type: fallback # &quot;fallback&quot; to global restart strategy by default# Deployment properties allow for describing the cluster to which table programs are submitted to.deployment: response-timeout: 5000 flink-1.7.2 cat /tmp/input.csv1,hello2,world3,hello world1,ok3,bye bye4,yes 启动 SQL Client： 12345678910111213141516171819202122232425262728293031323334 flink-1.7.2 ./bin/sql-client.sh embedded -e /tmp/env.yamlNo default environment specified.Searching for &apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf/sql-client-defaults.yaml&apos;...found.Reading default environment from: file:/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf/sql-client-defaults.yamlReading session environment from: file:/tmp/env.yamlValidating current environment...done.Flink SQL&gt; show tables;MyCustomViewMyTableSinkMyTableSourceFlink SQL&gt; describe MyTableSource;root |-- MyField1: Integer |-- MyField2: StringFlink SQL&gt; describe MyCustomView;root |-- MyField2: StringFlink SQL&gt; create view MyView1 as select MyField1 from MyTableSource;[INFO] View has been created.Flink SQL&gt; show tables;MyCustomViewMyTableSourceMyView1Flink SQL&gt; describe MyView1;root |-- MyField1: IntegerFlink SQL&gt; select * from MyTableSource; 使用 insert into 写入结果表： 123456Flink SQL&gt; insert into MyTableSink select * from MyTableSource;[INFO] Submitting SQL update statement to the cluster...[INFO] Table update statement has been successfully submitted to the cluster:Cluster ID: StandaloneClusterIdJob ID: 3fac2be1fd891e3e07595c684bb7b7a0Web interface: http://localhost:8081 查询生成的结果数据文件： 1234567 flink-1.7.2 cat /tmp/output.csv1,hello2,world3,hello world1,ok3,bye bye4,yes 也可以在 Environment 文件里面定义 UDF，在 SQL Client 里面通过 「HOW FUNCTIONS」查询和使用，这里就不再说明了。 SQL Client 功能社区还在开发中，详见 FLIP-24。 3.4 Restful API接下来我们演示如何通过 Rest API 来提交 Jar 包和执行任务。 更详细的操作请参考 Flink 的 Restful API 文档：https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/rest_api.html 1234567891011 flink-1.7.2 curl http://127.0.0.1:8081/overview&#123;&quot;taskmanagers&quot;:1,&quot;slots-total&quot;:4,&quot;slots-available&quot;:0,&quot;jobs-running&quot;:3,&quot;jobs-finished&quot;:0,&quot;jobs-cancelled&quot;:0,&quot;jobs-failed&quot;:0,&quot;flink-version&quot;:&quot;1.7.2&quot;,&quot;flink-commit&quot;:&quot;ceba8af&quot;&#125;% flink-1.7.2 curl -X POST -H &quot;Expect:&quot; -F &quot;jarfile=@/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/examples/streaming/TopSpeedWindowing.jar&quot; http://127.0.0.1:8081/jars/upload&#123;&quot;filename&quot;:&quot;/var/folders/2b/r6d49pcs23z43b8fqsyz885c0000gn/T/flink-web-124c4895-cf08-4eec-8e15-8263d347efc2/flink-web-upload/6077eca7-6db0-4570-a4d0-4c3e05a5dc59_TopSpeedWindowing.jar&quot;,&quot;status&quot;:&quot;success&quot;&#125;% flink-1.7.2 curl http://127.0.0.1:8081/jars&#123;&quot;address&quot;:&quot;http://localhost:8081&quot;,&quot;files&quot;:[&#123;&quot;id&quot;:&quot;6077eca7-6db0-4570-a4d0-4c3e05a5dc59_TopSpeedWindowing.jar&quot;,&quot;name&quot;:&quot;TopSpeedWindowing.jar&quot;,&quot;uploaded&quot;:1553743438000,&quot;entry&quot;:[&#123;&quot;name&quot;:&quot;org.apache.flink.streaming.examples.windowing.TopSpeedWindowing&quot;,&quot;description&quot;:null&#125;]&#125;]&#125;% flink-1.7.2 curl http://127.0.0.1:8081/jars/6077eca7-6db0-4570-a4d0-4c3e05a5dc59_TopSpeedWindowing.jar/plan&#123;&quot;plan&quot;:&#123;&quot;jid&quot;:&quot;41029eb3feb9132619e454ec9b2a89fb&quot;,&quot;name&quot;:&quot;CarTopSpeedWindowingExample&quot;,&quot;nodes&quot;:[&#123;&quot;id&quot;:&quot;90bea66de1c231edf33913ecd54406c1&quot;,&quot;parallelism&quot;:1,&quot;operator&quot;:&quot;&quot;,&quot;operator_strategy&quot;:&quot;&quot;,&quot;description&quot;:&quot;Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -&gt; Sink: Print to Std. Out&quot;,&quot;inputs&quot;:[&#123;&quot;num&quot;:0,&quot;id&quot;:&quot;cbc357ccb763df2852fee8c4fc7d55f2&quot;,&quot;ship_strategy&quot;:&quot;HASH&quot;,&quot;exchange&quot;:&quot;pipelined_bounded&quot;&#125;],&quot;optimizer_properties&quot;:&#123;&#125;&#125;,&#123;&quot;id&quot;:&quot;cbc357ccb763df2852fee8c4fc7d55f2&quot;,&quot;parallelism&quot;:1,&quot;operator&quot;:&quot;&quot;,&quot;operator_strategy&quot;:&quot;&quot;,&quot;description&quot;:&quot;Source: Custom Source -&gt; Timestamps/Watermarks&quot;,&quot;optimizer_properties&quot;:&#123;&#125;&#125;]&#125;&#125;% flink-1.7.2 curl -X POST http://127.0.0.1:8081/jars/6077eca7-6db0-4570-a4d0-4c3e05a5dc59_TopSpeedWindowing.jar/run&#123;&quot;jobid&quot;:&quot;04d80a24b076523d3dc5fbaa0ad5e1ad&quot;&#125;% Restful API 还提供了很多监控和 Metrics 相关的功能，对于任务提交的操作也支持的比较全面。 3.5 Web在 Flink Dashboard 页面左侧可以看到有个「Submit new Job」的地方，用户可以上传 Jar 包和显示执行计划和提交任务。Web 提交功能主要用于新手入门和演示用。 4.结束本期的课程到这里就结束了，我们主要讲解了 Flink 的 5 种任务提交的方式。熟练掌握各种任务提交方式，有利于提高我们日常的开发和运维效率。 视频回顾：https://zh.ververica.com/developers/flink-training-course2/]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 全领域干货合集]]></title>
    <url>%2FApache%20Flink%20%E5%85%A8%E9%A2%86%E5%9F%9F%E5%B9%B2%E8%B4%A7%E5%90%88%E9%9B%86.html</url>
    <content type="text"><![CDATA[【重要资讯】《重磅揭晓！Flink Forward Asia 2019 议程完整出炉》《史上超强阵容！大数据及人工智能领域顶级盛会，Flink Forward Asia 2019 不容错过！》《阿里巴巴高级技术专家章剑锋：大数据发展的 8 个要点》《修改代码150万行！与 Blink 合并后的 Apache Flink 1.9.0 究竟有哪些重大变更？》杨克特（鲁尼）《Apache Flink 1.9重磅发布！首次合并阿里内部版本Blink重要功能》Flink社区 【入门教程】进阶篇《Apache Flink 进阶（四）：Flink on Yarn/K8s 原理剖析及实践》周凯波（宝牛）《Apache Flink 进阶（三）：Checkpoint 原理解析与应用实践》唐云（茶干）《Apache Flink 进阶入门（二）：Time 深度解析》崔星灿《Apache Flink 进阶（一）：Runtime 核心机制剖析》高赟（云骞） 零基础入门《Apache Flink 零基础入门（一）：基础概念解析》陈守元、戴资力《Apache Flink 零基础入门（二）：开发环境搭建和应用的配置、部署及运行》沙晟阳（成阳）《Apache Flink 零基础入门（三）：DataStream API 编程》崔星灿、高赟《Apache Flink 零基础入门（四）：客户端操作的 5 种模式》周凯波（宝牛）《Apache Flink 零基础入门（五）：流处理核心组件 Time&amp;Window 深度解析》邱从贤（山智）《Apache Flink 零基础入门（六）：状态管理及容错机制》孙梦瑶（美团点评）《Apache Flink 零基础入门（七）：Table API 编程》程鹤群（军长）《Apache Flink 零基础入门（八）： SQL 编程实践》伍翀（云邪） 【应用案例】《如何构建批流一体数据融合平台的一致性语义保证？》陈肃（Datapipeline）《Apache Flink 在同程艺龙实时计算平台的研发与应用实践》同程艺龙数据中心 Flink 小分队（谢磊、周生乾、李苏兴）《OPPO数据中台之基石：基于Flink SQL构建实数据仓库》张俊（OPPO）《日均处理万亿数据！Flink在快手的应用实践与技术演进之路》董亭亭（快手）《小红书如何实现高效推荐？解密背后的大数据计算平台架构》郭一（小红书）《用Flink取代Spark Streaming！知乎实时数仓架构演进》知乎数据工程团队《58 集团大规模 Storm 任务平滑迁移至 Flink 的秘密》万石康、冯海涛（58集团）《监控系统哪家强？eBay 在监控系统上的实践应用！》eBay Unified Monitoring Platform 【开发者实践】《Flink Kafka Connector 与 Exactly Once 剖析》史天舒（TalkingData）《Flink SQL 系列 | 5 个 TableEnvironment 我该用哪个？》徐榜江（雪尽）《开篇 | 揭秘 Flink 1.9 新架构，Blink Planner 你会用了吗？》徐榜江（雪尽）《Flink on YARN（下）：常见问题与排查思路》杨弢（搏远）《Flink on YARN（上）：一张图轻松掌握基础架构与启动流程》杨弢（搏远）《Flink Checkpoint 问题排查实用指南》邱从贤（山智）《如果你也想做实时数仓…》郭华（付空）《如何在 Apache Flink 中使用 Python API？》孙金城（金竹）《Flink 1.9 实战：使用 SQL 读取 Kafka 并写入 MySQL》伍翀（云邪）《如何正确使用 Flink Connector？》董亭亭（快手）《如何在 Flink 1.9 中使用 Hive？》李锐《Apache Flink 1.9.0 为什么将支持 Python API ？》孙金城（金竹） 【Meetup精彩回顾&amp;PPT下载】9月7日《Apache Flink Meetup ·上海站（附PPT下载链接）》8月31日《Kafka x Flink Meetup 与世界人工智能大会大数据 AI 专场精彩回顾（附PPT下载）》]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流式处理术语解释Exactly-once与Effectively-once]]></title>
    <url>%2F%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E6%9C%AF%E8%AF%AD%E8%A7%A3%E9%87%8AExactly-once%E4%B8%8EEffectively-once.html</url>
    <content type="text"><![CDATA[分布式事件流处理已逐渐成为大数据领域的热点话题。该领域主要的流处理引擎（SPE）包括 Apache Storm、Apache Flink、Heron、Apache Kafka（Kafka Streams）以及 Apache Spark（Spark Streaming）等。处理语义是围绕 SPE 最受关注，讨论最多的话题之一，其中“严格一次（Exactly-once）”是很多引擎追求的目标之一，很多 SPE 均宣称可提供“严格一次”的处理语义。 然而“严格一次”具体指什么，需要具备哪些能力，当 SPE 宣称可支持时这实际上意味着什么，对于这些问题还有很多误解和歧义。使用“严格一次”来描述处理语义，这本身也容易造成误导。本文将探讨各大主要 SPE 在“严格一次”处理语义方面的差异，以及为什么“严格一次”更适合称之为“实际一次（Effectively-once）”。同时本文还将探讨在实现所谓“严格一次”的语义过程中，各类常用技术之间需要进行的取舍。 背景流处理通常也被称之为事件处理，简单来说是指持续不断地处理一系列无穷无尽地数据或事件地过程。流处理或事件处理应用程序大致可以看作一种有向图（Directed graph），大部分情况（但也并非总是如此）下也可以看作有向非循环图（Directed acyclic graph，DAG）。在这种图中，每个边缘（Edge）可代表一个数据或事件流，每个顶点（Vertex）代表使用应用程序定义的逻辑处理来自相邻边缘的数据或事件的运算符（Operator）。有两种特殊类型的顶点，通常称之为 Source 和 Sink，Source 会消耗外部数据 / 事件并将其注入应用程序，而 Sink 通常负责收集应用程序生成的结果。图 1 展示了这样的一个流应用程序范例。 图 1：一个典型的 Heron 处理拓扑 执行流 / 事件处理应用程序的 SPE 通常可供用户指定可靠性模式或处理语义，这代表了在跨越整个应用程序图处理数据时所能提供的保证。这些保证是有一定意义的，因为我们始终可以假设由于网络、计算机等原因遇到失败进而导致数据丢失的概率。在描述 SPE 能为应用程序提供的数据处理语义时，通常会使用三种模式 / 标签：最多一次（At-most-once）、最少一次（At-least-once），以及严格一次（Exactly-once）。 这些不同处理语义可粗略理解如下： 最多一次这其实是一种“尽力而为”的方法。数据或事件可以保证被应用程序中的所有运算符最多处理一次。这意味着如果在流应用程序最终成功处理之前就已丢失，则不会额外试图重试或重新传输事件。图 2 列举了一个范例。 图 2：最多一次处理语义 最少一次数据或事件可保证被应用程序图中的所有运算符最少处理一次。这通常意味着如果在流应用程序最终成功处理之前就已丢失，那么事件将从来源重播（Replayed）或重新传输。然而因为可以重新传输，有时候一个事件可能被多次处理，因此这种方式被称之为“最少一次”。图 3 展示了一个范例。在本例中，第一个运算符最初处理事件时失败了，随后重试并成功，随后再次重试并再次成功，然而再次重试实际上是不必要的。 图 3：最少一次处理语义 严格一次事件可保证被流应用程序中的所有运算符“严格一次”处理，哪怕遇到各种失败。 为了实现“严格一次”处理语义，通常主要会使用下列两种机制： 分布式快照 / 状态检查点 最少一次事件交付，外加消息去重 通过分布式快照 / 状态检查点方法实现的“严格一次”是由 Chandy-Lamport 分布式快照算法[1]启发而来的。在这种机制中，会定期为流应用程序中每个运算符的所有状态创建检查点，一旦系统中任何位置出现失败，每个运算符的所有状态会回滚至最新的全局一致检查点。回滚过程中所有处理工作会暂停。随后源也会重置为与最新检查点相符的偏移量。整个流应用程序基本上会被“倒带”到最新一致状态，并从该状态开始重新处理。图 4 展示了这种机制的一些基本概念。 图 4：分布式快照 在图 4 中，流应用程序在 T1 时正在正常运行，并创建了状态检查点。然而在 T2 时，运算符在处理传入的数据时失败了。此时 S = 4 这个状态值已经被保存到持久存储中，而 S = 12 状态值正位于运算符的内存中。为了调和这种矛盾，在 T3 时处理图将状态回退至 S = 4，并“重播”了流中直至最新状态前每个连续的状态，并处理了每个数据。最终结果是有些数据被处理了多次，但这也没问题，因为无论回滚多少次，结果状态都是相同的。 实现“严格一致”的另一种方法是在实现至少一次事件交付的同时在每个运算符一端进行事件去重。使用这种方法的 SPE 会重播失败的事件并再次尝试处理，并从每个运算符中移除重复的事件，随后才将结果事件发送给用户在运算符中定义的逻辑。这种机制要求为每个运算符保存事务日志，借此才能追踪哪些事件已经处理过了。为此 SPE 通常会使用诸如 Google 的 MillWheel[2]以及Apache Kafka Streams等机制。图 5 展示了这种机制的概况。 图 5：至少一次交付外加去重 严格一次真的就一次吗？接着重新考虑一下“严格一次”处理语义实际上能为最终用户提供怎样的保证。“严格一次”这样的标签对于到底什么只执行一次其实起到了一定的误导效果。 有些人可能认为“严格一次”描述了一种保证：在事件处理过程中，流中的每个事件只被处理一次。实际上任何 SPE 都不能完全保证真的只处理一次。面对各种可能的失败，根本不可能保证每个运算符中包含的，由用户定义的逻辑针对每个事件只执行一次，因为用户代码的不完整执行（Partial execution）这种可能性始终会出现。 假设这样一个场景：有个流处理运算符需要执行 Map 操作输出传入事件的 ID，随后返回无改变的事件。例如这个操作可能使用了如下的虚构代码： 复制代码 1 Map (Event event) { Print &quot;Event ID: &quot; + event.getId() Return event } 每个事件有自己的 GUID（全局唯一 ID）。如果用户逻辑的严格一次执行可以得到保证，那么事件 ID 将只输出一次。然而这一点永远无法保证，因为用户定义的逻辑执行过程中可能随时随地发生失败。SPE 无法自行判断用户定义的处理逻辑到底执行到哪一步了。因此任何用户定义的逻辑都无法保证只执行一次。这也意味着用户定义逻辑中实现的外部操作，例如数据库写入也无法严格保证只执行一次。此类操作依然需要通过幂等的方式实现。 那么当 SPE 宣称提供“严格一次”的处理语义保证时，它们指的到底是什么？如果用户逻辑无法严格保证只执行一次，那么到底是什么东西只执行了一次？当 SPE 宣称“严格一次”处理语义时，它们真正的含义在于可以保证在对 SPE 管理的状态进行更新时，可以只向后端的持久存储提交一次。 上文提到的两种机制均使用持久的后端存储作为事实来源（Source of truth），用于保存每个操作符的状态，并自动提交状态更新。对于机制 1（分布式快照 / 状态检查点），这个持久的后端存储可用于保存流应用程序中全局一致的状态检查点（每个运算符的状态检查点）；对于机制 2（至少一次事件交付，外加去重），这个持久的后端存储可用于保存每个运算符的状态，以及为了追踪哪些事件已经被成功处理过而为每个运算符生成的事务日志。 状态的提交或对事实来源的持久后端进行的更新可描述为事件（Occurring）的严格一次。然而在计算状态的更新 / 改动，例如所处理的事件正在针对事件执行各种用户定义的逻辑时，如果失败则可能进行多次，这一点正如上文所述。换句话说，事件的处理可能会进行多次，但处理的最终结果只会在持久的后端状态存储中体现一次。因此 Streamlio 认为“实际一次（Effectively-once）”可以更精确地描述这样地处理语义。 分布式快照，与至少一次事件交付外加去重机制的对比从语义的角度来看，分布式快照，以及至少一次事件交付外加去重，这两种机制可以提供相同的保证。然而由于两种机制在实现方面的差异，可能会对性能产生巨大的影响。 基于机制 1（分布式快照 / 状态检查点）的 SPE 在性能方面的开销可能是最低的，因为基本上，SPE 只需要在通过流应用程序照常处理事件的过程之外发送少量特殊事件，而状态检查点操作可以在后台以异步的方式进行。但是对于大型流应用程序，失败的概率将会更高，这会导致 SPE 需要暂停应用程序并回滚所有操作符的状态，这会对性能产生较大影响。流应用程序规模越大，遇到失败的频率就会越高，因此性能方面受到的影响也会越大。然而需要再次提醒的是，这种机制是非侵入式的，只会对资源的使用造成最少量的影响。 机制 2（至少一次事件交付外加去重）可能需要更多资源，尤其是存储资源。在这种机制中，SPE 需要能追踪已经被运算符的每个实例成功处理的每个元组（Tuple），借此才能执行去重并实现自身在每个事件中的去重。这可能需要追踪非常大量的数据，尤其是当流应用程序规模非常大，或运行了很多应用程序的时候。每个运算符中的每个事件执行去重操作，这本身也会产生巨大的性能开销。然而对于这种机制，流应用程序的性能不太可能受到应用程序规模的影响。对于机制 1，如果任何运算符遇到任何失败，均需要全局暂停并状态回滚；对于机制 2，失败只能影响到局部。如果某个运算符遇到失败，只需要从上游来源重播 / 重新传输尚未成功处理的事件，对性能的影响可隔离在流应用程序中实际发生失败的地方，只会对流应用程序中其他运算符的性能产生最少量的影响。从性能的角度来看，两种机制各有利弊，具体情况可参阅下文表格。 分布式快照 / 状态检查点 利 弊 性能和资源开销小 从失败中恢复时的性能影响大 随着拓扑规模逐渐增大，对性能的潜在影响将增高 至少一次交付外加去重 利 弊 失败对性能的影响更为局部 可能需要存储与基础架构提供更多支持 失败的影响未必随着拓扑规模一起增加 每个运算符处理每个事件均会产生性能开销 虽然从理论上看，分布式快照，和至少一次事件交付外加去重，这两种机制之间存在差异，但两者均可理解为至少一次处理外加幂等。对于这两种机制，如果遇到失败事件将会重播 / 重新传输（为了实现至少一次），而在状态回滚或事件去重时，如果从内部更新所管理的状态，运算符实际上将具备幂等的特性。 结论希望本文可以帮助大家意识到“严格一次”这个术语极具误导性。提供“严格一次”的处理语义实际上意味着在对流处理引擎所管理的运算符的状态进行各种更新后，结果将仅体现一次。“严格一次”完全无法保证事件的处理（例如执行各类用户定义的逻辑）只需要进行一次。因此 Streamlio 更愿意使用“最终一次”这个属于来描述这种保证，因为没必要确保处理工作只进行一次，只要保证由 SPE 管理的状态的最终结果只体现一次就够了。分布式快照和消息去重，这两种主流机制就是为了实现严格 / 实际一次的处理语义。在消息处理和状态更新方面，这两种机制均可提供相同的语义保证，但在性能方面可能有所差异。本文并不是为了探讨哪种机制更胜一筹，因为每种机制都各有利弊]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流处理系统中的“exactly Once”语义保证]]></title>
    <url>%2F%E6%B5%81%E5%A4%84%E7%90%86%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E2%80%9Cexactly%20Once%E2%80%9D%E8%AF%AD%E4%B9%89%E4%BF%9D%E8%AF%81.html</url>
    <content type="text"><![CDATA[前言最近在学习一些流处理相关的知识，对比与笔者接触比较多的离线处理系统，实时流处理的有些地方还是比较有意思的。在这里面，最常被人提到的词应该是“Exactly Once”语义 ，在工作面试中，如果做过实时流系统，肯定免不了被问到“xx框架是如何做到Exactly Once的”？笔者最近在阅读Spark Streaming的官方文档中，提到了这一点，于是来做个小小的总结归纳。如果感兴趣的同学，请继续往下阅读。 语义定义 在流处理系统中，我们对应数据记录的处理，有3种级别的语义定义，以此来衡量这个流处理系统的能力。 At most once（最多一次）。每条数据记录最多被处理一次，潜台词也表明数据会有丢失（没被处理掉）的可能。 At least once（最少一次）。每条数据记录至少被处理一次。这个比上一点强的地方在于这里至少保证数据不会丢，至少被处理过，唯一不足之处在于数据可能会被重复处理。 Exactly once（恰好一次）。每条数据记录正好被处理一次。没有数据丢失，也没有重复的数据处理。这一点是3个语义里要求最高的。 基本语义 上一节介绍的语义是比较宽泛意义上的语义，这里我们再细分下里面的语义操作。比如说，我们可以把一个记录处理操作再划分为下面3个子操作： 接收数据的操作。从数据源接收数据的操作。 转换处理数据的操作。在这里面数据会被事先定义好的各种操作语义所处理。 输出数据操作。将处理好后的结果数据输出到外部系统文件系统，或数据集等等。 为什么这里笔者要提到上面涉及到的更细粒度级别的操作呢？其实在流处理过程中，失败现象就可能发生在上面3个步骤中的任何一步。如果要拿最高标准“Exactly Once”标准来看，我们要达到的理想效果应该是： 数据只被处理过一次，这里面可以包括曾经处理失败，然后再读取原始数据进行处理。 对于一个原始数据，我们保证最后结果数据输出是一致的，我们并不是说输出操作只是一次执行的。 所以从这里面我们可以看出，要想达到最高的“Exactly Once”标准，中间的处理操作是最最关键的。因为它会有各种意外情况发生。 “Exactly Once”的重要保证：输入数据源的可依赖性 因为数据在处理过程中有可能会有各种情况发生，所以这里的一个必要前提保证是：原始数据的可依赖性，或者是可访问性。简单的用一句话来说，就是数据处理失败了，我还能够访问到原始输入数，然后再执行处理操作。所以归结为一个本质问题：操作的“Exactly Once”语义问题实质上是输入数据源的可依赖程度。 这里我们将焦点转向输入数据源上，输入数据源可以分为以下2种。 基于文件系统的数据源 数据从文件中读取而来，因为文件本身存在于已经本身具备容错能力的文件系统（比如HDFS），所以我们可以认为这样的数据源是可以支持“Exactly Once”语义的，因为任何的数据处理失败恢复都可以从原始文件系统中进行数据的再次读取。 基于外部接收器的数据源 基于外部接收器的数据源指的是我们的数据从外部系统中（比如说Kafka）读取过来的。像这类情况，同样地，我们要分这些接收数据器是可信赖的还是不可信赖的。 可信赖的接收器。接收器收到数据后并且在多个节点做了replica副本操作，然后确认回复给数据源，之后出现失败情况时数据源就不会重发数据给接收器对象。否则，接收数据源在接收器重启的时候，会重发数据给接收器对象。 不可信赖的接收器。这种接收器不发任何确认回复消息给数据源，这就有数据丢失的可能性了，当worker节点挂掉的时候。 以上就是笔者简单学习总结的内容了，希望对对此感兴趣的同学有帮助。 参考资料[1].http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-semantics]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零基础入门：从0到1学会apache Flink]]></title>
    <url>%2F%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%EF%BC%9A%E4%BB%8E0%E5%88%B01%E5%AD%A6%E4%BC%9Aapache%20Flink.html</url>
    <content type="text"><![CDATA[Flink是如何部署的 Flink 和Spark、Storm区别 Flink特点 Flink Runtime 层的主要架构是什么 Flink Runtime Master 组件有哪些？分别有什么作用 Flink 资源有哪些模式 Flink Session 模式适用什么类型任务 Flink Per-Job 模式适用什么类型任务 Flink Session 模式下任务提交流程是怎样的 Flink Per-Job 模式下任务提交流程是怎样的 Flink 资源管理模式和与作业调度是怎样的 Flink 任务 Slot 选择策略 Flink 中按什么顺序来调度 Task 的 Flink 中资源管理的实现是什么过程 Flink 中JobManager 是如何申请 Task 的资源的？ Flink Task 结束，Slot释放流程是怎样的 Flink Slot结束为什么使用延时释放 Flink 如何解决 Slot可能存在的状态不一致问题 Flink 中 Task 执行错误，如何恢复错误 Flink 中集群的 Master 进行发生异常如何恢复 Flink 后续版本大概会有哪些优化和扩展 Flink Api主要分为几层 Flink 中有哪些时间类型 如何判断 Flink 任务中应该使用哪种时间类型 Flink中 Processing Time 和 Event Time 数据特性有什么不同 Flink 中 WaterMark 作用是什么 Flink 中 WaterMark 生成方式有哪些 Flink 中 WaterMark 生成策略哪些？ Flink WaterMark 处理流程 ProcessFunction 和时间相关的功能有哪些 Flink Table/SQL 如何注册一个 Table ？ Flink 中默认窗口有哪些 Flink 中数据进入窗口流程是怎样的 Flink 窗口触发器是什么 Flink 窗口触发器（Trigger）有哪些状态？ Flink 窗口过滤器（Evictor）作用是什么 Flink 内置窗口过滤器（Evictor）有几种 什么是 Flink State Flink State 根据状态分为几种 如何做好 State 管理 Flink Checkpoint 与 state 的关系 Flink KeyedState 支持哪些类型状态？ Flink OperatorState 支持哪些类型状态？ Flink 中什么是 keyed state Flink 中什么是 operator state Flink 原始状态和托管状态有什么区别 Flink 状态 Statebackend 的分类 Flink HeapKeyedStateBackend 是如何实现的 Flink RocksDBStateBackend 适用场景有哪些 Flink 状态持久化策略有哪些？ Flink Checkpoint 执行流程详解 什么是 FLink 状态重分布 Flink OperatorState 是如何重分布的 Flink KeyedState 是如何重分布的 Savepoint 与 Checkpoint 的区别 Flink 如何实现 Checkpoint 的 EXACTLY_ONCE 语义 Flink DataStream 状态过期是怎样配置的 Flink 状态过期是如何清理的？ Flink 有哪些运行模式 JobManager 功能有哪些 TaskManager 里面的主要组件有什么 Flink Standalone 模式有什么特点 Flink 中一个 Standalone 任务的运行过程是怎样的 Flink 运行时的一些组件有什么 Yarn 集群的组件有哪些 Yarn 交互流程是怎样的 Yarn 中任务提交流程是怎样的 Flink on Yarn 中的 Per Job 模式是指什么 Flink on Yarn 中的 Per Job 模式运行流程是怎样的 Flink on Yarn 中的 Session 模式是什么 Flink on Yarn 中的 Session 模式运行流程是怎样的 Flink on Yarn 的 Session 模式和 Per Job 模式的应用场景有什么区别 Flink on Yarn 有什么优缺点 Kubernetes 中基础组件有哪些 Kubernetes 运行流程是怎样的 Kubernetes 核心概念有哪些 Flink on Kubernetes 任务在 Kubernetes 上运行的步骤是怎样的 Flink on Kubernetes JobManager 的执行过程是怎样的 Flink on Kubernetes 交互流程是怎样的 Flink on Kubernetes 中 k8s 各个组件具体的作用是什么 Flink 在 K8s 上可以通过 Operator 方式提交任务吗？ Flink 在 K8s 集群上如果不使用 Zookeeper 有没有其他高可用（HA）的方案？ Flink on K8s 在任务启动时需要指定 TaskManager 的个数，有和 Yarn 一样的动态资源申请方式吗？ 为什么要为 Flink 量身定制序列化框架 数据分析引擎面对 Java 对象存储密度较低，是怎样解决的 Flink 中数据类型有哪些 Flink 内部是如何表示数据类型的 Flink 中 TypeInformation 有哪些类型 Flink 中 TypeInformation 作用是什么 Flink 的序列化过程是怎样的 Flink 常见的序列化应用场景有哪些 Flink 通信层的序列化是怎样的 在 Flink 通信层的序列化中，何时确定 Function 的输入输出类型 在 Flink 通信层的序列化中，何时确定 Function 的序列化 / 反序列化器 在 Flink 通信层的序列化中，何时进行真正的序列化 / 反序列化操作？ Flink 对于无法序列化的类型是如何处理的 Flink 序列化过程中，进行序列化操作的序列化器从何而来？ Flink Pojo 的序列化与反序列化的类型规则是怎样的 Flink 类型信息系统如何拓展 Flink 任务执行时如何转换的 Flink 任务 Program 到 StreamGraph 是怎样转化 Flink 中 StreamNode 和 StreamEdge 是什么 Flink 任务 StreamGraph 到 JobGraph 是怎样转化 Flink 任务中为什么要为每个 operator 生成 hash 值？ Flink 中每个 operator 是怎样生成 hash 值的？ Flink operator 合并具体条件有哪些 Flink operator 自动生成 hash 值因素有哪些 Flink JobGraph 到 ExexcutionGraph 以及物理执行计划是怎样的 Fink on Yarn 的缺陷具体是什么 Flink Dispatcher 是什么 为什么 Flink 要引入 Dispatcher 为什么需要网络流控 网络流控是怎样实现的 静态网络限速有什么限制 Flink 1.5 以前如何实现 feedback 机制？ Flink 1.5 TCP 流控机制是怎样的 Flink TCP-based 反压机制是怎样的 Flink TCP-based 反压机制弊端有哪些 Flink 是如何实现的流控机制的 Flink Credit-based 反压过程是怎样的 有了动态反压，静态限速是不是完全没有作用了？ Flink Metric 有哪些类型 Flink 中获取 Metrics 有哪些方法 如何定位 Flink 任务运行慢的问题 Metrics 是系统内部的监控，那是否可以作为 Flink 日志分析的输出？ Flink Reporter 是有专门的线程吗？ Flink 有哪些方式可以跟外界进行数据交换？ Flink 预定义 source 和 sink 有哪些 Apache Bahir 中提供了哪些 Flink connecter FlinkKafkaConsumer 中消费起始位置如何设置？ FlinkKafkaConsumer 如何在不重启作业情况下动态感知新扩容的 partition FlinkKafkaConsumer offset 是如何提交的？ FlinkKakfaConsumer Watermark 如何生成 Flink kafka Producer 分区是怎样的 Flink kafka Producer 如何达到 at-least-once 语义 在 Flink consumer 的并行度的设置是对应 topic 的 partitions 个数吗？要是有多个主题数据源，并行度是设置成总体的 partitions 数吗？ 在构造 FlinkKafkaProducer 时, 如果 partitioner 传 null 的时候是 round-robin 发到每一个 partition ？如果有 key 的时候行为是 kafka 那种按照 key 分布到具体分区的行为吗？ 如果 checkpoint 时间过长，offset 未提交到 kafka，此时节点宕机了，重启之后的重复消费如何保证呢？ Flink operator state 以及 keyed state 有什么区别 Flink Operator state 使用有哪些注意事项 Flink Keyed state 使用有哪些注意事项 Flink Checkpoint 使用有哪些注意事项 典型的机器学习工作流程有哪些问题 Flink 如何实现对机器学习的抽象、扩展 Flink TableEnvironment 组成部分有哪些 Flink TableEnvironment 的适用场景有什么 Flink Table API&amp;SQL 解析验证 Flink 为什么选择支持 Python Flink Python Table API 架构是怎样的 Flink PythonVM 和 JavaVM 是怎样通讯的? Flink Python Table API 环境搭建流程是怎样的 Flink Python Table API 作业是如何提交的 容错保证语义有哪些层次 Flink 检查点保留策略有哪些 Flink 作业自动重启策略有哪些 Flink 在作业有变更请求， 有哪些操作可能会导致无法从检查点恢复 Flink 检查点协调器是什么 Flink 中检查点协调器在 Checkpoint 中作用是什么 Flink 检查点信息中有哪些信息 为什么 Flink 使用轻量级分布式快照 Flink 分布式快照如何切分数据流 Flink 算子接收到 Barrier 会如何处理 Flink Checkpoint 时，为什么要对齐 Barrier Flink Checkpoint 时，Barrier 是如何对齐的 Flink Checkpoint JobMaster 执行需要做哪些检查 Flink Checkpoint TaskExecutor 需要做哪些工作 Flink Source/Sink 如何才能支持端到端严格一次 Flink 分布式多个并发执行中如何实现端到端严格一次 Flink 两阶段提交流程是怎样的 Flink 为什么选择使用自主的内存管理 Flink 中 Java 对象内存中是如何存储的 Flink 1.10 以前的内存模型有哪些缺陷 Flink TaskManager 内存模型是怎样的 Flink TaskManager Flink本身使用的内存是如何分布的 Flink TaskManager JVM 使用的内存是如何分布的 Flink 中各个内存区块大小是在什么地方计算 Flink 中各个内存区块大小是如何计算的 Flink 内存结构是如何划分的 Flink 中内存段是什么 Flink 中内存页是什么 Flink 中内存段的关键属性有哪些 字节顺序 Little Endian 和 Big Endian 有什么区别 Flink 内存页是如何使用的 Flink 中流任务和批任务内存申请是如何进行的 Flink 中内存什么时候会释放 Flink 中内存是如何释放的 Flink 中调度器是什么 Flink 中调度器有哪些能力 Flink 中有哪些调度器 Flink 作业有哪些生命周期状态 《零基础入门：从0到1学会Apache Flink》下载地址：https://developer.aliyun.com/topic/download?id=35]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 零基础入门（三）：DataStream API 编程]]></title>
    <url>%2FApache%20Flink%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9ADataStream%20API%20%E7%BC%96%E7%A8%8B.html</url>
    <content type="text"><![CDATA[前面已经为大家介绍了 Flink 的基本概念以及安装部署的过程，从而希望能够帮助读者建立起对 Flink 的初步印象。本次课程开始，我们将进入第二部分，即 Flink 实际开发的相关内容。本次课程将首先介绍 Flink 开发中比较核心的 DataStream API 。我们首先将回顾分布式流处理的一些基本概念，这些概念对于理解实际的 DataStream API 有非常大的作用。然后，我们将详细介绍 DataStream API 的设计，最后我们将通过一个例子来演示 DataStream API 的使用。 1. 流处理基本概念对于什么是流处理，从不同的角度有不同的定义。其实流处理与批处理这两个概念是对立统一的，它们的关系有点类似于对于 Java 中的 ArrayList 中的元素，是直接看作一个有限数据集并用下标去访问，还是用迭代器去访问。 图1. 左图硬币分类器。硬币分类器也可以看作一个流处理系统，用于硬币分类的各部分组件提前串联在一起，硬币不断进入系统，并最终被输出到不同的队列中供后续使用。右图同理。 流处理系统本身有很多自己的特点。一般来说，由于需要支持无限数据集的处理，流处理系统一般采用一种数据驱动的处理方式。它会提前设置一些算子，然后等到数据到达后对数据进行处理。为了表达复杂的计算逻辑，包括 Flink 在内的分布式流处理引擎一般采用 DAG 图来表示整个计算逻辑，其中 DAG 图中的每一个点就代表一个基本的逻辑单元，也就是前面说的算子。由于计算逻辑被组织成有向图，数据会按照边的方向，从一些特殊的 Source 节点流入系统，然后通过网络传输、本地传输等不同的数据传输方式在算子之间进行发送和处理，最后会通过另外一些特殊的 Sink 节点将计算结果发送到某个外部系统或数据库中。 图2. 一个 DAG 计算逻辑图与实际的物理时模型。逻辑图中的每个算子在物理图中可能有多个并发。 对于实际的分布式流处理引擎，它们的实际运行时物理模型要更复杂一些，这是由于每个算子都可能有多个实例。如图 2 所示，作为 Source 的 A 算子有两个实例，中间算子 C 也有两个实例。在逻辑模型中，A 和 B 是 C 的上游节点，而在对应的物理逻辑中，C 的所有实例和 A、B 的所有实例之间可能都存在数据交换。在物理模型中，我们会根据计算逻辑，采用系统自动优化或人为指定的方式将计算工作分布到不同的实例中。只有当算子实例分布到不同进程上时，才需要通过网络进行数据传输，而同一进程中的多个实例之间的数据传输通常是不需要通过网络的。 表1. Apache Storm 构造 DAG 计算图。Apache Storm 的接口定义更加“面向操作”，因此更加底层。 12345TopologyBuilder builder = new TopologyBuilder();builder.setSpout(&quot;spout&quot;, new RandomSentenceSpout(), 5);builder.setBolt(&quot;split&quot;, new SplitSentence(), 8).shuffleGrouping(&quot;spout&quot;);builder.setBolt(&quot;count&quot;, new WordCount(), 12).fieldsGrouping(&quot;split&quot;, new Fields(&quot;word&quot;)); 表2. Apache Flink 构造 DAG 计算图。Apache Flink 的接口定义更加“面向数据”，因此更加高层。 12345StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;String&gt; text = env.readTextFile (&quot;input&quot;);DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = text.flatMap(new Tokenizer()).keyBy(0).sum(1);counts.writeAsText(&quot;output&quot;); 由于流处理的计算逻辑是通过 DAG 图来表示的，因此它们的大部分 API 都是围绕构建这种计算逻辑图来设计的。例如，对于几年前非常流行的 Apache Storm，它的 Word Count 的示例如表 1 所示。基于 Apache Storm 用户需要在图中添加 Spout 或 Bolt 这种算子，并指定算子之前的连接方式。这样，在完成整个图的构建之后，就可以将图提交到远程或本地集群运行。 与之对比，Apache Flink 的接口虽然也是在构建计算逻辑图，但是 Flink 的 API 定义更加面向数据本身的处理逻辑，它把数据流抽象成为一个无限集，然后定义了一组集合上的操作，然后在底层自动构建相应的 DAG 图。可以看出，Flink 的 API 要更“上层”一些。许多研究者在进行实验时，可能会更喜欢自由度高的 Storm，因为它更容易保证实现预想的图结构；而在工业界则更喜欢 Flink 这类高级 API，因为它使用更加简单。 2. Flink DataStream API 概览基于前面对流处理的基本概念，本节将详细介绍 Flink DataStream API 的使用方式。我们首先还是从一个简单的例子开始看起。表3是一个流式 Word Count 的示例，虽然它只有 5 行代码，但是它给出了基于 Flink DataStream API 开发程序的基本结构。 表3. 基于 Flink DataStream API 的 Word Count 示例. 12345678910//1、设置运行环境StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();//2、配置数据源读取数据DataStream&lt;String&gt; text = env.readTextFile (&quot;input&quot;);//3、进行一系列转换DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = text.flatMap(new Tokenizer()).keyBy(0).sum(1);//4、配置数据汇写出数据counts.writeAsText(&quot;output&quot;);//5、提交执行env.execute(&quot;Streaming WordCount&quot;); 为了实现流式 Word Count，我们首先要先获得一个 StreamExecutionEnvironment 对象。它是我们构建图过程中的上下文对象。基于这个对象，我们可以添加一些算子。对于流处理程度，我们一般需要首先创建一个数据源去接入数据。在这个例子中，我们使用了 Environment 对象中内置的读取文件的数据源。这一步之后，我们拿到的是一个 DataStream 对象，它可以看作一个无限的数据集，可以在该集合上进行一序列的操作。例如，在 Word Count 例子中，我们首先将每一条记录（即文件中的一行）分隔为单词，这是通过 FlatMap 操作来实现的。调用 FlatMap 将会在底层的 DAG 图中添加一个 FlatMap 算子。然后，我们得到了一个记录是单词的流。我们将流中的单词进行分组（keyBy），然后累积计算每一个单词的数据（sum(1)）。计算出的单词的数据组成了一个新的流，我们将它写入到输出文件中。 最后，我们需要调用 env#execute 方法来开始程序的执行。需要强调的是，前面我们调用的所有方法，都不是在实际处理数据，而是在构通表达计算逻辑的 DAG 图。只有当我们将整个图构建完成并显式的调用 Execute 方法后，框架才会把计算图提供到集群中，接入数据并执行实际的逻辑。 基于流式 Word Count 的例子可以看出，基于 Flink 的 DataStream API 来编写流处理程序一般需要三步：通过 Source 接入数据、进行一系统列的处理以及将数据写出。最后，不要忘记显式调用 Execute 方式，否则前面编写的逻辑并不会真正执行。 图3. Flink DataStream 操作概览 从上面的例子中还可以看出，Flink DataStream API 的核心，就是代表流数据的 DataStream 对象。整个计算逻辑图的构建就是围绕调用 DataStream 对象上的不同操作产生新的 DataStream 对象展开的。整体来说，DataStream 上的操作可以分为四类。第一类是对于单条记录的操作，比如筛除掉不符合要求的记录（Filter 操作），或者将每条记录都做一个转换（Map 操作）。第二类是对多条记录的操作。比如说统计一个小时内的订单总成交量，就需要将一个小时内的所有订单记录的成交量加到一起。为了支持这种类型的操作，就得通过 Window 将需要的记录关联到一起进行处理。第三类是对多个流进行操作并转换为单个流。例如，多个流可以通过 Union、Join 或 Connect 等操作合到一起。这些操作合并的逻辑不同，但是它们最终都会产生了一个新的统一的流，从而可以进行一些跨流的操作。最后， DataStream 还支持与合并对称的操作，即把一个流按一定规则拆分为多个流（Split 操作），每个流是之前流的一个子集，这样我们就可以对不同的流作不同的处理。 图4. 不同类型的 DataStream 子类型。不同的子类型支持不同的操作集合。 为了支持这些不同的流操作，Flink 引入了一组不同的流类型，用来表示某些操作的中间流数据集类型。完整的类型转换关系如图4所示。首先，对于一些针对单条记录的操作，如 Map 等，操作的结果仍然是是基本的 DataStream 类型。然后，对于 Split 操作，它会首先产生一个 SplitStream，基于 SplitStream 可以使用 Select 方法来筛选出符合要求的记录并再将得到一个基本的流。 类似的，对于 Connect 操作，在调用 streamA.connect(streamB)后可以得到一个专门的 ConnectedStream。ConnectedStream 支持的操作与普通的 DataStream 有所区别，由于它代表两个不同的流混合的结果，因此它允许用户对两个流中的记录分别指定不同的处理逻辑，然后它们的处理结果形成一个新的 DataStream 流。由于不同记录的处理是在同一个算子中进行的，因此它们在处理时可以方便的共享一些状态信息。上层的一些 Join 操作，在底层也是需要依赖于 Connect 操作来实现的。 另外，如前所述，我们可以通过 Window 操作对流可以按时间或者个数进行一些切分，从而将流切分成一个个较小的分组。具体的切分逻辑可以由用户进行选择。当一个分组中所有记录都到达后，用户可以拿到该分组中的所有记录，从而可以进行一些遍历或者累加操作。这样，对每个分组的处理都可以得到一组输出数据，这些输出数据形成了一个新的基本流。 对于普通的 DataStream，我们必须使用 allWindow 操作，它代表对整个流进行统一的 Window 处理，因此是不能使用多个算子实例进行同时计算的。针对这一问题，就需要我们首先使用 KeyBy 方法对记录按 Key 进行分组，然后才可以并行的对不同 Key 对应的记录进行单独的 Window 操作。KeyBy 操作是我们日常编程中最重要的操作之一，下面我们会更详细的介绍。 图5. 基本流上的 Window 操作与 KeyedStream 上的 Window 操对比。KeyedStream 上的 Window 操作使采用多个实例并发处理成为了可能。 基本 DataStream 对象上的 allWindow 与 KeyedStream 上的 Window 操作的对比如图5所示。为了能够在多个并发实例上并行的对数据进行处理，我们需要通过 KeyBy 将数据进行分组。KeyBy 和 Window 操作都是对数据进行分组，但是 KeyBy 是在水平分向对流进行切分，而 Window 是在垂直方式对流进行切分。 使用 KeyBy 进行数据切分之后，后续算子的每一个实例可以只处理特定 Key 集合对应的数据。除了处理本身外，Flink 中允许算子维护一部分状态（State），在KeyedStream 算子的状态也是可以分布式存储的。由于 KeyBy 是一种确定的数据分配方式（下文将介绍其它分配方式），因此即使发生 Failover 作业重启，甚至发生了并发度的改变，Flink 都可以重新分配 Key 分组并保证处理某个 Key 的分组一定包含该 Key 的状态，从而保证一致性。 最后需要强调的是，KeyBy 操作只有当 Key 的数量超过算子的并发实例数才可以较好的工作。由于同一个 Key 对应的所有数据都会发送到同一个实例上，因此如果Key 的数量比实例数量少时，就会导致部分实例收不到数据，从而导致计算能力不能充分发挥。 3. 其它问题除 KeyBy 之外，Flink 在算子之前交换数据时还支持其它的物理分组方式。如图 1 所示，Flink DataStream 中物理分组方式包括： Global: 上游算子将所有记录发送给下游算子的第一个实例。 Broadcast: 上游算子将每一条记录发送给下游算子的所有实例。 Forward：只适用于上游算子实例数与下游算子相同时，每个上游算子实例将记录发送给下游算子对应的实例。 Shuffle：上游算子对每条记录随机选择一个下游算子进行发送。 Rebalance：上游算子通过轮询的方式发送数据。 Rescale：当上游和下游算子的实例数为 n 或 m 时，如果 n &lt; m，则每个上游实例向ceil(m/n)或floor(m/n)个下游实例轮询发送数据；如果 n &gt; m，则 floor(n/m) 或 ceil(n/m) 个上游实例向下游实例轮询发送数据。 PartitionCustomer：当上述内置分配方式不满足需求时，用户还可以选择自定义分组方式。 图6. 除keyBy外其它的物理分组方式。 除分组方式外，Flink DataStream API 中另一个重要概念就是类型系统。图 7 所示，Flink DataStream 对像都是强类型的，每一个 DataStream 对象都需要指定元素的类型，Flink 自己底层的序列化机制正是依赖于这些信息对序列化等进行优化。具体来说，在 Flink 底层，它是使用 TypeInformation 对象对类型进行描述的，TypeInformation 对象定义了一组类型相关的信息供序列化框架使用。 图7. Flink DataStream API 中的类型系统 Flink 内置了一部分常用的基本类型，对于这些类型，Flink 也内置了它们的TypeInformation，用户一般可以直接使用而不需要额外的声明，Flink 自己可以通过类型推断机制识别出相应的类型。但是也会有一些例外的情况，比如，Flink DataStream API 同时支持 Java 和 Scala，Scala API 许多接口是通过隐式的参数来传递类型信息的，所以如果需要通过 Java 调用 Scala 的 API，则需要把这些类型信息通过隐式参数传递过去。另一个例子是 Java 中对泛型存在类型擦除，如果流的类型本身是一个泛型的话，则可能在擦除之后无法推断出类型信息，这时候也需要显式的指定。 在 Flink 中，一般 Java 接口采用 Tuple 类型来组合多个字段，而 Scala 则更经常使用 Row 类型或 Case Class。相对于 Row，Tuple 类型存在两个问题，一个是字段个数不能超过 25 个，此外，所有字段不允许有 null 值。最后，Flink 也支持用户自定义新的类型和 TypeInformation，并通过 Kryo 来实现序列化，但是这种方式可带来一些迁移等方面的问题，所以尽量不要使用自定义的类型。 4.示例然后，我们再看一个更复杂的例子。假设我们有一个数据源，它监控系统中订单的情况，当有新订单时，它使用 Tuple2 输出订单中商品的类型和交易额。然后，我们希望实时统计每个类别的交易额，以及实时统计全部类别的交易额。 表4. 实时订单统计示例。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class GroupedProcessingTimeWindowSample &#123; private static class DataSource extends RichParallelSourceFunction&lt;Tuple2&lt;String, Integer&gt;&gt; &#123; private volatile boolean isRunning = true; @Override public void run(SourceContext&lt;Tuple2&lt;String, Integer&gt;&gt; ctx) throws Exception &#123; Random random = new Random(); while (isRunning) &#123; Thread.sleep((getRuntimeContext().getIndexOfThisSubtask() + 1) * 1000 * 5); String key = &quot;类别&quot; + (char) (&apos;A&apos; + random.nextInt(3)); int value = random.nextInt(10) + 1; System.out.println(String.format(&quot;Emits\t(%s, %d)&quot;, key, value)); ctx.collect(new Tuple2&lt;&gt;(key, value)); &#125; &#125; @Override public void cancel() &#123; isRunning = false; &#125; &#125; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(2); DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; ds = env.addSource(new DataSource()); KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = ds.keyBy(0); keyedStream.sum(1).keyBy(new KeySelector&lt;Tuple2&lt;String, Integer&gt;, Object&gt;() &#123; @Override public Object getKey(Tuple2&lt;String, Integer&gt; stringIntegerTuple2) throws Exception &#123; return &quot;&quot;; &#125; &#125;).fold(new HashMap&lt;String, Integer&gt;(), new FoldFunction&lt;Tuple2&lt;String, Integer&gt;, HashMap&lt;String, Integer&gt;&gt;() &#123; @Override public HashMap&lt;String, Integer&gt; fold(HashMap&lt;String, Integer&gt; accumulator, Tuple2&lt;String, Integer&gt; value) throws Exception &#123; accumulator.put(value.f0, value.f1); return accumulator; &#125; &#125;).addSink(new SinkFunction&lt;HashMap&lt;String, Integer&gt;&gt;() &#123; @Override public void invoke(HashMap&lt;String, Integer&gt; value, Context context) throws Exception &#123; // 每个类型的商品成交量 System.out.println(value); // 商品成交总量 System.out.println(value.values().stream().mapToInt(v -&gt; v).sum()); &#125; &#125;); env.execute(); &#125;&#125; 示例的实现如表4所示。首先，在该实现中，我们首先实现了一个模拟的数据源，它继承自 RichParallelSourceFunction，它是可以有多个实例的 SourceFunction 的接口。它有两个方法需要实现，一个是 Run 方法，Flink 在运行时对 Source 会直接调用该方法，该方法需要不断的输出数据，从而形成初始的流。在 Run 方法的实现中，我们随机的产生商品类别和交易量的记录，然后通过 ctx#collect 方法进行发送。另一个方法是 Cancel 方法，当 Flink 需要 Cancel Source Task 的时候会调用该方法，我们使用一个 Volatile 类型的变量来标记和控制执行的状态。 然后，我们在 Main 方法中就可以开始图的构建。我们首先创建了一个 StreamExecutioniEnviroment 对象。创建对象调用的 getExecutionEnvironment 方法会自动判断所处的环境，从而创建合适的对象。例如，如果我们在 IDE 中直接右键运行，则会创建 LocalStreamExecutionEnvironment 对象；如果是在一个实际的环境中，则会创建 RemoteStreamExecutionEnvironment 对象。 基于 Environment 对象，我们首先创建了一个 Source，从而得到初始的&lt;商品类型，成交量&gt;流。然后，为了统计每种类别的成交量，我们使用 KeyBy 按 Tuple 的第 1 个字段（即商品类型）对输入流进行分组，并对每一个 Key 对应的记录的第 2 个字段（即成交量）进行求合。在底层，Sum 算子内部会使用 State 来维护每个Key（即商品类型）对应的成交量之和。当有新记录到达时，Sum 算子内部会更新所维护的成交量之和，并输出一条&lt;商品类型，更新后的成交量&gt;记录。 如果只统计各个类型的成交量，则程序可以到此为止，我们可以直接在 Sum 后添加一个 Sink 算子对不断更新的各类型成交量进行输出。但是，我们还需要统计所有类型的总成交量。为了做到这一点，我们需要将所有记录输出到同一个计算节点的实例上。我们可以通过 KeyBy 并且对所有记录返回同一个 Key，将所有记录分到同一个组中，从而可以全部发送到同一个实例上。 然后，我们使用 Fold 方法来在算子中维护每种类型商品的成交量。注意虽然目前 Fold 方法已经被标记为 Deprecated，但是在 DataStream API 中暂时还没有能替代它的其它操作，所以我们仍然使用 Fold 方法。这一方法接收一个初始值，然后当后续流中每条记录到达的时候，算子会调用所传递的 FoldFunction 对初始值进行更新，并发送更新后的值。我们使用一个 HashMap 来对各个类别的当前成交量进行维护，当有一条新的&lt;商品类别，成交量&gt;到达时，我们就更新该 HashMap。这样在 Sink 中，我们收到的是最新的商品类别和成交量的 HashMap，我们可以依赖这个值来输出各个商品的成交量和总的成交量。 需要指出的是，这个例子主要是用来演示 DataStream API 的用法，实际上还会有更高效的写法，此外，更上层的 Table / SQL 还支持 Retraction 机制，可以更好的处理这种情况。 图8. API 原理图 最后，我们对 DataStream API 的原理进行简要的介绍。当我们调用 DataStream#map 算法时，Flink 在底层会创建一个 Transformation 对象，这一对象就代表我们计算逻辑图中的节点。它其中就记录了我们传入的 MapFunction，也就是 UDF（User Define Function）。随着我们调用更多的方法，我们创建了更多的 DataStream 对象，每个对象在内部都有一个 Transformation 对象，这些对象根据计算依赖关系组成一个图结构，就是我们的计算图。后续 Flink 将对这个图结构进行进一步的转换，从而最终生成提交作业所需要的 JobGraph。 5. 总结本文主要介绍了 Flink DataStream API，它是当前 Flink 中比较底层的一套 API。在实际的开发中，基于该 API 需要用户自己处理 State 与 Time 等一些概念，因此需要较大的工作量。后续课程还会介绍更上层的 Table / SQL 层的 API，未来 Table / SQL 可能会成为 Flink 主流的 API，但是对于接口来说，越底层的接口表达能力越强，在一些需要精细操作的情况下，仍然需要依赖于 DataStream API。 原文地址]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 零基础入门（二）：开发环境搭建和应用的配置、部署及运行]]></title>
    <url>%2FApache%20Flink%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E5%92%8C%E5%BA%94%E7%94%A8%E7%9A%84%E9%85%8D%E7%BD%AE%E3%80%81%E9%83%A8%E7%BD%B2%E5%8F%8A%E8%BF%90%E8%A1%8C.html</url>
    <content type="text"><![CDATA[前言本文主要面向于初次接触 Flink、或者对 Flink 有了解但是没有实际操作过的同学。希望帮助大家更顺利地上手使用 Flink，并着手相关开发调试工作。 课程内容包括： Flink 开发环境的部署和配置 运行 Flink 应用（包括：单机 Standalone 模式、多机 Standalone 模式和 Yarn 集群模式） 一、Flink 开发环境部署和配置Flink 是一个以 Java 及 Scala 作为开发语言的开源大数据项目，代码开源在 GitHub 上，并使用 Maven 来编译和构建项目。对于大部分使用 Flink 的同学来说，Java、Maven 和 Git 这三个工具是必不可少的，另外一个强大的 IDE 有助于我们更快的阅读代码、开发新功能以及修复 Bug。因为篇幅所限，我们不会详述每个工具的安装细节，但会给出必要的安装建议。 关于开发测试环境，Mac OS、Linux 系统或者 Windows 都可以。如果使用的是 Windows 10 系统，建议使用 Windows 10 系统的 Linux 子系统来编译和运行。 工具 注释 Java Java 版本至少是Java 8，且最好选用 Java 8u51 及以上版本 Maven 必须使用 Maven 3，建议使用 Maven 3.2.5。Maven 3.3.x 能够编译成功，但是在 Shade 一些 Dependencies 的过程中有些问题 Git Flink 的代码仓库是： https://github.com/apache/flink 建议选用社区已发布的稳定分支，比如 Release-1.6 或者 Release-1.7。 1. 编译 Flink 代码在我们配置好之前的几个工具后，编译 Flink 就非常简单了，执行如下命令即可： 123mvn clean install -DskipTests# 或者mvn clean package -DskipTests 常用编译参数： 123-Dfast 主要是忽略QA plugins和JavaDocs的编译-Dhadoop.version=2.6.1 指定hadoop版本--settings=$&#123;maven_file_path&#125; 显式指定maven settings.xml配置文件 当成功编译完成后，能在当前 Flink 代码目录下的 flink-dist/target/子目录 中看到如下文件（不同的 Flink 代码分支编译出的版本号不同，这里的版本号是 Flink 1.5.1）： 其中有三个文件可以留意一下： 版本 注释 flink-1.5.1.tar.gz Binary 的压缩包 flink-1.5.1-bin/flink-1.5.1 解压后的 Flink binary 目录 flink-dist_2.11-1.5.1.jar 包含 Flink 核心功能的 jar 包 注意： 国内用户在编译时可能遇到编译失败“Build Failure”（且有 MapR 相关报错），一般都和 MapR 相关依赖的下载失败有关，即使使用了推荐的 settings.xml 配置（其中 Aliyun Maven 源专门为 MapR 相关依赖做了代理），还是可能出现下载失败的情况。问题主要和 MapR 的 Jar 包比较大有关。遇到这些问题时，重试即可。在重试之前，要先根据失败信息删除 Maven local repository 中对应的目录，否则需要等待 Maven 下载的超时时间才能再次出发下载依赖到本地。 2. 开发环境准备推荐使用 IntelliJ IDEA IDE 作为 Flink 的 IDE 工具。官方不建议使用 Eclipse IDE，主要原因是 Eclipse 的 Scala IDE 和 Flink 用 Scala 的不兼容。 如果你需要做一些 Flink 代码的开发工作，则需要根据 Flink 代码的 tools/maven/目录 下的配置文件来配置 Checkstyle ，因为 Flink 在编译时会强制代码风格的检查，如果代码风格不符合规范，可能会直接编译失败。 二、运行 Flink 应用1. 基本概念运行 Flink 应用其实非常简单，但是在运行 Flink 应用之前，还是有必要了解 Flink 运行时的各个组件，因为这涉及到 Flink 应用的配置问题。图 1 所示，这是用户用 DataStream API 写的一个数据处理程序。可以看到，在一个 DAG 图中不能被 Chain 在一起的 Operator 会被分隔到不同的 Task 中，也就是说 Task 是 Flink 中资源调度的最小单位。 图 1 Parallel Dataflows 图 2 所示，Flink 实际运行时包括两类进程： JobManager（又称为 JobMaster）：协调 Task 的分布式执行，包括调度 Task、协调创 Checkpoint 以及当 Job failover 时协调各个 Task 从 Checkpoint 恢复等。 TaskManager（又称为 Worker）：执行 Dataflow 中的 Tasks，包括内存 Buffer 的分配、Data Stream 的传递等。 图 2 Flink Runtime 架构图 图 3 所示，Task Slot 是一个 TaskManager 中的最小资源分配单位，一个 TaskManager 中有多少个 Task Slot 就意味着能支持多少并发的 Task 处理。需要注意的是，一个 Task Slot 中可以执行多个 Operator，一般这些 Operator 是能被 Chain 在一起处理的。 图 3 Process 2. 运行环境准备 准备 Flink binary直接从 Flink 官网上下载 Flink binary 的压缩包 或者从 Flink 源码编译而来 安装 Java，并配置 JAVA_HOME 环境变量 3. 单机 Standalone 的方式运行 Flink（1）基本的启动流程最简单的运行 Flink 应用的方法就是以单机 Standalone 的方式运行。 启动集群： 1./bin/start-cluster.sh 打开 http://127.0.0.1:8081/ 就能看到 Flink 的 Web 界面。尝试提交 Word Count 任务： 1./bin/flink run examples/streaming/WordCount.jar 大家可以自行探索 Web 界面中展示的信息，比如，我们可以看看 TaskManager 的 stdout 日志，就可以看到 Word Count 的计算结果。 我们还可以尝试通过“–input”参数指定我们自己的本地文件作为输入，然后执行： 1./bin/flink run examples/streaming/WordCount.jar --input $&#123;your_source_file&#125; 停止集群： 1./bin/stop-cluster.sh （2）常用配置介绍 conf / slaves conf / slaves 用于配置 TaskManager 的部署，默认配置下只会启动一个 TaskManager 进程，如果想增加一个 TaskManager 进程的，只需要文件中追加一行“localhost”。 也可以直接通过“ ./bin/taskmanager.sh start ”这个命令来追加一个新的 TaskManager： 1./bin/taskmanager.sh start|start-foreground|stop|stop-all conf/flink-conf.yaml conf/flink-conf.yaml 用于配置 JM 和 TM 的运行参数，常用配置有： 1234567891011# The heap size for the JobManager JVMjobmanager.heap.mb: 1024# The heap size for the TaskManager JVMtaskmanager.heap.mb: 1024# The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.taskmanager.numberOfTaskSlots: 4# the managed memory size for each task manager.taskmanager.managed.memory.size: 256 Standalone 集群启动后，我们可以尝试分析一下 Flink 相关进程的运行情况。执行 jps 命令，可以看到 Flink 相关的进程主要有两个，一个是 JobManager 进程，另一个是 TaskManager 进程。我们可以进一步用 ps 命令看看进程的启动参数中“-Xmx”和“-Xms”的配置。然后我们可以尝试修改 flink-conf.yaml 中若干配置，然后重启 Standalone 集群看看发生了什么变化。 需要补充的是，在 Blink 开源分支上，TaskManager 的内存计算上相对于现在的社区版本要更精细化，TaskManager 进程的堆内存限制（-Xmx）一般的计算方法是： 1TotalHeapMemory = taskmanager.heap.mb + taskmanager.managed.memory.size + taskmanager.process.heap.memory.mb（默认值为128MB） 而最新的 Flink 社区版本 Release-1.7 中 JobManager 和 TaskManager 默认内存配置方式为： 12345# The heap size for the JobManager JVMjobmanager.heap.size: 1024m# The heap size for the TaskManager JVMtaskmanager.heap.size: 1024m Flink 社区 Release-1.7 版本中的“taskmanager.heap.size”配置实际上指的不是 Java heap 的内存限制，而是 TaskManager 进程总的内存限制。我们可以同样用上述方法查看 Release-1.7 版本的 Flink binary 启动的 TaskManager 进程的 -Xmx 配置，会发现实际进程上的 -Xmx 要小于配置的“taskmanager.heap.size”的值，原因在于从中扣除了 Network buffer 用的内存，因为 Network buffer 用的内存一定是 Direct memory，所以不应该算在堆内存限制中。 （3）日志的查看和配置JobManager 和 TaskManager 的启动日志可以在 Flink binary 目录下的 Log 子目录中找到。Log 目录中以“flink-${user}-standalonesession-${id}-${hostname}”为前缀的文件对应的是 JobManager 的输出，其中有三个文件： flink-${user}-standalonesession-${id}-${hostname}.log：代码中的日志输出 flink-${user}-standalonesession-${id}-${hostname}.out：进程执行时的stdout输出 flink-${user}-standalonesession-${id}-${hostname}-gc.log：JVM的GC的日志 Log 目录中以“flink-${user}-taskexecutor-${id}-${hostname}”为前缀的文件对应的是 TaskManager 的输出，也包括三个文件，和 JobManager 的输出一致。 日志的配置文件在 Flink binary 目录的 conf 子目录下，其中： log4j-cli.properties：用 Flink 命令行时用的 log 配置，比如执行“ flink run”命令 log4j-yarn-session.properties：用 yarn-session.sh 启动时命令行执行时用的 log 配置 log4j.properties：无论是 Standalone 还是 Yarn 模式，JobManager 和 TaskManager 上用的 log 配置都是 log4j.properties 这三个“log4j.properties”文件分别有三个“logback.xml”文件与之对应，如果想使用 Logback 的同学，之需要把与之对应的“log4j.*properties”文件删掉即可，对应关系如下： log4j-cli.properties -&gt; logback-console.xml log4j-yarn-session.properties -&gt; logback-yarn.xml log4j.properties -&gt; logback.xml 需要注意的是，“flink-${user}-standalonesession-${id}-${hostname}”和“flink-${user}-taskexecutor-${id}-${hostname}”都带有“${id}”，“${id}”表示本进程在本机上该角色（JobManager 或 TaskManager）的所有进程中的启动顺序，默认从 0 开始。 （4）进一步探索尝试重复执行“./bin/start-cluster.sh”命令，然后看看 Web 页面（或者执行jps命令），看看会发生什么？可以尝试看看启动脚本，分析一下原因。接着可以重复执行“./bin/stop-cluster.sh”，每次执行完后，看看会发生什么。 4. 多机部署 Flink Standalone 集群部署前要注意的要点： 每台机器上配置好 Java 以及 JAVA_HOME 环境变量 每台机器上部署的 Flink binary 的目录要保证是同一个目录 如果需要用 HDFS，需要配置 HADOOP_CONF_DIR 环境变量配置 根据你的集群信息修改 conf/masters 和 conf/slaves 配置。 修改 conf/flink-conf.yaml 配置，注意要确保和 Masters 文件中的地址一致： 1jobmanager.rpc.address: z05f06378.sqa.zth.tbsite.net 确保所有机器的 Flink binary 目录中 conf 中的配置文件相同，特别是以下三个： 123conf/mastersconf/slavesconf/flink-conf.yaml 然后启动 Flink 集群： 1./bin/start-cluster.sh 提交 WordCount 作业： 1./bin/flink run examples/streaming/WordCount.jar 上传 WordCount 的 Input 文件： 1hdfs dfs -copyFromLocal story /test_dir/input_dir/story 提交读写 HDFS 的 WordCount 作业： 1./bin/flink run examples/streaming/WordCount.jar --input hdfs:///test_dir/input_dir/story --output hdfs:///test_dir/output_dir/output 增加 WordCount 作业的并发度（注意输出文件重名会提交失败）： 1./bin/flink run examples/streaming/WordCount.jar --input hdfs:///test_dir/input_dir/story --output hdfs:///test_dir/output_dir/output --parallelism 20 5. Standalone 模式的 HighAvailability（HA）部署和配置通过图 2 Flink Runtime 架构图，我们可以看到 JobManager 是整个系统中最可能导致系统不可用的角色。如果一个 TaskManager 挂了，在资源足够的情况下，只需要把相关 Task 调度到其他空闲 TaskSlot 上，然后 Job 从 Checkpoint 中恢复即可。而如果当前集群中只配置了一个 JobManager，则一旦 JobManager 挂了，就必须等待这个 JobManager 重新恢复，如果恢复时间过长，就可能导致整个 Job 失败。 因此如果在生产业务使用 Standalone 模式，则需要部署配置 HighAvailability，这样同时可以有多个 JobManager 待命，从而使得 JobManager 能够持续服务。 图 4 Flink JobManager HA 示意图 注意： 如果想使用 Flink standalone HA 模式，需要确保基于 Flink Release-1.6.1 及以上版本，因为这里社区有个 bug 会导致这个模式下主 JobManager 不能正常工作。 接下来的实验中需要用到 HDFS，所以需要下载带有 Hadoop 支持的 Flink Binary 包。 （1）（可选）使用 Flink 自带的脚本部署 ZKFlink 目前支持基于 Zookeeper 的 HA。如果你的集群中没有部署 ZK，Flink 提供了启动 Zookeeper 集群的脚本。首先修改配置文件“conf/zoo.cfg”，根据你要部署的 Zookeeper Server 的机器数来配置“server.X=addressX:peerPort:leaderPort”，其中“X”是一个 Zookeeper Server的唯一 ID，且必须是数字。 123456# The port at which the clients will connectclientPort=3181server.1=z05f06378.sqa.zth.tbsite.net:4888:5888server.2=z05c19426.sqa.zth.tbsite.net:4888:5888server.3=z05f10219.sqa.zth.tbsite.net:4888:5888 然后启动 Zookeeper： 1./bin/start-zookeeper-quorum.sh jps 命令看到 ZK 进程已经启动： 停掉 Zookeeper 集群的命令： 1./bin/stop-zookeeper-quorum.sh （2）修改 Flink Standalone 集群的配置修改 conf/masters 文件，增加一个 JobManager： 123$cat conf/mastersz05f06378.sqa.zth.tbsite.net:8081z05c19426.sqa.zth.tbsite.net:8081 之前修改过的 conf/slaves 文件保持不变： 1234$cat conf/slavesz05f06378.sqa.zth.tbsite.netz05c19426.sqa.zth.tbsite.netz05f10219.sqa.zth.tbsite.net 修改 conf/flink-conf.yaml 文件： 1234567891011121314# 配置high-availability modehigh-availability: zookeeper# 配置zookeeper quorum（hostname和端口需要依据对应zk的实际配置）high-availability.zookeeper.quorum: z05f02321.sqa.zth.tbsite.net:2181,z05f10215.sqa.zth.tbsite.net:2181# （可选）设置zookeeper的root目录high-availability.zookeeper.path.root: /test_dir/test_standalone2_root# （可选）相当于是这个standalone集群中创建的zk node的namespacehigh-availability.cluster-id: /test_dir/test_standalone2# JobManager的meta信息放在dfs，在zk上主要会保存一个指向dfs路径的指针high-availability.storageDir: hdfs:///test_dir/recovery2/ 需要注意的是，在 HA 模式下 conf/flink-conf.yaml 中的这两个配置都失效了（想想为什么）。 12jobmanager.rpc.addressjobmanager.rpc.port 修改完成后，确保配置同步到其他机器。 启动 Zookeeper 集群： 1./bin/start-zookeeper-quorum.sh 再启动 Standalone 集群（要确保之前的 Standalone 集群已经停掉）： 1./bin/start-cluster.sh 分别打开两个 Master 节点上的 JobManager Web 页面： http://z05f06378.sqa.zth.tbsite.net:8081http://z05c19426.sqa.zth.tbsite.net:8081 可以看到两个页面最后都转到了同一个地址上，这个地址就是当前主 JobManager 所在机器，另一个就是 Standby JobManager。以上我们就完成了 Standalone 模式下 HA 的配置。 接下来我们可以测试验证 HA 的有效性。当我们知道主 JobManager 的机器后，我们可以把主 JobManager 进程 Kill 掉，比如当前主 JobManager 在 z05c19426.sqa.zth.tbsite.net 这个机器上，就把这个进程杀掉。 接着，再打开这两个链接：http://z05f06378.sqa.zth.tbsite.net:8081http://z05c19426.sqa.zth.tbsite.net:8081 可以发现后一个链接已经不能展示了，而前一个链接可以展示，说明发生主备切换。然后我们再重启前一次的主 JobManager： 1./bin/jobmanager.sh start z05c19426.sqa.zth.tbsite.net 8081 再打开 http://z05c19426.sqa.zth.tbsite.net:8081 这个链接，会发现现在这个链接可以转到 http://z05f06378.sqa.zth.tbsite.net:8081 这个页面上了。说明这个 JobManager 完成了一个 Failover Recovery。 6. 使用 Yarn 模式跑 Flink job 图 5 Flink Yarn 部署流程图 相对于 Standalone 模式，Yarn 模式允许 Flink job 的好处有： 资源按需使用，提高集群的资源利用率 任务有优先级，根据优先级运行作业 基于 Yarn 调度系统，能够自动化地处理各个角色的 Failover○ JobManager 进程和 TaskManager 进程都由 Yarn NodeManager 监控 ○ 如果 JobManager 进程异常退出，则 Yarn ResourceManager 会重新调度 JobManager 到其他机器○ 如果 TaskManager 进程异常退出，JobManager 会收到消息并重新向 Yarn ResourceManager 申请资源，重新启动 TaskManager （1）在 Yarn 上启动 Long Running 的 Flink 集群（Session Cluster 模式）查看命令参数： 1./bin/yarn-session.sh -h 创建一个 Yarn 模式的 Flink 集群： 1./bin/yarn-session.sh -n 4 -jm 1024m -tm 4096m 其中用到的参数是： -n,–container Number of TaskManagers -jm,–jobManagerMemory Memory for JobManager Container with optional unit (default: MB) -tm,–taskManagerMemory Memory per TaskManager Container with optional unit (default: MB) -qu,–queue Specify YARN queue. -s,–slots Number of slots per TaskManager -t,–ship Ship files in the specified directory (t for transfer) 提交一个 Flink job 到 Flink 集群： 1./bin/flink run examples/streaming/WordCount.jar --input hdfs:///test_dir/input_dir/story --output hdfs:///test_dir/output_dir/output 这次提交 Flink job，虽然没有指定对应 Yarn application 的信息，却可以提交到对应的 Flink 集群，原因在于“/tmp/.yarn-properties-${user}”文件中保存了上一次创建 Yarn session 的集群信息。所以如果同一用户在同一机器上再次创建一个 Yarn session，则这个文件会被覆盖掉。 如果删掉“/tmp/.yarn-properties-${user}”或者在另一个机器上提交作业能否提交到预期到yarn session中呢？可以配置了“high-availability.cluster-id”参数，据此从 Zookeeper 上获取到 JobManager 的地址和端口，从而提交作业。 如果 Yarn session 没有配置 HA，又该如何提交呢？ 这个时候就必须要在提交 Flink job 的命令中指明 Yarn 上的 Application ID，通过“-yid”参数传入： 1/bin/flink run -yid application_1548056325049_0048 examples/streaming/WordCount.jar --input hdfs:///test_dir/input_dir/story --output hdfs:///test_dir/output_dir/output 我们可以发现，每次跑完任务不久，TaskManager 就被释放了，下次在提交任务的时候，TaskManager 又会重新拉起来。如果希望延长空闲 TaskManager 的超时时间，可以在 conf/flink-conf.yaml 文件中配置下面这个参数，单位是 milliseconds： 12slotmanager.taskmanager-timeout: 30000L # deprecated, used in release-1.5resourcemanager.taskmanager-timeout: 30000L （2）在 Yarn 上运行单个 Flink job（Job Cluster 模式）如果你只想运行单个 Flink Job 后就退出，那么可以用下面这个命令： 1./bin/flink run -m yarn-cluster -yn 2 examples/streaming/WordCount.jar --input hdfs:///test_dir/input_dir/story --output hdfs:///test_dir/output_dir/output 常用的配置有： -yn,–yarncontainer Number of Task Managers -yqu,–yarnqueue Specify YARN queue. -ys,–yarnslots Number of slots per TaskManager -yqu,–yarnqueue Specify YARN queue. 可以通过 Help 命令查看 Run 的可用参数： 1./bin/flink run -h 我们可以看到，“./bin/flink run -h”看到的“Options for yarn-cluster mode”中的“-y”和“–yarn”为前缀的参数其实和“./bin/yarn-session.sh -h”命令是一一对应的，语义上也基本一致。 关于“-n”（在yarn session模式下）、“-yn”在（yarn single job模式下）与“-p”参数的关系： “-n”和“-yn”在社区版本中（Release-1.5 ～ Release-1.7）中没有实际的控制作用，实际的资源是根据“-p”参数来申请的，并且 TM 使用完后就会归还 在 Blink 的开源版本中，“-n”（在 Yarn Session 模式下）的作用就是一开始启动指定数量的 TaskManager，之后即使 Job 需要更多的 Slot，也不会申请新的 TaskManager 在 Blink 的开源版本中，Yarn single job 模式“-yn”表示的是初始 TaskManager 的数量，不设置 TaskManager 的上限。（需要特别注意的是，只有加上“-yd”参数才能用 Single job 模式（例如：命令“./bin/flink run -yd -m yarn-cluster xxx”） 7. Yarn 模式下的 HighAvailability 配置首先要确保启动 Yarn 集群用的“yarn-site.xml”文件中的这个配置，这个是 Yarn 集群级别 AM 重启的上限。 1234 &lt;property&gt; &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt; &lt;value&gt;100&lt;/value&gt;&lt;/property&gt; 然后在 conf/flink-conf.yaml 文件中配置这个 Flink job 的 JobManager 能够重启的次数。 1yarn.application-attempts: 10 # 1+ 9 retries 最后再在 conf/flink-conf.yaml 文件中配置上 ZK 相关配置，这几个配置的配置方法和 Standalone 的 HA 配置方法基本一致，如下所示。 1234567891011121314# 配置high-availability modehigh-availability: zookeeper# 配置zookeeper quorum（hostname和端口需要依据对应zk的实际配置）high-availability.zookeeper.quorum: z05f02321.sqa.zth.tbsite.net:2181,z05f10215.sqa.zth.tbsite.net:2181# （可选）设置zookeeper的root目录high-availability.zookeeper.path.root: /test_dir/test_standalone2_root# 删除这个配置# high-availability.cluster-id: /test_dir/test_standalone2# JobManager的meta信息放在dfs，在zk上主要会保存一个指向dfs路径的指针high-availability.storageDir: hdfs:///test_dir/recovery2/ 需要特别注意的是：“high-availability.cluster-id”这个配置最好去掉，因为在 Yarn（以及Mesos）模式下，cluster-id 如果不配置的话，会配置成 Yarn 上的 Application ID ，从而可以保证唯一性。 原文地址]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Flink 零基础入门（一）：基础概念解析]]></title>
    <url>%2FApache%20Flink%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E8%A7%A3%E6%9E%90.html</url>
    <content type="text"><![CDATA[一、Apache Flink 的定义、架构及原理Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态或无状态的计算，能够部署在各种集群环境，对各种规模大小的数据进行快速计算。 1. Flink Application了解 Flink 应用开发需要先理解 Flink 的 Streams、State、Time 等基础处理语义以及 Flink 兼顾灵活性和方便性的多层次 API。 Streams：流，分为有限数据流与无限数据流，unbounded stream 是有始无终的数据流，即无限数据流；而 bounded stream 是限定大小的有始有终的数据集合，即有限数据流，二者的区别在于无限数据流的数据会随时间的推演而持续增加，计算持续进行且不存在结束的状态，相对的有限数据流数据大小固定，计算最终会完成并处于结束的状态。 State，状态是计算过程中的数据信息，在容错恢复和 Checkpoint 中有重要的作用，流计算在本质上是 Incremental Processing，因此需要不断查询保持状态；另外，为了确保 Exactly- once 语义，需要数据能够写入到状态中；而持久化存储，能够保证在整个分布式系统运行失败或者挂掉的情况下做到 Exactly- once，这是状态的另外一个价值。 Time，分为 Event time、Ingestion time、Processing time，Flink 的无限数据流是一个持续的过程，时间是我们判断业务状态是否滞后，数据处理是否及时的重要依据。 API，API 通常分为三层，由上而下可分为 SQL / Table API、DataStream API、ProcessFunction 三层，API 的表达能力及业务抽象能力都非常强大，但越接近 SQL 层，表达能力会逐步减弱，抽象能力会增强，反之，ProcessFunction 层 API 的表达能力非常强，可以进行多种灵活方便的操作，但抽象能力也相对越小。 2.Flink Architecture在架构部分，主要分为以下四点： 第一， Flink 具备统一的框架处理有界和无界两种数据流的能力 第二， 部署灵活，Flink 底层支持多种资源调度器，包括 Yarn、Kubernetes 等。Flink 自身带的 Standalone 的调度器，在部署上也十分灵活。 第三， 极高的可伸缩性，可伸缩性对于分布式系统十分重要，阿里巴巴双11大屏采用 Flink 处理海量数据，使用过程中测得 Flink 峰值可达 17 亿/秒。 第四， 极致的流式处理性能。Flink 相对于 Storm 最大的特点是将状态语义完全抽象到框架中，支持本地状态读取，避免了大量网络 IO，可以极大提升状态存取的性能。 3.Flink Operation后面会有专门课程讲解，此处简单分享 Flink 关于运维及业务监控的内容： Flink 具备 7 X 24 小时高可用的 SOA（面向服务架构），原因是在实现上 Flink 提供了一致性的 Checkpoint。Checkpoint 是 Flink 实现容错机制的核心，它周期性的记录计算过程中 Operator 的状态，并生成快照持久化存储。当 Flink 作业发生故障崩溃时，可以有选择的从 Checkpoint 中恢复，保证了计算的一致性。 Flink 本身提供监控、运维等功能或接口，并有内置的 WebUI，对运行的作业提供 DAG 图以及各种 Metric 等，协助用户管理作业状态。 4.Flink 的应用场景4.1 Flink 的应用场景：Data Pipeline Data Pipeline 的核心场景类似于数据搬运并在搬运的过程中进行部分数据清洗或者处理，而整个业务架构图的左边是 Periodic ETL，它提供了流式 ETL 或者实时 ETL，能够订阅消息队列的消息并进行处理，清洗完成后实时写入到下游的 Database 或 File system 中。场景举例： 实时数仓 当下游要构建实时数仓时，上游则可能需要实时的 Stream ETL。这个过程会进行实时清洗或扩展数据，清洗完成后写入到下游的实时数仓的整个链路中，可保证数据查询的时效性，形成实时数据采集、实时数据处理以及下游的实时 Query。 搜索引擎推荐 搜索引擎这块以淘宝为例，当卖家上线新商品时，后台会实时产生消息流，该消息流经过 Flink 系统时会进行数据的处理、扩展。然后将处理及扩展后的数据生成实时索引，写入到搜索引擎中。这样当淘宝卖家上线新商品时，能在秒级或者分钟级实现搜索引擎的搜索。 4.2 Flink 应用场景：Data Analytics Data Analytics，如图，左边是 Batch Analytics，右边是 Streaming Analytics。Batch Analysis 就是传统意义上使用类似于 Map Reduce、Hive、Spark Batch 等，对作业进行分析、处理、生成离线报表，Streaming Analytics 使用流式分析引擎如 Storm，Flink 实时处理分析数据，应用较多的场景如实时大屏、实时报表。 4.3 Flink 应用场景：Data Driven 从某种程度上来说，所有的实时的数据处理或者是流式数据处理都是属于 Data Driven，流计算本质上是 Data Driven 计算。应用较多的如风控系统，当风控系统需要处理各种各样复杂的规则时，Data Driven 就会把处理的规则和逻辑写入到Datastream 的 API 或者是 ProcessFunction 的 API 中，然后将逻辑抽象到整个 Flink 引擎中，当外面的数据流或者是事件进入就会触发相应的规则，这就是 Data Driven 的原理。在触发某些规则后，Data Driven 会进行处理或者是进行预警，这些预警会发到下游产生业务通知，这是 Data Driven 的应用场景，Data Driven 在应用上更多应用于复杂事件的处理。 二、「有状态的流式处理」概念解析1.传统批处理 传统批处理方法是持续收取数据，以时间作为划分多个批次的依据，再周期性地执行批次运算。但假设需要计算每小时出现事件转换的次数，如果事件转换跨越了所定义的时间划分，传统批处理会将中介运算结果带到下一个批次进行计算；除此之外，当出现接收到的事件顺序颠倒情况下，传统批处理仍会将中介状态带到下一批次的运算结果中，这种处理方式也不尽如人意。 2.理想方法 第一点，要有理想方法，这个理想方法是引擎必须要有能力可以累积状态和维护状态，累积状态代表着过去历史中接收过的所有事件，会影响到输出。 第二点，时间，时间意味着引擎对于数据完整性有机制可以操控，当所有数据都完全接受到后，输出计算结果。 第三点，理想方法模型需要实时产生结果，但更重要的是采用新的持续性数据处理模型来处理实时数据，这样才最符合 continuous data 的特性。 3.流式处理 流式处理简单来讲即有一个无穷无尽的数据源在持续收取数据，以代码作为数据处理的基础逻辑，数据源的数据经过代码处理后产生出结果，然后输出，这就是流式处理的基本原理。 4.分布式流式处理 假设 Input Streams 有很多个使用者，每个使用者都有自己的 ID，如果计算每个使用者出现的次数，我们需要让同一个使用者的出现事件流到同一运算代码，这跟其他批次需要做 group by 是同样的概念，所以跟 Stream 一样需要做分区，设定相应的 key，然后让同样的 key 流到同一个 computation instance 做同样的运算。 5.有状态分布式流式处理 如图，上述代码中定义了变数 X，X 在数据处理过程中会进行读和写，在最后输出结果时，可以依据变数 X 决定输出的内容，即状态 X 会影响最终的输出结果。这个过程中，第一个重点是先进行了状态 co-partitioned key by，同样的 key 都会流到 computation instance，与使用者出现次数的原理相同，次数即所谓的状态，这个状态一定会跟同一个 key 的事件累积在同一个 computation instance。 相当于根据输入流的 key 重新分区的 状态，当分区进入 stream 之后，这个 stream 会累积起来的状态也变成 copartiton 了。第二个重点是 embeded local state backend。有状态分散式流式处理的引擎，状态可能会累积到非常大，当 key 非常多时，状态可能就会超出单一节点的 memory 的负荷量，这时候状态必须有状态后端去维护它；在这个状态后端在正常状况下，用 in-memory 维护即可。 三、Apache Flink 的优势1.状态容错当我们考虑状态容错时难免会想到精确一次的状态容错，应用在运算时累积的状态，每笔输入的事件反映到状态，更改状态都是精确一次，如果修改超过一次的话也意味着数据引擎产生的结果是不可靠的。 如何确保状态拥有精确一次（Exactly-once guarantee）的容错保证？ 如何在分散式场景下替多个拥有本地状态的运算子产生一个全域一致的快照（Global consistent snapshot）？ 更重要的是，如何在不中断运算的前提下产生快照？ 1.1 简单场景的精确一次容错方法还是以使用者出现次数来看，如果某个使用者出现的次数计算不准确，不是精确一次，那么产生的结果是无法作为参考的。在考虑精确的容错保证前，我们先考虑最简单的使用场景，如无限流的数据进入，后面单一的 Process 进行运算，每处理完一笔计算即会累积一次状态，这种情况下如果要确保 Process 产生精确一次的状态容错，每处理完一笔数据，更改完状态后进行一次快照，快照包含在队列中并与相应的状态进行对比，完成一致的快照，就能确保精确一次。 1.2 分布式状态容错Flink 作为分布式的处理引擎，在分布式的场景下，进行多个本地状态的运算，只产生一个全域一致的快照，如需要在不中断运算值的前提下产生全域一致的快照，就涉及到分散式状态容错。 Global consistent snapshot 关于 Global consistent snapshot，当 Operator 在分布式的环境中，在各个节点做运算，首先产生 Global consistent snapshot 的方式就是处理每一笔数据的快照点是连续的，这笔运算流过所有的运算值，更改完所有的运算值后，能够看到每一个运算值的状态与该笔运算的位置，即可称为 consistent snapshot，当然，Global consistent snapshot 也是简易场景的延伸。 容错恢复 首先了解一下 Checkpoint，上面提到连续性快照每个 Operator 运算值本地的状态后端都要维护状态，也就是每次将产生检查点时会将它们传入共享的 DFS 中。当任何一个 Process 挂掉后，可以直接从三个完整的 Checkpoint 将所有的运算值的状态恢复，重新设定到相应位置。Checkpoint 的存在使整个 Process 能够实现分散式环境中的 Exactly-once。 1.3 分散式快照（Distributed Snapshots）方法 关于 Flink 如何在不中断运算的状况下持续产生 Global consistent snapshot，其方式是基于用 simple lamport 演算法机制下延伸的。已知的一个点 Checkpoint barrier， Flink 在某个 Datastream 中会一直安插 Checkpoint barrier，Checkpoint barrier 也会 N — 1等等，Checkpoint barrier N 代表着所有在这个范围里面的数据都是Checkpoint barrier N。 举例：假设现在需要产生 Checkpoint barrier N，但实际上在 Flink 中是由 job manager 触发 Checkpoint，Checkpoint 被触发后开始从数据源产生 Checkpoint barrier。当 job 开始做 Checkpoint barrier N 的时候，可以理解为 Checkpoint barrier N 需要逐步填充左下角的表格。 如图，当部分事件标为红色，Checkpoint barrier N 也是红色时，代表着这些数据或事件都由 Checkpoint barrier N 负责。Checkpoint barrier N 后面白色部分的数据或事件则不属于 Checkpoint barrier N。 在以上的基础上，当数据源收到 Checkpoint barrier N 之后会先将自己的状态保存，以读取 Kafka 资料为例，数据源的状态就是目前它在 Kafka 分区的位置，这个状态也会写入到上面提到的表格中。下游的 Operator 1 会开始运算属于 Checkpoint barrier N 的数据，当 Checkpoint barrier N 跟着这些数据流动到 Operator 1 之后,Operator 1 也将属于 Checkpoint barrier N 的所有数据都反映在状态中，当收到 Checkpoint barrier N 时也会直接对 Checkpoint 去做快照。 当快照完成后继续往下游走，Operator 2 也会接收到所有数据，然后搜索 Checkpoint barrier N 的数据并直接反映到状态，当状态收到 Checkpoint barrier N 之后也会直接写入到 Checkpoint N 中。以上过程到此可以看到 Checkpoint barrier N 已经完成了一个完整的表格，这个表格叫做 Distributed Snapshots，即分布式快照。分布式快照可以用来做状态容错，任何一个节点挂掉的时候可以在之前的 Checkpoint 中将其恢复。继续以上 Process，当多个 Checkpoint 同时进行，Checkpoint barrier N 已经流到 job manager 2，Flink job manager 可以触发其他的 Checkpoint，比如 Checkpoint N + 1，Checkpoint N + 2 等等也同步进行，利用这种机制，可以在不阻挡运算的状况下持续地产生 Checkpoint。 2.状态维护状态维护即用一段代码在本地维护状态值，当状态值非常大时需要本地的状态后端来支持。 如图，在 Flink 程序中，可以采用 getRuntimeContext().getState(desc); 这组 API 去注册状态。Flink 有多种状态后端，采用 API 注册状态后，读取状态时都是通过状态后端来读取的。Flink 有两种不同的状态值，也有两种不同的状态后端： JVM Heap状态后端，适合数量较小的状态，当状态量不大时就可以采用 JVM Heap 的状态后端。JVM Heap 状态后端会在每一次运算值需要读取状态时，用 Java object read / writes 进行读或写，不会产生较大代价，但当 Checkpoint 需要将每一个运算值的本地状态放入 Distributed Snapshots 的时候，就需要进行序列化了。 RocksDB 状态后端，它是一种 out of core 的状态后端。在 Runtime 的本地状态后端让使用者去读取状态的时候会经过磁盘，相当于将状态维护在磁盘里，与之对应的代价可能就是每次读取状态时，都需要经过序列化和反序列化的过程。当需要进行快照时只将应用序列化即可，序列化后的数据直接传输到中央的共享 DFS 中。 Flink 目前支持以上两种状态后端，一种是纯 memory 的状态后端，另一种是有资源磁盘的状态后端，在维护状态时可以根据状态的数量选择相应的状态后端。 3.Event - Time3.1 不同时间种类 在 Flink 及其他进阶的流式处理引擎出现之前，大数据处理引擎一直只支持 Processing-time 的处理。假设定义一个运算 windows 的窗口，windows 运算设定每小时进行结算。以 Processing-time 进行运算时可以发现数据引擎将 3 点至 4 点间收到的数据做结算。实际上在做报表或者分析结果时是想了解真实世界中 3 点至 4 点之间实际产生数据的输出结果，了解实际数据的输出结果就必须采用 Event – Time 了。 如图，Event - Time 相当于事件，它在数据最源头产生时带有时间戳，后面都需要用时间戳来进行运算。用图来表示，最开始的队列收到数据，每小时对数据划分一个批次，这就是 Event - Time Process 在做的事情。 3.2 Event - Time 处理 Event - Time 是用事件真实产生的时间戳去做 Re-bucketing，把对应时间 3 点到 4 点的数据放在 3 点到 4 点的 Bucket，然后 Bucket 产生结果。所以 Event - Time 跟 Processing - time 的概念是这样对比的存在。 Event - Time 的重要性在于记录引擎输出运算结果的时间。简单来说，流式引擎连续 24 小时在运行、搜集资料，假设 Pipeline 里有一个 windows Operator 正在做运算，每小时能产生结果，何时输出 windows 的运算值，这个时间点就是 Event - Time 处理的精髓，用来表示该收的数据已经收到。 3.3 Watermarks Flink 实际上是用 watermarks 来实现 Event - Time 的功能。Watermarks 在 Flink 中也属于特殊事件，其精髓在于当某个运算值收到带有时间戳“ T ”的 watermarks 时就意味着它不会接收到新的数据了。使用 watermarks 的好处在于可以准确预估收到数据的截止时间。举例，假设预期收到数据时间与输出结果时间的时间差延迟 5 分钟，那么 Flink 中所有的 windows Operator 搜索 3 点至 4 点的数据，但因为存在延迟需要再多等5分钟直至收集完 4：05 分的数据，此时方能判定 4 点钟的资料收集完成了，然后才会产出 3 点至 4 点的数据结果。这个时间段的结果对应的就是 watermarks 的部分。 4.状态保存与迁移流式处理应用无时无刻不在运行，运维上有几个重要考量： 更改应用逻辑/修 bug 等，如何将前一执行的状态迁移到新的执行？ 如何重新定义运行的平行化程度？ 如何升级运算丛集的版本号？ Checkpoint 完美符合以上需求，不过 Flink 中还有另外一个名词保存点（Savepoint），当手动产生一个 Checkpoint 的时候，就叫做一个 Savepoint。Savepoint 跟 Checkpoint 的差别在于检查点是 Flink 对于一个有状态应用在运行中利用分布式快照持续周期性的产生 Checkpoint，而 Savepoint 则是手动产生的 Checkpoint，Savepoint 记录着流式应用中所有运算元的状态。 如图，Savepoint A 和 Savepoint B，无论是变更底层代码逻辑、修 bug 或是升级 Flink 版本，重新定义应用、计算的平行化程度等，最先需要做的事情就是产生 Savepoint。 Savepoint 产生的原理是在 Checkpoint barrier 流动到所有的 Pipeline 中手动插入从而产生分布式快照，这些分布式快照点即 Savepoint。Savepoint 可以放在任何位置保存，当完成变更时，可以直接从 Savepoint 恢复、执行。 从 Savepoint 的恢复执行需要注意，在变更应用的过程中时间在持续，如 Kafka 在持续收集资料，当从 Savepoint 恢复时，Savepoint 保存着 Checkpoint 产生的时间以及 Kafka 的相应位置，因此它需要恢复到最新的数据。无论是任何运算，Event - Time 都可以确保产生的结果完全一致。 假设恢复后的重新运算用 Process Event - Time，将 windows 窗口设为 1 小时，重新运算能够在 10 分钟内将所有的运算结果都包含到单一的 windows 中。而如果使用 Event – Time，则类似于做 Bucketing。在 Bucketing 的状况下，无论重新运算的数量多大，最终重新运算的时间以及 windows 产生的结果都一定能保证完全一致。 四、总结本文首先从 Apache Flink 的定义、架构、基本原理入手，对大数据流计算相关的基本概念进行辨析，在此基础上简单回顾了大数据处理方式的历史演进以及有状态的流式数据处理的原理，最后从目前有状态的流式处理面临的挑战分析 Apache Flink 作为业界公认为最好的流计算引擎之一所具备的天然优势。希望有助于大家厘清大数据流式处理引擎涉及的基本概念，能够更加得心应手的使用 Flink。 原文地址]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图数据库洞察数据间的关联价值]]></title>
    <url>%2F%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93%E6%B4%9E%E5%AF%9F%E6%95%B0%E6%8D%AE%E9%97%B4%E7%9A%84%E5%85%B3%E8%81%94%E4%BB%B7%E5%80%BC.html</url>
    <content type="text"><![CDATA[大家好，我是吴敏。今天分享一个叫图数据库的技术产品。 01、 什么是图和图数据库 先来介绍一下什么是图和图数据库，所谓的图和平常认知的图片其实不是同一个概念，图（Graph）在计算机科学里面是一种数据结构，这种数据结构有三个比较主要的概念：点、边和属性。 通俗的说，图结构还有其他的叫法，比如：网络结构、拓扑结构，大致上都是描述的同一种数据结构。 举个例子，上面这个图是典型的图结构（网络结构），人和公司，公司与公司都存在关联关系。这里面存在几个重要的概念，在网络结构中一家公司、一个人可以是一个点；还有另外一个概念是边，描述的是点与点之间的关系，对应上图中母公司和子公司之间的一个控股关系，也可以是某一个人是另外一个公司的董事长，这样的一个身份关系。点和边基本上组成一个网络结构。 图结构在工业界使用的时候还会加上一个概念就是属性。比如，中间的这个人（点）可以有他的身份证、性别、年龄属性，关系就是边上也可以有一些属性，比如说投资某家公司的投资金额、投资的比例、投资的时间等等，都可以构成这个投资关系的属性。 像基于上图这样的工商关系，微众银行、腾讯和我们（Nebula Graph）也都是有各自合作的项目。 1、图数据库的案例 1：反欺诈 下面来介绍下图的应用。 一般来说，图在安全场景里面的应用会比较多，上面这种图的中间部分是和 360金融合作的一个项目，主要用于识别诈骗团伙。 具体来说，现在是互联网时代，很多 APP 支持申请贷款，不管是持牌或者持其他牌照的平台，都可以提供一定的贷款能力。与此同时，也存在团伙“作案”，当然抢银行的少，诈骗、骗贷的团伙多，而这些团伙是有特征的，可以通过一定的方式进行关联。 在左边的这个例子里，有些的黑产团伙，他们控制的账号会登录一些设备（手机），这些设备和 Wi-Fi 有关联关系。通过这样的 账号-设备-WiFi 关联关系可以识别出来这些团伙。这些团伙被识别出来后，如果团伙相关的人来贷款或者说是来申请授信时，在风控环节会先识别出来。 在中间这个例子里，红色的点是已知存在风险的账号，最中间的那个区域就是一个风险的团伙，这些节点就是被识别出来的风险节点，它们基于 Wi-Fi 关系将其他点关联到了一起。 360 金融通过用图的方法大概识别了接近 100 万个有风险的团伙，所以这些团伙哪怕换一个马甲或者其他设备也能第一时间把他们识别出来，进行屏蔽。右边案例图是一些受害人，蓝色的点是诈骗团伙，诈骗团伙还是有挺明显的特征存在的。 2 图数据库的案例 2：公司信用分 上面的例子主要是识别有风险的人，在这个例子里主要讲下 BOSS 直聘的公司风控。在上图中显示了 BOSS 直聘的一些公司，有些公司是很早入驻 BOSS 直聘平台，有些是新注册的。当中存在部分公司不一定可信，需要对这些公司作区分给一定信用分。比如说，公司信用等级好的对它的运营策略会放松点，信用等级差的公司对它的运营审核严格些。因为不停地有新的公司在注册，可以通过不同的运营方式得到这些公司的不同信息，上图这里用的是同这些公司有关联关系的公司。举个例子，我新注册一家公司的时候，这家公司还是会和其它公司有一些互动和关联，例如：该公司的分公司，或是公司同失信被执行人之间有关联关系，通过一轮轮的迭代算出风险评级和信用评级，为新出来的公司提供一个启动初始信用分，这个方法类似于网页权重中使用的 Page Rank。每天晚上 BOSS 直聘会跑几百万社区的权重。 3 图数据库的案例 3：可信设备 这个例子比较直观。现在每个人基本上都有手机，然后会有常用的手机设备，也可能你会临时换一个设备。这个常用设备下，鉴权的要求比较低。而用临时设备鉴权的要求较高，风控等级较高。还有一种情况就是家人临时使用彼此的设备，这种情况下的鉴权要求可以不需要那么高。因为只要换一个新设备就判断为高风险场景的话，其实对于用户的侵入和打扰很明显。 4 图数据库的案例 4：智能助理 这个是和美团合作的项目，本身其实是有两方面，一方面是和知识图谱相关，一方面是和深度学习相关。目前来说大多数公司的推荐系统都有基于深度学习的模型。那么会存在一个问题：这个推荐出来的内容可解释性差。简单来说，用户不知道为什么产生这样的列表。因此，美团结合图谱做了一些应用，把所有的菜、地点、人、人与人之间的关系还有这些东西组成大的网，当深度学习模型算出推荐列表之后，取用户和商家之间所有可能的关系，取出一条路径或者多条路径的，在多条路径之间做一些加权或者说计算一些路径规则分，呈现给用户看上去更可理解的理由。比如，这里挺有意思的理由，叫“在北京喜欢北京菜的山东老乡都说这家店很赞”，看到这个理由的时候，你会觉得这个推荐略微合理。当然类似的方法也可以用于像问答机器人这样的 KBQA 的系统。 5 图数据库的案例 5：外部作弊与刷单 这是一个刷单的例子，其实很多公司会有运营经费，特别是对新用户会有运营经费，但是这会招来一些羊毛党。这些专业的羊毛党技术很好，他们来薅羊毛的速度比一般的消费者速度快很多，一般前期的大多数的运营经费不是给新用户用掉而是给羊毛党薅走了，羊毛党一般就是那些人，把他们识别出来之后，就可以降低运营经费被薅走的概率。 6 图数据库的案例 6：数据治理系统 这个例子是关于 IT 系统的。我相信现在大多数的公司都是有一个庞大的数仓，几万张甚至几百万张的表，表与表之间又有比较强的依赖关系。例如：一张表或者某几张表取当中几个字段，通过一个 job 清洗，生成下一张表。某一天 DBA 或者某个业务人员发现有一个数据不太对，想知道哪个环节出错了，一层一层往上查，上百层的依赖，用图的方式关联就可以很快的查到哪个地方更有可能出错。这也是我们和微众银行合作的，他们现在正在使用的东西。 7 图数据库的案例 7、8：服务依赖分析 / 代码依赖分析 左边是和运维相关的，右边是和研发相关的事。因为现在基本都是微服务化了，整个微服务之间的调用关系其实是很庞大的。特别是一个大型集团内的RPC 调用关系，运维自己都不一定搞得清楚全局是什么依赖情况。 这个是美团的例子，把所有的调用关系写到图谱里，大概是百万级别的调用关系。比如说运维想知道过去 7 天可用率低于 4 个 9 的链路有哪些，可以非常快速地识别出来，深度可以是 10 层也可以 100 层。 右边是一个辅助开发过程的小工具，对研发人员来说挺方便的。对于一个大型的代码仓库，函数之间相互调用。比如说研发今天想改用一个接口，但是我不知道有多少人在调用这个接口，是怎么调用的。对于测试来说，也不知道要测试哪些边界场景。那可以把这些关系都放到图谱里面去，这样大约是一个千万级别的调用关系，把整个调用关系全抽出来之后，那研发说我想看一眼这个接口被多少人调用了，调用方是怎么使用的；QA 要做测试的时候，可能有哪些边界场景受到影响也可以很快地知道。 02 为什么使用图数据库 刚才说的其实就是一些图的应用，当然其实这些应用不用图这种数据结构来处理，也是可以的。比如在数仓用 Spark 或者写 SQL 来做也可以。但是为什么更推荐用图数据库呢？有以下几个原因。 1 一图胜千言 一个是图的表达能力更强。左边是用表的结构方式来处理人物关系和社区关系。右边当中人的是比较重要的节点，他在左边的表中对应的某一行，右边是用图的方式来看。通过不同的着色可以很容易地看出来不同的社区，然后不同的社区之间通过某些特殊的节点来关联。这样远比用表的方式直观得多，特别是在右边图里面查找中心节点比起在左边的表中查找属性值大小要方便的多。 2 繁琐 vs 简洁 第二个是对于图的遍历这种操作来说——相当于表操作中 join。如果用 SQL 来写，大约是左边这么长，也不是算非常复杂；但是用图的查询语言（右侧）来写的话，其实例子核心就是一句话，沿着一个点开始沿着这样一个路径取 Person 数据。所以对于图遍历操作，图专用的查询语言要更简洁。 3 更快！ 使用图还有一个优势是更快，行业内的经典例子就是查询的数据深度越多的时候，图数据库的优势越加明显。对于 4、 5 层深度的查询，小时级别的时延和秒级别的时延，是两种不同的业务形态。 4 流行趋势 最后一个原因是关于流行趋势。在国际上，用于统计各种数据库类型流行情况的 DB-Engines 上，可以看到图数据库的趋势。上图这是这个月最新的数据，绿色是图这种数据库类型流行的趋势，最下面红色的线是关系型数据库的流行趋势。可以看到，图数据库在过去 8 年内保持了比较好的增速，增长了 11 倍。 03 为什么选择 Nebula Graph 当然，在整个图数据库领域，产品并不是只有 Nebula Graph 一个，也有很多的其他公司。今天早上也有其他同行在会场上，我想解释一下为什么会推荐 Nebula Graph。 一般大家选型基于不同的需求看的重点不一样，我想可能会对可靠性、成本性能、功能各个方面进行权衡。 Nebula Graph 是一个开源的产品，源代码是开放在 GitHub 上的。虽然产品的研发时间不长，从 2018 年 10 月开始第一行代码，但是整个项目很活跃。 上图左下角是 Nebula Graph 中文论坛的情况，在国内有大量的使用者。而 Nebula Graph 本身是开源的项目，整个项目除了我们公司人员之外也有很多国内外贡献者，很多人在使用 Nebula 之后会发现一些 bug 这样就会 file 个 issue，也有不少人会自己动手 fix 和贡献 feature，这样提升了整个研发迭代速度。 右边是所有 Nebula 的 GitHub 贡献者列表，这些是公开情况，你可以在 GitHub 上面看到。总的来说，贡献者来源很多，并不是背后只有一家公司在研发。 上图是在生产环境使用 Nebula Graph 的公司。 既然是 Nebula Graph 是开源代码的，那么所有人可以下载和评测。所有的用户都可以根据自己的业务场景做的评测，会更贴近自己的实际场景。而不像某些供应商自己提供的评测，用户难验证这样的评测里面隐藏了多少坑。 这张图第一个例子是去年和微信合作的项目，他们现在的生产环境单个集群是 50 台机器的规模，它的更新数据量大概是 4,000 亿这样的级别。第二个是美团的例子，美团所做的 Nebula 和其它竞争对手产品的评测。因为美团也是有一个非常高的可用需求，基本上都是要两地多机房。第三个是 BOSS 直聘的评测，从友商产品迁移到 Nebula 之后，从最初只能做 50 亿量级的边的产品，提升到做千亿点边的项目。下面这是贝壳做的评测，公开在今年的DTCC，也是和友商产品的的对比。右边是 360 金融做的评测，生产环境服务器数量减少到原先集群的 1/3，性能是原来的 20 倍以上。 虽然软件本身是开源的，但是开源软件是可以商业化的。这个在国内外也是一个比较普遍的事情。Nebula 的源代码是开放的，所以不管是社区版也好，企业版也好，在产品功能内核、可视化、生态方面基本上没有太大的差别。主要的差别是在服务上，社区版如果有问题可以通过开源社区的方式来解决，按照开源协议（Apache 2.0）的约定。而如果是企业版的话，那会提供企业版的严格的 SLA。另外，云这几年流行和增速也非常的快，云目前是在受邀公测的阶段。大家有什么兴趣可以联系我们。 文章原址]]></content>
      <categories>
        <category>图数据库</category>
      </categories>
      <tags>
        <tag>NebulaGraph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink知识图谱]]></title>
    <url>%2FFlink%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1.html</url>
    <content type="text"><![CDATA[社区整理了这样一份知识图谱，由 Apache Flink Committer 执笔，四位 PMC 成员审核，将 Flink 9 大技术版块详细拆分，突出重点内容并搭配全面的学习素材。看完这份图谱，才算真的搞懂 Flink！ 相关链接 ververica中文网站: https://ververica.cn/ Apache Flink 视频教程: https://github.com/flink-china/flink-training-course Flink Forward Asia 2019: https://ververica.cn/developers/flink-forward-asia-2019/ Flink Forward China 2018: https://github.com/flink-china/flink-forward-china-2018 Flink Forward San Francisco 2015-2017: https://www.youtube.com/channel/UCY8_lgiZLZErZPF47a2hXMA (需要翻墙) ◀ Flink 中文社区微信公众号 Apache Flink Stateful Computations over Data Streams 唯一官方发布渠道 ​ 扫码关注，获取更多技术干货 https://github.com/flink-china/flink-training-course#34-flink-cep-%E5%AE%9E%E6%88%98 State Processor API: https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/libs/state_processor_api.html Gelly: https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/libs/gelly/ Table API &amp; SQL 相关链接 Dynamic Table: https://flink.apache.org/news/2017/04/04/dynamic-tables.html https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/dynamic_tables.html Time Attributes: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/time_attributes.html Temporal Tables: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/temporal_tables.html Query Configuration: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/query_configuration.html Flink SQL 编程: https://github.com/flink-china/flink-training-course#19-flink-sql-%E7%BC%96%E7%A8%8B Create Table/Databases/Function: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/create.html Drop Table/Databases/Function: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/drop.html Alter Table/Databases/Function: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/alter.html Query: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html SQL Client: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sqlClient.html Flink Table API 编程: https://github.com/flink-china/flink-training-course#18-flink-table-api-%E7%BC%96%E7%A8%8B Simplify Machine Learning With Flink TableAPI: https://files.alicdn.com/tpsservice/69181d1fd85d15635a7fe64ebafbf140.pdf https://yq.aliyun.com/live/703 (大概从视频52:00开始) Apache Flink Python API 现状及规划: https://github.com/flink-china/flink-training-course#214-apache-flink-python-api-%E7%8E%B0%E7%8A%B6%E5%8F%8A%E8%A7%84%E5%88%92 Built-in Functions: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/functions/systemFunctions.html User Defined Functions: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/functions/udfs.html Connect to External Systems: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connect.html User-defined Table Sources &amp; Sinks: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sourceSinks.html 深度探索 Flink SQL: https://github.com/flink-china/flink-training-course#213-%E6%B7%B1%E5%BA%A6%E6%8E%A2%E7%B4%A2-flink-sql Batch as a Special Case of Streaming and Alibaba’s contribution of Blink: https://flink.apache.org/news/2019/02/13/unified-batch-streaming-blink.html Modules: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/modules.html Catalog: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/catalogs.html Hive Integration: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/ Deployment and Operations 相关链接 Flink 安装部署、环境配置及运行应用程序: https://github.com/flink-china/flink-training-course#13-flink-%E5%AE%89%E8%A3%85%E9%83 %A8%E7%BD%B2%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%8F%8A%E8%BF%90% E8%A1%8C%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F Local Cluster: https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/local.html Standalone Cluster: https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/cluster_setup.html YARN: https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/yarn_setup.html Mesos: https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/mesos.html Introducing Docker Images for Apache Flink: https://flink.apache.org/news/2017/05/16/official-docker-image.html Running Apache Flink on Kubernetes with KUDO: https://flink.apache.org/news/2019/12/09/flink-kubernetes-kudo.html High Availability: https://ci.apache.org/projects/flink/flink-docs-master/ops/jobmanager_high_availability.html Command-Line Interface: https://ci.apache.org/projects/flink/flink-docs-master/ops/cli.html Python REPL: https://ci.apache.org/projects/flink/flink-docs-master/ops/python_shell.html Scala REPL: https://ci.apache.org/projects/flink/flink-docs-master/ops/scala_shell.html Kerberos: https://ci.apache.org/projects/flink/flink-docs-master/ops/security-kerberos.html SSL: https://ci.apache.org/projects/flink/flink-docs-master/ops/security-ssl.html Libraries 相关链接 Flink CEP 实战:]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019_Flink社区年度文章合集]]></title>
    <url>%2F2019-Flink%E7%A4%BE%E5%8C%BA%E5%B9%B4%E5%BA%A6%E6%96%87%E7%AB%A0%E5%90%88%E9%9B%86.html</url>
    <content type="text"><![CDATA[在过去的一年中 Apache Flink 社区官方微信公众号为小伙伴们推送了大数据及 Flink 相关活动资讯 46 篇；Apache Flink 的系列基础教程 19 篇、企业级应用实践 20+、实操系列以及实时数仓、风控、CEP 等典型应用场景及技术干货 50+。 实时资讯部分 Flink 社区实时资讯： | 最新消息！Cloudera 全球发行版正式集成 Apache Flink| 实至名归！Flink 再度成为 Apache 基金会最活跃的开源项目| 仅 1 年 GitHub Star 数翻倍，Flink 做了什么？| 年度回顾 | 2019 年的 Apache Flink| Flink Forward Asia 2019 - 总结和展望| Flink Weekly | 每周社区动态更新 - 20200114| Flink Weekly | 每周社区动态更新 - 20200107| Flink Weekly | 每周社区动态更新 - 20201231| Flink Weekly | 每周社区动态更新 - 20201224 企业实践企业实践部分优质内容： | 小米流式平台架构演进与实践| 美团点评基于 Flink 的实时数仓平台实践| 监控指标10K+！携程实时智能检测平台实践| Lyft 基于 Flink 的大规模准实时数据分析平台| 基于 Flink 构建 CEP 引擎的挑战和实践| 趣头条基于 Flink 的实时平台建设实践| Apache Flink 的迁移之路，2 年处理效果提升 5 倍| 日均百亿级日志处理：微博基于 Flink 的实时计算平台建设| Flink 在同程艺龙实时计算平台的研发与应用实践| Apache Flink 在 eBay 监控系统上的实践和应用| 每天30亿条笔记展示，小红书如何实现实时高效推荐？| 58 集团大规模 Storm 任务平滑迁移至 Flink 的秘密| 从 Storm 到 Flink，汽车之家基于 Flink 的实时 SQL 平台设计思路与实践| 日均处理万亿数据！Apache Flink在快手的应用实践与技术演进之路 系列基础教程Apache Flink 零基础入门教程： ▼ 进阶篇 ▼ |Flink 基础概念解析|Flink 开发环境搭建和应用的配置、部署及运行|Flink Datastream API 编程|Flink 客户端操作|Flink Time &amp; Window|Flink 状态管理及容错机制|Flink Table API 编程|Flink SQL 编程实践|Runtime 核心机制剖析|Checkpoint 原理剖析与应用实践 技术干货部分技术干货文章节选: | Apache Flink CEP 实战| Flink Batch SQL 1.10 实践| Flink State 有可能代替数据库吗？| Flink SQL 如何实现数据流的 Join？| Flink SQL 系列 | 开篇，新架构与 Planner| 如何在 Flink 1.9 中使用 Hive？| Apache Flink 1.9.0 为什么将支持 Python API ？| 在 Cloudera Data Flow 上运行你的第一个 Flink 例子| Flink on YARN（上）：一张图轻松掌握基础架构与启动流程| Flink on YARN（下）：常见问题与排查思路| 修改代码150万行！Apache Flink 1.9.0做了这些重大修改！| 如何在 PyFlink 1.10 中自定义 Python UDF？]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式学习]]></title>
    <url>%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%AD%A6%E4%B9%A0.html</url>
    <content type="text"><![CDATA[正则表达式在几乎所有语言中都可以使用，无论是前端的JavaScript、还是后端的Java、c#。他们都提供相应的接口/函数支持正则表达式。 但很神奇的是：无论你大学选择哪一门计算机语言，都没有关于正则表达式的课程给你修，在你学会正则之前，你只能看着那些正则大师们，写了一串外星文似的字符串，替代了你用一大篇幅的if else代码来做一些数据校验。 既然喜欢，那就动手学呗，可当你百度出一一堆相关资料时，你发现无一不例外的枯燥至极，难以学习。 本文旨在用最通俗的语言讲述最枯燥的基本知识！ 正则基础知识点：1.元字符万物皆有缘，正则也是如此，元字符是构造正则表达式的一种基本元素。我们先来记几个常用的元字符： 有了元字符之后，我们就可以利用这些元字符来写一些简单的正则表达式了，比如： 匹配有abc开头的字符串： 11 \babc或者^abc \2. 匹配8位数字的QQ号码： 11 ^\d\d\d\d\d\d\d\d$ \3. 匹配1开头11位数字的手机号码： 11 ^1\d\d\d\d\d\d\d\d\d\d$ 2. 重复限定符有了元字符就可以写不少的正则表达式了，但细心的你们可能会发现：别人写的正则简洁明了，而不理君写的正则一堆乱七八糟而且重复的元字符组成的。正则没提供办法处理这些重复的元字符吗？ 答案是有的！为了处理这些重复问题，正则表达式中一些重复限定符，把重复部分用合适的限定符替代，下面我们来看一些限定符： 有了这些限定符之后，我们就可以对之前的正则表达式进行改造了，比如： \1. 匹配8位数字的QQ号码： 11 ^\d&#123;8&#125;$ \2. 匹配1开头11位数字的手机号码： 11 ^1\d&#123;10&#125;$ \3. 匹配银行卡号是14~18位的数字： 11 ^\d&#123;14,18&#125;$ \4. 匹配以a开头的，0个或多个b结尾的字符串 11 ^ab*$ 3. 分组从上面的例子（4）中看到，限定符是作用在与他左边最近的一个字符，那么问题来了，如果我想要ab同时被限定那怎么办呢？ 正则表达式中用小括号()来做分组，也就是括号中的内容作为一个整体。 因此当我们要匹配多个ab时，我们可以这样如：匹配字符串中包含0到多个ab开头： 11 ^(ab)* 4. 转义我们看到正则表达式用小括号来做分组，那么问题来了： 如果要匹配的字符串中本身就包含小括号，那是不是冲突？应该怎么办？ 针对这种情况，正则提供了转义的方式，也就是要把这些元字符、限定符或者关键字转义成普通的字符，做法很简答，就是在要转义的字符前面加个斜杠，也就是\即可。如：要匹配以(ab)开头： 11 ^(\(ab\))* 5. 条件或回到我们刚才的手机号匹配，我们都知道：国内号码都来自三大网，它们都有属于自己的号段，比如联通有130/131/132/155/156/185/186/145/176等号段，假如让我们匹配一个联通的号码，那按照我们目前所学到的正则，应该无从下手的，因为这里包含了一些并列的条件，也就是“或”，那么在正则中是如何表示“或”的呢？ 正则用符号 | 来表示或，也叫做分支条件，当满足正则里的分支条件的任何一种条件时，都会当成是匹配成功。 那么我们就可以用或条件来处理这个问题 11 ^(130|131|132|155|156|185|186|145|176)\d&#123;8&#125;$ 6. 区间看到上面的例子，是不是看到有什么规律？是不是还有一种想要简化的冲动？实际是有的 正则提供一个元字符中括号 [] 来表示区间条件。 限定0到9 可以写成[0-9] 限定A-Z 写成[A-Z] 限定某些数字 [165] 那上面的正则我们还改成这样： 11 ^((13[0-2])|(15[56])|(18[5-6])|145|176)\d&#123;8&#125;$ 好了，正则表达式的基本用法就讲到这里了，其实它还有非常多的知识点以及元字符，我们在此只列举了部分元字符和语法来讲，旨在给那些不懂正则或者想学正则但有看不下去文档的人做一个快速入门级的教程，看完本教程，即使你不能写出高大上的正则，至少也能写一些简单的正则或者看得懂别人写的正则了。 正则进阶知识点：1. 零宽断言 无论是零宽还是断言，听起来都古古怪怪的，那先解释一下这两个词。 断言：俗话的断言就是“我断定什么什么”，而正则中的断言，就是说正则可以指明在指定的内容的前面或后面会出现满足指定规则的内容，意思正则也可以像人类那样断定什么什么，比如”ss1aa2bb3”,正则可以用断言找出aa2前面有bb3，也可以找出aa2后面有ss1. 零宽：就是没有宽度，在正则中，断言只是匹配位置，不占字符，也就是说，匹配结果里是不会返回断言本身。 意思是讲明白了，那他有什么用呢？我们来举个栗子：假设我们要用爬虫抓取csdn里的文章阅读量。通过查看源代码可以看到文章阅读量这个内容是这样的结构 11 &quot;&lt;span class=&quot;read-count&quot;&gt;阅读数：641&lt;/span&gt;&quot; 其中也就‘641’这个是变量，也就是说不同文章不同的值，当我们拿到这个字符串时，需要获得这里边的‘641’有很多种办法，但如果正则应该怎么匹配呢？ 下面先来讲几种类型的断言： 正向先行断言（正前瞻）： 语法：（?=pattern） 作用：匹配pattern表达式的前面内容，不返回本身。 这样子说，还是一脸懵逼，好吧，回归刚才那个栗子，要取到阅读量，在正则表达式中就意味着要能匹配到‘’前面的数字内容按照上所说的正向先行断言可以匹配表达式前面的内容，那意思就是:(?=) 就可以匹配到前面的内容了。匹配什么内容呢？如果要所有内容那就是： 123456789101112 1 String reg=".+(?=&lt;/span&gt;)"; 2 3 String test = "&lt;span class=\"read-count\"&gt;阅读数：641&lt;/span&gt;"; 4 Pattern pattern = Pattern.compile(reg); 5 Matcher mc= pattern.matcher(test); 6 while(mc.find())&#123; 7 System.out.println("匹配结果：") 8 System.out.println(mc.group()); 9 &#125;1011 //匹配结果：12 //&lt;span class="read-count"&gt;阅读数：641 可是老哥我们要的只是前面的数字呀，那也简单咯，匹配数字 \d,那可以改成： 1234567891 String reg="\\d+(?=&lt;/span&gt;)";2 String test = "&lt;span class=\"read-count\"&gt;阅读数：641&lt;/span&gt;";3 Pattern pattern = Pattern.compile(reg);4 Matcher mc= pattern.matcher(test);5 while(mc.find())&#123;6 System.out.println(mc.group());7 &#125;8 //匹配结果：9 //641 大功告成！ \2. 正向后行断言（正后顾）: 语法：（?&lt;=pattern） 作用：匹配pattern表达式的后面的内容，不返回本身。 有先行就有后行，先行是匹配前面的内容，那后行就是匹配后面的内容啦。上面的栗子，我们也可以用后行断言来处理. 1234567891011 1 //(?&lt;=&lt;span class="read-count"&gt;阅读数：)\d+ 2 String reg="(?&lt;=&lt;span class=\"read-count\"&gt;阅读数：)\\d+"; 3 4 String test = "&lt;span class=\"read-count\"&gt;阅读数：641&lt;/span&gt;"; 5 Pattern pattern = Pattern.compile(reg); 6 Matcher mc= pattern.matcher(test); 7 while(mc.find())&#123; 8 System.out.println(mc.group()); 9 &#125;10 //匹配结果：11 //641 就这么简单。 \3. 负向先行断言（负前瞻） 语法：(?!pattern) 作用：匹配非pattern表达式的前面内容，不返回本身。 有正向也有负向，负向在这里其实就是非的意思。举个栗子：比如有一句 “我爱祖国，我是祖国的花朵”现在要找到不是’的花朵’前面的祖国用正则就可以这样写： 11 祖国(?!的花朵) \4. 负向后行断言（负后顾） 语法：(?&lt;!pattern) 作用：匹配非pattern表达式的后面内容，不返回本身。 2. 捕获和非捕获单纯说到捕获，他的意思是匹配表达式，但捕获通常和分组联系在一起，也就是“捕获组” 捕获组：匹配子表达式的内容，把匹配结果保存到内存中中数字编号或显示命名的组里，以深度优先进行编号，之后可以通过序号或名称来使用这些匹配结果。 而根据命名方式的不同，又可以分为两种组： \1. 数字编号捕获组：语法：(exp)解释：从表达式左侧开始，每出现一个左括号和它对应的右括号之间的内容为一个分组，在分组中，第0组为整个表达式，第一组开始为分组。比如固定电话的：020-85653333他的正则表达式为：(0\d{2})-(\d{8})按照左括号的顺序，这个表达式有如下分组： 我们用Java来验证一下： 12345678910 1 String test = "020-85653333"; 2 String reg="(0\\d&#123;2&#125;)-(\\d&#123;8&#125;)"; 3 Pattern pattern = Pattern.compile(reg); 4 Matcher mc= pattern.matcher(test); 5 if(mc.find())&#123; 6 System.out.println("分组的个数有："+mc.groupCount()); 7 for(int i=0;i&lt;=mc.groupCount();i++)&#123; 8 System.out.println("第"+i+"个分组为："+mc.group(i)); 9 &#125;10 &#125; 输出结果： 12341 分组的个数有：22 第0个分组为：020-856533333 第1个分组为：0204 第2个分组为：85653333 可见，分组个数是2，但是因为第0个为整个表达式本身，因此也一起输出了。 \2. 命名编号捕获组：语法：(?exp)解释：分组的命名由表达式中的name指定比如区号也可以这样写:(?\0\d{2})-(?\d{8})按照左括号的顺序，这个表达式有如下分组： 用代码来验证一下： 1234567891 String test = "020-85653333";2 String reg="(?&lt;quhao&gt;0\\d&#123;2&#125;)-(?&lt;haoma&gt;\\d&#123;8&#125;)";3 Pattern pattern = Pattern.compile(reg);4 Matcher mc= pattern.matcher(test);5 if(mc.find())&#123;6 System.out.println("分组的个数有："+mc.groupCount());7 System.out.println(mc.group("quhao"));8 System.out.println(mc.group("haoma"));9 &#125; 输出结果： 1231 分组的个数有：22 分组名称为:quhao,匹配内容为：0203 分组名称为:haoma,匹配内容为：85653333 \3. 非捕获组：语法：(?:exp)解释：和捕获组刚好相反，它用来标识那些不需要捕获的分组，说的通俗一点，就是你可以根据需要去保存你的分组。 比如上面的正则表达式，程序不需要用到第一个分组，那就可以这样写： 11 (?:\0\d&#123;2&#125;)-(\d&#123;8&#125;) 验证一下： 12345678910 1 String test = "020-85653333"; 2 String reg="(?:0\\d&#123;2&#125;)-(\\d&#123;8&#125;)"; 3 Pattern pattern = Pattern.compile(reg); 4 Matcher mc= pattern.matcher(test); 5 if(mc.find())&#123; 6 System.out.println("分组的个数有："+mc.groupCount()); 7 for(int i=0;i&lt;=mc.groupCount();i++)&#123; 8 System.out.println("第"+i+"个分组为："+mc.group(i)); 9 &#125;10 &#125; 输出结果： 1231 分组的个数有：12 第0个分组为：020-856533333 第1个分组为：85653333 3. 反向引用上面讲到捕获，我们知道：捕获会返回一个捕获组，这个分组是保存在内存中，不仅可以在正则表达式外部通过程序进行引用，也可以在正则表达式内部进行引用，这种引用方式就是反向引用。 根据捕获组的命名规则，反向引用可分为： 数字编号组反向引用：\k或\number 命名编号组反向引用：\k或者\’name’ 好了 讲完了，懂吗？不懂！！！可能连前面讲的捕获有什么用都还不懂吧？其实只是看完捕获不懂不会用是很正常的！因为捕获组通常是和反向引用一起使用的 上面说到捕获组是匹配子表达式的内容按序号或者命名保存起来以便使用注意两个字眼：“内容” 和 “使用”这里所说的“内容”，是匹配结果，而不是子表达式本身，强调这个有什么用？嗯，先记住那这里所说的“使用”是怎样使用呢？ 因为它的作用主要是用来查找一些重复的内容或者做替换指定字符。 还是举栗子吧：比如要查找一串字母”aabbbbgbddesddfiid”里成对的字母如果按照我们之前学到的正则，什么区间啊限定啊断言啊可能是办不到的，现在我们先用程序思维理一下思路： 1）匹配到一个字母 2）匹配第下一个字母，检查是否和上一个字母是否一样 3）如果一样，则匹配成功，否则失败 这里的思路2中匹配下一个字母时，需要用到上一个字母，那怎么记住上一个字母呢？？？这下子捕获就有用处啦，我们可以利用捕获把上一个匹配成功的内容用来作为本次匹配的条件好了，有思路就要实践首先匹配一个字母：\w我们需要做成分组才能捕获，因此写成这样：(\w) 那这个表达式就有一个捕获组：（\w）然后我们要用这个捕获组作为条件，那就可以：(\w)\1这样就大功告成了可能有人不明白了，\1是什么意思呢？还记得捕获组有两种命名方式吗，一种是是根据捕获分组顺序命名，一种是自定义命名来作为捕获组的命名在默认情况下都是以数字来命名，而且数字命名的顺序是从1开始的\因此要引用第一个捕获组，根据反向引用的数字命名规则 就需要 \k或者\1当然，通常都是是后者。我们来测试一下： 12345671 String test = "aabbbbgbddesddfiid";2 Pattern pattern = Pattern.compile("(\\w)\\1");3 Matcher mc= pattern.matcher(test);4 while(mc.find())&#123;5 System.out.println(mc.group());67 &#125; 输出结果： 1234561 aa2 bb3 bb4 dd5 dd6 ii 嗯，这就是我们想要的了。 在举个替换的例子，假如想要把字符串中abc换成a 1231 String test = "abcbbabcbcgbddesddfiid";2 String reg="(a)(b)c";3 System.out.println(test.replaceAll(reg, "$1"));; 输出结果： 11 abbabcgbddesddfiid 4. 贪婪和非贪婪1.贪婪 我们都知道，贪婪就是不满足，尽可能多的要。在正则中，贪婪也是差不多的意思: 贪婪匹配：当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能多的字符，这匹配方式叫做贪婪匹配。特性：一次性读入整个字符串进行匹配，每当不匹配就舍弃最右边一个字符，继续匹配，依次匹配和舍弃（这种匹配-舍弃的方式也叫做回溯），直到匹配成功或者把整个字符串舍弃完为止，因此它是一种最大化的数据返回，能多不会少。 前面我们讲过重复限定符，其实这些限定符就是贪婪量词，比如表达式： 11 \d&#123;3,6&#125; 用来匹配3到6位数字，在这种情况下，它是一种贪婪模式的匹配，也就是假如字符串里有6个个数字可以匹配，那它就是全部匹配到。如 1234567891 String reg="\\d&#123;3,6&#125;"; 2 String test="61762828 176 2991 871";3 System.out.println("文本："+test);4 System.out.println("贪婪模式："+reg);5 Pattern p1 =Pattern.compile(reg);6 Matcher m1 = p1.matcher(test);7 while(m1.find())&#123;8 System.out.println("匹配结果："+m1.group(0));9 &#125; 输出结果： 1234561 文本：61762828 176 2991 44 8712 贪婪模式：\d&#123;3,6&#125;3 匹配结果：6176284 匹配结果：1765 匹配结果：29916 匹配结果：871 由结果可见：本来字符串中的“61762828”这一段，其实只需要出现3个（617）就已经匹配成功了的，但是他并不满足，而是匹配到了最大能匹配的字符，也就是6个。一个量词就如此贪婪了，那有人会问，如果多个贪婪量词凑在一起，那他们是如何支配自己的匹配权的呢？ 是这样的，多个贪婪在一起时，如果字符串能满足他们各自最大程度的匹配时，就互不干扰，但如果不能满足时，会根据深度优先原则，也就是从左到右的每一个贪婪量词，优先最大数量的满足，剩余再分配下一个量词匹配。 1234567891 String reg="(\\d&#123;1,2&#125;)(\\d&#123;3,4&#125;)"; 2 String test="61762828 176 2991 87321";3 System.out.println("文本："+test);4 System.out.println("贪婪模式："+reg);5 Pattern p1 =Pattern.compile(reg);6 Matcher m1 = p1.matcher(test);7 while(m1.find())&#123;8 System.out.println("匹配结果："+m1.group(0));9 &#125; 输出结果： 123451 文本：61762828 176 2991 873212 贪婪模式：(\d&#123;1,2&#125;)(\d&#123;3,4&#125;)3 匹配结果：6176284 匹配结果：29915 匹配结果：87321 “617628” 是前面的\d{1,2}匹配出了61，后面的匹配出了7628 “2991” 是前面的\d{1,2}匹配出了29 ，后面的匹配出了91 “87321”是前面的\d{1,2}匹配出了87，后面的匹配出了321 2. 懒惰（非贪婪） 懒惰匹配：当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能少的字符，这匹配方式叫做懒惰匹配。特性：从左到右，从字符串的最左边开始匹配，每次试图不读入字符匹配，匹配成功，则完成匹配，否则读入一个字符再匹配，依此循环（读入字符、匹配）直到匹配成功或者把字符串的字符匹配完为止。 懒惰量词是在贪婪量词后面加个“？” 1234567891 String reg="(\\d&#123;1,2&#125;?)(\\d&#123;3,4&#125;)"; 2 String test="61762828 176 2991 87321";3 System.out.println("文本："+test);4 System.out.println("贪婪模式："+reg);5 Pattern p1 =Pattern.compile(reg);6 Matcher m1 = p1.matcher(test);7 while(m1.find())&#123;8 System.out.println("匹配结果："+m1.group(0));9 &#125; 输出结果： 123451 文本：61762828 176 2991 873212 贪婪模式：(\d&#123;1,2&#125;?)(\d&#123;3,4&#125;)3 匹配结果：617624 匹配结果：29915 匹配结果：87321 解答： “61762” 是左边的懒惰匹配出6，右边的贪婪匹配出1762“2991” 是左边的懒惰匹配出2，右边的贪婪匹配出991“87321” 左边的懒惰匹配出8，右边的贪婪匹配出7321 5. 反义前面说到元字符的都是要匹配什么什么，当然如果你想反着来，不想匹配某些字符，正则也提供了一些常用的反义元字符： 正则进阶知识就讲到这里，正则是一门博大精深的语言，其实学会它的一些语法和知识点还算不太难，但想要做到真正学以致用能写出非常6的正则，还有很远的距离，只有真正对它感兴趣的，并且经常研究和使用它，才会渐渐的理解它的博大精深之处，我就带你们走到这，剩下的，靠自己啦。 作者：老刘链接：https://www.zhihu.com/question/48219401/answer/742444326来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux netcat 命令——网络工具中的瑞士军刀]]></title>
    <url>%2F%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7%E4%B8%AD%E7%9A%84%E7%91%9E%E5%A3%AB%E5%86%9B%E5%88%80.html</url>
    <content type="text"><![CDATA[netcat是网络工具中的瑞士军刀，它能通过TCP和UDP在网络中读写数据。通过与其他工具结合和重定向，你可以在脚本中以多种方式使用它。使用netcat命令所能完成的事情令人惊讶。 netcat所做的就是在两台电脑之间建立链接并返回两个数据流，在这之后所能做的事就看你的想像力了。你能建立一个服务器，传输文件，与朋友聊天，传输流媒体或者用它作为其它协议的独立客户端。 下面是一些使用netcat的例子. [A(172.31.100.7) B(172.31.100.23)] Linux netcat 命令实例： 1，端口扫描端口扫描经常被系统管理员和黑客用来发现在一些机器上开放端口，帮助他们识别系统中的漏洞。 1$nc -z -v -n 172.31.100.7 21-25 可以运行在TCP或者UDP模式，默认是TCP，-u参数调整为udp. z 参数告诉netcat使用0 IO,指的是一旦连接关闭，不进行数据交换(译者注：这里翻译不准，如有其它更好的，请指出) v 参数指使用冗余选项（译者注：即详细输出） n 参数告诉netcat 不要使用DNS反向查询IP地址的域名 这个命令会打印21到25 所有开放的端口。Banner是一个文本，Banner是一个你连接的服务发送给你的文本信息。当你试图鉴别漏洞或者服务的类型和版本的时候，Banner信息是非常有用的。但是，并不是所有的服务都会发送banner。 一旦你发现开放的端口，你可以容易的使用netcat 连接服务抓取他们的banner。 1$ nc -v 172.31.100.7 21 netcat 命令会连接开放端口21并且打印运行在这个端口上服务的banner信息。 Chat Server假如你想和你的朋友聊聊，有很多的软件和信息服务可以供你使用。但是，如果你没有这么奢侈的配置，比如你在计算机实验室，所有的对外的连接都是被限制的，你怎样和整天坐在隔壁房间的朋友沟通那？不要郁闷了，netcat提供了这样一种方法，你只需要创建一个Chat服务器，一个预先确定好的端口，这样子他就可以联系到你了。 Server 1$nc -l 1567 netcat 命令在1567端口启动了一个tcp 服务器，所有的标准输出和输入会输出到该端口。输出和输入都在此shell中展示。 Client 1$nc 172.31.100.7 1567 不管你在机器B上键入什么都会出现在机器A上。 3，文件传输大部分时间中，我们都在试图通过网络或者其他工具传输文件。有很多种方法，比如FTP,SCP,SMB等等，但是当你只是需要临时或者一次传输文件，真的值得浪费时间来安装配置一个软件到你的机器上嘛。假设，你想要传一个文件file.txt 从A 到B。A或者B都可以作为服务器或者客户端，以下，让A作为服务器，B为客户端。 Server 1$nc -l 1567 &lt; file.txt Client 1$nc -n 172.31.100.7 1567 &gt; file.txt 这里我们创建了一个服务器在A上并且重定向netcat的输入为文件file.txt，那么当任何成功连接到该端口，netcat会发送file的文件内容。 在客户端我们重定向输出到file.txt，当B连接到A，A发送文件内容，B保存文件内容到file.txt. 没有必要创建文件源作为Server，我们也可以相反的方法使用。像下面的我们发送文件从B到A，但是服务器创建在A上，这次我们仅需要重定向netcat的输出并且重定向B的输入文件。 B作为Server Server 1$nc -l 1567 &gt; file.txt Client 1nc 172.31.100.23 1567 &lt; file.txt 4，目录传输发送一个文件很简单，但是如果我们想要发送多个文件，或者整个目录，一样很简单，只需要使用压缩工具tar，压缩后发送压缩包。 如果你想要通过网络传输一个目录从A到B。 Server 1$tar -cvf – dir_name | nc -l 1567 Client 1$nc -n 172.31.100.7 1567 | tar -xvf - 这里在A服务器上，我们创建一个tar归档包并且通过-在控制台重定向它，然后使用管道，重定向给netcat，netcat可以通过网络发送它。 在客户端我们下载该压缩包通过netcat 管道然后打开文件。 如果想要节省带宽传输压缩包，我们可以使用bzip2或者其他工具压缩。 Server 1$tar -cvf – dir_name| bzip2 -z | nc -l 1567 通过bzip2压缩 Client 1$nc -n 172.31.100.7 1567 | bzip2 -d |tar -xvf - 使用bzip2解压 5. 加密你通过网络发送的数据如果你担心你在网络上发送数据的安全，你可以在发送你的数据之前用如mcrypt的工具加密。 服务端 1$nc localhost 1567 | mcrypt –flush –bare -F -q -d -m ecb &gt; file.txt 使用mcrypt工具加密数据。 客户端 1$mcrypt –flush –bare -F -q -m ecb &lt; file.txt | nc -l 1567 使用mcrypt工具解密数据。 以上两个命令会提示需要密码，确保两端使用相同的密码。 这里我们是使用mcrypt用来加密，使用其它任意加密工具都可以。 6. 流视频虽然不是生成流视频的最好方法，但如果服务器上没有特定的工具，使用netcat，我们仍然有希望做成这件事。 服务端 1$cat video.avi | nc -l 1567 这里我们只是从一个视频文件中读入并重定向输出到netcat客户端 1$nc 172.31.100.7 1567 | mplayer -vo x11 -cache 3000 - 这里我们从socket中读入数据并重定向到mplayer。 7，克隆一个设备如果你已经安装配置一台Linux机器并且需要重复同样的操作对其他的机器，而你不想在重复配置一遍。不在需要重复配置安装的过程，只启动另一台机器的一些引导可以随身碟和克隆你的机器。 克隆Linux PC很简单，假如你的系统在磁盘/dev/sda上 Server 1$dd if=/dev/sda | nc -l 1567 Client 1$nc -n 172.31.100.7 1567 | dd of=/dev/sda dd是一个从磁盘读取原始数据的工具，我通过netcat服务器重定向它的输出流到其他机器并且写入到磁盘中，它会随着分区表拷贝所有的信息。但是如果我们已经做过分区并且只需要克隆root分区，我们可以根据我们系统root分区的位置，更改sda 为sda1，sda2.等等。 8，打开一个shell我们已经用过远程shell-使用telnet和ssh，但是如果这两个命令没有安装并且我们没有权限安装他们，我们也可以使用netcat创建远程shell。 假设你的netcat支持 -c -e 参数(默认 netcat) Server 1$nc -l 1567 -e /bin/bash -i Client 1$nc 172.31.100.7 1567 这里我们已经创建了一个netcat服务器并且表示当它连接成功时执行/bin/bash 假如netcat 不支持-c 或者 -e 参数（openbsd netcat）,我们仍然能够创建远程shell Server 12$mkfifo /tmp/tmp_fifo$cat /tmp/tmp_fifo | /bin/sh -i 2&gt;&amp;1 | nc -l 1567 &gt; /tmp/tmp_fifo 这里我们创建了一个fifo文件，然后使用管道命令把这个fifo文件内容定向到shell 2&gt;&amp;1中。是用来重定向标准错误输出和标准输出，然后管道到netcat 运行的端口1567上。至此，我们已经把netcat的输出重定向到fifo文件中。 说明： 从网络收到的输入写到fifo文件中 cat 命令读取fifo文件并且其内容发送给sh命令 sh命令进程受到输入并把它写回到netcat。 netcat 通过网络发送输出到client 至于为什么会成功是因为管道使命令平行执行，fifo文件用来替代正常文件，因为fifo使读取等待而如果是一个普通文件，cat命令会尽快结束并开始读取空文件。 在客户端仅仅简单连接到服务器 Client 1$nc -n 172.31.100.7 1567 你会得到一个shell提示符在客户端 反向shell反向shell是人曾经在客户端打开的shell。反向shell这样命名是因为不同于其他配置，这里服务器使用的是由客户提供的服务。 服务端 1$nc -l 1567 在客户端，简单地告诉netcat在连接完成后，执行shell。 客户端 1$nc 172.31.100.7 1567 -e /bin/bash 现在，什么是反向shell的特别之处呢反向shell经常被用来绕过防火墙的限制，如阻止入站连接。例如，我有一个专用IP地址为172.31.100.7，我使用代理服务器连接到外部网络。如果我想从网络外部访问 这台机器如1.2.3.4的shell，那么我会用反向外壳用于这一目的。 10. 指定源端口假设你的防火墙过滤除25端口外其它所有端口，你需要使用-p选项指定源端口。 服务器端 1$nc -l 1567 客户端 1$nc 172.31.100.7 1567 -p 25 使用1024以内的端口需要root权限。 该命令将在客户端开启25端口用于通讯，否则将使用随机端口。 11. 指定源地址假设你的机器有多个地址，希望明确指定使用哪个地址用于外部数据通讯。我们可以在netcat中使用-s选项指定ip地址。 服务器端 1$nc -u -l 1567 &lt; file.txt 客户端 1$nc -u 172.31.100.7 1567 -s 172.31.100.5 &gt; file.txt 该命令将绑定地址172.31.100.5。 这仅仅是使用netcat的一些示例。 其它用途有： 使用-t选项模拟Telnet客户端， HTTP客户端用于下载文件， 连接到邮件服务器，使用SMTP协议检查邮件， 使用ffmpeg截取屏幕并通过流式传输分享，等等。其它更多用途。 简单来说，只要你了解协议就可以使用netcat作为网络通讯媒介，实现各种客户端。 原文地址：http://www.oschina.net/translate/linux-netcat-command]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>netstat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应对Hadoop集群数据疯长，这里祭出了4个治理对策！]]></title>
    <url>%2F%E5%BA%94%E5%AF%B9Hadoop%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E7%96%AF%E9%95%BF%EF%BC%8C%E8%BF%99%E9%87%8C%E7%A5%AD%E5%87%BA%E4%BA%864%E4%B8%AA%E6%B2%BB%E7%90%86%E5%AF%B9%E7%AD%96%EF%BC%81.html</url>
    <content type="text"><![CDATA[一、背景 在目前规模比较大的互联网公司中，总数据量能达到10PB甚至几十PB数据量的公司，我认为中国已经有超过了20家了。而在这些公司中，也有很多家公司的日数据增长达到100TB+了。 所以我们每天都要观察集群的数据增长，观察是否有哪一天、哪个路径增长过猛了，是否增长了很多垃圾数据；继续深挖下去，看看是不是可以删掉无用的数据。 此外我们还要做“容量预估“，把未来的数据增长规划出来，主要是依靠数据增长斜率计算出未来一个季度后的数据量，再把机器采购需求汇报出去。 在上一篇《基于FsImage的HDFS数据深度分析》（https://zhuanlan.zhihu.com/p/32203951）中，我们创建了基于Fsimage的HDFS数据分析仓库，并创建了一些分析图表，比如“HDFS增长趋势图”，充分地解决了发现“数据增长异常”的问题。 今天，我们会探讨以下4个问题： 怎样观察“数据增长” 怎样治理“数据过快增长” 怎样清理“无用的冷数据” 怎样管理“数据存活时间” HDFS增长趋势图 先来算一笔账：当下，一台不错的Datanode机器配置，能挂12-16块盘，每块盘挂上比较大的3TB的硬盘。单台机器的存储量大致在50TB。若按每天增长100TB算，就需要2台机器；按每天增长500TB，则是10台机器。这个数字实在是Terrible ！ 数据疯狂增长带来的问题 1、加机器对公司的财务预算要求很高 服务器再便宜，也是钱。以一周买几十台服务器这种速度来看，即便是财务运转再好的IT公司也不愿意看到数据增长失控。 2、对集群负载的压力 Hadoop是一个可以水平扩展的技术栈，且大多数分布式系统也都是把“水平Scalable”作为主要的功能点设计， 但在工程中，是否真的能做到“无限水平扩展”呢？ 首先Hadoop中有一些“Master节点”，这些 Master 节点要实时地和所有“ Slave节点 ”保持心跳通信。在集群规模较小时，由于“心跳”只做简单的网络通信，且所有的 Datanode 都互相错峰汇报“心跳”， 所以网络元数据交换并不是 Hadoop 系统水平扩张的瓶颈。 但在Hadoop集群的规模达到了大几千甚至上万台后，网络就是Namenode的瓶颈。这些心跳的RPC请求会和“用户Client”的RPC请求一起抢占Namenode有限的CPU资源。 3、对运维团队的运维压力 运维团队每周／月都要安装新机器到Hadoop集群里。做这些事情是重复又无聊的，即使自动化做得再好，也需要人来处理某些环节。脏活累活对追求“高技术密集度”的精英工程师团队，是有危害的。 那为什么我们会有这么多数据增长问题？直接删掉无用的数据不就行了吗？这个事情在公司内部很难做吗？ 为什么这个问题在公司难做？ 对于数据的增长，Hadoop Admin应该要对此负责的，但很多公司并没有做好这件事情，原因如下： 1、一些公司在自己的数据量级并不是很大的时候，不愿意重视这个问题。对他们而言，与其请2个人去做这个事情，花掉半年时间， 不如先把钱拿来买机器，这样的情形大多发生在 B轮／C轮的公司里。 业务增长是主要矛盾。数据每天增长5T，两个程序员半年的成本确实大于买一些机器先解燃眉之急。 等到公司业务越来越大，问题暴露得越来越严重时，公司才开始意识到严重性，这往往已经晚了，毕竟建立整套的HDFS分析系统和报表系统也不是一时半会就能搞定的。 2、受限于管理上的问题。在公司里，“业务事业部“和 “基础架构部”是平级的，那作为“基础架构部”的普通员工，哪怕是“基础架构部”的领导，都很难推动其它“业务部门”去Clean他们的数据存储。比如： 清一下没用的冷数据 给用户行为日志加一些 Data Retention策略 “业务部门”总认为自己的核心任务就是业务开发，能为公司产生更大的利益，因此在做数据清理的任务时，总把排期靠后或是设定为低优先级，总有“干不完”的开发任务，所以清理数据在公司内部很难推动。 3、Hadoop Admin 能拿出足够的证据，让“业务部门”删除冷数据吗？实际上，“业务部门”通常会这样搪塞： /path/a 到底最后访问的时间是什么时候，凭什么说没人用了？ /path/b 有100TB，可我都是有用的数据，别人也这么大，为什么不删别人的？ 你总让“我们部门”删数据，我们到底用了多少存储空间？别的组如果比我们更多呢？ 带着上面这三个问题，继续往下看。 第一个问题似乎是一个很难避免的问题，需要CTO有掌舵的能力，那笔者则希望有志于利用起Hadoop技术栈的中小公司CTO在看了这篇文章后，都能增加这个意识。 第二个问题是管理上的问题。一般牵扯到制度上的变革，最好是要有Involve更高层领导参与的。多和高层提“成本”和“省钱”，少提“技术”，我认为高层会意识到这个问题的价值。 第三个问题是本文的重点，即如何摆事实、拿证据证明我们可以针对Path做数据优化呢？ 哪些Path可以删掉？哪些Path应该加Data Retention 策略？ 二、行为（Action） 我们需要定期进行一些行为，来保持集群的数据可控。 每日行为 每天来到公司，做这样几件事： 集群增长常规日分析 当集群“日增长有异样”时，分析具体哪个Path增长占主导 发现“异常路径“属于哪个User或Team 发送邮件给对应User／Team，给出Solution 季度行为 每个季度结束时 ： 哪个Team增长最猛，统计Team日增长平均量 找到“环比”增长最猛的Team，找到本季度“新增数据最猛”的新路径，一般为一些新Hive表 发邮件给对应Team，给出Solution 要做到上述行为，我们要对每个Team，每个Path的“数据增长”都有详尽的数据支持。试想一下，在理想情况下，我们需要有哪些数据才能搞定？ 针对“每日行为””，我们需要确定： 每天增长最大的文件是哪些？ 针对确定的“异常日增长路径”，能查到这个路径的历史数据增长，因为要清楚“平均增长值”，才能看出“某日增长量为异常”，然后再查到其下哪个子路径贡献了最大的增长，进一步深入查找问题。 针对“季度行为”，我们需要： 1、所有“数据团队”对集群存储的使用情况，按HDFS的使用量做KPI考核；不仅了解每一个“数据团队”都有哪些“重要路径”，还需要知道这些路径的“增长状况怎么样”。 TeamA 一共使用的存储空间，占公司总量有多少？ TeamA 过去一个季度的环比增长速度如何？ TeamA 过去一个季度的绝对增长量如何？ TeamA 下的路径里，是否新建了很多新数据，比如新Hive表？是否有 Data Retention策略？ 2、针对一个Team新增的“异常增长路径”，我们要能查到这个路径的历史数据增长，要知道“平均增长值”，才能看出“某日增长量为异常”，然后查到其下哪个子路径贡献了最大的增长，进一步深入查找问题。 3、针对“某些”很大的、Size很久没有变化过的Folder，我们要知道这个Folder最后的访问时间是什么时候、它超过半年没访问过的文件占比有多少、超过1年没访问过的文件有多少，然后我们才能和所属的Team联系，优先决定是否能删除它。 在前文《基于FsImage的HDFS数据深度分析》（https://zhuanlan.zhihu.com/p/32203951）中 ，我们建立了HDFS数据仓库，这相当于我们存下了HDFS每一天的快照，所以每一条Path的元数据历史问题解决了。 再来说说Team Level，每一个Team的数据，都是由一些文件夹下的数据组成的。比如“推荐系统团队”，在/hive/warehouse/reco.db下，所有的推荐相关的表数据都存在于这个下面。另外/user/reco下也存放了很多这个组的数据，这几个路径，都属于“推荐数据组”的“顶级路径”。 所有“顶级路径”的“增长聚合”，就是整个组的“数据增长”。 数据组的顶级路径 每个Folder聚合其下每一个文件的Last_access_time，作为最终这个Folder的Last_access_time. 这个功能对Hive表非常好用。 有些Hive表很久都没有人访问过，后面我也会详细叙述如何清理Hive表。 三、例子 接下来我将用我司的自动化运维系统中的一些报表来做解决问题的展示。 这些报表都是我们根据解决问题的方法论创建出来的，我们希望贯彻“让一切人的决策基于数据”这一宗旨。让我们判断问题、找到问题，甚至说服“数据团队”，都用Data Driven。 每日行为例子 1、查看集群每日增长，发现没什么大问题。 2、查看增长贡献，发现几个/User下的用户增长过猛。 3、查看这个路径，与本路径历史增长做比较，发现昨日确实是在不正常地增长。 4、分析具体是什么子文件导致了这个目录的异常增长。 原来是这个用户删除了一些其它路径的大文件，划归到User目录自己的~/.Trash下了。那这就不用太担心，因为HDFS第二天会自动清理掉~/.Trash下的垃圾文件。 季度行为例子 1、分析“数据团队”季度增长量 可以看到： TeamA的数据总量很大，环比增长也很大，是首要的分析目标。 TeamB 和 TeamC 相比，虽然TeamB绝对值增量比TeamC大了很多，但还是一个数量级，但TeamC环比增速太高，很可能业务上发生了很大的变化，所以 TeamC是第二目标。 在运维人员有限的工作时间内，一定要把“精力”花在刀刃上。对一个Team的数据进行深度分析，往往要用去个把小时，一定要在单位时间上产出最大化。 2、深度分析Team数据 深度分析也是遵循“单位时间产出最大化，抓最主要矛盾”这一思想。接下来还是拿我司的“推荐“团队做例子： 这些所有的顶级路径，都代表了某种业务的“细分”顶级路径。 在每个细分“顶级路径”下，我们要观察： 哪些路径的“绝对数据量”很大，一头大象体重增长10%比一只老鼠多生一窝产生的体重多得多； 所有“第二档次数据贡献量”的路径，分别调查其“日增长量”和“环比增速”，即“增速”的相对值和绝对值。 还是拿Recoteam数据来举例子： 根据数据统计，我们分出第一轮目标和第二轮目标。 第一个路径： hdfs://beaconstore/user/hadoop/reco/report 它的特点是每日增速固定， 但最近访问时间“很新”，且“平均文件大小”偏小。 所以策略可能是 ： “每日数据量优化”； “减少天分区数”； 未来对“文件平均大小”做优化。因为文件数量很多，可以节省出很多内存。 第二个路径： hdfs://beaconstore/user/hadoop/reco/report 它的特点是已经许久不新增数据， 但最近访问时间“很新”。 所以策略可能是 ： 找到哪些子数据经常访问； 删掉不访问的子数据 ； 是否有生命周期，有的话记得在未来删除。 第三个路径： hdfs://beaconstore/user/hadoop/reco/online_log 它的特点是很久很久不新增数据， 且最近访问时间“很老”。 策略是——建议删除。 可以看出，不同的HDFS路径，其存在的问题不尽相同，这真的需要具体问题具体分析。 如果通过分析“第一目标清单”，已经能够达到控制集群存储的目的，大幅降低数据存储，那么可以适当地忽略“第二目标清单”，记住那个目标“单位时间产出比”。这时可以把时间省下来做更多有意义的事情。 3、最后我们会出具一个Report，给相关的组发送Email，指明应该做哪些优化。 四、数据增长之Hive篇 前文在讲述治理HDFSS的数据增长问题时提到了： 每日独立“异常路径”数据增长治理 每季度数据增长过快的“异常数据Team”的深度数据治理 现在我们就把目标锁定到Hadoop的数据仓库Hive，谈谈数据增长之Hive。 方法论 笔者认为Hive的“数据增长治理”，也分为两点： 每日观察“新增Hive表”，查看“每日增速过快的”以及“总量过快的”。新增的Hive表，被限定在30天（一个月内）新创建的Hive表。Hive表的创建时间，在Hive-metastore的数据里可以得到。 每季度观察“冷Hive表”，重点抓“Size最大的，最冷的Hive表”。 找到可优化“目标Hive表后”，按照前文提及的步骤来优化Hive表背后的HDFS路径，一个控制增量，一个优化存量。 细节 1、控制增量 Hadoop管理员每天早上花时间扫一眼最近一个月新建的Hive表里有没有很大的表。 这里的“大”指的是： 30天总量达到10个TB。 这个很好理解,”月总量值”是可配的，可以随着业务增长放大。 “每天平均日增量”达到1个TB。 因为有些表的生命周期可能只有几天，日增量大的表，都要进入“待观察”列表，最好都能强制“被管控”（Data Retention 策略，TTL等） 2、优化存量 Hive表的底层数据，是存储在HDFS上的文件夹。我们可以通过使用SQL，从Hive－metastore这个MySQL数据库里，查询到Hive表的HDFS路径，Owner等元信息。 select TBL_NAME, location, owner, db.NAME from TBLS tb left join SDS s on tb.SD_ID=s.SD_ID left join DBS db on tb.DB_ID=db.DB_ID 在查找到Hive表 －》 HDFS路径的对应关系后，我们又可以根据前文 《基于FsImage的HDFS数据深度分析》所建立的HDFS文件系统数据仓库，查询到HDFS路径的“Last_access_time”，以及路径的Size，平均文件大小等元数据，这保证并确认了hive表的“最后访问时间”是可知的。 最后，Hive表的几项元信息，都会被缓存到另一张经过ETL后的的数据表Hive_meta_after_etl。这张表的结构如下： hive_meta_after_etl 有了Hive_meta_after_etl表元数据的数据库，我们就可以设计查询入口： 在选出了运维的目标Hive表后，按照前文中分析HDFS“异常路径”的方法，进行进一步分析即可。 架构 总结 总之，为了防患“无止境的数据增长”，公司最好每天都观察数据增长5分钟，并在每个季度Review每个数据Team的增长。 这里总结了一些通用的原则，供大家参考： 要明确谁占用的“资源多”，谁Cost的成本高，方便给CTO汇报。 要打通数据分析系统，在数据团队有疑问、甚至不配合工作时，给他们摆事实、讲道理。 要把不同数据团队的KPI做排名、做比较，让数据存储上做得差的团队有“羞耻感”。 在推动整体数据治理这件事时，有必要Involve更高级别的领导，甚至CTO。 要梳理清楚Team的顶级路径，严格规定路径的使用。承诺只有放在Team顶级路径下的文件是安全的，否则都可能在系统过载时被管理员删除。 在有限的时间内，让产出最大化，把精力花在最有价值的点上。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-聚类分析-出租车案例]]></title>
    <url>%2FSpark-%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90-%E5%87%BA%E7%A7%9F%E8%BD%A6%E6%A1%88%E4%BE%8B.html</url>
    <content type="text"><![CDATA[通过分析出租车数据，然后使用KMeans对经纬度进行聚类，然后按照（类别，时间）进行分类，再统计每个类别每个时段的次数。 数据地址 链接: https://pan.baidu.com/s/166dKRUpryHWZ2F8wLA3eyw 密码: g9dz 数据格式以及意义： 111,30.655325,104.072573,173749111,30.655346,104.072363,173828111,30.655377,104.120252,124057111,30.655439,104.088812,142016 列一：出租车ID 列二：经度 列三：纬度 列四：时间（例如：142016表示14点20分16秒） 步骤： 1.整理数据，分割成训练数据和测试数据，且使其符合KMeans模型训练的格式 2.使用训练好的模型对测试数据进行预测，然后对结果以（类别，小时时间 ）进行count统计，结果为每个类别每个小时的总次数。 12345678910111213141516171819202122232425262728293031323334353637import org.apache.spark.ml.clustering.KMeansimport org.apache.spark.ml.feature.VectorAssemblerimport org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.types.&#123;StructField, StructType&#125;import org.apache.spark.sql.types._import org.apache.spark.sql.functions._object Tax1 &#123; def main(arg:Array[String]):Unit=&#123; val spark = SparkSession.builder().appName("Taxi1").master("local[*]").getOrCreate()//为读取的数据创建schema val taxiSchema = StructType(Array( StructField("id",IntegerType,true), StructField("tid",DoubleType,true), StructField("lat",DoubleType,true), StructField("time",StringType,true) )) val path = "/home/enche/data/taxi.csv" val data = spark.read.schema(taxiSchema).csv(path) //将tid和lat转换成训练使用的Vector类型 val assembler = new VectorAssembler() val tid_lat = Array("tid","lat") assembler.setInputCols(tid_lat).setOutputCol("feature").transform(data) //按照8：2的比例随即分割数据，分别用于训练和测试 val splitRait = Array(0.8, 0.2) val Array(train, test) = data.randomSplit(splitRait) //建立Kmeans，设置类别数为10 val kmeans = new KMeans().setK(10).setFeaturesCol("feature").setPredictionCol("prediction") //模型训练 val model = kmeans.fit(train) //使用模型预测 测试数据 val testResult = model.transform(test) //导入隐式转换，不然$"time"会出现错误 $ not e member of StringContext import spark.implicits._ val time_prediction = testResult.select(substring($"time", 0, 2).alias("hour"), $"prediction") time_prediction.groupBy("hour","prediction").agg(("prediction","count")).orderBy(desc("count")).filter(row=&gt;row.getAs(0)==15).take(10) &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark项目智慧城市车流量分析项目之固定卡口下车辆的行车轨迹]]></title>
    <url>%2FSpark%E9%A1%B9%E7%9B%AE%E6%99%BA%E6%85%A7%E5%9F%8E%E5%B8%82%E8%BD%A6%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%9B%BA%E5%AE%9A%E5%8D%A1%E5%8F%A3%E4%B8%8B%E8%BD%A6%E8%BE%86%E7%9A%84%E8%A1%8C%E8%BD%A6%E8%BD%A8%E8%BF%B9.html</url>
    <content type="text"><![CDATA[数据集 日期 卡口ID 摄像头编号 车牌号 拍摄时间 date monitor_id camera_id car action_time车速 道路ID 区域IDspeed road_id area_id 模拟数据 2018-06-27 0007 00536 京R66884 2018-06-27 11:30:25 30 41 082018-06-27 0005 01726 闵P89564 2018-06-27 09:34:03 19 7 032018-06-27 0005 01272 闵P89564 2018-06-27 09:50:39 187 19 052018-06-27 0002 00082 闵P89564 2018-06-27 09:34:47 1 28 052018-06-27 0003 08417 闵P89564 2018-06-27 09:23:05 171 42 022018-06-27 0003 02757 闵P89564 2018-06-27 09:52:35 32 50 042018-06-27 0000 03759 沪W87972 2018-06-27 20:02:43 243 25 052018-06-27 0002 08652 沪W87972 2018-06-27 20:05:10 51 11 05 代码逻辑 本地模拟代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108package test;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.Function;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.api.java.function.VoidFunction;import org.apache.spark.broadcast.Broadcast;import scala.Tuple2;import java.util.*;public class Carlearn &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf(); conf.setMaster("local").setAppName("Carlearn"); JavaSparkContext sc = new JavaSparkContext(conf); JavaRDD&lt;String&gt; lineRDD = sc.textFile("monitor_flow_action"); //mapToPair 形成一个tuple&lt;卡扣id,车牌号&gt; JavaPairRDD&lt;String, String&gt; rdd2 = lineRDD.mapToPair(new PairFunction&lt;String, String, String&gt;() &#123; @Override public Tuple2&lt;String, String&gt; call(String s) throws Exception &#123; String[] split = s.split("\t"); return new Tuple2&lt;&gt;(split[1], split[3]); &#125; &#125;); //过滤只留下一个卡扣id为001的信息 JavaPairRDD&lt;String, String&gt; rdd3 = rdd2.filter(new Function&lt;Tuple2&lt;String, String&gt;, Boolean&gt;() &#123; @Override public Boolean call(Tuple2&lt;String, String&gt; tuple2) throws Exception &#123; return tuple2._1.equals("0001"); &#125; &#125;); //map 形成一个个的car JavaRDD&lt;String&gt; rdd4 = rdd3.map(new Function&lt;Tuple2&lt;String, String&gt;, String&gt;() &#123; @Override public String call(Tuple2&lt;String, String&gt; tuple2) throws Exception &#123; return tuple2._2; &#125; &#125;); //去重 JavaRDD&lt;String&gt; rdd5 = rdd4.distinct(); //转换成集合 List&lt;String&gt; result = rdd5.collect(); //广播变量 final Broadcast&lt;List&lt;String&gt;&gt; broadcast = sc.broadcast(result); /** * mapToPair转换成Tuple&lt;车牌号,一行的值&gt; */ JavaPairRDD&lt;String, String&gt; rddA = lineRDD.mapToPair(new PairFunction&lt;String, String, String&gt;() &#123; @Override public Tuple2&lt;String, String&gt; call(String s) throws Exception &#123; String[] split = s.split("\t"); return new Tuple2&lt;&gt;(split[3], s); &#125; &#125;); //过滤掉只留下卡扣id为001信息的车, JavaPairRDD&lt;String, Iterable&lt;String&gt;&gt; rddC = rddA.filter(new Function&lt;Tuple2&lt;String, String&gt;, Boolean&gt;() &#123; @Override public Boolean call(Tuple2&lt;String, String&gt; tuple2) throws Exception &#123; List&lt;String&gt; carList = broadcast.value(); String carId = tuple2._2.split("\t")[3]; return carList.contains(carId); &#125; &#125;).groupByKey(); rddC.foreach(new VoidFunction&lt;Tuple2&lt;String, Iterable&lt;String&gt;&gt;&gt;() &#123; @Override public void call(Tuple2&lt;String, Iterable&lt;String&gt;&gt; tuple2) throws Exception &#123; Iterator&lt;String&gt; iterator = tuple2._2.iterator(); List&lt;Tuple2&lt;String, String&gt;&gt; map = new ArrayList&lt;&gt;(); //将时间和卡扣号封装成一个list while (iterator.hasNext()) &#123; String next = iterator.next(); String time = next.split("\t")[4]; String kakouId = next.split("\t")[1]; Tuple2&lt;String, String&gt; time_kakou = new Tuple2&lt;&gt;(time, kakouId); map.add(time_kakou); &#125; //通过时间排序,整理出汽车的行车轨迹 Collections.sort(map, new Comparator&lt;Tuple2&lt;String, String&gt;&gt;() &#123; @Override public int compare(Tuple2&lt;String, String&gt; o1, Tuple2&lt;String, String&gt; o2) &#123; return o1._1.compareTo(o2._1); &#125; &#125;); //打印下行车轨迹 String chepai = tuple2._1; System.out.println("汽车号:"+chepai); for (Tuple2&lt;String, String&gt; s : map) &#123; System.out.print("时间:"+s._1+"卡扣:"+s._2+"===&gt;"); &#125; System.out.println(); &#125; &#125;); &#125;&#125; 结果 06-27 22:39:26卡扣:0005===&gt;时间:2018-06-27 22:40:31卡扣:0008===&gt;时间:2018-06-27 22:41:37卡扣:0005===&gt;时间:2018-06-27 22:42:02卡扣:0000===&gt;时间:2018-06-27 22:43:07卡扣:0002===&gt;时间:2018-06-27 22:44:28卡扣:0004===&gt;时间:2018-06-27 22:45:49卡扣:0001===&gt;时间:2018-06-27 22:51:40卡扣:0005===&gt;时间:2018-06-27 22:53:26卡扣:0008===&gt;时间:2018-06-27 22:54:06卡扣:0007===&gt;时间:2018-06-27 22:57:26卡扣:0004===&gt;时间:2018-06-27 22:57:56卡扣:0005===&gt;时间:2018-06-27 22:58:07卡扣:0006===&gt;时间:2018-06-27 23:00:40卡扣:0006===&gt;时间:2018-06-27 23:01:56卡扣:0008===&gt;时间:2018-06-27 23:04:27卡扣:0004===&gt;时间:2018-06-27 23:04:43卡扣:0003===&gt;时间:2018-06-27 23:05:38卡扣:0001===&gt;时间:2018-06-27 23:09:03卡扣:0007===&gt;时间:2018-06-27 23:09:06卡扣:0005===&gt;时间:2018-06-27 23:09:37卡扣:0004===&gt;时间:2018-06-27 23:11:11卡扣:0007===&gt;时间:2018-06-27 23:12:06卡扣:0001===&gt;时间:2018-06-27 23:12:12卡扣:0008===&gt;时间:2018-06-27 23:14:31卡扣:0002===&gt;时间:2018-06-27 23:16:25卡扣:0007===&gt;时间:2018-06-27 23:23:47卡扣:0000===&gt;时间:2018-06-27 23:24:03卡扣:0008===&gt;时间:2018-06-27 23:26:27卡扣:0003===&gt;时间:2018-06-27 23:29:44卡扣:0001===&gt;时间:2018-06-27 23:30:23卡扣:0003===&gt;时间:2018-06-27 23:33:27卡扣:0007===&gt;时间:2018-06-27 23:34:18卡扣:0002===&gt;时间:2018-06-27 23:34:28卡扣:0002===&gt;时间:2018-06-27 23:34:39卡扣:0001===&gt;时间:2018-06-27 23:39:59卡扣:0005===&gt;时间:2018-06-27 23:45:13卡扣:0001===&gt;时间:2018-06-27 23:48:47卡扣:0004===&gt;汽车号:京D44143时间:2018-06-27 02:00:01卡扣:0007===&gt;时间:2018-06-27 02:00:59卡扣:0001===&gt;时间:2018-06-27 02:02:11卡扣:0008===&gt;时间:2018-06-27 02:02:29卡扣:0001===&gt;时间:2018-06-27 02:02:31卡扣:0001===&gt;时间:2018-06-27 02:04:11卡扣:0005===&gt;时间:2018-06-27 02:06:20卡扣:0000===&gt;时间:2018-06-27 02:07:36卡扣:0007===&gt;时间:2018-06-27 02:14:36卡扣:0004===&gt;时间:2018-06-27 02:14:58卡扣:0005===&gt;时间:2018-06-27 02:15:01卡扣:0007===&gt;时间:2018-06-27 02:15:14卡扣:0003===&gt;时间:2018-06-27 02:17:16卡扣:0005===&gt;时间:2018-06-27 02:20:59卡扣:0001===&gt;时间:2018-06-27 02:21:26卡扣:0008===&gt;时间:2018-06-27 02:21:27卡扣:0000===&gt;时间:2018-06-27 02:22:19卡扣:0004===&gt;时间:2018-06-27 02:26:20卡扣:0007===&gt;时间:2018-06-27 02:35:02卡扣:0002===&gt;时间:2018-06-27 02:35:28卡扣:0006===&gt;时间:2018-06-27 02:37:23卡扣:0001===&gt;时间:2018-06-27 02:38:13卡扣:0005===&gt;时间:2018-06-27 02:38:33卡扣:0002===&gt;时间:2018-06-27 02:39:15卡扣:0000===&gt;时间:2018-06-27 02:39:42卡扣:0008===&gt;时间:2018-06-27 02:41:53卡扣:0001===&gt;时间:2018-06-27 02:46:54卡扣:0007===&gt;时间:2018-06-27 02:47:23卡扣:0008===&gt;时间:2018-06-27 02:52:27卡扣:0001===&gt;时间:2018-06-27 02:53:31卡扣:0006===&gt;时间:2018-06-27 03:08:55卡扣:0007===&gt;汽车号:京R34631时间:2018-06-27 14:01:24卡扣:0004===&gt;时间:2018-06-27 14:03:36卡扣:0005===&gt;时间:2018-06-27 14:05:04卡扣:0006===&gt;时间:2018-06-27 14:08:40卡扣:0007===&gt;时间:2018-06-27 14:09:29卡扣:0002===&gt;时间:2018-06-27 14:09:51卡扣:0008===&gt;时间:2018-06-27 14:13:40卡扣:0008===&gt;时间:2018-06-27 14:18:55卡扣:0005===&gt;时间:2018-06-27 14:26:54卡扣:0001===&gt;时间:2018-06-27 14:27:20卡扣:0000===&gt;时间:2018-06-27 14:34:02卡扣:0001===&gt;时间:2018-06-27 14:34:39卡扣:0006===&gt;时间:2018-06-27 14:36:10卡扣:0003===&gt;时间:2018-06-27 14:37:08卡扣:0006===&gt;时间:2018-06-27 14:37:30卡扣:0003===&gt;时间:2018-06-27 14:39:25卡扣:0005===&gt;时间:2018-06-27 14:41:00卡扣:0007===&gt;时间:2018-06-27 14:42:03卡扣:0001===&gt;时间:2018-06-27 14:47:36卡扣:0001===&gt;时间:2018-06-27 14:48:16卡扣:0003===&gt;时间:2018-06-27 14:53:22卡扣:0001===&gt;时间:2018-06-27 14:53:34卡扣:0005===&gt;时间:2018-06-27 14:53:54卡扣:0001===&gt;时间:2018-06-27 14:53:56卡扣:0002===&gt;时间:2018-06-27 14:54:14卡扣:0003===&gt;时间:2018-06-27 14:55:44卡扣:0003===&gt;时间:2018-06-27 14:57:27卡扣:0006===&gt;时间:2018-06-27 14:57:53卡扣:0000===&gt;时间:2018-06-27 14:58:12卡扣:0008===&gt;时间:2018-06-27 14:59:11卡扣:0008===&gt;汽车号:沪V82625 转载至]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于WIFI探针的商业大数据分析系统（hadoop+spark+hbase+bootstrap+echarts）]]></title>
    <url>%2F%E5%9F%BA%E4%BA%8EWIFI%E6%8E%A2%E9%92%88%E7%9A%84%E5%95%86%E4%B8%9A%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F%EF%BC%88hadoop-spark-hbase-bootstrap-echarts%EF%BC%89.html</url>
    <content type="text"><![CDATA[简介服务端主要接收探针每三秒发送一次的数据，于接收端搭建Tomcat纵向集群，有效处理1300台以上的并发请求，将数据保存到数据分析平台待用，文件系统使用HDFS分布式文件系统。数据分析平台搭建于Linux系统，采用Spark&amp;Habse的分布式搭建模式，后台数据分析程序在3秒之内分析完实时数据，传至前台可视化，实现实时数据的展示。作品较好的完成包括客流量、入店量、入店率、来访周期、新老顾客、顾客活跃度、驻店时长、来访周期、跳出率、深访率在内的九大基础指标的分析。并于前端页面上注重直观展示数据的变化趋势，数据分析所涉及的范围根据店铺具体情况支持自定义阕值。对于探针功能的拓展：探针绑定短信模块，后台实现短信接口，从而对探针实现远程控制与状态监控。针对不同店铺大小推出小店铺探针，中性店铺三探针，大型店铺多探针模式，中型以上店铺支持定位，从而进一步分析呈现区域热点。此外作品特色的添加了分析预测的功能，并基于分析预测功能结合历史数据智能的为商家提供商业决策支持，其中包括营销方案的推送，店铺排名波动的提醒功能。 wifi探针数据分析本项目实现的主要功能： 通过探针设备采集可监测范围内的手机MAC地址、与探针距离、时间、地理位置等信息: 探针采集的数据可以定时发送到服务端保存: 利用大数据技术对数据进行人流量等指标的分析。最终以合理的方式展示数据处理结果。 1、数据收集数据收集由服务器和探针设备共同完成，探针采集数据并发送到服务器，服务器接收探针设备的数据，处理成定格式保存至分布式文件系统(HDFS)中，供数据处理使用。 1.1 术语介绍: STA: (station) 工作站，指手机或者电脑等连接WiFi的设备。 AP: (AcessPoint)接入点，指无线路由器等产生WiFi热点的设备。 SSID: ( Service Set dentifer)服务集标识，就是WiFi的名字。 1.2 探针采集数据的原理： 在无线领域中STA总是不断试图寻找周边存在的AP,所以我们可以利用这种特性来发现一个未连接 AP的STA,而对于一个已经连接到AP的STA,也可以通过截狭它发出的数据帧来获取MAC、与探针之间的距离和它当前连接的SSID等信息。 2、数据清洗探针上传的数据是一种半结构化数据，主要格式参数如下： id：嗅探设备ID mmac：嗅探器设备自身Wifi MAC rate：发送频率 wssid：嗅探器设备连接的WiFi的MAC地址 time：时间戳，采集这些MAC的时间 lat：纬度 lon：经度 addr：地址 mac：采集到的手机的MAC地址 rssi：手机的信号强度 range：手机距离嗅探设备的距离 ts：目标ssid，手机连接的WiFi的ssid tmc：手机连接的WiFi的地址 tc：是否与路由器连接 ds：手机是否睡眠 essidn：曾今连接的WiFi的SSID 该数据属于半结构化数据，其中包含探针设备ID,设备自身WFIMAC,发送频率，设备连接的WFi的SID设备连接的WFI的MAC地址、时间戳，采集到这些MAC的时间、纬度、经度、地址信息，以及一组被探测到的设备信息， 设备信息包括手机的MAC、信号强度、与探针之间的距离、手机连接WiFi的SID手机连接的WFI的MAC地址、手机曾经连接过的WiFi的SSID。需要在清洗过程中去除所有无用的数据，使之变成结构化的文件，到这里数据清洗的第一步就完成了。 第二步使用Spark SQL完成，在这一步中完成时间点到时间段的转化，即在处理之前每一条记录表示一个终端在某 一时间点的状态，而在结果中一条记录表示一 个终端在一段时间内的状态。 经过数据清洗，不仅大大减小了数据集的容量，也为后续的数据处理提供了极大的方便。 3、数据保存 处理后的数据直接保存为文本文件，保存在HDFS中。 处理后的数据导入关系型数据库，供后续生成图表使用，该展示系统使用PHP做后台，前端使用HTML和JS生成图表。 4、客流数据分析4.1 数据表设计 1、HDFS中是原始数据集：（data表，通过Spark SQL得到） 原始数据data 表主要字段: tanzhen_id：探针设备的id mac：用户设备的MAC.（不同的mac代表不同客户） time：探测到当前设备的时间 range：该设备与探什之间的距离 2、visit表（取出同一mac所有数据，按照time遍历，得到每一个用户的每一次访问记录） visit表主要字段： mac：标识不同用户 start_time：用户入店时间 leave_time：用户离店时间 stay_time：用户停留时间 思路：data表首先抽取每一个用户（MAC）的数据，对每个用户数据进行遍历，得到每个用户每一次的访问记录。通过visit表得到：客流量、入店率，来访周期，新老顾客，顾客活跃度等等。 原始数据表是数据接收服务器最终存储到HDFS中的数据，中间结果表是经过第2次收据清洗后的输出结果。 上传数据到HDFS（因为是用python等语言处理过的，所以传的数据格式为只提取有用的数据） timeArray有四个time是因为时间格式划分为四块。 4.2 指标说明 上面我们完成了数据的初步处理，我们将得到以下指标: 客流量:店铺或区域整体客流及趋势。 入店量:进入店铺或区域的客流及趋势。 入店率:进入店铺或区域的客流占全部客流的比例及趋势。来访周期:进入店铺或区域的顾客距离上次来店的间隔。 来访周期 : 进入店铺或区域的顾客距离上次来店的问隔。 新老顾客:一定时间段内首次/两次以上进入店铺的顾客。 顾客活跃度:按顾客距离上次来访间隔划分为不同活跃度(高活跃度、中活跃度、低活跃度、沉睡活跃度) . 驻店时长:进店铺的顾客在店内的停留时长， 跳出率:进店铺后很快离店的顾客及占比(占总体客流) . 深访率:进店铺深度访问的顾客及占比(占总体客流，可以根据定位轨迹或者停留时长判定). 店铺外人流/客流量：在实时接收探针数据过程中根据range字段(范围)以及数据条数实时得到。 入店量/离店量：是对visit表分别按start_time、 leave_time字段从小到大遍历统计规定时间段内的记录条数。 跳出率/深访率/驻店时长：对visit 表按time字段从小到大遍历统计规定时间段内记录条数stay_time小于三分钟和大于20分钟的记录条数以及stay_time均值。 新老顾客数/顺客活跃度：对visit表按time字段排序，按一定定时间段遍历，新顾客收等于该时间段结束时刻之前所有的顺客数减去该时间段开始时刻之前所有的顾客数，老顾客数等于该时间段内顾客数减去新顾客数。 4.3 spark源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394package com.victor.spark.WiFiDatapackage com.victor.spark.WifiProject import org.apache.spark.sql.SparkSessionimport scala.util.control.Breaks object new_customer_extract &#123; // 实例化类 def main(args: Array[String]): Unit = &#123; // 主方法--入口（Unit无返回值） val spark = SparkSession.builder().appName("customer_extract").config( "spark.some.config.option","some-values").getOrCreate() // 变量1（不可改变） import java.io._ val writer = new PrintWriter(new File("/spark/data/re.txt")) //for the storage of result import spark.implicits._ val df = spark.read.json("/spark/data/log.json") //read the data //create the view data for df df.createOrReplaceTempView("data") spark.sql("cache table data") //get all MAC of all users val macArray = spark.sql("SELECT DISTINCT mac FROM data").collect() var i =0 val inner = new Breaks val lenth = macArray.length //loop for each user while(i&lt;lenth)&#123; var resultString = "" var mac = macArray(i)(0) val sql = "SELECT 'time' from data where mac='"+mac+"'order by 'time'" val timeArray = spark.sql(sql).collect() //to get timeList from timeArray import scala.collection.mutable.ListBuffer var timeList = new ListBuffer[Int] var list_length = timeArray.length var j = 0 while (j &lt; list_length)&#123; timeList += timeArray(i)(0).toString.toInt j = j+1 &#125; var k = 0 var oldTime = 0 var newTime = 0 var maxVisitTimeInterval = 300 var startTime = 0 var leaveTime = 0 while (k &lt; list_length)&#123; if(k == 0)&#123; oldTime = timeList(0) newTime = timeList(0) startTime = timeList(0) &#125; else if(k == (list_length - 1))&#123; leaveTime = timeList(k) var stayTime = leaveTime - startTime resultString += """&#123;"mac":"""" + mac + """,""" +""""in_time":"""+startTime+","+""""out_time":""" +leaveTime+","+""""stay_Time":"""+stayTime+"&#125;\n" &#125;else&#123; newTime = timeList(k) if ((newTime - oldTime) &gt; maxVisitTimeInterval)&#123; leaveTime = oldTime var stayTime = leaveTime-startTime resultString += """&#123;"mac":"""" + mac + """,""" +""""in_time":"""+startTime+","+""""out_time":""" +leaveTime+","+""""stay_Time":"""+stayTime+"&#125;\n" startTime = newTime oldTime = newTime &#125;else&#123; oldTime = newTime &#125; &#125; k = k +1 &#125; writer.write(resultString) i = i+1 &#125; writer.close() spark.sql("uncache table data") &#125;&#125; ———————项目地址 原文地址：https://blog.csdn.net/rainmaple20186/article/details/80340140]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于云计算和大数据的模拟车辆行车监控系统]]></title>
    <url>%2F%E5%9F%BA%E4%BA%8E%E4%BA%91%E8%AE%A1%E7%AE%97%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A8%A1%E6%8B%9F%E8%BD%A6%E8%BE%86%E8%A1%8C%E8%BD%A6%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F.html</url>
    <content type="text"><![CDATA[一、 系统架构这是基于云计算和大数据的模拟车辆行车监控系统，可模拟实现在线远程对车辆行车的信息记录以及数据处理。其中，记录信息其中包括车辆的id、经过的地点（经纬度）、时间，数据处理包括对数据的排序、错误数据的排查、通过时间以及地点在地图上获得车辆行驶的轨迹、车辆相遇次数。 系统包括数据产生模块、数据接受与处理模块、数据库模块、客户端模块。其中kafka进行数据的接收,并进行数据过滤，将过滤后的数据传递给Redis，Redis再将数据存入Hbase数据库，Spark从Hbase中获得数据，将处理后的数据再传递回Hbase，客户端从Hbase中获得数据并将其展示在前端。 逻辑架构如下：物理架构图如下： 二、数据流程分析1. 数据采集过程分析数据采集过程包括Kafka数据采集、Redis数据过滤、Hbase数据入库三部分，其中包括三个实体：Kafka生产者、Kafka消费者兼Redis发布者、Redis订阅者。 各自的作用如下：Kafka生产者：负责从json文件中以行为单位读取数据源，通过Kafka生产者代码编写生产消息，将json读取的消息发布在topic上。 Kafka消费者兼Redis发布者：负责从topic上消费Kafka生产者生产的消息，将消息通过Redis发布订阅功能发布到一个信道，等待订阅者接受消息。 Redis订阅者：负责订阅发布者相应的信道，接受发布者的消息，将消息存入Hbase数据库。数据过滤过程使用了Kafka streams对原始数据进行过滤，本小组采用HIGH-LEVEL STREAMS DSL进行处理。Kafka创建一个Filter流，流的源绑定filter-before topic，同时Kafka生成者将消息生产在这个topic上；流的出口绑定filter-after topic，Kafka消费者绑定这个topic消费消息。过滤器消息选择条件过滤掉不正确的经纬度数据，并将这部分数据存放在Redis filter 键里，合格的数据传送到filter-after topic上。 Redis的缓冲作用在Redis订阅者上，由于生产者生产消息过快，如果选择一条一条的存入数据库，会出现存取数据过慢，导致生产者的消息经过规定的时间(本小组设置的时间是90秒)没有被消费，报出Timeout错误。为避免这样的问题，选择每1000条数据存入数据库一次，这样的方式优点在于每1000条数据才请求连接数据库一次。请求连接数据库是较耗时的一个步骤，频繁的请求连接数据库会拖慢程序的运行时长。在基础项时，选择将所有数据存入list，然后一次请求数据库连接，将所有数据存入数据库，请求数据库连接的时间占比很小。 出现的问题以及解决方案1000条数据一次存入无法达到实时的记录，这是本小组项目的一个缺点，但同时，这个问题可以通过选择storm 流式框架数据处理来解决，直接在Kafka消费阶段对数据进行流式处理能达到实时效果。 2.数据查询和离线处理分析数据查询：数据采集完成，所有数据存入Hbase数据库的‘Record’表中，行键设计为eid、placeid、time组合键，在数据查询时，需要将行键截取，获取对应的数据，与查询条件比较，返回满足条件的数据。 spark处理：spark分析过程包含三个阶段——程序源码发布到master节点、master将map程序分配给map节点进行map操作、master将reduce程序分配给reduce节点进行reduce操作。数据流向是map节点从master节点获取Hbase数据索引，进而获取数据，接着运行map程序将数据分散处理。Map程序处理完的数据流入reduce进行聚合处理，最后将reduce结果存入Hbase数据库中。问题：在进行spark分析时，限于物理机，整个集群仅有一个master节点、一个map worker节点、一个reduce worker节点，在数据分析时出现的情况是map worker节点的工作任务量远远大于reduce worker的工作任务量。在任务启动时，集群中各个节点使用top命令查看当前节点的CPU占比，发现在整个任务中map worker 节点长时间高CPU占比工作，而reduce worker节点在map worker节点处理完成后有10秒左右的高CPU占比工作期，然后整个数据分析完成。鉴于上述的问题，考虑在主机充足的情况下，选择为map任务分配多台主机。使得任务执行量较均匀分布。 三、软件功能分析1、完成基本搭建系统，完成过车统计功能系统可根据输入的地点ID进行检索，显示通过该地点的车辆ID、时间、地点以及经纬度；或者根据输入的车辆ID，显示出该车辆经过的地点、经过时间以及对应地点的经纬度。结果展示： 2.系统附加功能分析（1）原始信息过滤原始数据包含若干条错误记录，如经纬度不合法等，需要实时对kafka中接收到的数据进行过滤处理，将处理后的数据传递给Redis。 （2）车辆行驶轨迹重现实现方式：我们想出了两种方法实现其轨迹重新。A、hbase方式建立一张新表，重新编排行键。在hbaseTest类中完成具体操作。首先使用HBaseConf类中的getConnection()方法与HBase数据库进行连接。然后利用HBaseConf类中getTableByName()方法得到对表“Record”表的操作句柄。同时使用相同的方法得到对Trace表的操作句柄。之后，使用Table类中的getScanner（）方法得到Record表中的所有数据，并记录中“result”中。因为重现轨迹的时候只需要车辆的标识信息（Eid）和车辆经过的时间（time）和经过地方的经纬度（latitude，longitude）所以我们只需要在“Trace”表中存入这些数据即可。现在我们已经将得到的所有的“Record”表中的数据都存在了“result”中。然后将result中的所有数据扫描一遍，同时将每条记录中的“Eid,time,latitude,longitude”信息记录下来，同时将每一条记录的这些信息作为新的一条记录，以“Eid”为rowKey且以“time”为列族的第一列放在Put类的对象中，最后通过Table类的put()方法将新的记录存在“Trace”表中。这样得到的“Trace”表中的数据即会以“time”自动排序。当所有数据被读取并被重新放入“Trace”表中后，关闭与数据库的连接，所有的信息即被重新规划好。 B、spark的MapReduce方式展示结果：输入要查询的车辆的ID，显示其行驶轨迹。鼠标点击地点，可显示其经纬度。 轨迹展示结果： （3）车辆相遇次数统计我们定义相遇为“两车之间出现在同一地点的时间间隔小于一分钟”。首先，通过Spark从Hbase表中读取数据，自身以地点为键进行join操作，计算除自身外的车辆是否相遇；再以地点为键进行分组，同一组内的数据按照时间进行排序，遍历整个列表，找出满足小于一分钟的数据。 结果展示：输入要查询的车辆Id，查询结果显示与之相遇过的车辆的ID以及次数。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark项目落地实战以及日常大数据开发注意事项]]></title>
    <url>%2FSpark%E9%A1%B9%E7%9B%AE%E8%90%BD%E5%9C%B0%E5%AE%9E%E6%88%98%E4%BB%A5%E5%8F%8A%E6%97%A5%E5%B8%B8%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9.html</url>
    <content type="text"><![CDATA[Spark简介 基于内存的分布式集群计算平台 可适配 Python、Java、Scala、SQL 拓展功能：机器学习、流式计算、图计算 Spark特点 高效 内存计算引擎 DAG图 比MapReduce快10～100倍 易用 提供丰富的API，支持Java，Scala， Python 代码量小 与Hadoop集成 读写HDFS、Hbase、Hive 和Yarn集成 与Oracle存过的对比 Spark应用场景 数据仓库 机器学习 海量数据离线分析 实时数据流处理 基本概念 集群架构 集群资源管理器（Cluster Manager） 运行作业任务的工作节点（Worker Node） 每个应用的任务控制节点（Driver） 每个工作节点上负责具体任务的执行进程 （Executor） 资源管理器Mesos或YARN 任务执行流程 首先为应用构建起基本的运行环境，即由 Driver创建一个SparkContext，进行资源 的申请、任务的分配和监控 资源管理器为Executor分配资源，并启动 Executor进程 SparkContext根据RDD的依赖关系构建 DAG图，DAG图提交给DAGScheduler解 析成Stage，然后把一个个TaskSet提交给 底层调度器TaskScheduler处理； Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行， 并提供应用程序代码 Task在Executor上运行，把执行结果反馈 给TaskScheduler，然后反馈给 DAGScheduler，运行完毕后写入数据并 释放所有资源。 数据处理过程 读入外部数据源 转换算子进行数据处理 动作算子进行处理流程触发 处理完成输出结果 常用算子-转换 开发案例–集团电信三码低效资产分析 Spark很香、也很坑 坑1：无法自定义自增序列 坑2：Spark Stage之间的血缘冗长 坑3：直连Oracle读取慢 坑4：时间格式支持不友好 常见问题1-无法自定义自增序列 问题阐述： 在不同的业务逻辑中，由于会存在多种维度的分析，但是他们的结果是写入到同一张表格中的。在oracle中执行的时候是根据oracle中定义的序列来保证ID的唯一性，但是 我们代码实现的时候采用的数据加载模式时无法加载oracle中的序列，并且读取序列也会收到oracle序列缓冲的影响。所以在业务逻辑处理上我们得自己定义一个属于我们 业务的ID序列，并且需要保证唯一性。 常见问题2-血缘关系冗长 问题阐述： 由于SparkSQL在解析成ATS树时会向上追溯血缘并重复解析，且随着血缘关系的增长ATS树会变的越来越复杂，导致任务执行效率会严重降低。具体表象为 Spark任务在执行过程中会卡住不动，程序继续卡顿几个小时之后才会开始继续运行。 方案一：checkpoints方式切割方式 方案二：hdfs落地，使用时二次读取 常见问题3-读Oracle速率慢 问题阐述： 在读取Oracle时，数据表未做分区，程序无法通过指定分区并行加载数据，且为了减小数据库IO压力，采用限制高频、数据读取限制等策略，导致读取Oracle速 率很慢，影响计算效率。 常见问题4-Oracle时间格式支持不友好 问题阐述： park在读写Oracle时date类型数据容易丢失精度，例如： Oracle中 2019-12-20 05:44:30读取后为2019-12-20， Spark中2019-12-20 05:44:30写入后变成2019-12-20 00:00:00 解决方案： Oracle方言,即自定义一种数据库解释语言，实际上的实现 为数据的类型转换。OracleDateTypeInit.oracleInit()]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch查询速度为什么这么快？]]></title>
    <url>%2FElasticsearch%E6%9F%A5%E8%AF%A2%E9%80%9F%E5%BA%A6%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E5%BF%AB%EF%BC%9F.html</url>
    <content type="text"><![CDATA[ES 是基于 Lucene 的全文检索引擎，它会对数据进行分词后保存索引，擅长管理大量的索引数据，相对于 MySQL 来说不擅长经常更新数据及关联查询。 说的不是很透彻，没有解析相关的原理;不过既然反复提到了索引，那我们就从索引的角度来对比下两者的差异。 MySQL 索引先从 MySQL 说起，索引这个词想必大家也是烂熟于心，通常存在于一些查询的场景，是典型的空间换时间的案例。以下内容以 InnoDB 引擎为例。 常见的数据结构假设由我们自己来设计 MySQL 的索引，大概会有哪些选择呢? ①散列表首先我们应当想到的是散列表，这是一个非常常见且高效的查询、写入的数据结构，对应到 Java 中就是 HashMap。 这个数据结构应该不需要过多介绍了，它的写入效率很高 O(1)，比如我们要查询 id=3 的数据时，需要将 3 进行哈希运算，然后再这个数组中找到对应的位置即可。 但如果我们想查询 1≤id≤6 这样的区间数据时，散列表就不能很好的满足了，由于它是无序的，所以得将所有数据遍历一遍才能知道哪些数据属于这个区间。 ②有序数组 有序数组的查询效率也很高，当我们要查询 id=4 的数据时，只需要通过二分查找也能高效定位到数据 O(logn)。 同时由于数据也是有序的，所以自然也能支持区间查询;这么看来有序数组适合用做索引咯? 自然是不行，它有另一个重大问题;假设我们插入了 id=2.5 的数据，就得同时将后续的所有数据都移动一位，这个写入效率就会变得非常低。 ③平衡二叉树既然有序数组的写入效率不高，那我们就来看看写入效率高的，很容易就能想到二叉树。 这里我们以平衡二叉树为例： 由于平衡二叉树的特性：左节点小于父节点、右节点大于父节点。 所以假设我们要查询 id=11 的数据，只需要查询 10→12→11 便能最终找到数据，时间复杂度为 O(logn)，同理写入数据时也为 O(logn)。 但依然不能很好的支持区间范围查找，假设我们要查询 5≤id≤20 的数据时，需要先查询 10 节点的左子树再查询 10 节点的右子树最终才能查询到所有数据。导致这样的查询效率并不高。 ④跳表跳表可能不像上边提到的散列表、有序数组、二叉树那样日常见的比较多，但其实 Redis 中的 sort set 就采用了跳表实现。这里我们简单介绍下跳表实现的数据结构有何优势。 我们都知道即便是对一个有序链表进行查询效率也不高，由于它不能使用数组下标进行二分查找，所以时间复杂度是 o(n)。 但我们也可以巧妙的优化链表来变相的实现二分查找，如下图： 我们可以为最底层的数据提取出一级索引、二级索引，根据数据量的不同，我们可以提取出 N 级索引。当我们查询时便可以利用这里的索引变相的实现了二分查找。 假设现在要查询 id=13 的数据，只需要遍历 1→7→10→13 四个节点便可以查询到数据，当数越多时，效率提升会更明显。 同时区间查询也是支持，和刚才的查询单个节点类似，只需要查询到起始节点，然后依次往后遍历(链表有序)到目标节点便能将整个范围的数据查询出来。 同时由于我们在索引上不会存储真正的数据，只是存放一个指针，相对于最底层存放数据的链表来说占用的空间便可以忽略不计了。 平衡二叉树的优化但其实 MySQL 中的 InnoDB 并没有采用跳表，而是使用的一个叫做 B+ 树的数据结构。 这个数据结构不像是二叉树那样大学老师当做基础数据结构经常讲到，由于这类数据结构都是在实际工程中根据需求场景在基础数据结构中演化而来。 比如这里的 B+ 树就可以认为是由平衡二叉树演化而来。刚才我们提到二叉树的区间查询效率不高，针对这一点便可进行优化： 在原有二叉树的基础上优化后：所有的非叶子都不存放数据，只是作为叶子节点的索引，数据全部都存放在叶子节点。 这样所有叶子节点的数据都是有序存放的，便能很好的支持区间查询。只需要先通过查询到起始节点的位置，然后在叶子节点中依次往后遍历即可。 当数据量巨大时，很明显索引文件是不能存放于内存中，虽然速度很快但消耗的资源也不小;所以 MySQL 会将索引文件直接存放于磁盘中。 这点和后文提到 Elasticsearch 的索引略有不同。由于索引存放于磁盘中，所以我们要尽可能的减少与磁盘的 IO(磁盘 IO 的效率与内存不在一个数量级)。 通过上图可以看出，我们要查询一条数据至少得进行 4 次IO，很明显这个 IO 次数是与树的高度密切相关的，树的高度越低 IO 次数就会越少，同时性能也会越好。 那怎样才能降低树的高度呢? 我们可以尝试把二叉树变为三叉树，这样树的高度就会下降很多，这样查询数据时的 IO 次数自然也会降低，同时查询效率也会提高许多。这其实就是 B+ 树的由来。 使用索引的一些建议其实通过上图对 B+树的理解，也能优化日常工作的一些小细节;比如为什么需要最好是有序递增的? 假设我们写入的主键数据是无序的，那么有可能后写入数据的 id 小于之前写入的，这样在维护 B+树索引时便有可能需要移动已经写好数据。 如果是按照递增写入数据时则不会有这个考虑，每次只需要依次写入即可。所以我们才会要求数据库主键尽量是趋势递增的，不考虑分表的情况时最合理的就是自增主键。 整体来看思路和跳表类似，只是针对使用场景做了相关的调整(比如数据全部存储于叶子节点)。 ES 索引MySQL 聊完了，现在来看看 Elasticsearch 是如何来使用索引的。 正排索引在 ES 中采用的是一种名叫倒排索引的数据结构;在正式讲倒排索引之前先来聊聊和他相反的正排索引。 以上图为例，我们可以通过 doc_id 查询到具体对象的方式称为使用正排索引，其实也能理解为一种散列表。 本质是通过 key 来查找 value。比如通过 doc_id=4 便能很快查询到 name=jetty wang，age=20 这条数据。 倒排索引那如果反过来我想查询 name 中包含了 li 的数据有哪些?这样如何高效查询呢? 仅仅通过上文提到的正排索引显然起不到什么作用，只能依次将所有数据遍历后判断名称中是否包含 li ;这样效率十分低下。 但如果我们重新构建一个索引结构： 当要查询 name 中包含 li 的数据时，只需要通过这个索引结构查询到 Posting List 中所包含的数据，再通过映射的方式查询到最终的数据。 这个索引结构其实就是倒排索引。 Term Dictionary但如何高效的在这个索引结构中查询到 li 呢，结合我们之前的经验，只要我们将 Term 有序排列，便可以使用二叉树搜索树的数据结构在 o(logn) 下查询到数据。 将一个文本拆分成一个一个独立Term 的过程其实就是我们常说的分词。 而将所有 Term 合并在一起就是一个 Term Dictionary，也可以叫做单词词典。 英文的分词相对简单，只需要通过空格、标点符号将文本分隔便能拆词，中文则相对复杂，但也有许多开源工具做支持(由于不是本文重点，对分词感兴趣的可以自行搜索)。 当我们的文本量巨大时，分词后的 Term 也会很多，这样一个倒排索引的数据结构如果存放于内存那肯定是不够存的，但如果像 MySQL 那样存放于磁盘，效率也没那么高。 Term Index所以我们可以选择一个折中的方法，既然无法将整个 Term Dictionary 放入内存中，那我们可以为 Term Dictionary 创建一个索引然后放入内存中。 这样便可以高效的查询 Term Dictionary ，最后再通过 Term Dictionary 查询到 Posting List。 相对于 MySQL 中的 B+树来说也会减少了几次磁盘 IO。 这个 Term Index 我们可以使用这样的 Trie 树，也就是我们常说的字典树来存放。 如果我们是以 j 开头的 Term 进行搜索，首先第一步就是通过在内存中的 Term Index 查询出以 j 打头的 Term 在 Term Dictionary 字典文件中的哪个位置(这个位置可以是一个文件指针，可能是一个区间范围)。 紧接着在将这个位置区间中的所有 Term 取出，由于已经排好序，便可通过二分查找快速定位到具体位置;这样便可查询出 Posting List。 最终通过 Posting List 中的位置信息便可在原始文件中将目标数据检索出来。 更多优化当然 Elasticsearch 还做了许多针对性的优化，当我们对两个字段进行检索时，就可以利用 Bitmap 进行优化。 比如现在需要查询 name=li and age=18 的数据，这时我们需要通过这两个字段将各自的结果 Posting List 取出。 最简单的方法是分别遍历两个集合，取出重复的数据，但这个明显效率低下。 这时我们便可使用 Bitmap 的方式进行存储(还节省存储空间)，同时利用先天的位与计算便可得出结果。 123[1, 3, 5] ⇒ 10101 [1, 2, 4, 5] ⇒ 11011 这样两个二进制数组求与便可得出结果： 110001 ⇒ [1, 5] 最终反解出 Posting List 为 [1, 5]，这样的效率自然是要高上许多。同样的查询需求在 MySQL 中并没有特殊优化，只是先将数据量小的数据筛选出来之后再筛选第二个字段，效率自然也就没有 ES 高。 当然在最新版的 ES 中也会对 Posting List 进行压缩，具体压缩规则可以查看官方文档，这里就不具体介绍了。 总结最后我们来总结一下： 通过以上内容可以看出再复杂的产品最终都是基础数据结构组成，只是会对不同应用场景针对性的优化，所以打好数据结构与算法的基础后再看某个新的技术或中间件时才能快速上手，甚至自己就能知道优化方向。]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>ES</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据系统的Lambda架构]]></title>
    <url>%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B3%BB%E7%BB%9F%E7%9A%84Lambda%E6%9E%B6%E6%9E%84.html</url>
    <content type="text"><![CDATA[Nathan Marz的大作Big Data: Principles and best practices of scalable real-time data systems介绍了Labmda Architecture的概念，用于在大数据架构中，如何让real-time与batch job更好地结合起来，以达成对大数据的实时处理。 传统系统的问题在传统数据库的设计中，无法很好地支持系统的可伸缩性。当用户访问量增加时，数据库无法满足日益增长的用户请求负载，从而导致数据库服务器无法及时响应用户请求，出现超时错误。 解决的办法是在Web服务器与数据库之间增加一个异步处理的队列。如下图所示： 引入队列 当Web Server收到页面请求时，会将消息添加到队列中。在DB端，创建一个Worker定期从队列中取出消息进行处理，例如每次读取100条消息。这相当于在两者之间建立了一个缓冲。 但是，这一方案并没有从本质上解决数据库overload的问题，且当worker无法跟上writer的请求时，就需要增加多个worker并发执行，数据库又将再次成为响应请求的瓶颈。一个解决办法是对数据库进行分区（horizontal partitioning或者sharding）。分区的方式通常以Hash值作为key。这样就需要应用程序端知道如何去寻找每个key所在的分区。 问题仍然会随着用户请求的增加接踵而来。当之前的分区无法满足负载时，就需要增加更多分区，这时就需要对数据库进行reshard。resharding的工作非常耗时而痛苦，因为需要协调很多工作，例如数据的迁移、更新客户端访问的分区地址，更新应用程序代码。如果系统本身还提供了在线访问服务，对运维的要求就更高。稍有不慎，就可能导致数据写到错误的分区，因此必须要编写脚本来自动完成，且需要充分的测试。 即使分区能够解决数据库负载问题，却还存在容错性（Fault-Tolerance）的问题。解决办法： 改变queue/worker的实现。当消息发送给不可用的分区时，将消息放到“pending”队列，然后每隔一段时间对pending队列中的消息进行处理。 使用数据库的replication功能，为每个分区增加slave。 问题并没有得到完美地解决。假设系统出现问题，例如在应用系统代码端不小心引入了一个bug，使得对页面的请求重复提交了一次，这就导致了重复的请求数据。糟糕的是，直到24小时之后才发现了该问题，此时对数据的破坏已经造成了。即使每周的数据备份也无法解决此问题，因为它不知道到底是哪些数据受到了破坏（corrupiton）。由于人为错误总是不可避免的，我们在架构时应该如何规避此问题？ 现在，架构变得越来越复杂，增加了队列、分区、复制、重分区脚本（resharding scripts）。应用程序还需要了解数据库的schema，并能访问到正确的分区。问题在于：数据库对于分区是不了解的，无法帮助你应对分区、复制与分布式查询。最糟糕的问题是系统并没有为人为错误进行工程设计，仅靠备份是不能治本的。归根结底，系统还需要限制因为人为错误导致的破坏。 数据系统的概念大数据处理技术需要解决这种可伸缩性与复杂性。首先要认识到这种分布式的本质，要很好地处理分区与复制，不会导致错误分区引起查询失败，而是要将这些逻辑内化到数据库中。当需要扩展系统时，可以非常方便地增加节点，系统也能够针对新节点进行rebalance。 其次是要让数据成为不可变的。原始数据永远都不能被修改，这样即使犯了错误，写了错误数据，原来好的数据并不会受到破坏。 何谓“数据系统”？Nathan Marz认为： 如果数据系统通过查找过去的数据去回答问题，则通常需要访问整个数据集。 因此可以给data system的最通用的定义： 1Query = function(all data) 接下来，本书作者介绍了Big Data System所需具备的属性： 健壮性和容错性（Robustness和Fault Tolerance） 低延迟的读与更新（Low Latency reads and updates） 可伸缩性（Scalability） 通用性（Generalization） 可扩展性（Extensibility） 内置查询（Ad hoc queries） 维护最小（Minimal maintenance） 可调试性（Debuggability） Lambda架构Lambda架构的主要思想就是将大数据系统构建为多个层次，如下图所示： lambda layer 理想状态下，任何数据访问都可以从表达式Query = function(all data)开始，但是，若数据达到相当大的一个级别（例如PB），且还需要支持实时查询时，就需要耗费非常庞大的资源。 一个解决方式是预运算查询函数（precomputed query funciton）。书中将这种预运算查询函数称之为Batch View，这样当需要执行查询时，可以从Batch View中读取结果。这样一个预先运算好的View是可以建立索引的，因而可以支持随机读取。于是系统就变成： 12batch view = function(all data)query = function(batch view) Batch Layer在Lambda架构中，实现batch view = function(all data)的部分被称之为batch layer。它承担了两个职责： 存储Master Dataset，这是一个不变的持续增长的数据集 针对这个Master Dataset进行预运算 显然，Batch Layer执行的是批量处理，例如Hadoop或者Spark支持的Map-Reduce方式。 它的执行方式可以用一段伪代码来表示： 123function runBatchLayer(): while (true): recomputeBatchViews() 例如这样一段代码： 12345Api.execute(Api.hfsSeqfile("/tmp/pageview-counts"), new Subquery("?url", "?count") .predicate(Api.hfsSeqfile("/data/pageviews"), "?url", "?user", "?timestamp") .predicate(new Count(), "?count"); 代码并行地对hdfs文件夹下的page views进行统计（count），合并结果，并将最终结果保存在pageview-counts文件夹下。 利用Batch Layer进行预运算的作用实际上就是将大数据变小，从而有效地利用资源，改善实时查询的性能。但这里有一个前提，就是我们需要预先知道查询需要的数据，如此才能在Batch Layer中安排执行计划，定期对数据进行批量处理。此外，还要求这些预运算的统计数据是支持合并（merge）的。 Serving LayerBatch Layer通过对master dataset执行查询获得了batch view，而Serving Layer就要负责对batch view进行操作，从而为最终的实时查询提供支撑。因此Serving Layer的职责包含： 对batch view的随机访问 更新batch view Serving Layer应该是一个专用的分布式数据库，例如Elephant DB，以支持对batch view的加载、随机读取以及更新。注意，它并不支持对batch view的随机写，因为随机写会为数据库引来许多复杂性。简单的特性才能使系统变得更健壮、可预测、易配置，也易于运维。 Speed Layer只要batch layer完成对batch view的预计算，serving layer就会对其进行更新。这意味着在运行预计算时进入的数据不会马上呈现到batch view中。这对于要求完全实时的数据系统而言是不能接受的。要解决这个问题，就要通过speed layer。从对数据的处理来看，speed layer与batch layer非常相似，它们之间最大的区别是前者只处理最近的数据，后者则要处理所有的数据。另一个区别是为了满足最小的延迟，speed layer并不会在同一时间读取所有的新数据，相反，它会在接收到新数据时，更新realtime view，而不会像batch layer那样重新运算整个view。speed layer是一种增量的计算，而非重新运算（recomputation）。 因而，Speed Layer的作用包括： 对更新到serving layer带来的高延迟的一种补充 快速、增量的算法 最终Batch Layer会覆盖speed layer Speed Layer的等式表达如下所示： 1realtime view = function(realtime view, new data) 注意，realtime view是基于新数据和已有的realtime view。 总结下来，Lambda架构就是如下的三个等式： 123batch view = function(all data)realtime view = function(realtime view, new data)query = function(batch view . realtime view) 整个Lambda架构如下图所示： lambda architecture 基于Lambda架构，一旦数据通过batch layer进入到serving layer，在realtime view中的相应结果就不再需要了。 作者：张逸链接：https://www.jianshu.com/p/b6bf05332c18来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kappa架构的层级及其特点]]></title>
    <url>%2Fkappa%E6%9E%B6%E6%9E%84%E7%9A%84%E5%B1%82%E7%BA%A7%E5%8F%8A%E5%85%B6%E7%89%B9%E7%82%B9.html</url>
    <content type="text"><![CDATA[kappa架构的本质可以说是只依赖一套流处理系统来作为大数据处理解决方案。 一、概括起来，kappa架构包括两个层级 1、消息传输层 这一层有如下特点 持久性——数据可任意设定存储时间 分布式——数据分布式存储 数据可重放——数据可以被replay，从头重新处理 高性能——能够提供高性能数据读写访问 有了这几点保证之后，数据便可以在某个需要限度内全量存储，这可将生产者和消费者解耦，并进行分布式容错以提高可用性，可重放很重要，这确保了在必要情况下系统可进行重算。 消息传输层的意义在于弹性容纳并提供流计算引擎的输入数据，并在必要时从头开始读取重新计算，从而获得可靠结果。 通常使用消息队列如Kafka来作为消息传输层。 2、流处理层 这一层即是大数据流处理引擎，可用于进行流分布式实时计算。理想情况下，流处理层也应该具备如下特点 低延迟——保证快速响应 高吞吐——同时处理庞大数据量 具有容错与恢复能力——保证系统稳定可用 一致性保证——适用任何强一致性需求的应用(如金融级需求) 如今比较流行的大数据处理系统要数基于Spark引擎的Spark Streaming、Struct Streaming还有Flink、Storm了。我们看看业界最常用的Spark和Flink在流处理这方面的高下！在延迟方面Flink要更胜于Spark流处理。另一方面，在一致性方面Flink借鉴 Chandy-Lamport 分布式快照算法实现的Asynchronous Barrier Snapshots算法来提供一致性保证而Spark流处理无法保证。 这里我的另一点体会是，Flink是连续运算符长运行的计算模式，所以其在失败时进行状态重放，从断点继续往下执行即可，而批次的处理却不可以，批次的重新处理时其加载的仍然是批。 二、然后我们来看几个概念 1、幂等性 幂等性也就是指相同的操作无论执行多少次，均会产生一样的结果。 这里为什么提到幂等性呢？因为分布式中的一致性通常是难以保证的，因为伴随着分布式的一致性而来的除了耗资源还产生延迟以及网络通信问题。所以如Spark流处理输出时是无法保证Exactly-once（失败时部分已经写出、而还有部分未写出，重启时从上一个检查点开始，造成部分重复），这个时候可以通过幂等输出来解决。 举个例子，在往kafka写出时，可以通过外部存储的唯一键来辅助设置enable.auto.commit为false，在幂等写出时再手动提交，这样便可保证写入到kafka中的数据不会重复。 2、Exactly-once语义 Struct Streaming的微批次处理模式中声称的exactly-once语义并不是说输入数据只被处理一次，因为Spark引擎支持任务重试（根据沿袭图），所以很有可能task失败导致数据重新被加载再次处理。这里的exactly-once实际是指不管执行多少次，其最终的执行结果都是一样的。 三、kappa的优缺点 这个在前边的文章已有描述，但其核心优点即是无需维护离线批计算、实时计算两套系统，对于业务开发也只需一套程序即可，不用担心因两套系统导致的执行结果不一致。另外，其从头开始计算的能力也是主要考虑的其不足的一点。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kappa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kappa架构与Lambda架构比较]]></title>
    <url>%2FKappa%E6%9E%B6%E6%9E%84%E4%B8%8ELambda%E6%9E%B6%E6%9E%84%E6%AF%94%E8%BE%83.html</url>
    <content type="text"><![CDATA[Lambda架构Nathan Marz针对通用的，可扩展的和容错的数据处理架构提出了术语Lambda Architecture。它是一种旨在通过利用批处理和流处理这两者的优势来处理大量数据的数据处理架构。 图层从宏观角度看，它的处理流程如下： 所有进入系统的数据都被分配到批处理层和速度层进行处理。批处理层管理主数据集（一个不可变的，仅可扩展的原始数据集）并预先计算批处理视图。服务层对批处理视图进行索引，以便可以在低延迟的情况下进行点对点查询。速度层只处理最近的数据。任何传入的查询都必须通过合并来自批量视图和实时视图的结果来得到结果。 数据的相关性如前所述，任何传入查询都必须通过合并来自批量视图和实时视图的结果来得到答案，因此这些视图需要可合并性。需要注意的一点是，实时视图是以前的实时视图和新数据增量的函数，因此可以使用增量算法。批处理视图是所有数据的函数，因此应该在那里使用重算算法。 权衡我们生活中的每一件事都是一种折衷，而Lambda Architecture也不是一个例外。通常，我们需要解决一些主要的折衷： 完全重新计算与部分重新计算 在某些情况下，可以使用Bloom过滤器来避免完全重新计算 重算算法与增量算法 使用增量算法有很大的诱惑力，但根据指南我们必须使用重新计算算法，即使它使达到相同的结果变得更加困难。 加法算法与近似算法 Lambda Architecture与加法算法很好地协作。因此，这是我们需要考虑使用近似算法的另一种情况，例如，HyperLogLog用于计数不同的问题等。 实现有多种实现Lambda体系结构的方法，因为它对于每个层的底层解决方案都是不可知的。每一层都需要底层实现的特定功能，这可能有助于做出更好的选择并避免过度的决定： 批处理层：一次写入，批量读取多次 服务层：随机读取，不随机写入; 批量计算和批量写入 速度层：随机读取，随机写入; 增量计算 Kappa架构正如前面提到的，Lambda Architecture有其优点和缺点，人们也划分成支持者和反对者两派。Kappa 架构是LinkedIn的Jay Kreps结合实际经验和个人体会，针对Lambda架构进行深度剖析，分析其优缺点并采用的替代方案。Lambda 架构的一个很明显的问题是需要维护两套分别跑在批处理和实时计算系统上面的代码，而且这两套代码得产出一模一样的结果。因此对于设计这类系统的人来讲，要面对的问题是：为什么我们不能改进流计算系统让它能处理这些问题？为什么不能让流系统来解决数据全量处理的问题？流计算天然的分布式特性注定其扩展性比较好，能否加大并发量来处理海量的历史数据？基于种种问题的考虑，Jay提出了Kappa这种替代方案。Kappa架构 简化了Lambda架构。Kappa架构系统是删除了批处理系统的架构。要取代批处理，数据只需通过流式传输系统快速提供： 那如何用流计算系统对全量数据进行重新计算，步骤如下： 1、用Kafka或类似的分布式队列保存数据，需要几天数据量就保存几天。 2、当需要全量计算时，重新起一个流计算实例，从头开始读取数据进行处理，并输出到一个结果存储中。 3、当新的实例完成后，停止老的流计算实例，并把老的一引起结果删除。 一个典型的Kappa架构如下： 和Lambda架构相比，在Kappa架构下，只有在有必要的时候才会对历史数据进行重复计算，并且实时计算和批处理过程使用的是同一份代码。或许有些人会质疑流式处理对于历史数据的高吞吐量会力不从心，但是这可以通过控制新实例的并发数进行改善。 Kappa架构的核心思想包括以下三点： 用Kafka或者类似的分布式队列系统保存数据，你需要几天的数据量就保存几天。 当需要全量重新计算时，重新起一个流计算实例，从头开始读取数据进行处理，并输出到一个新的结果存储中。 当新的实例做完后，停止老的流计算实例，并把老的一些结果删除。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Kappa</tag>
        <tag>Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala快速入门]]></title>
    <url>%2FScala%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html</url>
    <content type="text"><![CDATA[变量定义1234567891011121314151617181920val hello="Hello Scala"val hello:String="Hello Scala"val hello:java.lang.String="Hello Scala"lazy val hello="Hello Scala"val hello="Hello Scala"val x=0x29 //十六进制定义整数val x=41 //十进制定义整数val doubleNumber = 3.14159 //Double类型定义，直接输入浮点数，编译器会将其自动推断为Double类型val floatNumber=3.14159F //定义Float类型浮点数，需要在浮点数后面加F或fval floatNumber=0.1314e1var letter = 'A' //字符定义，用单引号(')将字符包裹val hello = """Hello \n \t \b \\ Scala"""//如果需要原样输出字符串中的内容，则用三个双引号"""将字符串包裹起来var bool = true 注： val 相当于Java中的final关键字修饰的变量，一旦赋值便不能更改 当未指定类型时，Scala会根据实际类型进行类型推断,上面前三种方式结果相同 lazy关键字修饰的变量，定义时不赋值，真正使用时才赋值 var关键字修饰的变量，可以被重新赋值e也可以是大写E,0.1214e1 = 0.131410 e也可以是大写E,0.1214e2 = 0.13141010 函数定义12345678910111213def max(x:Int, y:Int):Int=&#123; if(x &gt; y) x else y&#125;def sub(a:Int,b:Int)=&#123; a-b&#125;def sub(a:Int,b:Int):Unit=&#123; a-b&#125; def：关键字，定义一个函数 max：自定义的方法名 (x:Int, y:Int)：方法名后小扩号中为参数列表 Int：参数后的Int为方法返回值类型 {…}：大扩号中为方法体 Scala函数返回值可以不加return,默认函数体最后一条语句为返回值 函数体不指定返回值时，scala会根据实际类型进行类型推断 Unit关键字表示函数不存在返回值，相当于java中的void关键字 Scala每行语句结束后的分号可加可不加 Scala基本类型 Value type Range Byte 8-bit Short 16-bit Int 32-bit Long 64-bit Char 16-bit Float 32-bit Double 64-bit Boolean true or false String 算术操作+ - * / % 在Scala中一切操作皆方法，这意味着Scala中的一切毕为对象 1+2整数求和时，编译器会将其转换为(1).+(2)执行,所以下面两种方式是相等的var sum =1+2var sum=(1).+(2) 操作符重载，编译器会将其转换为(1).+(2L)执行结果等于3var longsum = 1 + 2L Scala中可以用+ -符号来表示正负数，例如：-3+3var y= 1+ -3var y= -3+3 关系运算符= &lt; ⇐ ! == 逻辑运算符! &amp;&amp; || 位运算&amp;(按位与) |(按位或) ^(按位异或) ~(按位取反) «(左移) »(带符号右移) »&gt;(无符号右移) 12345678910111213141516171819202122232425262728293031323334353637383940//00000001//00000010//00000000scala&gt; 1 &amp; 2res0: Int = 0//00000001//00000010//00000011scala&gt; 1 | 2res0: Int = 3//00000001//1111110scala&gt; ~1res1: Int = -2//00000001//00000010scala&gt; 1 &lt;&lt; 1res3: Int = 2//00000001//00000000scala&gt; 1 &gt;&gt; 1res5: Int = 0//00000001//00000000scala&gt; 1 &gt;&gt;&gt; 1res7: Int = 0//00000001//00000010//操作的结果是如果某位不同则该位为1, 否则该位为0.scala&gt; 1 ^ 2res9: Int = 3scala&gt; 1 ^ 3res10: Int = 2 对象比较 Scala中的对象比较不同于Java中的对象比较 Scala基于内容的比较，而Java中比较的是引用地址 Scala eq 方法是基于引用地址的比较 1234567891011121314151617181920212223242526scala&gt; 1==1res16: Boolean = truescala&gt; 1==1.0res17: Boolean = truescala&gt; val x="hello"x: String = helloscala&gt; val y="hello"y: String = helloscala&gt; x==yres18: Boolean = truescala&gt; val x:String = new String("hello")x: String = helloscala&gt; val y:String = new String("hello")y: String = helloscala&gt; x==yres20: Boolean = truescala&gt; x.eq(y)res17: Boolean = false 运算符的优先级扩号优先级最高* / %+ -:= !&lt; &gt;&amp; 字符串的操作var str = “hello” 定义的String类型变量可以使用java中String的所有方法 隐式转换：Scala把String类型转换成StringOps,其中包括了所有的immutable sequence的方法。碰到reverse,map,drop和slice方法时编译器会自动插入这个转换。 Scala方法的调用可以不加点“.”,直接用空格调用，并且可以省略扩号 123456789101112str.indexOf(“o”)str indexOf 'o'str.toUpperCasestr.toLowerCasestr.reversestr.map(_.toUpper)str.drop(2)str drop 2str drop(2)str slice(1,4)str.slice(1,4)…. if的使用 Scala会自动进行类型推断 1234scala&gt; var x = if("hello"=="hello")1 else 2 x: Int = 1scala&gt; var x = if("hello"=="hello")"1" else "2"x: String = 1 while的使用12345678910111213141516def main(args: Array[String]): Unit = &#123; HelloScala loop 4 HelloScala loop(4) HelloScala.loop(4) var line="" while((line=readLine())!="") //readLine() 读取控制台的输入 println("read:"+line) &#125; def loop(x:Int)=&#123; var a=x while(a!=0)&#123; a=a-1 println(a) &#125; &#125; do while的使用12345678910111213141516171819def main(args: Array[String]): Unit = &#123; HelloScala loop(4,2) HelloScala.loop(4,2) var line="" do&#123; line=readLine() println("read:"+line) &#125; while(line!="") &#125; def loop(x:Int,y:Int)=&#123; var a=x do&#123; a=a-y println(a) &#125; while(a!=0) &#125; Scala具有函数式编程语方的特点，在函数式编程语方当中，一般不推荐使用var变量 利用if替代while控制结构,可以减少var变量的使用,程序结构也更简单，表达能力更强 123456def loop(x:Int):Unit=&#123; if(x!=0)&#123; println(x) loop(x-1) &#125; &#125; for循环的使用 打印当前目录所有文件名 ← 生成器(generator),在执行过程中，集合files中的Array[File]的元素将依次赋值给file,file类型为File,输出时调用其toStirng方法将文件名打印出来 123456val files = (new java.io.File(".")).listFilesfor(file &lt;- files )println(file)val files = (new java.io.File(".")).listFilesfor(i &lt;- 0 to files.length -1)println(files(i))var a = 1 to 2for(i &lt;- a)println("item:"+i) for循环中加入if过滤操作 12val files = (new java.io.File(".")).listFilesfor(file &lt;- files if file.getName.endsWith("iml"))println(file) 加入多个过滤条件 1for(file &lt;- files if file.isFile; if file.getName.endsWith("iml")) println(file) 4.多重循环的实现 1234567891011def main(args: Array[String]): Unit = &#123; grep("JAVA") &#125; def fileLines(file: File) = scala.io.Source.fromFile(file).getLines.toList def grep(pattern:String) = for ( file &lt;- (new File(".")).listFiles if file.getName.endsWith("iml"); line &lt;- fileLines(file) if line.trim.contains(pattern) ) println(file + ":" + line.trim) yield 关键字在循环中生成返回结果123456789101112131415161718192021def main(args: Array[String]): Unit = &#123; println(getFile(0)) &#125; def getFile = for&#123; file &lt;- (new File(".")).listFiles if file.getName.endsWith("iml") &#125;yield file def main(args: Array[String]): Unit = &#123; for(i &lt;- grep("orderEntry"))println(i) &#125; def fileLines(file: File) = scala.io.Source.fromFile(file).getLines.toList def grep(pattern:String) = for ( file &lt;- (new File(".")).listFiles if file.getName.endsWith("iml"); line &lt;- fileLines(file) if line.trim.contains(pattern) ) yield line.trim//返回Array[String] Array数组操作定长数组Array12345678910111213141516Scala中的Array是以Java中的Array方式实现的 var arr = 0 until 5 scala.collection.immutable.Range = Range 0 until 5 val arr1 = new Array[Int](10) 复杂对象类型的数组初使化值为null,数值型被初使化为0 var arr2 = Array(“aaa”,“bbb”) 定义一个数组同时赋值，这种调用方式是调用其apply方法进行数组创建操作的 arr2(1)=“test” 数组元素赋值,不同于Java,这里index的指定是用小扩号。 intArr.update(2,“test2”) 更新值 变长数组ArrayBuffer12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970scala.collection.mutable.ArrayBuffer var arr=ArrayBuffer[String]() arr+=“Hello” arr+=(“Hello”,“Scala”) += 在尾部添加元素 arr.insert(1,“test1”,“test2”,“test3”) arr.insertAll(3,arr) arr.insertAll(3,arr2) 在指定位置插入数据 arr.remove(0,2) 从索引0开始，删除2个元素 arr++=Array(“hello”,“test”) arr++=List(“value1”,“value2”) ++=用于向数组中追加内容，++=右侧可以是任何集合 arr.trimStart(2) 删除前面2个元素 arr.trimEnd(2) 删除未尾2个元素 arr.toArray 转换ArrayBuffer to Array arr.toBuffer 转换Array to ArrayBuffer 遍历数组 to 方式 for(i ← 0 to arr.length-1)println(arr(i)) until方式 for(i ← 0 until arr.length)println(arr(i)) foreach方式 for(i ← arr)print(“\t”+i) 指定步长 for(i← 0 until (arr.length,2)) print(“\t”+arr(i)) for(i← (0 to (arr.length-1,2))) print(“\t”+arr(i)) 倒序输出 for(i ← (0 until arr.length).reverse) print(“\t”+arr(i)) for(i ← (0 to arr.length -1).reverse) print(“\t”+arr(i)) for(i ← arr.reverse) print(“\t”+i) yield返回新的数组 var arr2 = for(i ← arr) yield i+“test” *原数组不变 *BufferArray转换后产生的仍然是BufferArray *Array转换后产生的仍然是Array 数组求和,最大值,最小值,格式化输出,过滤掉小于3数据 val intArr = Array(1,2,3,4,5) println(intArr.sum) println(intArr.max) println(intArr.min) println(intArr.mkString) println(intArr mkString “,”) println(intArr.mkString(“(”,“,”,“)”)) println(intArr.dropWhile(_&lt;3) mkString “,”) 多维数组 var multiArr = Array(Array(1,2,3),Array(4,5,6)) println(multiArr(1)(2)) for (elem ← multiArr) &#123;println(elem mkString(“,”))&#125; ListList的定义12345678910111213141516171819202122val cars = List(“bmw”,“volvo”) val cars = List.apply(“bmw”,“volvo”) val nums = List(1,2,3,4,5,6,7,7,8) var cars:List[Object]=List(“BMW”,“Volvo”) var list= List() var cars:List[String]=List() 采用 :: 及Nil进行列表构建 val nums = 1 :: (2 :: (3:: (4::Nil))) 相等于 val nums = List(1, 2, 3, 4) 相等于 val nums = 1::2::3::4::Nil 多重List val cars=List(List(“BMW”,“200万”,“red”),List(“BYD”,“20万”,“white”)) *list一旦被创建，其值不能被改变 *list具有递归结构(Recursive Structure)，例如链表结构 *List类型和其它集合类型一样，它具有协变性(Covariant),即对于类型S和T,如果S是T的子类型，则List[S]也是List[T]的子类型 *空的List,其类型为Nothing,Nothing在Scala的继承层次中的最低层,即Nothing是任何Scala其它类型如String,Object等的子类 \* :: 操作符的优先级是从右往左的 list常用操作1234567891011121314151617181920212223242526272829303132333435363738判断是否为空 cars.isEmpty 取第一个元素 cars.head 取第一个元素外的元素列表 cars.tail cars.reverse.init init取最后一个元素外的元素列表 cars.init cars.reverse.tail 取最后一个 cars:last 取第二个元素 cars.tail.head 取倒数第二个元素 cars.init.last 元素倒置 cars.reverse 丢弃前N个元素,返回元素列表 cars drop 1 获取前几个元素列表 cars take 2 cars.take(2) 将列表进行分割,从Index N的位置进行分割，返回前后两个元素列表，位置Ｎ算后面的元素列表 cars.splitAt(1) (cars.take(1),cars.drop1) zip 返回的是List类型的元组(Tuple) 12345678scala&gt; val arr = List("1","6","3","5","9") arr: List[String] = List(1, 6, 3, 5, 9) scala&gt; val num = List(1,6,3,5,9) num: List[Int] = List(1, 6, 3, 5, 9) scala&gt; arr zip num res99: List[(String, Int)] = List((1,1), (6,6), (3,3), (5,5), (9,9)) 插入排序算法 123456789def sort(xs:List[Int]) :List[Int] =&#123; if(xs.isEmpty) Nil else insert(xs.head, sort(xs.tail))&#125;def insert(x:Int, xs:List[Int]) :List[Int] =&#123; if(xs.isEmpty || x&lt;= xs.head) x:: xs else xs.head :: insert(x,xs.tail) &#125; List连接操作 ::: 12scala&gt; List(1,2):::List(3,4)res35: List[Int] = List(1, 2, 3, 4) list相等判断 123456val arr = List(1,6,3,5,9) val arr1 = List("1","6","3","5","9") aaa == arr1 //false val arr = List("1","6","3","5","9")val arr1 = List("1","6","3","5","9")arr == arr1 //true range方法，构建某一值范围内的Listval nums = List.range(2,9)range主法，指定步长N,范围不存在的返回空Listval nums = List.range(2,100, 5)val nums = List.range(100,20, -5) flatten,将列表平滑成第一个元素 12scala&gt; List(List("aaa","bbb"),List("111","222")).flatten res34: List[String] = List(aaa, bbb, 111, 222) concat列表链接 1scala&gt; List.concat(List("aaa","bbb"),List("111","222")) res32: List[String] = List(aaa, bbb, 111, 222) \\ SetSet 不可重复，无序的集合如果需要按添加顺序排序，可以使用LinkedHashSet实现 val set = Set(1,2,3)val mutableSet = mutable.Set(1,2,3) 添加元iset+5 遍历Setfor(i ← set+7)print(i) Mapval map = Map(1 → “one”,“two” → 2,2.3 → “three”)val mutableMap = mutable.Map(1 → “one”,“two” → 2,2.3 → “three”)println(map) 添加元素println(map.+(4 → “four”)) 删除元素println(map.-(“tow”)) 遍历map 1234567for(i&lt;-map)println(i+"\t"+i._1+"\t"+i._2) map.foreach(e=&gt;&#123; val (k,v)=e println(k,v) //通过模式匹配获取元组内容 println(e._1,e._2) //直接访问元组内容&#125;) mutable map可以使用put添加元素val mutableMap = mutable.Map(1 → “one”,“two” → 2,2.3 → “three”)mutableMap.put(“four”,4) 判断key是不存在 map.contains(1) map.get(“key”) Queue队列immutable queuescala&gt; var queue=scala.collection.immutable.Queue(1,2,3)queue: scala.collection.immutable.Queue[Int] = Queue(1, 2, 3) 出队scala&gt; queue.dequeueres38: (Int, scala.collection.immutable.Queue[Int]) = (1,Queue(2, 3)) 入队scala&gt; queue.enqueue(4)res40: scala.collection.immutable.Queue[Int] = Queue(1, 2, 3, 4) mutable queuescala&gt; var queue=scala.collection.mutable.Queue(1,2,3,4,5)queue: scala.collection.mutable.Queue[Int] = Queue(1, 2, 3, 4, 5) 入队操作scala&gt; queue += 5res43: scala.collection.mutable.Queue[Int] = Queue(1, 2, 3, 4, 5, 5) 集合方式scala&gt; queue ++= List(6,7,8)res45: scala.collection.mutable.Queue[Int] = Queue(1, 2, 3, 4, 5, 5, 6, 7, 8) Scala.collectionscala会默认导入以几个包：import java.langimport scala.import Predef._ Predef对象中包含了Set、Map等的定义 在Scala中，默认使用的immutable集合，如果需要使用mutable集合，需要在在程序中importimmutable 不可变的集合，一量被创建，便不能被改变，添加、删除、更新操作返回的是新的集合，原集合保持不变 mutable 可变的集合，可以更新或修改，添加、删除、修改元素将作用于原集合 scala.collection包中的集合类层次结构如下图: scala.collection.immutable包中的类层次结构: scala.collection.mutable包中的类层次结构: 可变集合与不可变集合对应关系： 函数字面量（值函数）函数字面量（function literal），也称值函数（function values），指的是函数可以赋值给变量。 12345678910111213141516val increase=(x:Int)=&gt;x+1//=&gt;左侧的表示输入参数，右侧的表示方法体//等同于def increase(x:Int)=x+1 //方法体中多个语句则使用&#123;&#125;val increase=(x:Int)=&gt;&#123; println("begin") println("end") x+1&#125;//数组的map方法中调用（写法1）//写法1println(Array(1,2,3).map(increase).mkString(“,”))//写法2 匿名函数写法println(Array(1,2,3).map((x:Int)=&gt;x+1) mkString “,”) 函数简化大扩号中只有一行操作时，大扩号可以省掉， 方法的调用可以省略“.”,用空格代替*参数类型推断写法 12println(Array(1,2,3) map((x)=&gt;x+1) mkString &quot;,&quot;)println(Array(1,2,3) map((x)=&gt;x+1) mkString(&quot;,&quot;)) *函数只有一个参数的话，可以省略() 12scala&gt; Array(1,2,3,4) map&#123;x=&gt;x+1&#125; mkString(&quot;,&quot;)res28: String = 2,3,4,5 *如果参数右边只出现一次，则可以进一步简化 12scala&gt; Array(1,2,3,4) map&#123;_+1&#125; mkString(&quot;,&quot;)res29: String = 2,3,4,5 *值函数简化 12val fun1=1+(_:Int)val fun2:(Double)=&gt;Double=1+_ *函数参数,即将一个函数做为参数传入 12345def convertIntToString(f:(Int)=&gt;String)=f(4)def multiply(factor:Double)=(x:Double)=&gt;factor*x var b = multiply(4) println(b(1)) 函数闭包(Closure)(x:Int)⇒x+more,这里的more是一个自由变量(Free Variable),more是一个没有指定含义的不定变量而x的类型确定，值在函数调用的时候被赋值，称这种变量为绑定变量(Bound Variable) 123456789101112131415(x:Int)=&gt;x+more var more=3 var fun = (x:Int)=&gt;x+more more =10 println(fun(1)) println(fun(13)) more =10.87 println(fun(1)) println(fun(13))var sum=0val list = List(1,2,3,4,5,6,7,88,9)list.foreach(sum+=_) *像这种运行时确定more类型及值的函数称为闭包，more是个自由变量，在运行时值和类型得以确定，这是一个由开放(free)到封闭的过程。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何开启一个线程，开启大量线程会有什么问题，如何优化？]]></title>
    <url>%2F%E5%A6%82%E4%BD%95%E5%BC%80%E5%90%AF%E4%B8%80%E4%B8%AA%E7%BA%BF%E7%A8%8B%EF%BC%8C%E5%BC%80%E5%90%AF%E5%A4%A7%E9%87%8F%E7%BA%BF%E7%A8%8B%E4%BC%9A%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%8C%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%EF%BC%9F.html</url>
    <content type="text"><![CDATA[这道题想考察什么？ 是否了解线程开启的方式？ 开启大量线程会引起什么问题？为什么？怎么优化？ 考察的知识点 线程的开启方式 开启大量线程的问题 线程池 考生应该如何回答1、首先，关于如何开启一个线程，大多数人可能都会说3种，Thread、Runnable、Callback嘛！但事实却不是这样的。看JDK里怎么说的。 1234567891011/** * ... * There are two ways to create a new thread of execution. One is to * declare a class to be a subclass of &lt;code&gt;Thread&lt;/code&gt;. * The other way to create a thread is to declare a class that * implements the &lt;code&gt;Runnable&lt;/code&gt; interface. * .... */public class Thread implements Runnable&#123; &#125; Thread源码的类描述中有这样一段，翻译一下，只有两种方法去创建一个执行线程，一种是声明一个Thread的子类，另一种是创建一个类去实现Runnable接口。惊不惊讶，并没有提到Callback。 继承Thread类 12345678910111213141516171819public class ThreadUnitTest &#123;​ @Test public void testThread() &#123; //创建MyThread实例 MyThread myThread = new MyThread(); //调用线程start的方法，进入可执行状态 myThread.start(); &#125;​ //继承Thread类，重写内部run方法 static class MyThread extends Thread &#123;​ @Override public void run() &#123; System.out.println("test MyThread run"); &#125; &#125;&#125; 实现Runnable接口 12345678910111213141516171819public class ThreadUnitTest &#123;​ @Test public void testRunnable() &#123; //创建MyRunnable实例，这其实只是一个任务，并不是线程 MyRunnable myRunnable = new MyRunnable(); //交给线程去执行 new Thread(myRunnable).start(); &#125;​ //实现Runnable接口，并实现内部run方法 static class MyRunnable implements Runnable &#123;​ @Override public void run() &#123; System.out.println("test MyRunnable run"); &#125; &#125;&#125; 还是看看Callable是怎么回事吧。 1234567891011121314151617181920212223242526272829303132public class ThreadUnitTest &#123;​ @Test public void testCallable() &#123; //创建MyCallable实例，需要与FutureTask结合使用 MyCallable myCallable = new MyCallable(); //创建FutureTask，与Runnable一样，也只能算是个任务 FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(myCallable); //交给线程去执行 new Thread(futureTask).start();​ try &#123; //get方法获取任务返回值，该方法是阻塞的 String result = futureTask.get(); System.out.println(result); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;​ //实现Callable接口，并实现call方法，不同之处是该方法有返回值 static class MyCallable implements Callable&lt;String&gt; &#123;​ @Override public String call() throws Exception &#123; Thread.sleep(10000); return "test MyCallable run"; &#125; &#125;&#125; Callable的方式必须与FutureTask结合使用，我们看看FutureTask的继承关系。 123456789//FutureTask实现了RunnableFuture接口public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123;​&#125;​//RunnableFuture接口继承Runnable和Future接口public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; void run();&#125; 真相大白了，其实实现Callback接口创建线程的方式，归根到底就是Runnable方式，只不过它是在Runnable的基础上又增加了一些能力，例如取消任务执行等。 2、开启线程的几种方式算是答上了，那开启大量线程，或者说频繁开启线程到底会引起什么问题呢？众所周知，在Java中，调用Thread的start方法后，该线程即置为就绪状态，等待CPU的调度。这个流程里有两个关注点需要去理解。 start内部怎样开启线程的？看看start方法是怎么实现的。 12345678910111213141516171819202122232425// Thread类的start方法public synchronized void start() &#123; // 一系列状态检查 if (threadStatus != 0) throw new IllegalThreadStateException(); group.add(this); boolean started = false; try &#123; //调用start0方法，真正启动java线程的地方 start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; &#125; &#125; &#125; //start0方法是一个native方法private native void start0(); JVM中，native方法与java方法存在一个映射关系，Java中的start0对应c层的JVM_StartThread方法，我们继续看一下。 1234567891011121314151617181920212223242526JVM_ENTRY(void, JVM_StartThread(JNIEnv* env, jobject jthread)) JVMWrapper("JVM_StartThread"); JavaThread *native_thread = NULL; bool throw_illegal_thread_state = false; &#123; MutexLocker mu(Threads_lock); // 判断Java线程是否已经启动，如果已经启动过，则会抛异常。 if (java_lang_Thread::thread(JNIHandles::resolve_non_null(jthread)) != NULL) &#123; throw_illegal_thread_state = true; &#125; else &#123; //如果没有启动过，走到这里else分支，去创建线程 //分配c++线程结构并创建native线程 jlong size = java_lang_Thread::stackSize(JNIHandles::resolve_non_null(jthread)); size_t sz = size &gt; 0 ? (size_t) size : 0; //注意这里new JavaThread native_thread = new JavaThread(&amp;thread_entry, sz); if (native_thread-&gt;osthread() != NULL) &#123; native_thread-&gt;prepare(jthread); &#125; &#125; &#125; ...... Thread::start(native_thread); 走到这里发现，Java层已经过渡到native层，但远远还没结束。 12345678910111213141516171819202122JavaThread::JavaThread(ThreadFunction entry_point, size_t stack_sz) : Thread() &#123; initialize(); _jni_attach_state = _not_attaching_via_jni; set_entry_point(entry_point); os::ThreadType thr_type = os::java_thread; thr_type = entry_point == &amp;compiler_thread_entry ? os::compiler_thread : os::java_thread; //根据平台，调用create_thread，创建真正的内核线程 os::create_thread(this, thr_type, stack_sz); &#125; bool os::create_thread(Thread* thread, ThreadType thr_type, size_t req_stack_size) &#123; ...... pthread_t tid; //利用pthread_create（）来创建线程 int ret = pthread_create(&amp;tid, &amp;attr, (void* (*)(void*)) thread_native_entry, thread); ...... return true;&#125; pthread_create方法，第三个参数表示启动这个线程后要执行的方法的入口，第四个参数表示要给这个方法传入的参数。 1234567static void *thread_native_entry(Thread *thread) &#123; ...... //thread_native_entry方法的最下面的run方法，这个thread就是上面传递下来的参数，也就是JavaThread thread-&gt;run(); ...... return 0;&#125; 终于开始执行run方法了！！！ 12345678910111213141516171819202122//thread.cpp类void JavaThread::run() &#123; ...... //调用内部thread_main_inner thread_main_inner();&#125; void JavaThread::thread_main_inner() &#123; if (!this-&gt;has_pending_exception() &amp;&amp; !java_lang_Thread::is_stillborn(this-&gt;threadObj())) &#123; &#123; ResourceMark rm(this); this-&gt;set_native_thread_name(this-&gt;get_thread_name()); &#125; HandleMark hm(this); //注意：内部通过JavaCalls模块，调用了Java线程要执行的run方法 this-&gt;entry_point()(this, this); &#125; DTRACE_THREAD_PROBE(stop, this); this-&gt;exit(false); delete this;&#125; 一条U字型代码调用链总算结束了，高呼一声，原来start之后做了这么多事情呀，稍微总结一下。 1) Java中调用Thread的star方法，通过JNI方式，调用到native层。 2) native层，JVM通过pthread_create方法创建一个系统内核线程，并指定内核线程的初始运行地址，即一个方法指针。 3) 在内核线程的初始运行方法中，利用JavaCalls模块，回调到java线程的run方法，开始java级别的线程执行。 线程开启后CPU调度会发生什么？ 计算机的世界里，CPU会分为若干时间片，通过各种算法分配时间片来执行任务，有耳熟能详时间片轮转调度算法、短进程优先算法、优先级算法等。当一个任务的时间片用完，就会切换到另一个任务。在切换之前会保存上一个任务的状态，当下次再切换到该任务，就会加载这个状态， 这就是所谓的线程的上下文切换。很明显，上下文的切换是有开销的，包括很多方面，操作系统保存和恢复上下文的开销、线程调度器调度线程的开销和高速缓存重新加载的开销等。 经过上面两个理论基础的回顾，开启大量线程引起的问题，总结起来，就两个字——开销。 消耗时间。线程的创建和销毁都需要时间，当数量太大的时候，会影响效率。 消耗内存。创建更多的线程会消耗更多的内存，这是毋庸置疑的。线程频繁创建与销毁，还有可能引起内存抖动，频繁触发GC，最直接的表现就是卡顿。长而久之，内存资源占用过多或者内存碎片过多，系统甚至会出现OOM。 消耗CPU。在操作系统中，CPU都是遵循时间片轮转机制进行处理任务，线程数过多，必然会引起CPU频繁的进行线程上下文切换。这个代价是昂贵的，某些场景下甚至超过任务本身的消耗。 3、针对上面提及到的问题，我们自然需要进行优化。线程的本质是为了执行任务，在计算机的世界里，任务分大致分为两类，CPU密集型任务和IO密集型任务。 CPU密集型任务，比如公式计算、资源解码等。这类任务要进行大量的计算，全都依赖CPU的运算能力，持久消耗CPU资源。所以针对这类任务，其实不应该开启大量线程。因为线程越多，花在线程切换的时间就越多，CPU执行效率就越低，一般CPU密集型任务同时进行的数量等于CPU的核心数，最多再加个1。 IO密集型任务，比如网络读写、文件读写等。这类任务不需要消耗太多的CPU资源，绝大部分时间是在IO操作上。所以针对这类任务，可以开启大量线程去提高CPU的执行效率，一般IO密集型任务同时进行的数量等于CPU的核心数的两倍。 另外，在无法避免，必须要开启大量线程的情况下，我们也可以使用线程池代替直接创建线程的做法进行优化。线程池的基本作用就是复用已有的线程，从而减少线程的创建，降低开销。在Java中，线程池的使用还是非常方便的，JDK中提供了现成的ThreadPoolExecutor类，我们只需要按照自己的需求进行相应的参数配置即可，这里提供一个示例。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/** * 线程池使用 */public class ThreadPoolService &#123;​ /** * 线程池变量 */ private ThreadPoolExecutor mThreadPoolExecutor;​ private static volatile ThreadPoolService sInstance = null;​ /** * 线程池中的核心线程数，默认情况下，核心线程一直存活在线程池中，即便他们在线程池中处于闲置状态。 * 除非我们将ThreadPoolExecutor的allowCoreThreadTimeOut属性设为true的时候，这时候处于闲置的核心 * 线程在等待新任务到来时会有超时策略，这个超时时间由keepAliveTime来指定。一旦超过所设置的超时时间，闲 * 置的核心线程就会被终止。 * CPU密集型任务 N+1 IO密集型任务 2*N */ private final int CORE_POOL_SIZE = Runtime.getRuntime().availableProcessors() + 1; /** * 线程池中所容纳的最大线程数，如果活动的线程达到这个数值以后，后续的新任务将会被阻塞。包含核心线程数+非* * 核心线程数。 */ private final int MAXIMUM_POOL_SIZE = Math.max(CORE_POOL_SIZE, 10); /** * 非核心线程闲置时的超时时长，对于非核心线程，闲置时间超过这个时间，非核心线程就会被回收。 * 只有对ThreadPoolExecutor的allowCoreThreadTimeOut属性设为true的时候，这个超时时间才会对核心线 * 程产生效果。 */ private final long KEEP_ALIVE_TIME = 2; /** * 用于指定keepAliveTime参数的时间单位。 */ private final TimeUnit UNIT = TimeUnit.SECONDS; /** * 线程池中保存等待执行的任务的阻塞队列 * ArrayBlockingQueue 基于数组实现的有界的阻塞队列 * LinkedBlockingQueue 基于链表实现的阻塞队列 * SynchronousQueue 内部没有任何容量的阻塞队列。在它内部没有任何的缓存空间 * PriorityBlockingQueue 具有优先级的无限阻塞队列。 */ private final BlockingQueue&lt;Runnable&gt; WORK_QUEUE = new LinkedBlockingDeque&lt;&gt;(); /** * 线程工厂，为线程池提供新线程的创建。ThreadFactory是一个接口，里面只有一个newThread方法。 默认为DefaultThreadFactory类。 */ private final ThreadFactory THREAD_FACTORY = Executors.defaultThreadFactory(); /** * 拒绝策略，当任务队列已满并且线程池中的活动线程已经达到所限定的最大值或者是无法成功执行任务，这时候 * ThreadPoolExecutor会调用RejectedExecutionHandler中的rejectedExecution方法。 * CallerRunsPolicy 只用调用者所在线程来运行任务。 * AbortPolicy 直接抛出RejectedExecutionException异常。 * DiscardPolicy 丢弃掉该任务，不进行处理。 * DiscardOldestPolicy 丢弃队列里最近的一个任务，并执行当前任务。 */ private final RejectedExecutionHandler REJECTED_HANDLER = new ThreadPoolExecutor.AbortPolicy();​ private ThreadPoolService() &#123; &#125;​ /** * 单例 * @return */ public static ThreadPoolService getInstance() &#123; if (sInstance == null) &#123; synchronized (ThreadPoolService.class) &#123; if (sInstance == null) &#123; sInstance = new ThreadPoolService(); sInstance.initThreadPool(); &#125; &#125; &#125; return sInstance; &#125;​ /** * 初始化线程池 */ private void initThreadPool() &#123; try &#123; mThreadPoolExecutor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAXIMUM_POOL_SIZE, KEEP_ALIVE_TIME, UNIT, WORK_QUEUE, THREAD_FACTORY, REJECTED_HANDLER); &#125; catch (Exception e) &#123; LogUtil.printStackTrace(e); &#125; &#125;​ /** * 向线程池提交任务,无返回值 * * @param runnable */ public void post(Runnable runnable) &#123; mThreadPoolExecutor.execute(runnable); &#125;​ /** * 向线程池提交任务,有返回值 * * @param callable */ public &lt;T&gt; Future&lt;T&gt; post(Callable&lt;T&gt; callable) &#123; RunnableFuture&lt;T&gt; task = new FutureTask&lt;T&gt;(callable); mThreadPoolExecutor.execute(task); return task; &#125;&#125; 备注：转载至知乎]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务特性-ACID]]></title>
    <url>%2F%E4%BA%8B%E5%8A%A1%E7%89%B9%E6%80%A7-ACID.html</url>
    <content type="text"><![CDATA[事务具有4个特征，分别是原子性、一致性、隔离性和持久性，简称事务的ACID特性； 原子性（atomicity)一个事务要么全部提交成功，要么全部失败回滚，不能只执行其中的一部分操作，这就是事务的原子性。 一致性（consistency)事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行之前和执行之后，数据库都必须处于一致性状态。 如果数据库系统在运行过程中发生故障，有些事务尚未完成就被迫中断，这些未完成的事务对数据库所作的修改有一部分已写入物理数据库，这是数据库就处于一种不正确的状态，也就是不一致的状态。 隔离性（isolation）事务的隔离性是指在并发环境中，并发的事务时相互隔离的，一个事务的执行不能不被其他事务干扰。不同的事务并发操作相同的数据时，每个事务都有各自完成的数据空间，即一个事务内部的操作及使用的数据对其他并发事务时隔离的，并发执行的各个事务之间不能相互干扰。 在标准SQL规范中，定义了4个事务隔离级别，不同的隔离级别对事务的处理不同，分别是：未授权读取，授权读取，可重复读取和串行化 1、读未提交（Read Uncommited），该隔离级别允许脏读取，其隔离级别最低；比如事务A和事务B同时进行，事务A在整个执行阶段，会将某数据的值从1开始一直加到10，然后进行事务提交，此时，事务B能够看到这个数据项在事务A操作过程中的所有中间值（如1变成2，2变成3等），而对这一系列的中间值的读取就是未授权读取 2、授权读取也称为已提交读（Read Commited），授权读取只允许获取已经提交的数据。比如事务A和事务B同时进行，事务A进行+1操作，此时，事务B无法看到这个数据项在事务A操作过程中的所有中间值，只能看到最终的10。另外，如果说有一个事务C，和事务A进行非常类似的操作，只是事务C是将数据项从10加到20，此时事务B也同样可以读取到20，即授权读取允许不可重复读取。 3、可重复读（Repeatable Read) 就是保证在事务处理过程中，多次读取同一个数据时，其值都和事务开始时刻是一致的，因此该事务级别禁止不可重复读取和脏读取，但是有可能出现幻影数据。所谓幻影数据，就是指同样的事务操作，在前后两个时间段内执行对同一个数据项的读取，可能出现不一致的结果。在上面的例子中，可重复读取隔离级别能够保证事务B在第一次事务操作过程中，始终对数据项读取到1，但是在下一次事务操作中，即使事务B（注意，事务名字虽然相同，但是指的是另一个事务操作）采用同样的查询方式，就可能读取到10或20； 4、串行化 是最严格的事务隔离级别，它要求所有事务被串行执行，即事务只能一个接一个的进行处理，不能并发执行。 持久性（durability）一旦事务提交，那么它对数据库中的对应数据的状态的变更就会永久保存到数据库中。–即使发生系统崩溃或机器宕机等故障，只要数据库能够重新启动，那么一定能够将其恢复到事务成功结束的状态。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>ACID</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OLAP和OLTP的区别]]></title>
    <url>%2FOLAP%E5%92%8COLTP%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content type="text"><![CDATA[OLAP（On-Line Analytical Processing）联机分析处理，也称为面向交易的处理过程，其基本特征是前台接收的用户数据可以立即传送到计算中心进行处理，并在很短的时间内给出处理结果，是对用户操作快速响应的方式之一。应用在数据仓库，使用对象是决策者。OLAP系统强调的是数据分析，响应速度要求没那么高。 OLTP（On-Line Transaction Processing）联机事务处理，它使分析人员能够迅速、一致、交互地从各个方面观察信息，以达到深入理解数据的目的。它具有FASMI(Fast Analysis of Shared Multidimensional Information)，即共享多维信息的快速分析的特征。主要应用是传统关系型数据库。OLTP系统强调的是内存效率，实时性比较高。 以下是OLAP和OLTP的比较：]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>OLAP,OLTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL8.0修改密码问题]]></title>
    <url>%2FMySQL8-0%E4%BF%AE%E6%94%B9%E5%AF%86%E7%A0%81%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[MySQL5.7和之前的用户修改密码方式： 123mysql -uroot -e "Set password=password(‘123’);"mysql -uroot -p123.com -e "use mysql;update user set authentication_string=password('456') where user='root';"update mysql.user set authentication_string=password("123") where user='root'; 以上三种方法在MySQL8.0以后版本中将不能使用，如果使用了将会导致在正确修改密码是报如下错误： 12mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '123';ERROR 1396 (HY000): Operation ALTER USER failed for 'root'@'localhost' 如遇上以上问题请使用update语句先清空authentication_string字段，然后再修改密码即可 12update user set authentication_string='' where user='root';ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '你的密码'; 所以特别提醒童鞋们： MySQL8.0后请使用alter修改用户密码，因为在MySQL8.0以后的加密方式为caching_sha2_password，如果使用update修改密码会给user表中root用户的authentication_string字段下设置newpassowrd值，当再使用alter user ‘root’@’localhost’ identified by ‘newpassword’修改密码时会一直报错，必须清空后再修改，因为authentication_string字段下只能是MySQL加密后的43位字符串密码，其他的会报格式错误，所以在MySQL8.0以后能修改密码的方法只能是：ALTER USER ‘root’@’localhost’ IDENTIFIED WITH mysql_native_password BY ‘你的密码’;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL8.0</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7.5+Ambari2.7.3部署安装]]></title>
    <url>%2Fcentos7-5-Ambari2-7-3%E9%83%A8%E7%BD%B2%E5%AE%89%E8%A3%85.html</url>
    <content type="text"><![CDATA[–环境信息：182.15.240.145 master182.15.240.146 slave1182.15.240.147 slave2 –安装所需的包：ambari-2.7.3.0-centos7.tar.gzHDP-3.1.0.0-centos7-rpm.tar.gzHDP-UTILS-1.1.0.22-centos7.tar.gzjdk-8u102-linux-x64.tar.gzmysql-5.7.27-1.el7.x86_64.rpm-bundle.tar 注：以下所有操作都是用root权限！ 一、卸载自带jdk - 所有机器 (1)查看自带JDk版本rpm -qa|grep java (2)卸载自带JDK 12rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.161-0.b14.el7_4.x86_64rpm -e --nodeps java-1.8.0-openjdk-1.8.0.161-0.b14.el7_4.x86_64 (3)上传JDk并解压jdk版本：jdk-8u102-linux-x64.tar.gz上传JDk包至/usr/local/jdk目录下，并切换到该目录（没有jdk目录则手动创建），执行远程拷贝命令如，每个节点的jdk安装路径一样。拷贝完后解压JDK包。 12scp jdk-8u102-linux-x64.tar.gz root@182.15.240.146:/usr/local/jdkscp jdk-8u102-linux-x64.tar.gz root@182.15.240.147:/usr/local/jdk 解压后路径：/usr/local/jdk/jdk1.8.0_102 (4)配置JDk每个节点都需要配置，且配置JDK的内容相同。vim /etc/profile末尾加入下配置： 123export JAVA_HOME=/usr/local/jdk/jdk1.8.0_102export CLASSPATH=$:CLASSPATH:$JAVA_HOME/lib/export PATH=$PATH:$JAVA_HOME/bin (5)jdk生效source /etc/profile 二、设置主机名称 - 所有机器vim /etc/hostname若有三个节点，选一个节点作为主节点，修改其hostname中为master，其他节点的主机名称依次修改为slave1、slave2。然后使用命令：reboot重启生效。 三、修改Hosts - 所有机器vim /etc/hosts 每个节点的hosts文件中添加如下配置： 123182.15.240.145 master182.15.240.146 slave1182.15.240.147 slave2 四、修改network - 所有机器vim /etc/sysconfig/network 添加如下配置： 12# Created by anacondaNETWORKING=yes 通过ping主机名看是否通讯正常。ping slave1 五、打开安全限制 - 所有机器vim /etc/security/limits.conf 文件末尾新增如下： 12345# End of file* soft nofile 65536* hard nofile 65536* soft nproc 131072* hard nproc 131072 六、关闭防火墙 - 所有机器 12[root@master~]#systemctl disable firewalld[root@master~]#systemctl stop firewalld 另外所有机器还需修改：[root@master ~]# vim /etc/selinux/config参数修改如下： 12SELINUX=disabledSELINUXTYPE=targeted 七、同步时钟1、安装chrony服务 - 所有机器yum -y install chrony 2、设置master为主服务器，开启nptd服务（主服务器） 主服务器上该配置文件修改项如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243vim /etc/chrony.conf----------------------------# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).# server 0.centos.pool.ntp.org iburst# server 1.centos.pool.ntp.org iburst# server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver 182.15.240.145 iburst# Record the rate at which the system clock gains/losses time.driftfile /var/lib/chrony/drift# Allow the system clock to be stepped in the first three updates# if its offset is larger than 1 second.makestep 1.0 3# Enable kernel synchronization of the real-time clock (RTC).rtcsync# Enable hardware timestamping on all interfaces that support it.#hwtimestamp *# Increase the minimum number of selectable sources required to adjust# the system clock.#minsources 2# Allow NTP client access from local network.#allow 192.168.0.0/16allow 182.15.240.0/24 # Serve time even if not synchronized to a time source.#local stratum 10local stratum 10# Specify file containing keys for NTP authentication.#keyfile /etc/chrony.keys# Specify directory for log files.logdir /var/log/chrony# Select which information is logged.#log measurements statistics tracking 修改保存后执行： 12[root@master ~]# systemctl restart chronyd.service #启动服务[root@master ~]# systemctl status chronyd.service #开机自启动 查看时间同步状态： 123chronyc -a makestepchronyc sourcestatschronyc sources -v 3、子节点时间同步配置vim /etc/chrony.conf注释原来的server并新增如下配置即可然后重启服务：server 182.15.240.145 [root@master ~]# systemctl restart chronyd.service #启动服务[root@master ~]# systemctl status chronyd.service #开机自启动 每个子节点的chrony.conf里都要配置server 182.15.240.145，并重启服务！ 八、SSH无密码登录 - 主节点1、免密钥操作 1234[root@master ~]# ssh-keygen -t rsa 连接提示选yes ，密码提示填登录密码[root@master ~]# ssh-copy-id slave1[root@master ~]# ssh-copy-id slave2[root@master ~]# ssh-copy-id master 测试是否实现了无密码登录: 1234[root@master ~]# ssh slave1 date ;ssh slave2 date;ssh master date;2019年 04月 23日 星期一 14:18:13 CST2019年 04月 23日 星期一 14:18:13 CST2019年 04月 23日 星期一 14:18:13 CST 2、保存密钥将创建的秘钥拷贝出来，因为后面ambari安装的时候需要上传这个秘钥。创建秘钥是在隐藏文件夹/root/.ssh/下面的，所以需要先把秘钥拷贝到可见区域，然后拷贝到电脑上。 123456[root@master ~]# cd /root/.ssh/[root@master .ssh]# lsauthorized_keys id_rsa id_rsa.pub known_hosts[root@master .ssh]# cp id_rsa /root/[root@master .ssh]# ls /root/anaconda-ks.cfg id_rsa jdk-8u161-linux-x64.rpm id_rsa为密钥！ 九、其他系统设置 - 所有机器 1root@master ~]# sudo sh -c "echo umask 0022 &gt;&gt; /etc/profile" 十、修改yum源，实现离线安装1、安装httpd服务 - 主服务器 1234[root@master ~]# yum -y install httpd[root@master ~]# service httpd restartRedirecting to /bin/systemctl restart httpd.service[root@master ~]# chkconfig httpd on 2、上传三个包放到/var/www/html目录下 - 主服务器安装完成后，会生成 /var/www/html目录（相当于Tomcat的webapps目录），进入到/var/www/html目录下，创建ambari和hdp目录，用来存放安装文件. 123456[root@yum ~]# mkdir /var/www/html/ambari[root@yum ~]# mkdir /var/www/html/hdp[root@yum ~]# mkdir /var/www/html/hdp/HDP-UTILS-1.1.0.22[root@yum ~]# tar -zxvf ambari-2.7.3.0-centos7.tar.gz -C /var/www/html/ambari/[root@yum ~]# tar -zxvf HDP-3.1.0.0-centos7-rpm.tar.gz -C /var/www/html/hdp/[root@yum ~]# tar -zxvf HDP-UTILS-1.1.0.22-centos7.tar.gz -C /var/www/html/hdp/HDP-UTILS-1.1.0.22/ 3、启动httpd服务： 123[root@yum ~]# systemctl start httpd # 启动httpd[root@yum ~]# systemctl status httpd # 查看httpd状态[root@yum ~]# systemctl enable httpd # 设置httpd开机自启 现在可以通过访问 12http://182.15.240.145/ambari/ http://182.15.240.145/hdp/ 十一、制作本地源https://docs.hortonworks.com/HDPDocuments/Ambari-2.7.3.0/bk_ambari-installation/content/ambari_repositories.htmlhttps://docs.hortonworks.com/HDPDocuments/Ambari-2.7.3.0/bk_ambari-installation/content/hdp_31_repositories.html 1、安装本地源 - 主服务器(1)下载ambari.repo文件 1[root@master ambari]# wget -O /etc/yum.repos.d/ambari.repo http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.3.0/ambari.repo 注：路径一定要配置正确！！！ 修改配置文件：vim /etc/yum.repos.d/ambari.repo 12345678910[root@master ambari]# vi ambari/centos7/2.7.3.0-139/ambari.repo#VERSION_NUMBER=2.7.3.0-139[ambari-2.7.3.0]#json.url = http://public-repo-1.hortonworks.com/HDP/hdp_urlinfo.jsonname=ambari Version - ambari-2.7.3.0baseurl=http://182.15.240.145/ambari/ambari/centos7/2.7.3.0-139gpgcheck=1gpgkey=http://182.15.240.145/ambari/ambari/centos7/2.7.3.0-139/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1 (2)配置HDP和HDP-TILS 123456789101112131415161718[root@yum yum.repos.d]# touch /etc/yum.repos.d/HDP.repo[root@master ambari]# vim HDP/centos7/3.1.0.0-78/hdp.repo#VERSION_NUMBER=3.1.0.0-78[HDP-3.1.0.0]name=HDP Version - HDP-3.1.0.0baseurl=http://182.15.240.145/hdp/HDP/centos7gpgcheck=1gpgkey=http://182.15.240.145/hdp/HDP/centos7/3.1.0.0-78/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1[HDP-UTILS-1.1.0.22]name=HDP-UTILS Version - HDP-UTILS-1.1.0.22baseurl=http://182.15.240.145/hdp/HDP-UTILS-1.1.0.22gpgcheck=1gpgkey=http://182.15.240.145/hdp/HDP-UTILS-1.1.0.22/HDP-UTILS/centos7/1.1.0.22/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1 2、清理一下yum的缓存 123[root@master ambari]# yum clean all[root@master ambari]# yum makecache[root@master ambari]# yum repolist 3、将创建好的文件拷贝到子节点 - 主服务器 123[root@master ambari]#cd /etc/yum.repos.d[root@master yum.repos.d]# scp ambari.repo HDP.repo slave1:$PWD[root@master yum.repos.d]# scp ambari.repo HDP.repo slave2:$PWD 十二、安装ambari-server1、以mysql为数据库安装 - 主服务器mysql包版本：mysql-5.7.27-1.el7.x86_64.rpm-bundle.tar上传到主节点/usr/local/mysql目录下并解压。 12rpm -qa |grep -i mysqlrpm -qa |grep -i mariadb 用rpm -e 删除包强制卸载mariadb 1rpm -e --nodeps mariadb-libs-5.5.44-2.el7.centos.x86_64 这里是为了避免安装mysql时出现依赖错误，提前删除系统已有的mariadb和mysql包！ 安装mysql可能用到的工具： 12yum -y install libaioyum -y install net-tools 2、安装mysql-server:按照common–&gt;libs–&gt;client–&gt;server的顺序。若不按照此顺序，也会有一定“依赖”关系的提醒。 1234567891011121314151617181920[root@hadoop mysql]# rpm -ivh mysql-community-common-5.7.18-1.el7.x86_64.rpm warning: mysql-community-common-5.7.18-1.el7.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing...1:mysql-community-common-5.7.18-1.e################################# [100%][root@hadoop mysql]# rpm -ivh mysql-community-libs-5.7.18-1.el7.x86_64.rpm warning: mysql-community-libs-5.7.18-1.el7.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing...1:mysql-community-libs-5.7.18-1.el7################################# [100%][root@hadoop mysql]# rpm -ivh mysql-community-client-5.7.18-1.el7.x86_64.rpm warning: mysql-community-client-5.7.18-1.el7.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing...1:mysql-community-client-5.7.18-1.e################################# [100%][root@hadoop mysql]# rpm -ivh mysql-community-server-5.7.18-1.el7.x86_64.rpm warning: mysql-community-server-5.7.18-1.el7.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYPreparing... ################################# [100%]Updating / installing...1:mysql-community-server-5.7.18-1.e################################# [100%] 3、初始化mysql 1[root@hadoop mysql]# mysqld --initialize 4、更改mysql数据库所属于用户及其所属于组 1[root@hadoop mysql]# chown mysql:mysql /var/lib/mysql -R 5、启动mysql数据库 12345[root@hadoop mysql]# cd /var/lib/mysql[root@hadoop mysql]# systemctl start mysqld.service[root@hadoop ~]# cd /var/log/[root@hadoop log]# grep 'password' mysqld.log 2019-02-26T04:33:06.989818Z 1 [Note] A temporary password is generated for root@localhost: mxeV&amp;htW-3VC 更改root用户密码，新版的mysql在第一次登录后更改密码前是不能执行任何命令的 123456789101112131415161718192021222324[root@hadoop log]# mysql -u root -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 4Server version: 5.7.18Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.更改密码mysql&gt; set password=password('oracle');Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt; grant all privileges on *.* to root@'%' identified by 'oracle' with grant option;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 安装和配置ambari-server 1yum -y install ambari-server 6、登录mysql创建ambari安装所需要的库 设置的账号后面配置ambari-server的时候会用到！！！ 执行如下语句： 12345678910111213141516171819202122232425262728293031CREATE DATABASE ambari; use ambari; CREATE USER 'ambari'@'%' IDENTIFIED BY 'ambari123'; GRANT ALL PRIVILEGES ON *.* TO 'ambari'@'%'; CREATE USER 'ambari'@'localhost' IDENTIFIED BY 'ambari123'; GRANT ALL PRIVILEGES ON *.* TO 'ambari'@'localhost'; CREATE USER 'ambari'@'master' IDENTIFIED BY 'ambari123'; GRANT ALL PRIVILEGES ON *.* TO 'ambari'@'master'; FLUSH PRIVILEGES; source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql show tables; use mysql; select Host User Password from user where user='ambari'; CREATE DATABASE hive; use hive; CREATE USER 'hive'@'%' IDENTIFIED BY 'hive'; GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%'; CREATE USER 'hive'@'localhost' IDENTIFIED BY 'hive'; GRANT ALL PRIVILEGES ON *.* TO 'hive'@'localhost'; CREATE USER 'hive'@'master' IDENTIFIED BY 'hive'; GRANT ALL PRIVILEGES ON *.* TO 'hive'@'master'; FLUSH PRIVILEGES; CREATE DATABASE oozie; use oozie; CREATE USER 'oozie'@'%' IDENTIFIED BY 'oozie'; GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'%'; CREATE USER 'oozie'@'localhost' IDENTIFIED BY 'oozie'; GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'localhost'; CREATE USER 'oozie'@'master' IDENTIFIED BY 'oozie'; GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'master'; FLUSH PRIVILEGES; 如果要重新执行以上语句，则需要先删除：\1. delete from mysql.user where user=’sonar’; 删除用户2.CREATE USER ‘sonar’@’%’IDENTIFIED BY ‘sonar’;天假用户\3. flush privileges; 清理缓存\4. select from mysql.user where user=’sonar’;查看该用户是否存在 ，结果是null\5. flush privileges;清理缓存\6. GRANT ALL ON sonar. TO ‘sonar’@’%’ IDENTIFIED BY ‘sonar’;用户操作授权 7、mysql与ambari-server的连接下载mysql-connector-java-5.1.40.jar放到root文件下 1234mkdir /usr/share/javacp /root/mysql-connector-java-5.1.40.jar /usr/share/java/mysql-connector-java.jarcp /usr/share/java/mysql-connector-java.jar /var/lib/ambari-server/resources/mysql-jdbc-driver.jarvi /etc/ambari-server/conf/ambari.properties 123456添加server.jdbc.driver.path=/usr/share/java/mysql-connector-java.jar8、初始化ambari-server[root@master ~]# ambari-server setup下面是配置执行流程，按照提示操作： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354（1） 提示是否自定义设置。输入：yCustomize user account for ambari-server daemon [y/n] (n)? y（2）ambari-server 账号。Enter user account for ambari-server daemon (root):如果直接回车就是默认选择root用户如果输入已经创建的用户就会显示：Enter user account for ambari-server daemon (root):ambariAdjusting ambari-server permissions and ownership...（3）检查防火墙是否关闭Adjusting ambari-server permissions and ownership...Checking firewall...WARNING: iptables is running. Confirm the necessary Ambari ports are accessible. Refer to the Ambari documentation for more details on ports.OK to continue [y/n] (y)?直接回车（4）设置JDK。输入：3Checking JDK...Do you want to change Oracle JDK [y/n] (n)? y[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8[2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7[3] Custom JDK==============================================================================Enter choice (1): 3如果上面选择3自定义JDK,则需要设置JAVA_HOME。输入：/usr/java/jdk1.8.0_161WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.Path to JAVA_HOME: /usr/java/jdk1.8.0_131Validating JDK on Ambari Server...done.Completing setup...（5）数据库配置。选择：yConfiguring database...Enter advanced database configuration [y/n] (n)? y（6）选择数据库类型。输入：3Configuring database...==============================================================================Choose one of the following options:[1] - PostgreSQL (Embedded)[2] - Oracle[3] - MySQL[4] - PostgreSQL[5] - Microsoft SQL Server (Tech Preview)[6] - SQL Anywhere==============================================================================Enter choice (3): 3（7）设置数据库的具体配置信息，根据实际情况输入，如果和括号内相同，则可以直接回车。如果想重命名，就输入。Hostname (localhost): masterPort (3306): 3306Database name (ambari): ambariUsername (ambari): ambariEnter Database Password (bigdata):ambari123Re-Enter password: ambari123（8）将Ambari数据库脚本导入到数据库WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql Proceed with configuring remote database connection properties [y/n] (y)? y[root@master ~]# ambari-server start 错误处理：如果出现错误，请注意查看日志，根据具体的错误内容进行处理，默认ambari-server的日志在/var/log/ambari-server/ambari-server.log里面。如果在处理日志的过程中或者后面安装的过程中出现一些莫名的错误，可以重置的安装。如果上面进行的默认数据库的配置，可以使用下面的代码重置ambari-server。 123[root@master ~]# ambari-server stop[root@master ~]# ambari-server reset[root@master ~]# ambari-server setup 如果选择的是第二种方式，就需要先执行上面的语句，然后手动将mysql里面创建的数据库进行删除。然后再重新执行第二步的操作 123456789101112131415[root@master ~]# mysql -uroot -pmysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || ambari || hive || oozie || performance_schema |+--------------------+rows in set (0.00 sec)mysql&gt; drop database ambari;mysql&gt; drop database hive;mysql&gt; drop database oozie; 先在mysql库的User表中删除ambari、hive和oozie用户，再重复第六小结的用户创建操作，然后执行第八小结的命令：ambari-server setup。 9、启动ambari-server服务 1234567891011121314[root@master ~]# ambari-server startUsing python /usr/bin/pythonStarting ambari-serverAmbari Server running with administrator privileges.Organizing resource files at /var/lib/ambari-server/resources...Ambari database consistency check started...Server PID at: /var/run/ambari-server/ambari-server.pidServer out at: /var/log/ambari-server/ambari-server.outServer log at: /var/log/ambari-server/ambari-server.logWaiting for server start..................................Server started listening on 8080DB configs consistency check: no errors and warnings were found.Ambari Server 'start' completed successfully. 十三、安装部署HDP集群 1、登录界面默认管理员账户登录， 账户：admin 密码：adminhttp://182.15.240.145:8080 2、安装向导(1)Launch Install Wizard(2)配置集群名称；(3)选择版本并修改本地源地址；选HDP-3.1.00;选Use Local Repository;选redhat7:HDP-3.1： http://182.15.240.145/hdp/HDP/centos7/HDP-3.1-GPL: http://public-repo-1.hortonworks.com/HDP-GPL/centos7/3.x/updates/3.1.0.0/HDP-UTILS-1.1.0.22: http://182.15.240.145/hdp/HDP-UTILS-1.1.0.22/ 配置节点和密钥Target hosts填masterslave1slave2 从主节点的/root目录下下载密钥id_rsa，并上传即可！ 使用第三方数据库mysql的时候需要执行：ambari-server setup –jdbc-db=mysql –jdbc-driver=/var/lib/ambari-agent/lib/mysql-connector-java-5.1.24.jar [root@master lib]# ambari-server setup –jdbc-db=mysql –jdbc-driver=/var/lib/ambari-agent/lib/mysql-connector-java-5.1.24.jarUsing python /usr/bin/pythonSetup ambari-serverCopying /var/lib/ambari-agent/lib/mysql-connector-java-5.1.24.jar to /var/lib/ambari-server/resources/mysql-connector-java-5.1.24.jarCreating symlink /var/lib/ambari-server/resources/mysql-connector-java-5.1.24.jar to /var/lib/ambari-server/resources/mysql-connector-java.jarIf you are updating existing jdbc driver jar for mysql with mysql-connector-java-5.1.24.jar. Please remove the old driver jar, from all hosts. Restarting services that need the driver, will automatically copy the new jar to the hosts.JDBC driver was successfully initialized.Ambari Server ‘setup’ completed successfully. Database URL: jdbc:mysql://master/hiveDatabase Password: hive/hive —————-最后一步的信息—————————————-Admin Name : adminCluster Name : leapTotal Hosts : 3 (3 new)Repositories:redhat7 (HDP-3.1):redhat7 (HDP-3.1-GPL):redhat7 (HDP-UTILS-1.1.0.22):Services:HDFS DataNode : 3 hostsNameNode : masterNFSGateway : 1 hostSNameNode : slave1 YARN + MapReduce2 Timeline Service V1.5 : slave1NodeManager : 3 hostsResourceManager : masterTimeline Service V2.0 Reader : masterRegistry DNS : master Tez Clients : 3 hosts Hive Metastore : slave1HiveServer2 : slave1Database : Existing MySQL / MariaDB Database HBase Master : masterRegionServer : 2 hostsPhoenix Query Server : 3 hosts ZooKeeper Server : 3 hosts Ambari Metrics Metrics Collector : slave2Grafana : master Kafka Broker : master SmartSense Activity Analyzer : masterActivity Explorer : masterHST Server : master Spark2 Livy for Spark2 Server : 2 hostsHistory Server : masterThrift Server : 2 hosts-———————————- 主页： http://182.15.240.145:8080 admin/adminyarn： http://master:8088/cluster]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>ambari,HDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH5.16.1集群企业真正离线部署]]></title>
    <url>%2FCDH5-16-1%E9%9B%86%E7%BE%A4%E4%BC%81%E4%B8%9A%E7%9C%9F%E6%AD%A3%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2.html</url>
    <content type="text"><![CDATA[视频:https://www.bilibili.com/video/av52167219PS:建议先看课程视频1-2篇，再根据视频或文档部署，如有问题，及时与@若泽数据J哥联系。 一.准备工作1.离线部署主要分为三块:a.MySQL离线部署b.CM离线部署c.Parcel文件离线源部署 2.规划: 节点 MySQL部署组件 Parcel文件离线源 CM服务进程 大数据组件 hadoop001 MySQL Parcel Activity Monitor NN RM DN NM hadoop002 Alert Publisher Event Server DN NM hadoop003 Host Monitor Service Monitor DN NM 3.下载源: CMcloudera-manager-centos7-cm5.16.1_x86_64.tar.gz ParcelCDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcelCDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1manifest.json JDKhttps://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html下载jdk-8u202-linux-x64.tar.gz MySQLhttps://dev.mysql.com/downloads/mysql/5.7.html#downloads下载mysql-5.7.26-el7-x86_64.tar.gz MySQL jdbc jarmysql-connector-java-5.1.47.jar下载完成后要重命名去掉版本号，mv mysql-connector-java-5.1.47.jar mysql-connector-java.jar 准备好百度云,下载安装包:链接:https://pan.baidu.com/s/10s-NaFLfztKuWImZTiBMjA 密码:viqp 二.集群节点初始化1.阿里云上海区购买3台，按量付费虚拟机CentOS7.2操作系统，2核8G最低配置 2.当前笔记本或台式机配置hosts文件 MAC: /etc/hosts Window: C:\windows\system32\drivers\etc\hosts 公网地址: 1231106.15.234.222 hadoop001 2106.15.235.200 hadoop002 3106.15.234.239 hadoop003 3.设置所有节点的hosts文件私有地址、内网地址: 1231echo &quot;172.19.7.96 hadoop001&quot;&gt;&gt; /etc/hosts2echo &quot;172.19.7.98 hadoop002&quot;&gt;&gt; /etc/hosts3echo &quot;172.19.7.97 hadoop003&quot;&gt;&gt; /etc/hosts 4.关闭所有节点的防火墙及清空规则1231systemctl stop firewalld 2systemctl disable firewalld3iptables -F 5.关闭所有节点的selinux1231vi /etc/selinux/config2将SELINUX=enforcing改为SELINUX=disabled 3设置后需要重启才能生效 6.设置所有节点的时区一致及时钟同步6.1.时区 1234567891011 1[root@hadoop001 ~]# date 2Sat May 11 10:07:53 CST 2019 3[root@hadoop001 ~]# timedatectl 4 Local time: Sat 2019-05-11 10:10:31 CST 5 Universal time: Sat 2019-05-11 02:10:31 UTC 6 RTC time: Sat 2019-05-11 10:10:29 7 Time zone: Asia/Shanghai (CST, +0800) 8 NTP enabled: yes 9NTP synchronized: yes10 RTC in local TZ: yes11 DST active: n/a 查看命令帮助，学习至关重要，无需百度，太👎1234567891011121314151617 1[root@hadoop001 ~]# timedatectl --help 2timedatectl [OPTIONS...] COMMAND ... 3Query or change system time and date settings. 4 -h --help Show this help message 5 --version Show package version 6 --no-pager Do not pipe output into a pager 7 --no-ask-password Do not prompt for password 8 -H --host=[USER@]HOST Operate on remote host 9 -M --machine=CONTAINER Operate on local container10 --adjust-system-clock Adjust system clock when changing local RTC mode11Commands:12 status Show current time settings13 set-time TIME Set system time14 set-timezone ZONE Set system time zone15 list-timezones Show known time zones16 set-local-rtc BOOL Control whether RTC is in local time17 set-ntp BOOL Control whether NTP is enabled 查看哪些时区12345671[root@hadoop001 ~]# timedatectl list-timezones2Africa/Abidjan3Africa/Accra4Africa/Addis_Ababa5Africa/Algiers6Africa/Asmara7Africa/Bamako 所有节点设置亚洲上海时区1231[root@hadoop001 ~]# timedatectl set-timezone Asia/Shanghai2[root@hadoop002 ~]# timedatectl set-timezone Asia/Shanghai3[root@hadoop003 ~]# timedatectl set-timezone Asia/Shanghai 6.2.时间 所有节点安装ntp11[root@hadoop001 ~]# yum install -y ntp 选取hadoop001为ntp的主节点11[root@hadoop001 ~]# vi /etc/ntp.conf time12341server 0.asia.pool.ntp.org2server 1.asia.pool.ntp.org3server 2.asia.pool.ntp.org4server 3.asia.pool.ntp.org 当外部时间不可用时，可使用本地硬件时间11server 127.127.1.0 iburst local clock 允许哪些网段的机器来同步时间11restrict 172.19.7.0 mask 255.255.255.0 nomodify notrap 开启ntpd及查看状态123456789101112 1[root@hadoop001 ~]# systemctl start ntpd 2[root@hadoop001 ~]# systemctl status ntpd 3● ntpd.service - Network Time Service 4 Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled) 5 Active: active (running) since Sat 2019-05-11 10:15:00 CST; 11min ago 6 Main PID: 18518 (ntpd) 7 CGroup: /system.slice/ntpd.service 8 └─18518 /usr/sbin/ntpd -u ntp:ntp -g 9May 11 10:15:00 hadoop001 systemd[1]: Starting Network Time Service...10May 11 10:15:00 hadoop001 ntpd[18518]: proto: precision = 0.088 usec11May 11 10:15:00 hadoop001 ntpd[18518]: 0.0.0.0 c01d 0d kern kernel time sync enabled12May 11 10:15:00 hadoop001 systemd[1]: Started Network Time Service. 验证12341[root@hadoop001 ~]# ntpq -p2 remote refid st t when poll reach delay offset jitter3==============================================================================4 LOCAL(0) .LOCL. 10 l 726 64 0 0.000 0.000 0.000 其他从节点停止禁用ntpd服务123451[root@hadoop002 ~]# systemctl stop ntpd2[root@hadoop002 ~]# systemctl disable ntpd3Removed symlink /etc/systemd/system/multi-user.target.wants/ntpd.service.4[root@hadoop002 ~]# /usr/sbin/ntpdate hadoop001511 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec 每天凌晨同步hadoop001节点时间12345678910 1[root@hadoop002 ~]# crontab -e 200 00 * * * /usr/sbin/ntpdate hadoop001 3[root@hadoop003 ~]# systemctl stop ntpd 4[root@hadoop004 ~]# systemctl disable ntpd 5Removed symlink /etc/systemd/system/multi-user.target.wants/ntpd.service. 6[root@hadoop005 ~]# /usr/sbin/ntpdate hadoop001 711 May 10:29:22 ntpdate[9370]: adjust time server 172.19.7.96 offset 0.000867 sec 8#每天凌晨同步hadoop001节点时间 9[root@hadoop003 ~]# crontab -e1000 00 * * * /usr/sbin/ntpdate hadoop001 7.部署集群的JDK123456781mkdir /usr/java2tar -xzvf jdk-8u45-linux-x64.tar.gz -C /usr/java/3#切记必须修正所属用户及用户组4chown -R root:root /usr/java/jdk1.8.0_455echo &quot;export JAVA_HOME=/usr/java/jdk1.8.0_45&quot; &gt;&gt; /etc/profile6echo &quot;export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;PATH&#125;&quot; &gt;&gt; /etc/profile7source /etc/profile8which java 8.hadoop001节点离线部署MySQL5.7(假如觉得困难哟，就自行百度RPM部署，因为该部署文档是我司生产文档) 文档链接:https://github.com/Hackeruncle/MySQL 视频链接:https://pan.baidu.com/s/1jdM8WeIg8syU0evL1-tDOQ 密码:whic 9.创建CDH的元数据库和用户、amon服务的数据库及用户123451create database cmf DEFAULT CHARACTER SET utf8;2create database amon DEFAULT CHARACTER SET utf8;3grant all on cmf.* TO &apos;cmf&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;4grant all on amon.* TO &apos;amon&apos;@&apos;%&apos; IDENTIFIED BY &apos;Ruozedata123456!&apos;;5flush privileges; 10.hadoop001节点部署mysql jdbc jar121mkdir -p /usr/share/java/2cp mysql-connector-java.jar /usr/share/java/ 三.CDH部署1.离线部署cm server及agent1234567891011121314151617 11.1.所有节点创建目录及解压 2mkdir /opt/cloudera-manager 3tar -zxvf cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz -C /opt/cloudera-manager/ 41.2.所有节点修改agent的配置，指向server的节点hadoop001 5sed -i &quot;s/server_host=localhost/server_host=hadoop001/g&quot; /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-agent/config.ini 61.3.主节点修改server的配置: 7vi /opt/cloudera-manager/cm-5.16.1/etc/cloudera-scm-server/db.properties 8com.cloudera.cmf.db.type=mysql 9com.cloudera.cmf.db.host=hadoop00110com.cloudera.cmf.db.name=cmf11com.cloudera.cmf.db.user=cmf12com.cloudera.cmf.db.password=Ruozedata123456!13com.cloudera.cmf.db.setupType=EXTERNAL141.4.所有节点创建用户15useradd --system --home=/opt/cloudera-manager/cm-5.16.1/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm161.5.目录修改用户及用户组17chown -R cloudera-scm:cloudera-scm /opt/cloudera-manager 2.hadoop001节点部署离线parcel源12345678910111213141516 12.1.部署离线parcel源 2$ mkdir -p /opt/cloudera/parcel-repo 3$ ll 4total 3081664 5-rw-r--r-- 1 root root 2127506677 May 9 18:04 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel 6-rw-r--r-- 1 root root 41 May 9 18:03 CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1 7-rw-r--r-- 1 root root 841524318 May 9 18:03 cloudera-manager-centos7-cm5.16.1_x86_64.tar.gz 8-rw-r--r-- 1 root root 185515842 Aug 10 2017 jdk-8u144-linux-x64.tar.gz 9-rw-r--r-- 1 root root 66538 May 9 18:03 manifest.json10-rw-r--r-- 1 root root 989495 May 25 2017 mysql-connector-java.jar11$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel /opt/cloudera/parcel-repo/12#切记cp时，重命名去掉1，不然在部署过程CM认为如上文件下载未完整，会持续下载13$ cp CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.16.1-1.cdh5.16.1.p0.3-el7.parcel.sha14$ cp manifest.json /opt/cloudera/parcel-repo/152.2.目录修改用户及用户组16$ chown -R cloudera-scm:cloudera-scm /opt/cloudera/ 3.所有节点创建软件安装目录、用户及用户组权限121mkdir -p /opt/cloudera/parcels2chown -R cloudera-scm:cloudera-scm /opt/cloudera/ 4.hadoop001节点启动Server1234514.1.启动server2/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-server start34.2.阿里云web界面，设置该hadoop001节点防火墙放开7180端口44.3.等待1min，打开 http://hadoop001:7180 账号密码:admin/admin54.4.假如打不开，去看server的log，根据错误仔细排查错误 5.所有节点启动Agent11/opt/cloudera-manager/cm-5.16.1/etc/init.d/cloudera-scm-agent start 6.接下来，全部Web界面操作http://hadoop001:7180/账号密码:admin/admin 7.欢迎使用Cloudera Manager–最终用户许可条款与条件。勾选 8.欢迎使用Cloudera Manager–您想要部署哪个版本？选择Cloudera Express免费版本 9.感谢您选择Cloudera Manager和CDH 10.为CDH集群安装指导主机。选择[当前管理的主机]，全部勾选 11.选择存储库 12.集群安装–正在安装选定Parcel假如本地parcel离线源配置正确，则”下载”阶段瞬间完成，其余阶段视节点数与内部网络情况决定。 13.检查主机正确性 123456789101112131415161718192021222324 113.1.建议将/proc/sys/vm/swappiness设置为最大值10。 2swappiness值控制操作系统尝试交换内存的积极； 3swappiness=0：表示最大限度使用物理内存，之后才是swap空间； 4swappiness=100：表示积极使用swap分区，并且把内存上的数据及时搬迁到swap空间； 5如果是混合服务器，不建议完全禁用swap，可以尝试降低swappiness。 6临时调整： 7sysctl vm.swappiness=10 8永久调整： 9cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.conf10# Adjust swappiness value11vm.swappiness=1012EOF1313.2.已启用透明大页面压缩，可能会导致重大性能问题，建议禁用此设置。14临时调整：15echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag16echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled17永久调整：18cat &lt;&lt; EOF &gt;&gt; /etc/rc.d/rc.local19# Disable transparent_hugepage20echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag21echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled22EOF23# centos7.x系统，需要为&quot;/etc/rc.d/rc.local&quot;文件赋予执行权限24chmod +x /etc/rc.d/rc.local 14.自定义服务，选择部署Zookeeper、HDFS、Yarn服务 15.自定义角色分配 16.数据库设置 17.审改设置，默认即可 18.首次运行 19.恭喜您! 20.主页 CDH全套课程目录，如有buy，加微信(ruoze_star)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 1 0.青云环境介绍和使用 2 1.Preparation 3 谈谈怎样入门大数据 4 谈谈怎样做好一个大数据平台的运营工作 5 Linux机器,各软件版本介绍及安装(录播) 6 2.Introduction 7 Cloudera、CM及CDH介绍 8 CDH版本选择 9 CDH安装几种方式解读 10 3.Install&amp;UnInstall 11 集群节点规划,环境准备(NTP,Jdk and etc) 12 MySQL编译安装及常用命令 13 推荐:CDH离线安装(踩坑心得,全面剖析) 14 解读暴力卸载脚本 15 4.CDH Management 16 CDH体系架构剖析 17 CDH配置文件深度解析 18 CM的常用命令 19 CDH集群正确启动和停止顺序 20 CDH Tsquery Language 21 CDH常规管理(监控/预警/配置/资源/日志/安全) 22 5.Maintenance Experiment 23 HDFS HA 配置 及hadoop/hdfs常规命令 24 Yarn HA 配置 及yarn常规命令 25 Other CDH Components HA 配置 26 CDH动态添加删除服务(hive/spark/hbase) 27 CDH动态添加删除机器 28 CDH动态添加删除及迁移DataNode进程等 29 CDH升级(5.10.0--&gt;5.12.0) 30 6.Resource Management 31 Linux Cgroups 32 静态资源池 33 动态资源池 34 多租户案例 35 7.Performance Tunning 36 Memory/CPU/Network/Disk及集群规划 37 Linux参数 38 HDFS参数 39 MapReduce及Yarn参数 40 其他服务参数 41 8.Cases Share 42 CDH4&amp;5之Alternatives命令 的研究 43 CDH5.8.2安装之Hash verification failed 44 记录一次CDH4.8.6 配置HDFS HA 坑 45 CDH5.0集群IP更改 46 CDH的active namenode exit(GC)和彩蛋分享 47 9. Kerberos 48 Kerberos简介49 Kerberos体系结构50 Kerberos工作机制51 Kerberos安装部署52 CDH启用kerberos53 Kerberos开发使用(真实代码)54 10.Summary 55 总结 Join us if you have a dream.若泽数据官网腾讯课堂，搜若泽数据Bilibili网站,搜若泽数据若泽大数据–官方博客若泽大数据–博客一览若泽大数据–内部学员面试题]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>CDH5.16.1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次Java线程池误用引发的血案和总结]]></title>
    <url>%2F%E4%B8%80%E6%AC%A1Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%AF%AF%E7%94%A8%E5%BC%95%E5%8F%91%E7%9A%84%E8%A1%80%E6%A1%88%E5%92%8C%E6%80%BB%E7%BB%93.html</url>
    <content type="text"><![CDATA[这是一个十分严重的问题自从最近的某年某月某天起，线上服务开始变得不那么稳定。在高峰期，时常有几台机器的内存持续飙升，并且无法回收，导致服务不可用。 例如GC时间采样曲线： 和内存使用曲线： 图中所示，18:50-19:00的阶段，已经处于服务不可用的状态了。上游服务的超时异常会增加，该台机器会触发熔断。熔断触发后，改台机器的流量会打到其他机器，其他机器发生类似的情况的可能性会提高，极端情况会引起所有服务宕机，曲线掉底。 因为线上内存过大，如果采用 jmap dump的方式，这个任务可能需要很久才可以执行完，同时把这么大的文件存放起来导入工具也是一件很难的事情。再看JVM启动参数，也很久没有变更过 Xms, Xmx, -XX:NewRatio, -XX:SurvivorRatio, 虽然没有仔细分析程序使用内存情况，但看起来也无大碍。 于是开始找代码，某年某天某月～ 嗯，注意到一段这样的代码提交： 12345678910111213private static ExecutorService executor = Executors.newFixedThreadPool(15);public static void push2Kafka(Object msg) &#123; executor.execute(new WriteTask(msg, false)); &#125; 相关代码的完整功能是，每次线上调用，都会把计算结果的日志打到 Kafka，Kafka消费方再继续后续的逻辑。内存被耗尽可能有一个原因是，因为使用了 newFixedThreadPool 线程池，而它的工作机制是，固定了N个线程，而提交给线程池的任务队列是不限制大小的，如果Kafka发消息被阻塞或者变慢，那么显然队列里面的内容会越来越多，也就会导致这样的问题。 为了验证这个想法，做了个小实验，把 newFixedThreadPool 线程池的线程个数调小一点，例如 1。果然压测了一下，很快就复现了内存耗尽，服务不可用的悲剧。 最后的修复策略是使用了自定义的线程池参数，而非 Executors 默认实现解决了问题。下面就把线程池相关的原理和参数总结一下，避免未来踩坑。 1. Java线程池虽然Java线程池理论，以及构造线程池的各种参数，以及 Executors 提供的默认实现之前研读过，不过线上还没有发生过线程池误用引发的事故，所以有必要把这些参数再仔细琢磨一遍。 优先补充一些线程池的工作理论，有助于展开下面的内容。线程池顾名思义，就是由很多线程构成的池子，来一个任务，就从池子中取一个线程，处理这个任务。这个理解是我在第一次接触到这个概念时候的理解，虽然整体基本切入到核心，但是实际上会比这个复杂。例如线程池肯定不会无限扩大的，否则资源会耗尽；当线程数到达一个阶段，提交的任务会被暂时存储在一个队列中，如果队列内容可以不断扩大，极端下也会耗尽资源，那选择什么类型的队列，当队列满如何处理任务，都有涉及很多内容。线程池总体的工作过程如下图： 线程池内的线程数的大小相关的概念有两个，一个是核心池大小，还有最大池大小。如果当前的线程个数比核心池个数小，当任务到来，会优先创建一个新的线程并执行任务。当已经到达核心池大小，则把任务放入队列，为了资源不被耗尽，队列的最大容量可能也是有上限的，如果达到队列上限则考虑继续创建新线程执行任务，如果此刻线程的个数已经到达最大池上限，则考虑把任务丢弃。 在 java.util.concurrent 包中，提供了 ThreadPoolExecutor 的实现。 1234567891011121314151617181920212223242526272829public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123;&#125; 既然有了刚刚对线程池工作原理对概述，这些参数就很容易理解了： corePoolSize- 核心池大小，既然如前原理部分所述。需要注意的是在初创建线程池时线程不会立即启动，直到有任务提交才开始启动线程并逐渐时线程数目达到corePoolSize。若想一开始就创建所有核心线程需调用prestartAllCoreThreads方法。 maximumPoolSize-池中允许的最大线程数。需要注意的是当核心线程满且阻塞队列也满时才会判断当前线程数是否小于最大线程数，并决定是否创建新线程。 keepAliveTime - 当线程数大于核心时，多于的空闲线程最多存活时间 unit - keepAliveTime 参数的时间单位。 workQueue - 当线程数目超过核心线程数时用于保存任务的队列。主要有3种类型的BlockingQueue可供选择：无界队列，有界队列和同步移交。将在下文中详细阐述。从参数中可以看到，此队列仅保存实现Runnable接口的任务。 别看这个参数位置很靠后，但是真的很重要，因为楼主的坑就因这个参数而起，这些细节有必要仔细了解清楚。 threadFactory - 执行程序创建新线程时使用的工厂。 handler - 阻塞队列已满且线程数达到最大值时所采取的饱和策略。java默认提供了4种饱和策略的实现方式：中止、抛弃、抛弃最旧的、调用者运行。将在下文中详细阐述。 2. 可选择的阻塞队列BlockingQueue详解在重复一下新任务进入时线程池的执行策略：如果运行的线程少于corePoolSize，则 Executor始终首选添加新的线程，而不进行排队。（如果当前运行的线程小于corePoolSize，则任务根本不会存入queue中，而是直接运行）如果运行的线程大于等于 corePoolSize，则 Executor始终首选将请求加入队列，而不添加新的线程。如果无法将请求加入队列，则创建新的线程，除非创建此线程超出 maximumPoolSize，在这种情况下，任务将被拒绝。主要有3种类型的BlockingQueue： 无界队列 队列大小无限制，常用的为无界的LinkedBlockingQueue，使用该队列做为阻塞队列时要尤其当心，当任务耗时较长时可能会导致大量新任务在队列中堆积最终导致OOM。阅读代码发现，Executors.newFixedThreadPool 采用就是 LinkedBlockingQueue，而楼主踩到的就是这个坑，当QPS很高，发送数据很大，大量的任务被添加到这个无界LinkedBlockingQueue 中，导致cpu和内存飙升服务器挂掉。 有界队列 常用的有两类，一类是遵循FIFO原则的队列如ArrayBlockingQueue与有界的LinkedBlockingQueue，另一类是优先级队列如PriorityBlockingQueue。PriorityBlockingQueue中的优先级由任务的Comparator决定。使用有界队列时队列大小需和线程池大小互相配合，线程池较小有界队列较大时可减少内存消耗，降低cpu使用率和上下文切换，但是可能会限制系统吞吐量。 在我们的修复方案中，选择的就是这个类型的队列，虽然会有部分任务被丢失，但是我们线上是排序日志搜集任务，所以对部分对丢失是可以容忍的。 同步移交队列 如果不希望任务在队列中等待而是希望将任务直接移交给工作线程，可使用SynchronousQueue作为等待队列。SynchronousQueue不是一个真正的队列，而是一种线程之间移交的机制。要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接收这个元素。只有在使用无界线程池或者有饱和策略时才建议使用该队列。 3. 可选择的饱和策略RejectedExecutionHandler详解JDK主要提供了4种饱和策略供选择。4种策略都做为静态内部类在ThreadPoolExcutor中进行实现。 3.1 AbortPolicy中止策略该策略是默认饱和策略。 1234567891011121314151617public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; throw new RejectedExecutionException(&quot;Task &quot; + r.toString() + &quot; rejected from &quot; + e.toString()); &#125; 使用该策略时在饱和时会抛出RejectedExecutionException（继承自RuntimeException），调用者可捕获该异常自行处理。 3.2 DiscardPolicy抛弃策略12345public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123;&#125; 如代码所示，不做任何处理直接抛弃任务 3.3 DiscardOldestPolicy抛弃旧任务策略123456789101112131415161718192021public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; e.getQueue().poll(); e.execute(r); &#125;&#125; 如代码，先将阻塞队列中的头元素出队抛弃，再尝试提交任务。如果此时阻塞队列使用PriorityBlockingQueue优先级队列，将会导致优先级最高的任务被抛弃，因此不建议将该种策略配合优先级队列使用。 3.4 CallerRunsPolicy调用者运行1234567891011121314151617public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; r.run(); &#125;&#125; 既不抛弃任务也不抛出异常，直接运行任务的run方法，换言之将任务回退给调用者来直接运行。使用该策略时线程池饱和后将由调用线程池的主线程自己来执行任务，因此在执行任务的这段时间里主线程无法再提交新任务，从而使线程池中工作线程有时间将正在处理的任务处理完成。 4. Java提供的四种常用线程池解析既然楼主踩坑就是使用了 JDK 的默认实现，那么再来看看这些默认实现到底干了什么，封装了哪些参数。简而言之 Executors 工厂方法Executors.newCachedThreadPool() 提供了无界线程池，可以进行自动线程回收；Executors.newFixedThreadPool(int) 提供了固定大小线程池，内部使用无界队列；Executors.newSingleThreadExecutor() 提供了单个后台线程。 详细介绍一下上述四种线程池。 4.1 newCachedThreadPool1234567891011121314151617public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 在newCachedThreadPool中如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。初看该构造函数时我有这样的疑惑：核心线程池为0，那按照前面所讲的线程池策略新任务来临时无法进入核心线程池，只能进入 SynchronousQueue中进行等待，而SynchronousQueue的大小为1，那岂不是第一个任务到达时只能等待在队列中，直到第二个任务到达发现无法进入队列才能创建第一个线程？这个问题的答案在上面讲SynchronousQueue时其实已经给出了，要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接收这个元素。因此即便SynchronousQueue一开始为空且大小为1，第一个任务也无法放入其中，因为没有线程在等待从SynchronousQueue中取走元素。因此第一个任务到达时便会创建一个新线程执行该任务。 4.2 newFixedThreadPool1234567891011121314151617public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 看代码一目了然了，线程数量固定，使用无限大的队列。再次强调，楼主就是踩的这个无限大队列的坑。 4.3 newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 123456789public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125; 在来看看ScheduledThreadPoolExecutor（）的构造函数 12345678910111213public ScheduledThreadPoolExecutor(int corePoolSize) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); &#125; ScheduledThreadPoolExecutor的父类即ThreadPoolExecutor，因此这里各参数含义和上面一样。值得关心的是DelayedWorkQueue这个阻塞对列，在上面没有介绍，它作为静态内部类就在ScheduledThreadPoolExecutor中进行了实现。简单的说，DelayedWorkQueue是一个无界队列，它能按一定的顺序对工作队列中的元素进行排列。 4.4 newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 12345678910111213public static ScheduledExecutorService newSingleThreadScheduledExecutor() &#123; return new DelegatedScheduledExecutorService (new ScheduledThreadPoolExecutor(1)); &#125; 首先new了一个线程数目为 1 的ScheduledThreadPoolExecutor，再把该对象传入DelegatedScheduledExecutorService中，看看DelegatedScheduledExecutorService的实现代码： 12345678910111213DelegatedScheduledExecutorService(ScheduledExecutorService executor) &#123; super(executor); e = executor;&#125; 在看看它的父类 123456789DelegatedExecutorService(ExecutorService executor) &#123; e = executor; &#125; 其实就是使用装饰模式增强了ScheduledExecutorService（1）的功能，不仅确保只有一个线程顺序执行任务，也保证线程意外终止后会重新创建一个线程继续执行任务。 结束语虽然之前学习了不少相关知识，但是只有在实践中踩坑才能印象深刻吧 原文地址]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程和线程池]]></title>
    <url>%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%B1%A0.html</url>
    <content type="text"><![CDATA[并发编程和线程池 练气期（并发编程基础）练气期一层（this）synchronized(this)和synchronized方法都是锁当前对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Test_01 &#123; private int count = 0; // 存在堆中 private Object o = new Object(); // 存在堆中 // 多个线程都能找到的，都能访问的对象叫临界资源对象 public void testSync1() &#123; synchronized (o) &#123; System.out.println(Thread.currentThread().getName() + " count = " + count++); &#125; &#125; public void testSync2() &#123; synchronized (this) &#123; System.out.println(Thread.currentThread().getName() + " count = " + count++); &#125; &#125; public synchronized void testSync3() &#123; System.out.println(Thread.currentThread().getName() + " count = " + count++); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; final Test_01 t = new Test_01(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.testSync3(); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.testSync3(); &#125; &#125;).start(); &#125;&#125; 练气期二层（static）静态同步方法，锁的是当前类型的类对象。 12345678910111213141516171819202122public class Test_02 &#123; private static int staticCount = 0; public static synchronized void testSync4()&#123; System.out.println(Thread.currentThread().getName() + " staticCount = " + staticCount++); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public static void testSync5()&#123; synchronized(Test_02.class)&#123; System.out.println(Thread.currentThread().getName() + " staticCount = " + staticCount++); &#125; &#125;&#125; 练气期三层（原子性）加锁的目的就是为了保证操作的原子性 123456789101112131415161718public class Test_03 implements Runnable &#123; private int count = 0; @Override public /*synchronized*/ void run() &#123; System.out.println(Thread.currentThread().getName() + " count = " + count++); &#125; public static void main(String[] args) &#123; Test_03 t = new Test_03(); for (int i = 0; i &lt; 5; i++) &#123; new Thread(t, "Thread - " + i).start(); &#125; &#125;&#125; 练气期四层（同步与非同步方法间调用）同步方法只影响锁定同一个锁对象的同步方法。不影响其他线程调用非同步方法，或调用其他锁资源的同步方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class Test_04 &#123; Object o = new Object(); public synchronized void m1() &#123; // 重量级的访问操作。 System.out.println("public synchronized void m1() start"); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("public synchronized void m1() end"); &#125; public void m3() &#123; synchronized (o) &#123; System.out.println("public void m3() start"); try &#123; Thread.sleep(1500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("public void m3() end"); &#125; &#125; public void m2() &#123; System.out.println("public void m2() start"); try &#123; Thread.sleep(1500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("public void m2() end"); &#125; public static class MyThread01 implements Runnable &#123; public MyThread01(int i, Test_04 t) &#123; this.i = i; this.t = t; &#125; int i; Test_04 t; public void run() &#123; if (i == 0) &#123; t.m1(); &#125; else if (i &gt; 0) &#123; t.m2(); &#125; else &#123; t.m3(); &#125; &#125; &#125; public static void main(String[] args) &#123; Test_04 t = new Test_04(); new Thread(new Test_04.MyThread01(0, t)).start(); new Thread(new Test_04.MyThread01(1, t)).start(); new Thread(new Test_04.MyThread01(-1, t)).start(); &#125;&#125; 练气期五层（存在原子性问题）同步方法只能保证当前方法的原子性，不能保证多个业务方法之间的互相访问的原子性。一般来说，商业项目中，不考虑业务逻辑上的脏读问题。如你买东西下订单后，提示订单已下，查询时候，可能看不到。一般我们只关注数据脏读。但是在金融领域，保险领域严格要求。 1234567891011121314151617181920212223242526272829303132333435public class Test_05 &#123; private double d = 0.0; public synchronized void m1(double d)&#123; try &#123; // 相当于复杂的业务逻辑代码。 TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; this.d = d; &#125; public double m2()&#123; return this.d; &#125; public static void main(String[] args) &#123; final Test_05 t = new Test_05(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m1(100); &#125; &#125;).start(); System.out.println(t.m2()); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(t.m2()); &#125;&#125; 练气期六层（锁可重入）同一个线程，多次调用同步代码，锁定同一个锁对象，可重入。 1234567891011121314151617181920212223242526272829public class Test_06 &#123; synchronized void m1()&#123; // 锁this System.out.println("m1 start"); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; m2(); System.out.println("m1 end"); &#125; synchronized void m2()&#123; // 锁this System.out.println("m2 start"); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("m2 end"); &#125; public static void main(String[] args) &#123; new Test_06().m1(); &#125;&#125; 练气期七层（调用父类的同步方法）子类同步方法覆盖父类同步方法，可以指定调用父类的同步方法， 相当于锁的重入。父类的方法 &lt;&lt;==&gt;&gt; 本类的方法 12345678910111213141516171819202122232425public class Test_07 &#123; synchronized void m() &#123; System.out.println("Super Class m start"); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("Super Class m end"); &#125; public static void main(String[] args) &#123; new Sub_Test_07().m(); &#125;&#125;class Sub_Test_07 extends Test_07 &#123; synchronized void m() &#123; System.out.println("Sub Class m start"); super.m(); System.out.println("Sub Class m end"); &#125;&#125; 练气期八层（锁与异常）当同步方法中发生异常的时候，自动释放锁资源，不会影响其他线程的执行。我们需要注意的是在同步业务逻辑中，如果发生异常如何处理——— try/catch 。如存钱时，发送网络中断，查询的时候查到多少钱，存的钱要返还 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Test_08 &#123; int i = 0; synchronized void m() &#123; System.out.println(Thread.currentThread().getName() + " - start"); while (true) &#123; i++; System.out.println(Thread.currentThread().getName() + " - " + i); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; /*if(i == 5)&#123; i = 1/0; &#125;*/ //模拟存钱，中断处理 if (i == 5) &#123; try &#123; i = 1 / 0; &#125; catch (Exception e) &#123; i = 0; &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; final Test_08 t = new Test_08(); // 锁的是当前对象 new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;, "t1").start(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;, "t2").start(); &#125;&#125; 练气期九层（volatile）cpu默认查询cpu的高速缓存区域，CPU中每一个核都有自己的缓存，当cpu有中断的时候，他可能清空高速缓存区域数据，重新从内存中读取数据。volatile改变内存中的数据，通知底层OS系统，每次使用b的时候，最好看下内存数据是否发生变动。即volatile做的是一个通知OS系统的作。 123456789101112131415161718192021222324252627282930public class Test_09 &#123; volatile boolean b = true; //线程可见性问题 void m()&#123; System.out.println("start"); while(b)&#123;&#125; System.out.println("end"); &#125; public static void main(String[] args) &#123; final Test_09 t = new Test_09(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; t.b = false; //堆空间的对象，线程共享 &#125;&#125; volatile的非原子性问题，只能保证可见性，不能保证原子性。 那什么时候使用volatile？棋牌室的人数，新增的人有一个线程去+1。这是可以使用volatile join()多个线程在运行结束时，我把多个线程再main线程的位置连在一起，当其他线程都结束，即保证在所有线程循环执行+1后，再执行main线程打印。 1234567891011121314151617181920212223242526272829303132333435public class Test_10 &#123; volatile int count = 0; /*synchronized*/ void m() &#123; //保证原子性的解决方法是使用synchronized或者是Atomic for (int i = 0; i &lt; 10000; i++) &#123; count++; &#125; &#125; public static void main(String[] args) &#123; final Test_10 t = new Test_10(); List&lt;Thread&gt; threads = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; threads.add(new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;)); &#125; for (Thread thread : threads) &#123; thread.start(); &#125; for (Thread thread : threads) &#123; try &#123; thread.join(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; System.out.println(t.count); //理论上是10w。实际少于这个数 &#125;&#125; 练气期十层（AtomicXxx）什么时候有原子性，没有可见性？ 答：所谓原子性是指多个线程访问一个变量时，其结果必须保证正确性。所谓可见性是指多线程间可以看最终结果的变量 1234567891011121314151617181920212223242526272829303132333435public class Test_11 &#123; AtomicInteger count = new AtomicInteger(0); void m() &#123; for (int i = 0; i &lt; 10000; i++) &#123; /*if(count.get() &lt; 1000)*/ count.incrementAndGet(); //相当于++count,count.getAndAccumulate()是count++; &#125; &#125; public static void main(String[] args) &#123; final Test_11 t = new Test_11(); List&lt;Thread&gt; threads = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; threads.add(new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;)); &#125; for (Thread thread : threads) &#123; thread.start(); &#125; for (Thread thread : threads) &#123; try &#123; thread.join(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; System.out.println(t.count.intValue()); &#125;&#125; 练气期十一层（锁对象变更） 同步代码一旦加锁后，那么会有一个临时的锁引用指向锁对象，和真实的引用无直接关联。在锁未释放之前，修改锁引用，不会影响同步代码的执行。 我们打印的是Test_13中的o。不是锁引用的_O;下面synchronized锁的是两个对象。打印的是同一个对象。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Test_13 &#123; Object o = new Object(); //变量引用 int i = 0; int a() &#123; try &#123; /* * return i -&gt; * int _returnValue = i; // 0; * return _returnValue; */ return i; &#125; finally &#123; i = 10; &#125; &#125; void m() &#123; System.out.println(Thread.currentThread().getName() + " start"); synchronized (o) &#123; //计算变量引用与变量引用不是一回事 while (true) &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " - " + o); &#125; &#125; &#125; public static void main(String[] args) &#123; final Test_13 t = new Test_13(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;, "thread1").start(); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; Thread thread2 = new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;, "thread2"); t.o = new Object(); thread2.start(); //更改临界资源对象 System.out.println(t.i); System.out.println(t.a()); System.out.println(t.i); &#125;&#125; 练气期十二层（CountDownLatch） 不会进入等待队列，可以和锁混合使用，或替代锁的功能。 一次性在门上挂多个锁。 作用如：init对象的时候有一个前后顺序的问题。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Test_15 &#123; CountDownLatch latch = new CountDownLatch(5); void m1() &#123; try &#123; latch.await();// 等待门闩开放。 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("m1() method"); &#125; void m2() &#123; for (int i = 0; i &lt; 10; i++) &#123; if (latch.getCount() != 0) &#123; System.out.println("latch count : " + latch.getCount()); latch.countDown(); // 减门闩上的锁。 &#125; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; System.out.println("m2() method : " + i); &#125; &#125; public static void main(String[] args) &#123; final Test_15 t = new Test_15(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m1(); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m2(); &#125; &#125;).start(); &#125;&#125; 练气期大圆满123456789101112131415161718192021222324252627282930313233343536373839404142public class Test_14 &#123; String s1 = "hello"; String s2 = new String("hello"); // new关键字，一定是在堆中创建一个新的对象。 Integer i1 = 1; // i1与i2是同一个变量，在常量池中，new是放在堆内存 Integer i2 = 1; void m1() &#123; synchronized (i1) &#123; //s1与s2 System.out.println("m1()"); while (true) &#123; &#125; &#125; &#125; void m2() &#123; synchronized (i2) &#123; System.out.println("m2()"); while (true) &#123; &#125; &#125; &#125; public static void main(String[] args) &#123; final Test_14 t = new Test_14(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m1(); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m2(); &#125; &#125;).start(); &#125;&#125; 自定义容器，提供新增元素（add）和获取元素数量（size）方法。启动两个线程。线程1向容器中新增10个数据。线程2监听容器元素数量，当容器元素数量为5时，线程2输出信息并终止。 使用volatile 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Test_01 &#123; public static void main(String[] args) &#123; final Test_01_Container t = new Test_01_Container(); new Thread(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; System.out.println("add Object to Container " + i); t.add(new Object()); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; while (true) &#123; if (t.size() == 5) &#123; System.out.println("size = 5"); break; &#125; &#125; &#125; &#125;).start(); &#125;&#125;class Test_01_Container &#123; volatile List&lt;Object&gt; container = new ArrayList&lt;&gt;(); public void add(Object o) &#123; this.container.add(o); &#125; public int size() &#123; return this.container.size(); &#125;&#125; 使用synchronized和wait(), 调用wait()将释放锁，并且进入等待队列中，生产者与消费者模型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Test_02 &#123; public static void main(String[] args) &#123; final Test_02_Container t = new Test_02_Container(); final Object lock = new Object(); new Thread(new Runnable()&#123; @Override public void run() &#123; synchronized (lock) &#123; if(t.size() != 5)&#123; try &#123; lock.wait(); // 线程进入等待队列。 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println("size = 5"); lock.notifyAll(); // 唤醒其他等待线程 &#125; &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; synchronized (lock) &#123; for(int i = 0; i &lt; 10; i++)&#123; System.out.println("add Object to Container " + i); t.add(new Object()); if(t.size() == 5)&#123; lock.notifyAll(); try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125;).start(); &#125;&#125;class Test_02_Container&#123; List&lt;Object&gt; container = new ArrayList&lt;&gt;(); public void add(Object o)&#123; this.container.add(o); &#125; public int size()&#123; return this.container.size(); &#125;&#125; 使用门闩避免进入等待队列，效率更高。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class Test_03 &#123; public static void main(String[] args) &#123; final Test_03_Container t = new Test_03_Container(); final CountDownLatch latch = new CountDownLatch(1); new Thread(new Runnable()&#123; @Override public void run() &#123; if(t.size() != 5)&#123; try &#123; latch.await(); // 等待门闩的开放。 不是进入等待队列 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println("size = 5"); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; for(int i = 0; i &lt; 10; i++)&#123; System.out.println("add Object to Container " + i); t.add(new Object()); if(t.size() == 5)&#123; latch.countDown(); // 门闩-1 &#125; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;).start(); &#125;&#125;class Test_03_Container&#123; List&lt;Object&gt; container = new ArrayList&lt;&gt;(); public void add(Object o)&#123; this.container.add(o); &#125; public int size()&#123; return this.container.size(); &#125;&#125; 小编是一枚Java Coder，业余写文章，现主营微信公众号《Java患者》，喜欢的话关注我的公众号或者加我微信我们一起学习Java 筑基期（ReentrantLock）筑基初期（lock等待锁） concurrent是jdk1.5后的包，避免synchronized的出现而设计出来的一种锁机制。 ReentrantLock 重入锁，在一个对象上加一个标记信息，这个标记信息代表锁机制。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Test_01 &#123; Lock lock = new ReentrantLock(); void m1() &#123; try &#123; lock.lock(); // 加锁 for (int i = 0; i &lt; 10; i++) &#123; TimeUnit.SECONDS.sleep(1); System.out.println("m1() method " + i); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); // 解锁 &#125; &#125; void m2() &#123; lock.lock(); System.out.println("m2() method"); lock.unlock(); &#125; public static void main(String[] args) &#123; final Test_01 t = new Test_01(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m1(); &#125; &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(new Runnable() &#123; @Override public void run() &#123; t.m2(); &#125; &#125;).start(); &#125;&#125; 筑基中期（tryLock尝试锁）尝试锁有阻塞和非阻塞两种 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class Test_02 &#123; Lock lock = new ReentrantLock(); void m1()&#123; try&#123; lock.lock(); for(int i = 0; i &lt; 10; i++)&#123; TimeUnit.SECONDS.sleep(1); System.out.println("m1() method " + i); &#125; &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125;finally&#123; lock.unlock(); &#125; &#125; void m2()&#123; boolean isLocked = false; try&#123; // 尝试锁， 如果有锁，无法获取锁标记，返回false。 // 非阻塞，如果获取锁标记，返回true // isLocked = lock.tryLock(); // 阻塞尝试锁，阻塞参数代表的时长，尝试获取锁标记。 // 如果超时，不等待。直接返回。 isLocked = lock.tryLock(5, TimeUnit.SECONDS); if(isLocked)&#123; System.out.println("m2() method synchronized"); &#125;else&#123; System.out.println("m2() method unsynchronized"); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(isLocked)&#123; // 尝试锁在解除锁标记的时候，一定要判断是否获取到锁标记。 // 如果当前线程没有获取到锁标记，会抛出异常。 lock.unlock(); &#125; &#125; &#125; public static void main(String[] args) &#123; final Test_02 t = new Test_02(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m1(); &#125; &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; new Thread(new Runnable() &#123; @Override public void run() &#123; t.m2(); &#125; &#125;).start(); &#125;&#125; 筑基后期（lockInterruptibly可打断锁） 阻塞状态有3种： 包括普通阻塞（不释放锁），等待队列（释放锁），锁池队列。 普通阻塞： sleep(10000)， 可以被打断。调用thread.interrupt()方法，可以打断阻塞状态，抛出异常。 等待队列： wait()方法被调用，也是一种阻塞状态，只能由notify唤醒。无法打断。 锁池队列： 执行过程中，遇到同步代码，无法获取锁标记。不是所有的锁池队列都可被打断。 使用ReentrantLock的lock方法，获取锁标记的时候，如果需要阻塞等待锁标记，无法被打断。 使用ReentrantLock的lockInterruptibly方法，获取锁标记的时候，如果需要阻塞等待，可以被打断。 可打断锁意义：软件锁死了，无响应，去去任务管理器结束任务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class Test_03 &#123; Lock lock = new ReentrantLock(); void m1() &#123; try &#123; lock.lock(); for (int i = 0; i &lt; 5; i++) &#123; TimeUnit.SECONDS.sleep(1); System.out.println("m1() method " + i); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; void m2() &#123; try &#123; // 线程执行到这里，本来是不能获得锁标记的，要进入等待队列的。 // 当通过调用当前线程的interrupt()，通过打断当前线程，抛出异常，使线程被唤醒，阻塞结束 lock.lockInterruptibly(); // 可尝试打断的，阻塞等待锁。可以被其他的线程打断阻塞状态 System.out.println("m2() method"); &#125; catch (InterruptedException e) &#123; // 被打断的异常，被打断与唤醒、阻塞结束都是不一样的 // sleep任何一个线程都可以把他打断，强行唤醒 // 如果是lock不可被打断的 // 如果是lockInterruptibly，阻塞等待这把锁，类似sleep，可以通过interrupt()打断 System.out.println("m2() method interrupted"); &#125; finally &#123; try &#123; lock.unlock(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; final Test_03 t = new Test_03(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m1(); &#125; &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; t.m2(); &#125; &#125;); t2.start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; // 不调用interrupt()方法，t2最后可以获得锁，继续执行 t2.interrupt();// 打断t2线程，锁的位置会抛出异常。 &#125;&#125; 筑基圆满（公平锁） 在cpu和os中本身线程竞争锁标记是不公平的，不考虑线程的等待时间的。 运用在轮询的场景，如打牌。 需要效果一部分的cpu资源计算等待的时间，性能有所降低。要仅能少用，并发量在10之内。 123456789101112131415161718192021222324252627282930313233343536373839public class Test_04 &#123; public static void main(String[] args) &#123; TestReentrantlock t = new TestReentrantlock(); //TestSync t = new TestSync(); Thread t1 = new Thread(t); Thread t2 = new Thread(t); t1.start(); t2.start(); &#125;&#125;class TestReentrantlock extends Thread &#123; // 定义一个公平锁 private static ReentrantLock lock = new ReentrantLock(true); public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + " get lock"); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;&#125;class TestSync extends Thread &#123; public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; //不公平的 synchronized (this) &#123; System.out.println(Thread.currentThread().getName() + " get lock in TestSync"); &#125; &#125; &#125;&#125; 小编是一枚Java Coder，业余写文章，现主营微信公众号《Java患者》，喜欢的话关注我的公众号或者加我微信我们一起学习Java 金丹期金丹初期（生产者&amp;消费者） ReenTrantLock建议应用在同步方式，相对效率比synchronized高，量级较轻。 synchronized在JDK1.5版本尝试优化，到JDK1.7后，优化效率已经非常好了。在绝对效率上不比ReenTrantLock差多少。 使用ReenTrantLock必须释放锁标记。一般在finally代码块释放锁标记的。 12练习（生产者消费者模式）：自定义同步容器，容器容量上限为10。可以在多线程中应用，并保证数据线程安全。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class TestContainer01&lt;E&gt; &#123; private final LinkedList&lt;E&gt; list = new LinkedList&lt;&gt;(); private final int MAX = 10; private int count = 0; public synchronized int getCount()&#123; return count; &#125; public synchronized void put(E e)&#123; while(list.size() == MAX)&#123; try &#123; this.wait(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; &#125; list.add(e); count++; this.notifyAll(); &#125; public synchronized E get()&#123; E e = null; while(list.size() == 0)&#123; try&#123; this.wait(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; &#125; e = list.removeFirst(); count--; this.notifyAll(); return e; &#125; public static void main(String[] args) &#123; final TestContainer01&lt;String&gt; c = new TestContainer01&lt;&gt;(); for(int i = 0; i &lt; 10; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; for(int j = 0; j &lt; 5; j++)&#123; System.out.println(c.get()); &#125; &#125; &#125;, "consumer"+i).start(); &#125; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; for(int i = 0; i &lt; 2; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; for(int j = 0; j &lt; 25; j++)&#123; c.put("container value " + j); &#125; &#125; &#125;, "producer"+i).start(); &#125; &#125;&#125; 使用ReentrantLock完成生产者-消费者 Condition， 为Lock增加条件。当条件满足时（生成了或者是被消费），做什么事情，如加锁或解锁。如等待或唤醒 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class TestContainer02&lt;E&gt; &#123; private final LinkedList&lt;E&gt; list = new LinkedList&lt;&gt;(); private final int MAX = 10; private int count = 0; private Lock lock = new ReentrantLock(); private Condition producer = lock.newCondition(); private Condition consumer = lock.newCondition(); public int getCount()&#123; return count; &#125; public void put(E e)&#123; lock.lock(); try &#123; while(list.size() == MAX)&#123; System.out.println(Thread.currentThread().getName() + " 等待。。。"); // 进入等待队列。释放锁标记。 // 借助条件，进入的等待队列。 producer.await(); &#125; System.out.println(Thread.currentThread().getName() + " put 。。。"); list.add(e); count++; // 借助条件，唤醒所有的消费者。 consumer.signalAll(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public E get()&#123; E e = null; lock.lock(); try &#123; while(list.size() == 0)&#123; System.out.println(Thread.currentThread().getName() + " 等待。。。"); // 借助条件，消费者进入等待队列 consumer.await(); &#125; System.out.println(Thread.currentThread().getName() + " get 。。。"); e = list.removeFirst(); count--; // 借助条件，唤醒所有的生产者 producer.signalAll(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; return e; &#125; public static void main(String[] args) &#123; final TestContainer02&lt;String&gt; c = new TestContainer02&lt;&gt;(); for(int i = 0; i &lt; 10; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; for(int j = 0; j &lt; 5; j++)&#123; System.out.println(c.get()); &#125; &#125; &#125;, "consumer"+i).start(); &#125; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; for(int i = 0; i &lt; 2; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; for(int j = 0; j &lt; 25; j++)&#123; c.put("container value " + j); &#125; &#125; &#125;, "producer"+i).start(); &#125; &#125;&#125; 金丹中期（锁的底层实现）​ Java 虚拟机中的同步(Synchronization)基于进入和退出管程(Monitor)对象实现。同步方法 并不是由 monitor enter 和 monitor exit 指令来实现同步的，而是由方法调用指令读取运行时常量池中方法的ACC_SYNCHRONIZED 标志来隐式实现的。注：monitor enter 和 monitor exit 指令是C语言的内容。 对象的内存模型（一个对象包含3部分，没有方法，方法是在方法区域中的） 对象头：存储对象的 hashCode、锁信息或分代年龄或 GC 标志，类型指针指向对象的类元数据，JVM 通过这个指针确定该对象是哪个类的实例等信息。（关注锁信息） 实例变量：存放类的属性数据信息，包括父类的属性信息 填充数据：由于虚拟机要求对象起始地址必须是 8 字节的整数倍。填充数据不是必须存在的，仅仅是为了字节对齐 monitor在栈中，但不是在线程栈中。 _Owner指向线程。 ​ 当线程在对象上加锁时，对象头都会指向monitor，记录锁信息。当执行 synchronized 同步方法或同步代码块时，会在对象头中记录锁标记，锁标记指向的是 monitor 对象（也称为管程或监视器锁）的起始地址。每个对象都存在着一个 monitor 与之关联，对象与其 monitor 之间的关系有存在多种实现方式，如 monitor 可以与对象一起创建销毁或当线程试图获取对象锁时自动生成，但当一个 monitor 被某个线程持有后，它便处于锁定状态。 ​ 另外的线程想获取对象头中的锁信息的时候，会发现对象头中已经记录一把锁（monitor），他就获取不到。monitor是互斥的，对象头记录的monitor就不会分配给其他线程了，此时这个线程就会进入阻塞状态。当执行中的线程发生异常，或者是释放锁标记，对象头的锁信息就会释放它记录的monitor。阻塞状态的线程就会弹出来争夺对象中的锁信息，重新在锁信息中记录monitor。 ​ ObjectMonitor 中有两个队列，_WaitSet 和 _EntryList，以及_Owner 标记。其中_WaitSet是用于管理等待队列(wait)线程的，_EntryList 是用于管理锁池阻塞线程的，_Owner 标记用于记录当前执行线程。 线程状态图 ​ 当多线程并发访问同一个同步代码时，首先会进入_EntryList，当线程获取锁标记后，monitor 中的_Owner 记录此线程，并在 monitor 中的计数器执行递增计算（+1），代表锁定，其他线程在_EntryList 中继续阻塞。若执行线程调用 wait 方法，则 monitor 中的计数器执行赋值为 0 计算，并将_Owner 标记赋值为 null，代表放弃锁，执行线程进如_WaitSet 中阻塞。若执行线程调用 notify/notifyAll 方法，_WaitSet 中的线程被唤醒，进入_EntryList 中阻塞，等待获取锁标记。若执行线程的同步代码执行结束，同样会释放锁标记，monitor 中的_Owner标记赋值为 null，且计数器赋值为 0 计算。 ​ interrupt() 方法可以任何打断阻塞状态的线程，以抛异常的代价。 ​ InterruptedException异常是阻塞异常。阻塞中的线程抛出的。 锁的重入 ​ 在 Java 中，同步锁是可以重入的。只有同一线程调用同步方法或执行同步代码块，对同一个对象加锁时才可重入。​ 当线程持有锁时，会在 monitor 的计数器中执行递增计算，若当前线程调用其他同步代码，且同步代码的锁对象相同时，monitor 中的计数器继续递增。每个同步代码执行结束，monitor 中的计数器都会递减，直至所有同步代码执行结束，monitor 中的计数器为 0 时，释放锁标记，_Owner 标记赋值为 null。 金丹后期（锁的种类） Java 中锁的种类包括偏向锁，自旋锁，轻量级锁，重量级锁。 锁的使用方式先提供偏向锁，如果不满足的时候，升级为轻量级锁，再不满足，升级为重量级锁。自旋锁是一个过渡的锁状态，不是一种实际的锁类型。锁只能升级，不能降级。 在金丹初期提到的就是重量级锁。 偏向锁： ​ 是一种编译解释锁。如果代码中不可能出现多线程并发争抢同一个锁的时候，JVM 编译代码，解释执行的时候，会自动的放弃同步信息。消除 synchronized 的同步代码结果。使用锁标记的形式记录锁状态。在 Monitor 中有变量 ACC_SYNCHRONIZED。当变量值使用的时候，代表偏向锁锁定。可以避免锁的争抢和锁池状态的维护。提高JVM解释效率。 123456Object o = new Object();public void m() &#123; o = new Object(); synchronized (o) &#123; &#125;&#125; 轻量级锁： ​ 是一个过渡锁。当偏向锁不满足，也就是有多线程并发访问，锁定同一个对象的时候，先提升为轻量级锁。也是使用标记 ACC_SYNCHRONIZED 标记记录的。ACC_UNSYNCHRONIZED 标记记录未获取到锁信息的线程。就是只有两个线程争抢锁标记的时候，优先使用轻量级锁。A线程和monitor有直接关联的。B线程不记录monitor，是monitor记录B线程，线程A结束后，B两个线程才找到monitor。也可能出现重量级锁。 自旋锁： ​ 是一个过渡锁，是偏向锁和轻量级锁的过渡。当获取锁的过程中，未获取到。为了提高效率，JVM 自动执行若干次空循环，再次申请锁，而不是进入阻塞状态的情况。称为自旋锁。自旋锁提高效率就是避免线程状态的变更。 金丹圆满（ThreadLocal） 就是一个Map。key 是Thread.getCurrentThread()，value 是线程需要保存的变量。 ThreadLocal.set(value)相当map.put(Thread.getCurrentThread(), value)。 ThreadLocal.get() 相当map.get(Thread.getCurrentThread())。 内存问题 ： 在并发量高的时候，可能有内存溢出。 使用ThreadLocal的时候，一定注意回收资源问题，每个线程结束之前，将当前线程保存的线程变量一定要删除 ，调用ThreadLocal.remove()，要不会发生泄露。run方法的finally代码块。 ​ 在一个操作系统中，线程和进程是有数量上限的。在操作系统中，确定线程和进程唯一性的唯一条件就是线程或进程 ID。操作系统在回收线程或进程的时候，不是一定杀死线程或进程，在繁忙的时候，只会做情况线程或进程栈数据的操作，重复使用线程或进程。 12345678910111213141516171819202122232425262728293031323334public class Test_01 &#123; volatile static String name = "zhangsan"; static ThreadLocal&lt;String&gt; tl = new ThreadLocal&lt;&gt;(); public static void main(String[] args) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(name); // lisi System.out.println(tl.get()); // null &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; name = "lisi"; tl.set("wangwu"); &#125; &#125;).start(); &#125;&#125; 小编是一枚Java Coder，业余写文章，现主营微信公众号《Java患者》，喜欢的话关注我的公众号或者加我微信我们一起学习Java 元婴期（并发容器）​ 解决并发情况下的容器线程安全问题的。给多线程环境准备一个线程安全的容器对象。线程安全的容器对象： Vector, Hashtable。线程安全容器对象，都是使用 synchronized方法实现的。​ concurrent 包中的同步容器，大多数是使用系统底层技术实现的线程安全。类似 native。Java8 中使用 CAS。 元婴前期（Map/Set） ConcurrentHashMap/ConcurrentHashSet底层哈希实现的同步 Map(Set)。效率高，线程安全。使用系统底层技术实现线程安全。量级较 synchronized 低。key 和 value 不能为 null。 ConcurrentSkipListMap/ConcurrentSkipListSet底层跳表（SkipList）实现的同步 Map(Set)。有序，效率比 ConcurrentHashMap 稍低。 1234567891011121314151617181920212223242526272829303132333435public class Test_01_ConcurrentMap &#123; public static void main(String[] args) &#123; final Map&lt;String, String&gt; map = new Hashtable&lt;&gt;(); // Collections.syncxxxxxx // final Map&lt;String, String&gt; map = new ConcurrentHashMap&lt;&gt;(); // final Map&lt;String, String&gt; map = new ConcurrentSkipListMap&lt;&gt;(); 数据结构跳表 final Random r = new Random(); Thread[] array = new Thread[100]; final CountDownLatch latch = new CountDownLatch(array.length); long begin = System.currentTimeMillis(); for (int i = 0; i &lt; array.length; i++) &#123; array[i] = new Thread(new Runnable() &#123; @Override public void run() &#123; for (int j = 0; j &lt; 10000; j++) &#123; map.put("key" + r.nextInt(100000), "value" + r.nextInt(100000)); &#125; latch.countDown(); &#125; &#125;); &#125; for (Thread t : array) &#123; t.start(); &#125; try &#123; latch.await(); //等待门闩开放 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; long end = System.currentTimeMillis(); System.out.println("执行时间为 ： " + (end - begin) + "毫秒！"); &#125;&#125; 调表机构：存10、18、15、20、19。 元婴中期（List） CopyOnWriteArrayList：写时复制集合，效率低，读取效率高。每次写入数据，都会创建一个新的底层数组。 浪费空间保证数据的安全。 初始容量1，每次新增的内容，创建容量+1。 取得时候，取最新的数组。remove最后一个数据，直接用上一个数组。 set和remove其他数据，重新创建数组。 存在幻读（写的时候，有读操作，不是最新添加的数据） 存在脏读（写的时候，有读操作，不是最新添加的数据） 123456789101112131415161718192021222324252627282930313233343536public class Test_02_CopyOnWriteList &#123; public static void main(String[] args) &#123; // final List&lt;String&gt; list = new ArrayList&lt;&gt;(); 线程不安全 // final List&lt;String&gt; list = new Vector&lt;&gt;(); 线程安全 更快 final List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); final Random r = new Random(); Thread[] array = new Thread[100]; final CountDownLatch latch = new CountDownLatch(array.length); long begin = System.currentTimeMillis(); for(int i = 0; i &lt; array.length; i++)&#123; array[i] = new Thread(new Runnable() &#123; @Override public void run() &#123; for(int j = 0; j &lt; 1000; j++)&#123; list.add("value" + r.nextInt(100000)); &#125; latch.countDown(); &#125; &#125;); &#125; for(Thread t : array)&#123; t.start(); &#125; try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; long end = System.currentTimeMillis(); System.out.println("执行时间为 ： " + (end-begin) + "毫秒！"); System.out.println("List.size() : " + list.size()); &#125;&#125; 元婴后期（Queue）ConcurrentLinkedQueue：基础链表同步队列。 123456789101112131415161718192021public class Test_03_ConcurrentLinkedQueue &#123; public static void main(String[] args) &#123; Queue&lt;String&gt; queue = new ConcurrentLinkedQueue&lt;&gt;(); for(int i = 0; i &lt; 10; i++)&#123; queue.offer("value" + i); &#125; System.out.println(queue); System.out.println(queue.size()); // peek() -&gt; 查看queue中的首数据 System.out.println(queue.peek()); System.out.println(queue.size()); // poll() -&gt; 获取queue中的首数据 System.out.println(queue.poll()); System.out.println(queue.size()); &#125;&#125; LinkedBlockingQueue：阻塞队列，队列容量不足自动阻塞，队列容量为 0 自动阻塞。 put自动阻塞， 队列容量满后，自动阻塞。 take自动阻塞方法， 队列容量为0后，自动阻塞。 12345678910111213141516171819202122232425262728293031323334353637383940public class Test_04_LinkedBlockingQueue &#123; final BlockingQueue&lt;String&gt; queue = new LinkedBlockingQueue&lt;&gt;(); final Random r = new Random(); public static void main(String[] args) &#123; final Test_04_LinkedBlockingQueue t = new Test_04_LinkedBlockingQueue(); new Thread(new Runnable() &#123; @Override public void run() &#123; while(true)&#123; try &#123; t.queue.put("value"+t.r.nextInt(1000)); TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;, "producer").start(); for(int i = 0; i &lt; 3; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; while(true)&#123; try &#123; System.out.println(Thread.currentThread().getName() + " - " + t.queue.take()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;, "consumer"+i).start(); &#125; &#125;&#125; ArrayBlockingQueue：底层数组实现的有界队列。自动阻塞。根据调用 API（add/put/offer）不同，有不同特性。当容量不足的时候，有阻塞能力。 add 方法在容量不足的时候，抛出异常。 put 方法在容量不足的时候，阻塞等待。 offer 方法， 单参数 offer 方法，不阻塞。容量不足的时候，返回 false。当前新增数据操作放弃。 三参数 offer 方法（offer(value,times,timeunit)），容量不足的时候，阻塞 times 时长（单位为 timeunit），如果在阻塞时长内，有容量空闲，新增数据返回 true。如果阻塞时长范围内，无容量空闲，放弃新增数据，返回 false。 1234567891011121314151617181920212223242526272829303132public class Test_05_ArrayBlockingQueue &#123; final BlockingQueue&lt;String&gt; queue = new ArrayBlockingQueue&lt;&gt;(3); public static void main(String[] args) &#123; final Test_05_ArrayBlockingQueue t = new Test_05_ArrayBlockingQueue(); for(int i = 0; i &lt; 5; i++)&#123; // System.out.println("add method : " + t.queue.add("value"+i)); --------------------------------------------------------------------- /*try &#123; t.queue.put("put"+i); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("put method : " + i);*/ --------------------------------------------------------------------- // System.out.println("offer method : " + t.queue.offer("value"+i)); --------------------------------------------------------------------- try &#123; System.out.println("offer method : " + t.queue.offer("value"+i, 1, TimeUnit.SECONDS)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(t.queue); &#125;&#125; DelayQueue：延时队列。根据比较机制，实现自定义处理顺序的队列。常用于定时任务。 通过比较方法，比较排列，获取。 可以保存的对象一定要实现Delayed接口。Delayed接口继承Comparable接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Test_06_DelayQueue &#123; static BlockingQueue&lt;MyTask_06&gt; queue = new DelayQueue&lt;&gt;(); public static void main(String[] args) throws InterruptedException &#123; long value = System.currentTimeMillis(); MyTask_06 task1 = new MyTask_06(value + 2000); MyTask_06 task2 = new MyTask_06(value + 1000); MyTask_06 task3 = new MyTask_06(value + 3000); MyTask_06 task4 = new MyTask_06(value + 2500); MyTask_06 task5 = new MyTask_06(value + 1500); queue.put(task1); queue.put(task2); queue.put(task3); queue.put(task4); queue.put(task5); System.out.println(queue); System.out.println(value); for(int i = 0; i &lt; 5; i++)&#123; System.out.println(queue.take()); &#125; &#125;&#125;class MyTask_06 implements Delayed &#123; private long compareValue; public MyTask_06(long compareValue)&#123; this.compareValue = compareValue; &#125; /** * 比较大小。自动实现升序 * 建议和getDelay方法配合完成。 * 如果在DelayQueue是需要按时间完成的计划任务，必须配合getDelay方法完成。 */ @Override public int compareTo(Delayed o) &#123; return (int)(this.getDelay(TimeUnit.MILLISECONDS) - o.getDelay(TimeUnit.MILLISECONDS)); &#125; /** * 获取计划时长的方法。 * 根据参数TimeUnit来决定，如何返回结果值。秒，毫秒 */ @Override public long getDelay(TimeUnit unit) &#123; return unit.convert(compareValue - System.currentTimeMillis(), TimeUnit.MILLISECONDS); &#125; @Override public String toString()&#123; return "Task compare value is : " + this.compareValue; &#125;&#125; LinkedTransferQueue：转移队列， 使用 transfer 方法，没有消费者，就阻塞。必须有消费者（take()方法的调用者），实现数据的即时处理（电话）。 无容量的，放数组，容量为零，这时候要阻塞。等另一个线程来拿，不经过容器的存储来转移数组。 使用 add方法，直接存在容器中。队列会保存数据，不做阻塞等待（短信）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class Test_07_TransferQueue &#123; TransferQueue&lt;String&gt; queue = new LinkedTransferQueue&lt;&gt;(); public static void main(String[] args) &#123; final Test_07_TransferQueue t = new Test_07_TransferQueue();//为了匿名内部累方法中获得queue引用。 /*new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(Thread.currentThread().getName() + " thread begin " ); System.out.println(Thread.currentThread().getName() + " - " + t.queue.take()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, "output thread").start(); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; try &#123; t.queue.transfer("test string"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;*/ new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; t.queue.transfer("test string"); // t.queue.add("test string"); System.out.println("add ok"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(Thread.currentThread().getName() + " thread begin " ); System.out.println(Thread.currentThread().getName() + " - " + t.queue.take()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, "output thread").start(); &#125;&#125; SynchronusQueue：同步队列，是一个容量为 0 的队列。是一个特殊的 TransferQueue。 必须现有消费线程等待，才能使用的队列。 add 方法，无阻塞。若没有消费线程阻塞等待数据，则抛出非阻塞异常。 put 方法，有阻塞。若没有消费线程阻塞等待数据，则阻塞。 场景：玩家与玩家之间的匹配。 12345678910111213141516171819202122232425262728293031323334353637383940public class Test_08_SynchronusQueue &#123; BlockingQueue&lt;String&gt; queue = new SynchronousQueue&lt;&gt;(); public static void main(String[] args) &#123; final Test_08_SynchronusQueue t = new Test_08_SynchronusQueue(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(Thread.currentThread().getName() + " thread begin " ); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " - " + t.queue.take()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, "output thread").start(); /*try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;*/ // t.queue.add("test add"); try &#123; t.queue.put("test put"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " queue size : " + t.queue.size()); &#125;&#125; 元婴圆满（线程池）​ Executor：线程池顶级接口。定义方法，void execute(Runnable)。方法是用于处理任务的一个服务方法。调用者提供 Runnable 接口的实现，线程池通过线程执行这个 Runnable。服务方法无返回值的。是 Runnable 接口中的 run 方法无返回值。常用方法 - void execute(Runnable)，作用是启动线程任务的。 ​ 他不是线程池，他是线程池线程池底层处理机制。在使用线程池的时候，底层如何处理本线程的逻辑。 123456789101112131415public class Test_01_MyExecutor implements Executor &#123; public static void main(String[] args) &#123; new Test_01_MyExecutor().execute(new Runnable() &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + " - test executor"); &#125; &#125;); &#125; @Override public void execute(Runnable command) &#123; new Thread(command).start(); &#125;&#125; ExecutorService：Executor 接口的子接口。提供了一个新的服务方法，submit。有返回值（Future 类型）。submit 方法提供了 overload 方法。其中有参数类型为 Runnable 的，不需要提供返回值的；有参数类型为 Callable，可以提供线程执行后的返回值。 他是线程池服务类型。所有的线程池类型都实现这个接口，实现这个接口，代表可以提供线程池能力。 Future是 submit 方法的返回值。代表未来，也就是线程执行结束后的一种结果。如返回值。 常见方法 void execute(Runnable) Future submit(Callable) Future submit(Runnable) shutdown()：优雅关闭。 不是强行关闭线程池，回收线程池中的资源。而是不再处理新的任务，将已接收的任务处理完毕后再关闭。 线程池状态 Running - 线程池正在执行中。活动状态。 ShuttingDown - 线程池正在关闭过程中。优雅关闭。一旦进入这个状态，线程池不再接收新的任务，处理所有已接收的任务，处理完毕后，关闭线程池。不能执行submit方法和execute方法。 Terminated - 线程池已经关闭。不能执行submit方法和execute方法。 Future：未来结果，代表线程任务执行结束后的结果。获取线程执行结果的方式是通过 get 方法获取的。 get 无参，阻塞等待线程执行结束，并得到结果。 get 有参，阻塞固定时长，等待线程执行结束后的结果，如果在阻塞时长范围内，线程未执行结束，抛出异常。 常用方法： T get() T get(long, TimeUnit) 123456789101112131415161718192021222324252627282930313233343536public class Test_03_Future &#123; public static void main(String[] args) throws InterruptedException, ExecutionException &#123; /*FutureTask&lt;String&gt; task = new FutureTask&lt;&gt;(new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; return "first future task"; &#125; &#125;); new Thread(task).start(); System.out.println(task.get());*/ // 上面代码和下面一模一样的 ExecutorService service = Executors.newFixedThreadPool(1); Future&lt;String&gt; future = service.submit(new Callable&lt;String&gt;() &#123; @Override public String call() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("aaa"); return Thread.currentThread().getName() + " - test executor"; &#125; &#125;); System.out.println(future); System.out.println(future.isDone()); // 查看线程是否结束， 任务是否完成。 call方法是否执行结束 System.out.println(future.get()); // 获取call方法的返回值。 System.out.println(future.isDone()); &#125;&#125; Callable：可执行接口， 类似 Runnable 接口，也是可以启动一个线程的接口。其中定义的方法是call，call 方法的作用和 Runnable 中的 run 方法完全一致，call 方法有返回值。 接口方法 ： Object call();相当于 Runnable 接口中的 run 方法。区别为此方法有返回值。不能抛出已检查异常。 和 Runnable 接口的选择 - 需要返回值或需要抛出异常时，使用 Callable，其他情况可任意选择。 Executors：工具类型，为 Executor 线程池提供工具方法。 可以快速的提供若干种线程池。如：固定容量的，无限容量的，容量为 1 等各种线程池。 线程池是一个进程级的重量级资源。默认的生命周期和 JVM 一致。当开启线程池后，直到 JVM 关闭为止，是线程池的默认生命周期。如果手工调用 shutdown 方法，那么线程池执行所有的任务后，自动关闭。不调用shutdown方法，程序一直不关闭的。 开始 - 创建线程池。 结束 - JVM 关闭或调用 shutdown 并处理完所有的任务。 类似 Arrays，Collections 等工具类型的功用。 FixedThreadPool：容量固定的线程池。活动状态和线程池容量是有上限的线程池。 所有的线程池中，都有一个任务队列。使用的是 BlockingQueue作为任务的载体。当任务数量大于线程池容量的时候，没有运行的任务保存在任务队列中，当线程有空闲的，自动从队列中取出任务执行。 使用场景： 大多数情况下，使用的线程池，首选推荐 FixedThreadPool。OS 系统和硬件是有线程支持上限。不能随意的无限制提供线程池。 线程池默认的容量上限是 Integer.MAX_VALUE。 常见的线程池容量： PC - 200。 服务器 - 1000~10000 queued tasks - 任务队列，completed tasks - 结束任务队列 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Test_02_FixedThreadPool &#123; public static void main(String[] args) &#123; ExecutorService service = Executors.newFixedThreadPool(5); for (int i = 0; i &lt; 6; i++) &#123; service.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " - test executor"); &#125; &#125;); &#125; //[Running, pool size = 5, active threads = 5, queued tasks = 1, completed tasks = 0] System.out.println(service); service.shutdown(); // 是否已经结束， 相当于回收了资源。 System.out.println(service.isTerminated()); // false // 是否已经关闭， 是否调用过shutdown方法 System.out.println(service.isShutdown()); //true // [Shutting down, pool size = 5, active threads = 5, queued tasks = 1, completed tasks = 0] System.out.println(service); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // service.shutdown(); System.out.println(service.isTerminated()); //true System.out.println(service.isShutdown()); //true // [Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 6] System.out.println(service); &#125;&#125; ​ CachedThreadPool：缓存的线程池。容量不限（Integer.MAX_VALUE）。自动扩容。容量管理策略：如果线程池中的线程数量不满足任务执行，创建新的线程。每次有新任务无法即时处理的时候，都会创建新的线程。 当线程池中的线程空闲时长达到一定的临界值（默认 60 秒），自动释放线程。 默认线程空闲 60 秒，自动销毁。 应用场景： 内部应用或测试应用。 内部应用，有条件的内部数据瞬间处理时应用，如：电信平台夜间执行数据整理（有把握在短时间内处理完所有工作，且对硬件和软件有足够的信心）。 测试应用，在测试的时候，尝试得到硬件或软件的最高负载量，用于提供FixedThreadPool 容量的指导。 1234567891011121314151617181920212223242526272829303132public class Test_05_CachedThreadPool &#123; public static void main(String[] args) &#123; ExecutorService service = Executors.newCachedThreadPool(); System.out.println(service); // 容量为0 for(int i = 0; i &lt; 5; i++)&#123; service.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " - test executor"); &#125; &#125;); &#125; System.out.println(service); // 容量为5 try &#123; TimeUnit.SECONDS.sleep(65); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(service); &#125;&#125; ScheduledThreadPool：计划任务线程池。可以根据计划自动执行任务的线程池。 scheduleAtFixedRate(Runnable, start_limit, limit, timeunit) runnable - 要执行的任务。 start_limit - 第一次任务执行的间隔。 limit - 多次任务执行的间隔。 timeunit - 多次任务执行间隔的时间单位。 他是阻塞的，效率低下。 他本质就是DelayedQueue 每间隔一定的时间，随机一个线程运行，并且运行完的线程，不会销毁，会继续等待下次选中运行。 使用场景： 计划任务时选用（具体与DelaydQueue比较后选择），如：电信行业中的数据整理，每分钟整理，每消失整理，每天整理等. 1234567891011121314151617181920212223public class Test_07_ScheduledThreadPool &#123; public static void main(String[] args) &#123; ScheduledExecutorService service = Executors.newScheduledThreadPool(3); System.out.println(service); // 定时完成任务。 scheduleAtFixedRate(Runnable, start_limit, limit, timeunit) // runnable - 要执行的任务。 service.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()); &#125; &#125;, 0, 300, TimeUnit.MILLISECONDS); &#125;&#125; SingleThreadExceutor：单一容量的线程池。使用场景： 所有任务交给它处理，保证任务顺序时使用。如： 游戏大厅中的公共频道聊天。秒杀。 1234567891011121314151617181920212223public class Test_06_SingleThreadExecutor &#123; public static void main(String[] args) &#123; ExecutorService service = Executors.newSingleThreadExecutor(); System.out.println(service); for(int i = 0; i &lt; 5; i++)&#123; service.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " - test executor"); &#125; &#125;); &#125; &#125;&#125; ForkJoinPool：分支合并线程池（mapduce 类似的设计思想，递归思想的运用）。适合用于处理复杂任务。 初始化线程容量与 CPU 核心数相关。 线程池中运行的内容必须是 ForkJoinTask 的子类型（RecursiveTask,RecursiveAction）。 ForkJoinPool - 分支合并线程池。 可以递归完成复杂任务。要求可分支合并的任务必须是 ForkJoinTask 类型的子类型。其中提供了分支和合并的能力。ForkJoinTask 类型提供了两个抽象子类型，RecursiveTask 有返回结果的分支合并任务,RecursiveAction 无返回结果的分支合并任务。（Callable/Runnable）compute 方法：就是任务的执行逻辑。 ForkJoinPool 没有所谓的容量。默认都是 1 个线程。根据任务自动的分支新的子线程。当子线程任务结束后，自动合并。所谓自动是根据 fork 和 join 两个方法实现的。 应用： 主要是做科学计算或天文计算的。数据分析的. 拿空间换时间，效率高，但要看CPU能力。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class Test_08_ForkJoinPool &#123; final static int[] numbers = new int[1000000]; final static int MAX_SIZE = 50000; final static Random r = new Random(); static&#123; for(int i = 0; i &lt; numbers.length; i++)&#123; numbers[i] = r.nextInt(1000); &#125; &#125; static class AddTask extends RecursiveTask&lt;Long&gt;&#123; // RecursiveAction int begin, end; public AddTask(int begin, int end)&#123; this.begin = begin; this.end = end; &#125; // protected Long compute()&#123; if((end - begin) &lt; MAX_SIZE)&#123; long sum = 0L; for(int i = begin; i &lt; end; i++)&#123; sum += numbers[i]; &#125; // System.out.println("form " + begin + " to " + end + " sum is : " + sum); return sum; &#125;else&#123; int middle = begin + (end - begin)/2; AddTask task1 = new AddTask(begin, middle); AddTask task2 = new AddTask(middle, end); task1.fork();// 就是用于开启新的任务的。 就是分支工作的。 就是开启一个新的线程任务。 task2.fork(); // join - 合并。将任务的结果获取。 这是一个阻塞方法。一定会得到结果数据。 return task1.join() + task2.join(); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException, ExecutionException, IOException &#123; long result = 0L; for(int i = 0; i &lt; numbers.length; i++)&#123; result += numbers[i]; &#125; System.out.println(result); ForkJoinPool pool = new ForkJoinPool(); AddTask task = new AddTask(0, numbers.length); Future&lt;Long&gt; future = pool.submit(task); System.out.println(future.get()); &#125;&#125; ThreadPoolExecutor：线程池底层实现。除 ForkJoinPool 外，其他常用线程池底层都是使用ThreadPoolExecutor实现的。public ThreadPoolExecutor(int corePoolSize，int maximumPoolSize，long keepAliveTime，TimeUnit unit，BlockingQueue workQueue); corePoolSize： 核心容量，创建线程池的时候，默认有多少线程。也是线程池保持的最少线程数 maximumPoolSize： 最大容量，线程池最多有多少线程 keepAliveTime：生命周期，0 为永久。当线程空闲多久后，自动回收。 unit：生命周期单位，为生命周期提供单位，如：秒，毫秒 workQueue：任务队列，阻塞队列。注意，泛型必须是 使用场景： 默认提供的线程池不满足条件时使用。如：初始线程数据 4，最大线程数200，线程空闲周期 30 秒。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Test_09_ThreadPoolExecutor &#123; public static void main(String[] args) &#123; // 模拟fixedThreadPool， 核心线程5个，最大容量5个，线程的生命周期无限。 ExecutorService service = new ThreadPoolExecutor(5, 5, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); for(int i = 0; i &lt; 6; i++)&#123; service.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " - test executor"); &#125; &#125;); &#125; System.out.println(service); service.shutdown(); System.out.println(service.isTerminated()); System.out.println(service.isShutdown()); System.out.println(service); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; service.shutdown(); System.out.println(service.isTerminated()); System.out.println(service.isShutdown()); System.out.println(service); &#125;&#125; 123练习：启动若干线程，并行访问同一个容器中的数据。保证获取容器中数据时没有数据错误，且线程安全。如：售票，秒杀等业务。 使用synchronized 1234567891011121314151617181920212223242526272829303132333435363738394041public class Test_01 &#123; static List&lt;String&gt; list = new ArrayList&lt;&gt;(); // static List&lt;String&gt; list = new Vector&lt;&gt;(); static&#123; for(int i = 0; i &lt; 10000; i++)&#123; list.add("String " + i); &#125; &#125; public static void main(String[] args) &#123; for(int i = 0; i &lt; 10; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; while(list.size() &gt; 0)&#123; System.out.println(Thread.currentThread().getName() + " - " + list.remove(0)); &#125; &#125; &#125;, "Thread" + i).start(); &#125; /*for(int i = 0; i &lt; 10; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; while(true)&#123; synchronized (list) &#123; if(list.size() &lt;= 0)&#123; break; &#125; System.out.println(Thread.currentThread().getName() + " - " + list.remove(0)); &#125; &#125; &#125; &#125;, "Thread" + i).start(); &#125;*/ &#125;&#125; 使用queue 12345678910111213141516171819202122232425262728public class Test_02 &#123; static Queue&lt;String&gt; list = new ConcurrentLinkedQueue&lt;&gt;(); static&#123; for(int i = 0; i &lt; 10000; i++)&#123; list.add(&quot;String &quot; + i); &#125; &#125; public static void main(String[] args) &#123; for(int i = 0; i &lt; 10; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; while(true)&#123; String str = list.poll(); if(str == null)&#123; break; &#125; System.out.println(Thread.currentThread().getName() + &quot; - &quot; + str); &#125; &#125; &#125;, &quot;Thread&quot; + i).start(); &#125; &#125;&#125; 小编是一枚Java Coder，业余写文章，现主营微信公众号《Java患者》，喜欢的话关注我的公众号或者加我微信我们一起学习Java 化神期（JVM1.7）化神前期（jvm结构）jvm基本结构图： 类加载子系统：类加载子系统负责从文件系统或者网络中加载 Class 信息，如ClassLoad这里面的组件。 方法区：加载的类信息存放于一块称为方法区的内存空间。除了类的信息外，方法区中可能还会存放运行时常量池信息，包括字符串字面量和数字常量（这部分常量信息是 Class 文件中常量池部分的内存映射）。 Java 堆：java 堆在虚拟机启动的时候建立，它是 java 程序最主要的内存工作区域。几乎所有的java 对象实例都存放在 java 堆中。堆空间是所有线程共享的，这是一块与 java 应用密切相关的内存空间。 直接内存：java 的 NIO 库允许 java 程序使用直接内存。直接内存是在 java 堆外的、直接向系统申请的内存空间。通常访问直接内存的速度会优于 java 堆。因此出于性能的考虑，读写频繁的场合可能会考虑使用直接内存。由于直接内存在 java 堆外，因此它的大小不会直接受限于 Xmx 指定的最大堆大小，但是系统内存是有限的，java 堆和直接内存的总和依然受限于操作系统能给出的最大内存。 垃圾回收系统：垃圾回收系统是 java 虚拟机的重要组成部分，垃圾回收器可以对方法区、java 堆和直接内存进行回收。其中，java 堆是垃圾收集器的工作重点。和 C/C++不同，java 中所有的对象空间释放都是隐式的，也就是说，java 中没有类似 free()或者 delete()这样的函数释放指定的内存区域。对于不再使用的垃圾对象，垃圾回收系统会在后台默默工作，默默查找、标识并释放垃圾对象，完成包括 java 堆、方法区和直接内存中的全自动化管理。 Java 栈：每一个 java 虚拟机线程都有一个私有的 java 栈，一个线程的 java 栈在线程创建的时候被创建，java 栈中保存着帧信息，java 栈中保存着局部变量、方法参数，同时和 java 方法的调用、返回密切相关。 本地方法栈：本地方法栈和 java 栈非常类似，最大的不同在于 java 栈用于方法的调用，而本地方法栈则用于本地方法的调用，作为对 java 虚拟机的重要扩展，java 虚拟机允许 java 直接调用本地方法（通常使用 C 编写）。 PC 寄存器：PC（Program Counter）寄存器也是每一个线程私有的空间，java 虚拟机会为每一个 java线程创建 PC 寄存器。在任意时刻，一个 java 线程总是在执行一个方法，这个正在被执行的方法称为当前方法。如果当前方法不是本地方法，PC 寄存器就会指向当前正在被执行的指令。如果当前方法是本地方法，那么 PC 寄存器的值就是 undefined。 Java HotSpot Client 模式和 Server 模式的区别：当虚拟机运行在-client 模式的时候,使用的是一个代号为 C1 的轻量级编译器, 而-server模式启动的虚拟机采用相对重量级,代号为 C2 的编译器. C2 比 C1 编译器编译的相对彻底,服务起来之后,性能更高。 JDK 安装目录/jre/lib/（x86、i386、amd32、amd64）/jvm.cfg文件中的内容，-server 和-client 哪一个配置在上，执行引擎就是哪一个。如果是 JDK1.5版本且是 64 位系统应用时，-client 无效。 –64 位系统内容-server KNOWN-client IGNORE –32 位系统内容-server KNOWN-client KNOWN 注意 ：在部分 JDK1.6 版本和后续的 JDK 版本 (64 位系统 ) 中， -client 参数已经不起作用了， Server 模式成为唯一。 化神中期（堆结构及对象分代） 什么是分代，分代的必要性是什么？ ​ Java 虚拟机根据对象存活的周期不同，把堆内存划分为几块，一般分为新生代、老年代和永久代（对 HotSpot 虚拟机而言），这就是 JVM 的内存分代策略。​ 堆内存是虚拟机管理的内存中最大的一块，也是垃圾回收最频繁的一块区域，我们程序所有的对象实例都存放在堆内存中。给堆内存分代是为了提高对象内存分配和垃圾回收的效率。试想一下，如果堆内存没有区域划分，所有的新创建的对象和生命周期很长的对象放在一起，随着程序的执行，堆内存需要频繁进行垃圾收集，而每次回收都要遍历所有的对象，遍历这些对象所花费的时间代价是巨大的，会严重影响我们的 GC 效率，并且会产生碎片。​ 有了内存分代，情况就不同了，新创建的对象会在新生代中分配内存，经过多次回收仍然存活下来的对象存放在老年代中，静态属性、类信息等存放在永久代中，新生代中的对象存活时间短，只需要在新生代区域中频繁进行 GC，老年代中对象生命周期长，内存回收的频率相对较低，不需要频繁进行回收，永久代中回收效果太差，一般不进行垃圾回收，还可以根据不同年代的特点采用合适的垃圾收集算法。分代收集大大提升了收集效率，这些都是内存分代带来的好处。 分代的划分Java ​ 虚拟机将堆内存划分为 新生代、老年代和永久代 ，永久代是 HotSpot 虚拟机特有的概念（JDK1.8 之后为 metaspace 替代永久代），它采用永久代的方式来实现方法区，其他的虚拟机实现没有这一概念，而且 HotSpot 也有取消永久代的趋势，在 JDK 1.7 中 HotSpot 已经开始了“去永久化”，把原本放在永久代的字符串常量池移出。永久主要存放常量、类信息、静态变量等数据，与垃圾回收关系不大，新生代和老年代是垃圾回收的主要区域。 新生代（Young Generation) ​ 新生成的对象优先存放在新生代中，新生代对象朝生夕死，存活率很低，在新生代Eden中，常规应用进行一次垃圾收集一般可以回收 70% ~ 95% 的空间，回收效率很高。​ HotSpot 将新生代划分为三块，一块较大的 Eden（伊甸）空间和两块较小的 Survivor（幸存者）空间，默认比例为 8：1：1。划分的目的是因为 HotSpot 采用复制算法来回收新生代，设置这个比例是为了充分利用内存空间，减少浪费。新生成的对象在 Eden 区分配（大对象除外，大对象直接进入老年代），当 Eden 区没有足够的空间进行分配时，虚拟机将发起一次Minor GC。​ GC 开始时，对象只会存在于 Eden 区和 From Survivor 区，To Survivor 区是空的（作为保留区域）。GC 进行时，Eden 区中所有存活的对象都会被复制到 To Survivor 区，而在 FromSurvivor 区中，仍存活的对象会根据它们的年龄值决定去向，年龄值达到年龄阀值（默认为15，新生代中的对象每熬过一轮垃圾回收，年龄值就加 1，GC 分代年龄存储在对象的 header中）的对象会被移到老年代中，没有达到阀值的对象会被复制到 To Survivor 区。接着清空Eden 区和 From Survivor 区，新生代中存活的对象都在 To Survivor 区。接着， From Survivor区和 To Survivor 区会交换它们的角色（复制算法减少碎片），也就是新的 To Survivor 区就是上次 GC 清空的 FromSurvivor 区，新的 From Survivor 区就是上次 GC 的 To Survivor 区，总之，不管怎样都会保证To Survivor 区在一轮 GC 后是空的。GC 时当 To Survivor 区没有足够的空间存放上一次新生代收集下来的存活对象时，需要依赖老年代进行分配担保，将这些对象存放在老年代中。 老年代（Old Generationn ） ​ 在新生代中经历了多次（具体看虚拟机配置的阀值）GC 后仍然存活下来的对象会进入老年代中。老年代中的对象生命周期较长，存活率比较高，在老年代中进行 GC 的频率相对而言较低，而且回收的速度也比较慢。 永久代（Permanent Generationn） ​ 永久代存储类信息、常量、静态变量、即时编译器编译后的代码等数据，对这一区域而言，Java 虚拟机规范指出可以不进行垃圾收集，一般而言不会进行垃圾回收。 化神后期（垃圾回收算法及分代垃圾） 常见 垃圾回收算法 引用计数 （Reference Counting ）：比较古老的回收算法。原理是此对象有一个引用，即增加一个计数，删除一个引用则减少一个计数。垃圾回收时，只用收集计数为 0 的对象。此算法最致命的是无法处理循环引用的问题。 复制（Copying）：此算法把内存空间划为两个相等的区域，每次只使用其中一个区域。垃圾回收时，遍历当前使用区域，把正在使用中的对象复制到另外一个区域中。此算法每次只处理正在使用中的对象，因此复制成本比较小，同时复制过去以后还能进行相应的内存整理，不会出现“碎片”问题。当然，此算法的缺点也是很明显的，就是需要两倍内存空间。简图如下： 标记- 清除（Mark-Sweep ）：最古老的算法，此算法执行分两阶段。第一阶段从引用根节点开始标记所有被引用的对象，第二阶段遍历整个堆，把未标记的对象清除。此算法需要暂停整个应用，同时，会产生内存碎片。简图如下： 标记- 整理（Mark-Compact ）：此算法结合了“标记-清除”和“复制”两个算法的优点。也是分两阶段，第一阶段从根节点开始标记所有被引用对象，第二阶段遍历整个堆，把清除未标记对象并且把存活对象“压缩”到堆的其中一块，按顺序排放。此算法避免了“标记-清除”的碎片问题，同时也避免了“复制”算法的空间问题。简图如下： 垃圾收集器的分类 次收集器：Scavenge GC，指发生在新生代的 GC，因为新生代的 Java 对象大多都是朝生夕死，所以Scavenge GC 非常频繁，一般回收速度也比较快。 当 Eden 空间不足以为对象分配内存时，会触发 Scavenge GC。 一般情况下，当新对象生成，并且在 Eden 申请空间失败时，就会触发 Scavenge GC，对Eden 区域进行 GC，清除非存活对象，并且把尚且存活的对象移动到 Survivor 区。然后整理Survivor 的两个区。这种方式的 GC 是对年轻代的 Eden 区进行，不会影响到年老代。因为大部分对象都是从 Eden 区开始的，同时 Eden 区不会分配的很大，所以 Eden 区的 GC 会频繁进行。因而，一般在这里需要使用速度快、效率高的算法，使 Eden 去能尽快空闲出来。 当年轻代堆空间紧张时会被触发 相对于全收集而言，收集间隔较短 全收集器：Full GC，指发生在老年代的 GC，出现了 Full GC 一般会伴随着至少一次的 Minor GC（老年代的对象大部分是 Scavenge GC 过程中从新生代进入老年代），比如：分配担保失败。FullGC 的速度一般会比 Scavenge GC 慢 10 倍以上。 当老年代内存不足或者显式调用 System.gc()方法时，会触发 Full GC。 当老年代或者持久代堆空间满了，会触发全收集操作。 可以使用 System.gc()方法来显式的启动全收集，全收集一般根据堆大小的不同，需要的时间不尽相同，但一般会比较长。 垃圾回收器的常规组合使用： Serial、ParNew、Parallel Scabenage构成新生代回收器。 Serial Old、Parallel Old、CMS是老年代回收器。 G1新老通用 分代垃圾收集器 串行收集器（Serial ）：JDK1.3之前JVM唯一一个次收集器（新生代收集器），1.5版本也是默认次收集器，它是串收集器。 Serial 收集器是 Hotspot 运行在 Client 模式下的默认新生代收集器。 它的特点是：只用一个 CPU（计算核心）/一条线程去完成 GC 工作, 且在进行垃圾收集时必须暂停其他所有的工作线程(“Stop The World” -后面简称 STW)。 可以使用-XX:+UseSerialGC 打开。虽然是单线程收集, 但它却简单而高效, 在 VM 管理内存不大的情况下(收集几十 M~一两百 M 的新生代), 停顿时间完全可以控制在几十毫秒~一百多毫秒内。 大多数收集器都是在串行收集器进行优化，减少他停顿的时间。 并行收集器（ParNew ）：ParNew 收集器其实是前面 Serial 的多线程版本,考虑用户等待的时间， 除使用多条线程进行 GC外, 包括 Serial可用的所有控制参数、收集算法、STW、对象分配规则、回收策略等都与 Serial 完全一样(也是VM启用 CMS 收集器-XX: +UseConcMarkSweepGC 的默认新生代收集器)。 由于存在线程切换的开销, ParNew 在单 CPU 的环境中比不上 Serial, 且在通过超线程技术实现的两个 CPU 的环境中也不能 100%保证能超越 Serial. 但随着可用的 CPU 数量的增加,收集效率肯定也会大大增加(ParNew 收集线程数与 CPU 的数量相同, 因此在 CPU 数量过大的环境中, 可用-XX:ParallelGCThreads=参数控制 GC 线程数，一般与CPU的线程数相同)。 Parallel Scavenge 收集器：与 ParNew 类似, Parallel Scavenge 也是使用复制算法, 也是并行多线程收集器. 但与其他收集器关注尽可能缩短垃圾收集时间不同, Parallel Scavenge 更关注系统吞吐量: 系统吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间) 停顿时间越短就越适用于用户交互的程序-良好的响应速度能提升用户的体验; 而高吞吐量则适用于后台运算而不需要太多交互的任务-可以最高效率地利用CPU时间,尽快地完成程序的运算任务. Parallel Scavenge 提供了如下参数设置系统吞吐量: Serial Old 收集器：Serial Old 是 Serial 收集器的老年代版本, 同样是单线程收集器,使用“标记-整理”算法 Parallel Old 收集器：Parallel Old 是 Parallel Scavenge 收集器的老年代版本, 使用多线程和“标记－整理”算法, 吞吐量优先, 主要与 Parallel Scavenge 配合在注重吞吐量及 CPU 资源敏感系统内使用； CMS 收集器 （Concurrent Mark Sweep ）：CMS(Concurrent Mark Sweep)收集器是一款具有划时代意义的收集器, 一款真正意义上的并发收集器, 虽然现在已经有了理论意义上表现更好的 G1 收集器, 但现在主流互联网企业线上选用的仍是 CMS(如 Taobao、微店)。 并发（concurrent）包含用户线程，并行（Parallel）不包含。 CMS是一种以获取最短回收停顿时间为目标的收集器(CMS又称多并发低暂停的收集器)，基于”标记-清除”算法实现， 整个 GC 过程分为以下 4 个步骤: 初始标记(CMS initial mark) 并发标记(CMS concurrent mark: GC Roots Tracing 过程) 重新标记(CMS remark) 并发清除(CMS concurrent sweep: 已死对象将会就地释放, 注意:此处没有压缩) 其中 1，3 两个步骤(初始标记、重新标记)仍需 STW. 但初始标记仅只标记一下 GC Roots能直接关联到的对象, 速度很快; 而重新标记则是为了修正并发标记期间因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录, 虽然一般比初始标记阶段稍长, 但要远小于并发标记时间。CMS 特点： CMS 默认启动的回收线程数=(CPU 数目+3)4，当 CPU 数&gt;4 时, GC线程一般占用不超过 25%的 CPU 资源, 但是当 CPU 数&lt;=4 时, GC线程可能就会过多的占用用户 CPU 资源, 从而导致应用程序变慢, 总吞吐量降低。 无法处理浮动垃圾, 可能出现 Promotion Failure、Concurrent Mode Failure 而导致另一次 Full GC 的产生: 浮动垃圾是指在 CMS 并发清理阶段用户线程运行而产生的新垃圾. 由于在 GC 阶段用户线程还需运行, 因此还需要预留足够的内存空间给用户线程使用, 导致 CMS不 能 像 其 他收 集 器那 样 等到 老 年 代几 乎 填满 了 再进 行 收 集. 因此 CMS 提 供 了-XX:CMSInitiatingOccupancyFraction 参 数 来 设 置 GC 的 触 发 百 分 比 ( 以 及-XX:+UseCMSInitiatingOccupancyOnly 来启用该触发百分比), 当老年代的使用空间超过该比例后 CMS 就会被触发(JDK 1.6 之后默认 92%). 但当 CMS 运行期间预留的内存无法满足程序需要, 就会出现上述 Promotion Failure 等失败, 这时 VM 将启动后备预案: 临时启用 Serial Old收集器来重新执行Full GC(CMS通常配合大内存使用, 一旦大内存转入串行的Serial GC, 那停顿的时间就是大家都不愿看到的了). 最后, 由于 CMS 采用”标记-清除”算法实现, 可能会产生大量内存碎片. 内存碎片过多 可 能 会 导 致 无 法 分 配 大 对 象 而 提 前 触 发 Full GC. 因 此 CMS 提 供 了-XX:+UseCMSCompactAtFullCollection 开关参数, 用于在 Full GC 后再执行一个碎片整理过程.但内存整理是无法并发的, 内存碎片问题虽然没有了, 但停顿时间也因此变长了, 因此 CMS还提供了另外一个参数-XX:CMSFullGCsBeforeCompaction 用于设置在执行 N 次不进行内存整理的 Full GC 后, 跟着来一次带整理的(默认为 0: 每次进入 Full GC 时都进行碎片整理). 分区收集- G1 收集器：G1(Garbage-First)是一款面向服务端应用的收集器, 主要目标用于配备多颗 CPU 的服务器治理大内存，-XX:+UseG1GC 启用 G1 收集器。 与其他基于分代的收集器不同, G1 将整个 Java 堆划分为多个大小相等的独立区域(Region), 虽然还保留有新生代和老年代的概念, 但新生代和老年代不再是物理隔离的了,它们都是一部分 Region(不需要连续)的集合.如： 每块区域既有可能属于 O 区、也有可能是 Y 区，因此不需要一次就对整个老年代/新生代回收。而是当线程并发寻找可回收的对象时，有些区块包含可回收的对象要比其他区块多很多。 虽然在清理这些区块时 G1 仍然需要暂停应用线程,，但可以用相对较少的时间优先回收垃圾较多的 Region。这种方式保证了 G1 可以在有限的时间内获取尽可能高的收集效率。 G1的新生代收集跟ParNew类似: 存活的对象被转移到一个或多个Survivor Regions.，如果存活时间达到阀值, 这部分对象就会被提升到老年代.如图： 其特定是： 一整块堆内存被分为多个 Regions. 存活对象被拷贝到新的 Survivor 区或老年代. 年轻代内存由一组不连续的 heap 区组成, 这种方法使得可以动态调整各代区域尺寸. Young GC 会有 STW 事件, 进行时所有应用程序线程都会被暂停. 多线程并发 GC. G1 老年代 GC 特点如下 : 并发标记阶段 在与应用程序并发执行的过程中会计算活跃度信息 . 这些活跃度信息标识出那些 regions 最适合在 STW 期间回收 (which regions will be bestto reclaim during an evacuation pause). 不像 CMS 有清理阶段 . 再次标记阶段 使用 Snapshot-at-the-Beginning(SATB) 算法比 CMS 快得多 . 空 region 直接被回收 . 拷贝 / 清理阶段 (Copying/Cleanup Phase) 年轻代与老年代同时回收 . 老年代内存回收会基于他的活跃度信息 化神圆满（JVM优化）JDK 常用 JVM 优化相关命令 jps jps - l：显示线程 id 和执行线程的主类名 jps -v：显示线程 id 和执行线程的主类名和 JVM 配置信息 jstat jstat -参数 线程 id 执行时间（单位毫秒） 执行次数 如：jstat -gc 4488 30 10 SXC - survivor 初始空间大小，单位字节。（X为survivor中X区域） SXU - survivor 使用空间大小， 单位字节。 EC - eden 初始空间大小 EU - eden 使用空间大小 OC - old 初始空间大小 OU - old 使用空间大小 PC - permanent 初始空间大小 PU - permanent 使用空间大小 YGC - youngGC 收集次数 YGCT - youngGC 收集使用时长， 单位秒 FGC - fullGC 收集次数 FGCT - fullGC 收集使用时长 GCT - 总计收集使用总时长 YGCT+FGCT jvisualvm：一个 JDK 内置的图形化 VM 监视管理工具。一般我们会在里面安装visualgc 插件。（工具、插件、可用插件），设置编辑url连接地址。 JVM 常见参数配置方式：java [options] MainClass [arguments]options - JVM 启动参数。 配置多个参数的时候，参数之间使用空格分隔。参数命名： 常见为 -参数名参数赋值： 常见为 -参数名=参数值 | -参数名:参数值 12345678910111213public class Test &#123; public static void main(String[] args) &#123; List&lt;GarbageCollectorMXBean&gt; l = ManagementFactory.getGarbageCollectorMXBeans(); for(GarbageCollectorMXBean b : l) &#123; System.out.println(b.getName()); // 收集器名称 &#125; try &#123; System.in.read(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 内存设置 -Xms:初始堆大小，JVM 启动的时候，给定堆空间大小。 -Xmx:最大堆大小，JVM 运行过程中，如果初始堆空间不足的时候，最大可以扩展到多少。 -Xmn：设置年轻代大小。整个堆大小=年轻代大小+年老代大小+持久代大小。持久代一般固定大小为 64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun 官方推荐配置为整个堆的 3/8。 -Xss： 设置每个线程的 Java 栈大小。JDK5.0 以后每个线程 Java 栈大小为 1M，以前每个线程堆栈大小为 256K。根据应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在 3000~5000 左右。 -XX:NewSize=n:设置年轻代大小 -XX:NewRatio=n:设置年轻代和年老代的比值。如:为 3，表示年轻代与年老代比值为 1：3，年轻代占整个年轻代+年老代和的 1/4 -XX:SurvivorRatio=n:年轻代中 Eden 区与两个 Survivor 区的比值。注意 Survivor 区有两个。如：3，表示 Eden：Survivor=3：2，一个 Survivor 区占整个年轻代的 1/5 -XX:MaxPermSize=n:设置持久代大小 -XX:MaxTenuringThreshold：设置垃圾最大年龄。如果设置为 0 的话，则年轻代对象不经过 Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在 Survivor 区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概率。 收集器设置 -XX:+UseSerialGC:设置串行收集器，年轻带收集器， 次收集器 -XX:+UseParallelGC:设置并行收集器 -XX:+UseParNewGC:设置年轻代为并行收集。可与 CMS 收集同时使用。JDK5.0 以上，JVM会根据系统配置自行设置，所以无需再设置此值。 -XX:+UseParallelOldGC:设置并行年老代收集器，JDK6.0 支持对年老代并行收集。 -XX:+UseConcMarkSweepGC:设置年老代并发收集器，测试中配置这个以后，-XX:NewRatio的配置失效，原因不明。所以，此时年轻代大小最好用-Xmn 设置。 -XX:+UseG1GC:设置 G1 收集器 垃圾回收统计信息，类似日志的配置信息。会有控制台相关信息输出。 商业项目上线的时候，使用 loggc -XX:+PrintGC -XX:+Printetails -XX:+PrintGCTimeStamps -Xloggc:filename 并行收集器设置 -XX:ParallelGCThreads=n:设置并行收集器收集时最大线程数使用的 CPU 数。并行收集线程数。 -XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间，单位毫秒。可以减少 STW 时间。 -XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为 1/(1+n)并发收集器设置 -XX:+CMSIncrementalMode:设置为增量模式。适用于单 CPU 情况。 -XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动选择年轻代区大小和相应的 Survivor 区比例，以达到目标系统规定的最低相应时间或者收集频率等，此值建议使用并行收集器时，一直打开。 -XX:CMSFullGCsBeforeCompaction=n：由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此值设置运行多少次 GC 以后对内存空间进行压缩、整理。 -XX:+UseCMSCompactAtFullCollection：打开对年老代的压缩。可能会影响性能，但是可以消除碎片 内存设置经验分享 JVM 中最大堆大小有三方面限制： 相关操作系统的数据模型（32-bt 还是 64-bit）限制； 系统的可用虚拟内存限制； 系统的可用物理内存限制。 32 位系统 下，一般限制在 1.5G~2G；64 为操作系统对内存无限制。 Tomcat 配置方式： 编写 catalina.bat|catalina.sh ，增加 JAVA_OPTS 参数设置。 windows和 linux 配置方式不同。 windows - set “JAVA_OPTS=%JAVA_OPTS% 自定义参数 “ ； linux -JAVA_OPTS=”$JAVA_OPTS 自定义参数 “常见设置： -Xmx3550m -Xms3550m -Xmn2g -Xss128k 适合开发过程的测试应用。要求物理内存大于4G。 设置JVM最大可用内存与初始内存相同，可以避免每次垃圾完成后JVM重新分配内存。 -Xmn2g，设置年轻代为2个g，持久代一般固定大小为64m，所以增大年轻代后，将会减小老年代大小。官方推荐设置成整个堆的3/8。 -Xss128k ，设置每个线程的堆栈大小，JDK1.5后每个线程栈大小为1M，以前每个线程栈大小为256k。在相同物理内存下，减少这个值能生成更多的线程，但是操作系统对进程内的线程数量是有限的，不能无线生成，经验值在3000~5000左右。 收集器设置经验分享 关于收集器的选择 JVM 给了三种选择：串行收集器、并行收集器、并发收集器，但是串行收集器只适用于小数据量的情况，所以这里的选择主要针对并行收集器和并发收集器。默认情况下，JDK5.0 以前都是使用串行收集器，如果想使用其他收集器需要在启动时加入相应参数。JDK5.0 以后，JVM 会根据当前系统配置进行判断。 常见配置： 并行收集器主要以到达一定的吞吐量为目标，适用于科学计算和后台处理等。 -Xmx3800m -Xms3800m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20 使用 ParallelGC 作为并行收集器， GC 线程为 20（CPU 核心数&gt;=20 时），内存问题根据硬件配置具体提供。建议使用物理内存的 80%左右作为 JVM 内存容量。 -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20-XX:+UseParallelOldGC 指定老年代收集器，在JDK5.0之后的版本，ParallelGC对应的全收集器就是ParallelOldGC。可以忽略 -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:MaxGCPauseMillis=100 指定 GC 时最大暂停时间。单位是毫秒。每次 GC 最长使用 100 毫秒。可以尽可能提高工作线程的执行资源。 -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:MaxGCPauseMillis=100-XX:+UseAdaptiveSizePolicy UseAdaptiveSizePolicy 是提高年轻代 GC 效率的配置。次收集器执行效率。 并发收集器主要是保证系统的响应时间，减少垃圾收集时的停顿时间。适用于应用服务器、电信领域、互联网领域等。 -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:ParallelGCThreads=20-XX:+UseConcMarkSweepGC -XX:+UseParNewGC 指定年轻代收集器为 ParNew，年老代收集器 ConcurrentMarkSweep，并发 GC 线程数为20（CPU 核心&gt;=20），并发 GC 的线程数建议使用（CPU 核心数+3）/4 或 CPU 核心数【不推荐使用】。 -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseConcMarkSweepGC-XX:CMSFullGCsBeforeCompaction=5 -XX:+UseCMSCompactAtFullCollection CMSFullGCsBeforeCompaction=5 执行 5 次 GC 后，运行一次内存的整理。 UseCMSCompactAtFullCollection 执行老年代内存整理。可以避免内存碎片，提高 GC 过程中的效率，减少停顿时间。 简单总结 年轻代大小选择 响应时间优先的应用：尽可能设大，直到接近系统的最低响应时间限制（根据实际情况选择）。在此种情况下，年轻代收集发生的频率也是最小的。同时，减少到达年老代的对象。 吞吐量优先的应用：尽可能的设置大，可能到达 Gbit 的程度。因为对响应时间没有要求，垃圾收集可以并行进行，一般适合 8CPU 以上的应用。 年老代大小选择 响应时间优先的应用： 年老代使用并发收集器，所以其大小需要小心设置，一般要考虑并发会话率和会话持续时间等一些参数。如果堆设置小了，可以会造成内存碎片、高回收频率以及应用暂停而使用传统的标记清除方式；如果堆大了，则需要较 长的收集时间。最优化的方案，一般需要参考以下数据获得： 并发垃圾收集信息 持久代并发收集次数 传统 GC 信息 花在年轻代和年老代回收上的时间比例 减少年轻代和年老代花费的时间，一般会提高应用的效率 吞吐量优先的应用：一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代。原因是，这样可以尽可能回收掉大部分短期对象，减少中期的对象，而年老代存放长期存活对象。 较小堆引起的碎片问题，因为年老代的并发收集器使用标记、清除算法，所以不会对堆进行压缩。当收集器回收时，他会把相邻的空间进行合并，这样可以分配给较大的对象。但是，当堆空间较小时，运行一段时间以后，就会出现“碎片”，如果并发收集器找不到足够的空间，那么并发收集器将会停止，然后使用传统的标记、整理方式进行回收。如果出现“碎片”，可能需要进行如下配置： -XX:+UseCMSCompactAtFullCollection：使用并发收集器时，开启对年老代的压缩。 -XX:CMSFullGCsBeforeCompaction=0：上面配置开启的情况下，这里设置多少次 Full GC后，对年老代进行压缩 小编是一枚Java Coder，业余写文章，现主营微信公众号《Java患者》，喜欢的话关注我的公众号或者加我微信我们一起学习Java 合体期（网络编程）合体前期（Socket）​ 首先注意，Socket不是Java中独有的概念，而是一个语言无关标准。任何可以实现网络编程的编程语言都有Socket。 什么是 Socket? ​ 网络上的两个程序通过一个双向的通信连接实现数据的交换，这个连接的一端称为一个socket。 ​ 建立网络通信连接至少要一个端口号。socket 本质是编程接口(API)，对 TCP/IP 的封装，TCP/IP 也要提供可供程序员做网络开发所用的接口，这就是 Socket 编程接口；HTTP 是轿车， 提供了封装或者显示数据的具体形式；Socket 是发动机，提供了网络通信的能力。 ​ Socket 的英文原义是“孔”或“插座”。作为 BSD UNIX 的进程通信机制，取后一种意思。通 常也称作“套接字“，用于描述 IP 地址和端口，是一个通信链的句柄，可以用来实现不同虚 拟机或不同计算机之间的通信。在 Internet 上的主机一般运行了多个服务软件，同时提供几 种服务。每种服务都打开一个 Socket，并绑定到一个端口上，不同的端口对应于不同的服务。Socket 正如其英文原义那样，像一个多孔插座。一台主机犹如布满各种插座的房间，每个插 座有一个编号，有的插座提供 220 伏交流电， 有的提供 110 伏交流电，有的则提供有线电 视节目。 客户软件将插头插到不同编号的插座，就可以得到不同的服务。 Socket 连接步骤 ​ 根据连接启动的方式以及本地套接字要连接的目标，套接字之间的连接过程可以分为三个步骤：服务器监听，客户端请求，连接确认。【如果包含数据交互+断开连接，那么一共是 五个步骤】 服务器监听：是服务器端套接字并不定位具体的客户端套接字，而是处于等待连接的状态，实时监控网络状态。 客户端请求：是指由客户端的套接字提出连接请求，要连接的目标是服务器端的套接字。为此，客户端的套接字必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址和端口号，然后就向服务器端套接字提出连接请求。 连接确认(三层握手)：是指当服务器端套接字监听到或者说接收到客户端套接字的连接请求【1】，它就响应客户端套接字的请求，建立一个新的线程，把服务器端套接字的描述发给客户端【2】，一旦客户端确认了此描述，连接就建立好了【3】。而服务器端套接字继续处于监听状态，继续接收其他客户端套接字的连接请求。 断开连接：客户端向服务发起一个请求关闭消息【1】，服务器根据自己状态，等到可以关闭时候，发一个可以关闭的消息给客户端【2】，并且服务器再向浏览器发一个关闭成功的消息【3】，客户端发一个光笔成功的消息，至于服务器可以收到不管【4】。 Java 中的 Socket ​ 在java.net包是网络编程的基础类库。其中ServerSocket和Socket是网络编程的基础类型ServerSocket是服务端应用类型。Socket是建立连接的类型。当连接建立成功后，服务器和客户端都会有一个 Socket对象示例，可以通过这个Socket 对象示例，完成会话的所有操作。 ​ 对于一个完整的网络连接来说，Socket是平等的，没有服务器客户端分级情况 什么是同步和异步 ​ 同步和异步是针对应用程序和内核OS的交互而言的，同步指的是用户进程触发 IO 操作并 等待或者轮询的去查看 IO 操作是否就绪，而异步是指用户进程触发 IO 操作以后便开始做自 己的事情，而当 IO 操作已经完成的时候会得到 IO 完成的通知，异步是OS底层支持的一种操作。以银行取款为例： 同步 ： 自己亲自出马持银行卡到银行取钱（使用同步 IO 时，Java 自己处理 IO 读写）； 异步 ： 委托一小弟拿银行卡到银行取钱，然后给你（使用异步 IO 时，Java 将 IO 读写 委托给 OS 处理，需要将数据缓冲区地址和大小传给 OS(银行卡和密码)，OS 需要支持异步 IO 操作 API） 什么是阻塞和非阻塞 ​ 阻塞和非阻塞是针对于进程在访问数据的时候，根据 IO 操作的就绪状态来采取的不同 方式，说白了是一种读取或者写入操作方法的实现方式，阻塞方式下读取或者写入函数将一直等待，而非阻塞方式下，读取或者写入方法会立即返回一个状态值。 以银行取款为例： 阻塞 ： ATM 排队取款，你只能等待（使用阻塞 IO 时，Java 调用会一直阻塞到读写完 成才返回）； 非阻塞 ： 柜台取款，取个号，然后坐在椅子上做其它事，等号广播会通知你办理，没到号你就不能去，你可以不断问大堂经理排到了没有，大堂经理如果说还没到你就不能去（使用非阻塞 IO 时，如果不能读写 Java 调用会马上返回，当 IO 事件分发器通知可读写时再继 续进行读写，不断循环直到读写完成） ajax是异步阻塞的。 合体中期（BIO、NIO 、AIO） BIO 编程 ：Blocking IO同步阻塞的编程方式。 BIO 编程方式通常是在 JDK1.4 版本之前常用的编程方式。编程实现过程为：首先在服务端启动一个 ServerSocket 来监听网络请求，客户端启动 Socket 发起网络请求，默认情况下 ServerSocket 回建立一个线程来处理此请求，如果服务端没有线程可用，客户端则会阻塞等待或遭到拒绝。 且建立好的连接，在通讯过程中，是同步的。在并发处理效率上比较低。大致结构如下： 每次请求都要创建一个server socket和一个thread 同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就 需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可 以通过线程池机制改善（有人把这种叫做伪异步，实际上不能实现任何的异步的操作，归根还是同步）。 BIO 方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4 以前的唯一选择，但程序直观简单易理解。 使用线程池机制改善后的 BIO 模型图如下: Client： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class Client &#123; public static void main(String[] args) &#123; String host = null; int port = 0; if(args.length &gt; 2)&#123; host = args[0]; port = Integer.parseInt(args[1]); &#125;else&#123; host = "127.0.0.1"; port = 9999; &#125; Socket socket = null; BufferedReader reader = null; PrintWriter writer = null; Scanner s = new Scanner(System.in); try&#123; socket = new Socket(host, port); String message = null; reader = new BufferedReader( new InputStreamReader(socket.getInputStream(), "UTF-8")); writer = new PrintWriter( socket.getOutputStream(), true); while(true)&#123; message = s.nextLine(); if(message.equals("exit"))&#123; break; &#125; writer.println(message); writer.flush(); System.out.println(reader.readLine()); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(socket != null)&#123; try &#123; socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; socket = null; if(reader != null)&#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; reader = null; if(writer != null)&#123; writer.close(); &#125; writer = null; &#125; &#125;&#125; server 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495public class Server &#123; public static void main(String[] args) &#123; int port = genPort(args); ServerSocket server = null; try&#123; server = new ServerSocket(port); System.out.println("server started!"); while(true)&#123; Socket socket = server.accept(); new Thread(new Handler(socket)).start(); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(server != null)&#123; try &#123; server.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; server = null; &#125; &#125; static class Handler implements Runnable&#123; Socket socket = null; public Handler(Socket socket)&#123; this.socket = socket; &#125; @Override public void run() &#123; BufferedReader reader = null; PrintWriter writer = null; try&#123; reader = new BufferedReader( new InputStreamReader(socket.getInputStream(), "UTF-8")); writer = new PrintWriter( new OutputStreamWriter(socket.getOutputStream(), "UTF-8")); String readMessage = null; while(true)&#123; System.out.println("server reading... "); if((readMessage = reader.readLine()) == null)&#123; break; &#125; System.out.println(readMessage); writer.println("server recive : " + readMessage); writer.flush(); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(socket != null)&#123; try &#123; socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; socket = null; if(reader != null)&#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; reader = null; if(writer != null)&#123; writer.close(); &#125; writer = null; &#125; &#125; &#125; private static int genPort(String[] args)&#123; if(args.length &gt; 0)&#123; try&#123; return Integer.parseInt(args[0]); &#125;catch(NumberFormatException e)&#123; return 9999; &#125; &#125;else&#123; return 9999; &#125; &#125;&#125; ThreadPool版的Server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596public class Server &#123; public static void main(String[] args) &#123; int port = genPort(args); ServerSocket server = null; ExecutorService service = Executors.newFixedThreadPool(50); try&#123; server = new ServerSocket(port); System.out.println("server started!"); while(true)&#123; Socket socket = server.accept(); service.execute(new Handler(socket)); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(server != null)&#123; try &#123; server.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; server = null; &#125; &#125; static class Handler implements Runnable&#123; Socket socket = null; public Handler(Socket socket)&#123; this.socket = socket; &#125; @Override public void run() &#123; BufferedReader reader = null; PrintWriter writer = null; try&#123; reader = new BufferedReader( new InputStreamReader(socket.getInputStream(), "UTF-8")); writer = new PrintWriter( new OutputStreamWriter(socket.getOutputStream(), "UTF-8")); String readMessage = null; while(true)&#123; System.out.println("server reading... "); if((readMessage = reader.readLine()) == null)&#123; break; &#125; System.out.println(readMessage); writer.println("server recive : " + readMessage); writer.flush(); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(socket != null)&#123; try &#123; socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; socket = null; if(reader != null)&#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; reader = null; if(writer != null)&#123; writer.close(); &#125; writer = null; &#125; &#125; &#125; private static int genPort(String[] args)&#123; if(args.length &gt; 0)&#123; try&#123; return Integer.parseInt(args[0]); &#125;catch(NumberFormatException e)&#123; return 9999; &#125; &#125;else&#123; return 9999; &#125; &#125;&#125; NIO 编程 Unblocking IO（New IO）： 同步非阻塞的编程方式。 NIO 本身是基于事件驱动思想来完成的，其主要想解决的是 BIO 的大并发问题，NIO 基于Reactor，当 socket 有流可读或可写入 socket 时，操作系统会相应的通知应用程序进行处理，应用再将流读取到缓冲区或写入操作系统。也就是说，这个时候，已经不是一个连接就要对应一个处理线程了，而是有效的请求，对应一个线程，当连接没有数据时，是没有工作线程来处理的（即一个线程对应多个有效请求）。 NIO 的最重要的地方是当一个连接创建后，不需要对应一个线程，这个连接会被注册到多路复用器上面，所以所有的连接只需要一个线程就可以搞定，当这个线程中的多路复用器进行轮询的时候，发现连接上有请求的话，才开启一个线程进行处理，也就是一个请求一个线程模式，并且面向缓存。 在 NIO 的处理方式中，当一个请求来的话，开启线程进行处理，可能会等待后端应用的资源(JDBC 连接等待)，其实这个线程就被阻塞了，当并发上来的话，还是会有 BIO 一样的问题。 同步非阻塞，服务器实现模式为一个请求一个通道，即客户端发送的连接请求都会注册 到多路复用器上，多路复用器轮询到连接有 I/O 请求时才启动一个线程进行处理。 NIO 方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局 限于应用中，编程复杂，JDK1.4 开始支持。 Buffer:ByteBuffer,CharBuffer,ShortBuffer,IntBuffer,LongBuffer,FloatBuffer,DoubleBuffer*。* Channel:SocketChannel,ServerSocketChannel*。* Selector:Selector,AbstractSelector SelectionKey:OP_READ,OP_WRITE,OP_CONNECT,OP_ACCEPT client 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class NIOClient &#123; public static void main(String[] args) &#123; // 远程地址创建 InetSocketAddress remote = new InetSocketAddress("localhost", 9999); SocketChannel channel = null; // 定义缓存。 ByteBuffer buffer = ByteBuffer.allocate(1024); try &#123; // 开启通道 channel = SocketChannel.open(); // 连接远程服务器。 channel.connect(remote); Scanner reader = new Scanner(System.in); while(true)&#123; System.out.print("put message for send to server &gt; "); String line = reader.nextLine(); if(line.equals("exit"))&#123; break; &#125; // 将控制台输入的数据写入到缓存。 buffer.put(line.getBytes("UTF-8")); // 重置缓存游标 buffer.flip(); // 将数据发送给服务器 channel.write(buffer); // 清空缓存数据。 buffer.clear(); // 读取服务器返回的数据 int readLength = channel.read(buffer); if(readLength == -1)&#123; break; &#125; // 重置缓存游标 buffer.flip(); byte[] datas = new byte[buffer.remaining()]; // 读取数据到字节数组。 buffer.get(datas); System.out.println("from server : " + new String(datas, "UTF-8")); // 清空缓存。 buffer.clear(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally&#123; if(null != channel)&#123; try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168public class NIOServer implements Runnable &#123; // 多路复用器， 选择器。 用于注册通道的。 private Selector selector; // 定义了两个缓存。分别用于读和写。 初始化空间大小单位为字节。 // Buffer1是不安全的，要想安全模仿BIO，独立定义为Handler对象 private ByteBuffer readBuffer = ByteBuffer.allocate(1024); private ByteBuffer writeBuffer = ByteBuffer.allocate(1024); public static void main(String[] args) &#123; new Thread(new NIOServer(9999)).start(); &#125; public NIOServer(int port) &#123; init(port); &#125; private void init(int port)&#123; try &#123; System.out.println("server starting at port " + port + " ..."); // 开启多路复用器 this.selector = Selector.open(); // 开启服务通道 ServerSocketChannel serverChannel = ServerSocketChannel.open(); // 非阻塞， 如果传递参数true，为阻塞模式。 serverChannel.configureBlocking(false); // 绑定端口 serverChannel.bind(new InetSocketAddress(port)); // 注册，并标记当前服务通道状态 /* * register(Selector, int) * int - 状态编码 * OP_ACCEPT ： 连接成功的标记位。 * OP_READ ： 可以读取数据的标记 * OP_WRITE ： 可以写入数据的标记 * OP_CONNECT ： 连接建立后的标记 */ serverChannel.register(this.selector, SelectionKey.OP_ACCEPT); System.out.println("server started."); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public void run()&#123; while(true)&#123; try &#123; // 阻塞方法，当至少一个通道被选中，此方法返回。 // 通道是否选择，由注册到多路复用器中的通道标记决定。 this.selector.select(); // 返回以选中的通道标记集合， 集合中保存的是通道的标记。相当于是通道的ID。 Iterator&lt;SelectionKey&gt; keys = this.selector.selectedKeys().iterator(); while(keys.hasNext())&#123; SelectionKey key = keys.next(); // 将本次要处理的通道从集合中删除，下次循环根据新的通道列表再次执行必要的业务逻辑 keys.remove(); // 通道是否有效 if(key.isValid())&#123; // 阻塞状态 try&#123; if(key.isAcceptable())&#123; accept(key); &#125; &#125;catch(CancelledKeyException cke)&#123; // 断开连接。 出现异常。 key.cancel(); &#125; // 可读状态 try&#123; if(key.isReadable())&#123; read(key); &#125; &#125;catch(CancelledKeyException cke)&#123; key.cancel(); &#125; // 可写状态 try&#123; if(key.isWritable())&#123; write(key); &#125; &#125;catch(CancelledKeyException cke)&#123; key.cancel(); &#125; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private void write(SelectionKey key)&#123; this.writeBuffer.clear(); SocketChannel channel = (SocketChannel)key.channel(); Scanner reader = new Scanner(System.in); try &#123; System.out.print("put message for send to client &gt; "); String line = reader.nextLine(); // 将控制台输入的字符串写入Buffer中。 写入的数据是一个字节数组。 writeBuffer.put(line.getBytes("UTF-8")); writeBuffer.flip(); channel.write(writeBuffer); channel.register(this.selector, SelectionKey.OP_READ); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private void read(SelectionKey key)&#123; try &#123; // 清空读缓存。 this.readBuffer.clear(); // 获取通道 SocketChannel channel = (SocketChannel)key.channel(); // 将通道中的数据读取到缓存中。通道中的数据，就是客户端发送给服务器的数据。 int readLength = channel.read(readBuffer); // 检查客户端是否写入数据。 if(readLength == -1)&#123; // 关闭通道 key.channel().close(); // 关闭连接 key.cancel(); return; &#125; /* * flip， NIO中最复杂的操作就是Buffer的控制。 * Buffer中有一个游标。游标信息在操作后不会归零，如果直接访问Buffer的话，数据有不一致的可能。 * flip是重置游标的方法。NIO编程中，flip方法是常用方法。 */ this.readBuffer.flip(); // 字节数组，保存具体数据的。 Buffer.remaining() -&gt; 是获取Buffer中有效数据长度的方法。 byte[] datas = new byte[readBuffer.remaining()]; // 是将Buffer中的有效数据保存到字节数组中。 readBuffer.get(datas); System.out.println("from " + channel.getRemoteAddress() + " client : " + new String(datas, "UTF-8")); // 注册通道， 标记为写操作。 channel.register(this.selector, SelectionKey.OP_WRITE); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); try &#123; key.channel().close(); key.cancel(); &#125; catch (IOException e1) &#123; e1.printStackTrace(); &#125; &#125; &#125; private void accept(SelectionKey key)&#123; try &#123; // 此通道为init方法中注册到Selector上的ServerSocketChannel ServerSocketChannel serverChannel = (ServerSocketChannel)key.channel(); // 阻塞方法，当客户端发起请求后返回。 此通道和客户端一一对应。 SocketChannel channel = serverChannel.accept(); channel.configureBlocking(false); // 设置对应客户端的通道标记状态，此通道为读取数据使用的。 channel.register(this.selector, SelectionKey.OP_READ); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; TestBuffer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * * Buffer的应用固定逻辑 * 写操作顺序 * 1. clear() * 2. put() -&gt; 写操作 * 3. flip() -&gt; 重置游标 * 4. SocketChannel.write(buffer); -&gt; 将缓存数据发送到网络的另一端 * 5. clear() * * 读操作顺序 * 1. clear() * 2. SocketChannel.read(buffer); -&gt; 从网络中读取数据 * 3. buffer.flip() -&gt; 重置游标 * 4. buffer.get() -&gt; 读取数据 * 5. buffer.clear() * */public class TestBuffer &#123; public static void main(String[] args) throws Exception &#123; ByteBuffer buffer = ByteBuffer.allocate(8); byte[] temp = new byte[]&#123;3,2,1&#125;; // 写入数据之前 ： java.nio.HeapByteBuffer[pos=0 lim=8 cap=8] // pos - 游标位置， lim - 限制数量， cap - 最大容量 System.out.println("写入数据之前 ： " + buffer); // 写入字节数组到缓存 buffer.put(temp); // 写入数据之后 ： java.nio.HeapByteBuffer[pos=3 lim=8 cap=8] // 游标为3， 限制为8， 容量为8，默认限制与容量一样大小 System.out.println("写入数据之后 ： " + buffer); // 重置游标 ， lim = pos ; pos = 0; buffer.flip();//把这行注释掉后，下面的循环就是5次了。 //在重置一次，pos为0，lim（可读写操作有效数据位数）为0，get会报错，也不让写。要写要clear // 重置游标之后 ： java.nio.HeapByteBuffer[pos=0 lim=3 cap=8] // 游标为0， 限制为3， System.out.println("重置游标之后 ： " + buffer); // 清空Buffer， pos = 0; lim = cap; // buffer.clear(); // get() -&gt; 获取当前游标指向的位置的数据。 // System.out.println(buffer.get()); // remaining是lim-pos /*for(int i = 0; i &lt; buffer.remaining(); i++)&#123; // get(int index) -&gt; 获取指定位置的数据。 int data = buffer.get(i); System.out.println(i + " - " + data); &#125;*/ &#125;&#125; AIO 编程 AsynchronousIO： 异步非阻塞的编程方式 与 NIO 不同，当进行读写操作时，只须直接调用 API 的 read 或 write 方法即可。这两种 方法均为异步的，对于读操作而言，当有流可读取时，操作系统会将可读的流传入 read 方 法的缓冲区，并通知应用程序；对于写操作而言，当操作系统将 write 方法传递的流写入完 毕时，操作系统主动通知应用程序。即可以理解为，read/write 方法都是异步的，完成后会 主动调用回调函数。 客户端向服务端发数据，首先是OS接收到，他会将数据写到buffer里，然后通知应用程序代码，数据已经准备好了，可以read拿走了。应用代码write时，也会不数据同步进入OS的Buffer里，通过反向通知告诉应用程序已经写完了。buffer数据会自动地反回给client。client与server交互借助OS实现异步操作。 在 JDK1.7 中，这部分内容被称作 NIO.2，主要在 java.nio.channels 包下增加了下面四个异步通道： AsynchronousSocketChannel AsynchronousServerSocketChannel AsynchronousFileChannel AsynchronousDatagramChannel 异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的 I/O 请求都是由 OS 先完成了再通知服务器应用去启动线程进行处理。 AIO 方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调 用 OS 参与并发操作，编程比较复杂，JDK7 开始支持。 server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class AIOServer &#123; // 线程池， 提高服务端效率。 private ExecutorService service; // 线程组 // private AsynchronousChannelGroup group; // 服务端通道， 针对服务器端定义的通道。 private AsynchronousServerSocketChannel serverChannel; public AIOServer(int port)&#123; init(9999); &#125; private void init(int port)&#123; try &#123; System.out.println("server starting at port : " + port + " ..."); // 定长线程池 service = Executors.newFixedThreadPool(4); /* 使用线程组 group = AsynchronousChannelGroup.withThreadPool(service); serverChannel = AsynchronousServerSocketChannel.open(group); */ // 开启服务端通道， 通过静态方法创建的。 serverChannel = AsynchronousServerSocketChannel.open(); // 绑定监听端口， 服务器启动成功，但是未监听请求。 serverChannel.bind(new InetSocketAddress(port)); System.out.println("server started."); // 开始监听 // accept(T attachment, CompletionHandler&lt;AsynchronousSocketChannel, ? super T&gt;) // AIO开发中，监听是一个类似递归的监听操作。每次监听到客户端请求后，都需要处理逻辑开启下一次的监听。 // 下一次的监听，需要服务器的资源继续支持。this传递到AIOServerHandler的completed方法中 serverChannel.accept(this, new AIOServerHandler()); try &#123; TimeUnit.SECONDS.sleep(Integer.MAX_VALUE); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; new AIOServer(9999); &#125; public ExecutorService getService() &#123; return service; &#125; public void setService(ExecutorService service) &#123; this.service = service; &#125; public AsynchronousServerSocketChannel getServerChannel() &#123; return serverChannel; &#125; public void setServerChannel(AsynchronousServerSocketChannel serverChannel) &#123; this.serverChannel = serverChannel; &#125;&#125; AIOServerHandler 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public class AIOServerHandler implements CompletionHandler&lt;AsynchronousSocketChannel, AIOServer&gt; &#123; /** * 业务处理逻辑， 当请求到来后，监听成功，应该做什么。 * 一定要实现的逻辑： 为下一次客户端请求开启监听。accept方法调用。 * result参数 ： 就是和客户端直接建立关联的通道。 * 无论BIO、NIO、AIO中，一旦连接建立，两端是平等的。 * result中有通道中的所有相关数据。如：OS操作系统准备好的读取数据缓存，或等待返回数据的缓存。 */ @Override public void completed(AsynchronousSocketChannel result, AIOServer attachment) &#123; // 处理下一次的客户端请求。类似递归逻辑。 attachment.getServerChannel().accept(attachment, this); doRead(result); &#125; /** * 异常处理逻辑， 当服务端代码出现异常的时候，做什么事情。 */ @Override public void failed(Throwable exc, AIOServer attachment) &#123; exc.printStackTrace(); &#125; /** * 真实项目中，服务器返回的结果应该是根据客户端的请求数据计算得到的。不是等待控制台输入的。 * @param result */ private void doWrite(AsynchronousSocketChannel result)&#123; try &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); System.out.print("enter message send to client &gt; "); Scanner s = new Scanner(System.in); String line = s.nextLine(); buffer.put(line.getBytes("UTF-8")); // 重点：必须复位，必须复位，必须复位 buffer.flip(); // write方法是一个异步操作。具体实现由OS实现。 可以增加get方法，实现阻塞，等待OS的写操作结束。 result.write(buffer); // result.write(buffer).get(); // 调用get代表服务端线程阻塞，等待写操作完成 &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125;/* catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125;*/ &#125; private void doRead(final AsynchronousSocketChannel channel)&#123; ByteBuffer buffer = ByteBuffer.allocate(1024); /* * 异步读操作， read(Buffer destination, A attachment, * CompletionHandler&lt;Integer, ? super A&gt; handler) * destination - 目的地， 是处理客户端传递数据的中转缓存。 可以不使用。 * attachment - 处理客户端传递数据的对象。 通常使用Buffer处理。 * handler - 处理逻辑 */ channel.read(buffer, buffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123; /** * 业务逻辑，读取客户端传输数据 * attachment - 在completed方法执行的时候，OS已经将客户端请求的数据写入到Buffer中了。 * 但是未复位（flip）。 使用前一定要复位。 */ @Override public void completed(Integer result, ByteBuffer attachment) &#123; try &#123; System.out.println(attachment.capacity()); // 复位 attachment.flip(); System.out.println("from client : " + new String(attachment.array(), "UTF-8")); doWrite(channel); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; exc.printStackTrace(); &#125; &#125;); &#125;&#125; client 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class AIOClient &#123; private AsynchronousSocketChannel channel; public AIOClient(String host, int port)&#123; init(host, port); &#125; private void init(String host, int port)&#123; try &#123; // 开启通道 channel = AsynchronousSocketChannel.open(); // 发起请求，建立连接。 channel.connect(new InetSocketAddress(host, port)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public void write(String line)&#123; try &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); buffer.put(line.getBytes("UTF-8")); buffer.flip(); channel.write(buffer); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; &#125; public void read()&#123; ByteBuffer buffer = ByteBuffer.allocate(1024); try &#123; // read方法是异步方法，OS实现的。get方法是一个阻塞方法，会等待OS处理结束后再返回，要不代码不等待，下面没数据打印出来。真实开发可以不加，其结果依靠OS自己去等待，拿到数据再返回通知， channel.read(buffer).get(); // channel.read(dst, attachment, handler); buffer.flip(); System.out.println("from server : " + new String(buffer.array(), "UTF-8")); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; &#125; public void doDestory()&#123; if(null != channel)&#123; try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; AIOClient client = new AIOClient("localhost", 9999); try&#123; System.out.print("enter message send to server &gt; "); Scanner s = new Scanner(System.in); String line = s.nextLine(); client.write(line); client.read(); &#125;finally&#123; client.doDestory(); &#125; &#125;&#125; 合体后期（Netty） 简介 ​ Netty 是由 JBOSS 提供的一个 java 开源框架。Netty 提供异步的、事件驱动的网络应用 程序框架和工具，用以快速开发高性能、高可靠性的网络服务器和客户端程序。 它是建立再NIO和AIO基础之上的。 ​ 也就是说，Netty 是一个基于 NIO 的客户、服务器端编程框架，使用 Netty 可以确保你 快速和简单的开发出一个网络应用，例如实现了某种协议的客户，服务端应用。Netty 相当 简化和流线化了网络应用的编程开发过程，例如，TCP 和 UDP 的 socket 服务开发。 ​ “快速”和“简单”并不用产生维护性或性能上的问题。Netty 是一个吸收了多种协议的实 现经验，这些协议包括 FTP,SMTP,HTTP，各种二进制，文本协议，并经过相当精心设计的项 目，最终，Netty* 成功的找到了一种方式，在保证易于开发的同时还保证了其应用的性能， 稳定性和伸缩性。 ​ Netty 从 4.x 版本开始，需要使用 JDK1.6 及以上版本提供基础支撑。 ​ 在设计上：针对多种传输类型的统一接口 - 阻塞和非阻塞；简单但更强大的线程模型； 真正的无连接的数据报套接字支持；链接逻辑支持复用； ​ 在性能上：比核心 Java API 更好的吞吐量，较低的延时；资源消耗更少，这个得益于 共享池和重用；减少内存拷贝 ​ 在健壮性上：消除由于慢，快，或重载连接产生的 OutOfMemoryError；消除经常发现 在 NIO 在高速网络中的应用中的不公平的读/写比 ​ 在安全上：完整的 SSL / TLS 和 StartTLS 的支持 ​ 且已得到大量商业应用的真实验证,如：Hadoop 项目的 Avro（RPC 框架）、Dubbo、Dubbox等 RPC 框架。 ​ Netty 的官网是：http://netty.io ​ 有 三 方 提 供 的 中 文 翻 译 Netty 用 户 手 册 （ 官 网 提 供 源 信 息 ）： http://ifeve.com/netty5-user-guide/ Netty 架构 线程模型（acceptor是一个监听线程） .jpg) 单线程模型 ​ 在 ServerBootstrap 调用方法 group 的时候，传递的参数是同一个线程组，且在构造线程 组的时候，构造参数为 1，这种开发方式，就是一个单线程模型。 个人机开发测试使用。不推荐。 ​ 在处理acceptor和runnable task的线程组合并成一个，并且只有一个线程。 123456// 初始化线程组,构建线程组的时候，如果不传递参数，则默认构建的线程组线程数是CPU核心数量。acceptorGroup = new NioEventLoopGroup(1);// 初始化服务的配置bootstrap = new ServerBootstrap();// 绑定线程组bootstrap.group(acceptorGroup, acceptorGroup); 多线程模型 ​ 在 ServerBootstrap 调用方法 group 的时候，传递的参数是两个不同的线程组。负责监听 的 acceptor 线程组，线程数为 1，也就是构造参数为 1。负责处理客户端任务的线程组，线 程数大于 1，也就是构造参数大于 1。这种开发方式，就是多线程模型。 ​ 长连接，且客户端数量较少，连接持续时间较长情况下使用。如：企业内部交流应用。 1234567// 初始化线程组,构建线程组的时候，如果不传递参数，则默认构建的线程组线程数是CPU核心数量。acceptorGroup = new NioEventLoopGroup(1);clientGroup = new NioEventLoopGroup(&gt;1);// 初始化服务的配置bootstrap = new ServerBootstrap();// 绑定线程组bootstrap.group(acceptorGroup, clientGroup); 主从多线程模型 ​ 在 ServerBootstrap 调用方法 group 的时候，传递的参数是两个不同的线程组。负责监听 的 acceptor 线程组，线程数大于 1，也就是构造参数大于 1。负责处理客户端任务的线程组， 线程数大于 1，也就是构造参数大于 1。这种开发方式，就是主从多线程模型。 ​ 长连接，客户端数量相对较多，连接持续时间比较长的情况下使用。如：对外提供服务 的相册服务器。 案例： 12345678910111213141516171819202122232425262728293031323334353637383940&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;5.0.0.Alpha2&lt;/version&gt; &lt;!-- &lt;version&gt;4.1.24.Final&lt;/version&gt; --&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-codec-http&lt;/artifactId&gt; &lt;version&gt;5.0.0.Alpha2&lt;/version&gt; &lt;!-- &lt;version&gt;4.1.24.Final&lt;/version&gt; --&gt; &lt;/dependency&gt; &lt;!-- 接收处理工具 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.marshalling&lt;/groupId&gt; &lt;artifactId&gt;jboss-marshalling-river&lt;/artifactId&gt; &lt;version&gt;1.4.11.Final&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 序列化处理工具 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.marshalling&lt;/groupId&gt; &lt;artifactId&gt;jboss-marshalling-serial&lt;/artifactId&gt; &lt;version&gt;1.4.11.Final&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 系统信息收集 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.hyperic.sigar&lt;/groupId&gt; &lt;artifactId&gt;com.springsource.org.hyperic.sigar&lt;/artifactId&gt; &lt;version&gt;1.6.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kaazing&lt;/groupId&gt; &lt;artifactId&gt;sigar.dist&lt;/artifactId&gt; &lt;version&gt;1.0.0.0&lt;/version&gt; &lt;classifier&gt;distribution&lt;/classifier&gt; &lt;type&gt;zip&lt;/type&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 工具类 GzipUtils 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class GzipUtils &#123; public static void main(String[] args) throws Exception &#123; FileInputStream fis = new FileInputStream("D:\\3\\1.jpg"); byte[] temp = new byte[fis.available()]; int length = fis.read(temp); System.out.println("长度 : " + length); byte[] zipArray = GzipUtils.zip(temp); System.out.println("压缩后的长度 : " + zipArray.length); byte[] unzipArray = GzipUtils.unzip(zipArray); System.out.println("解压缩后的长度 : " + unzipArray.length); FileOutputStream fos = new FileOutputStream("D:\\3\\101.jpg"); fos.write(unzipArray); fos.flush(); fos.close(); fis.close(); &#125; /** * 解压缩 * @param source 源数据。需要解压的数据。 * @return 解压后的数据。 恢复的数据。 * @throws Exception */ public static byte[] unzip(byte[] source) throws Exception&#123; ByteArrayOutputStream out = new ByteArrayOutputStream(); ByteArrayInputStream in = new ByteArrayInputStream(source); // JDK提供的。 专门用于压缩使用的流对象。可以处理字节数组数据。 GZIPInputStream zipIn = new GZIPInputStream(in); byte[] temp = new byte[256]; int length = 0; while((length = zipIn.read(temp, 0, temp.length)) != -1)&#123; out.write(temp, 0, length); &#125; // 将字节数组输出流中的数据，转换为一个字节数组。 byte[] target = out.toByteArray(); zipIn.close(); out.close(); return target; &#125; /** * 压缩 * @param source 源数据，需要压缩的数据 * @return 压缩后的数据。 * @throws Exception */ public static byte[] zip(byte[] source) throws Exception&#123; ByteArrayOutputStream out = new ByteArrayOutputStream(); // 输出流，JDK提供的，提供解压缩功能。 GZIPOutputStream zipOut = new GZIPOutputStream(out); // 将压缩信息写入到内存。 写入的过程会实现解压。 zipOut.write(source); // 结束。 zipOut.finish(); byte[] target = out.toByteArray(); zipOut.close(); return target; &#125;&#125; OSUtils 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265public class OSUtils &#123; public static void main(String[] args) &#123; try &#123; // System信息，从jvm获取 property(); System.out.println("----------------------------------"); // cpu信息 cpu(); System.out.println("----------------------------------"); // 内存信息 memory(); System.out.println("----------------------------------"); // 操作系统信息 os(); System.out.println("----------------------------------"); // 用户信息 who(); System.out.println("----------------------------------"); // 文件系统信息 file(); System.out.println("----------------------------------"); // 网络信息 net(); System.out.println("----------------------------------"); // 以太网信息 ethernet(); System.out.println("----------------------------------"); &#125; catch (Exception e1) &#123; e1.printStackTrace(); &#125; &#125; private static void property() throws UnknownHostException &#123; Runtime r = Runtime.getRuntime(); Properties props = System.getProperties(); InetAddress addr; addr = InetAddress.getLocalHost(); String ip = addr.getHostAddress(); Map&lt;String, String&gt; map = System.getenv(); String userName = map.get("USERNAME");// 获取用户名 String computerName = map.get("COMPUTERNAME");// 获取计算机名 String userDomain = map.get("USERDOMAIN");// 获取计算机域名 System.out.println("用户名: " + userName); System.out.println("计算机名: " + computerName); System.out.println("计算机域名: " + userDomain); System.out.println("本地ip地址: " + ip); System.out.println("本地主机名: " + addr.getHostName()); System.out.println("JVM可以使用的总内存: " + r.totalMemory()); System.out.println("JVM可以使用的剩余内存: " + r.freeMemory()); System.out.println("JVM可以使用的处理器个数: " + r.availableProcessors()); System.out.println("Java的运行环境版本： " + props.getProperty("java.version")); System.out.println("Java的运行环境供应商： " + props.getProperty("java.vendor")); System.out.println("Java供应商的URL： " + props.getProperty("java.vendor.url")); System.out.println("Java的安装路径： " + props.getProperty("java.home")); System.out.println("Java的虚拟机规范版本： " + props.getProperty("java.vm.specification.version")); System.out.println("Java的虚拟机规范供应商： " + props.getProperty("java.vm.specification.vendor")); System.out.println("Java的虚拟机规范名称： " + props.getProperty("java.vm.specification.name")); System.out.println("Java的虚拟机实现版本： " + props.getProperty("java.vm.version")); System.out.println("Java的虚拟机实现供应商： " + props.getProperty("java.vm.vendor")); System.out.println("Java的虚拟机实现名称： " + props.getProperty("java.vm.name")); System.out.println("Java运行时环境规范版本： " + props.getProperty("java.specification.version")); System.out.println("Java运行时环境规范供应商： " + props.getProperty("java.specification.vender")); System.out.println("Java运行时环境规范名称： " + props.getProperty("java.specification.name")); System.out.println("Java的类格式版本号： " + props.getProperty("java.class.version")); System.out.println("Java的类路径： " + props.getProperty("java.class.path")); System.out.println("加载库时搜索的路径列表： " + props.getProperty("java.library.path")); System.out.println("默认的临时文件路径： " + props.getProperty("java.io.tmpdir")); System.out.println("一个或多个扩展目录的路径： " + props.getProperty("java.ext.dirs")); System.out.println("操作系统的名称： " + props.getProperty("os.name")); System.out.println("操作系统的构架： " + props.getProperty("os.arch")); System.out.println("操作系统的版本： " + props.getProperty("os.version")); System.out.println("文件分隔符： " + props.getProperty("file.separator")); System.out.println("路径分隔符： " + props.getProperty("path.separator")); System.out.println("行分隔符： " + props.getProperty("line.separator")); System.out.println("用户的账户名称： " + props.getProperty("user.name")); System.out.println("用户的主目录： " + props.getProperty("user.home")); System.out.println("用户的当前工作目录： " + props.getProperty("user.dir")); &#125; private static void memory() throws SigarException &#123; Sigar sigar = new Sigar(); Mem mem = sigar.getMem(); // 内存总量 System.out.println("内存总量: " + mem.getTotal() / 1024L + "K av"); // 当前内存使用量 System.out.println("当前内存使用量: " + mem.getUsed() / 1024L + "K used"); // 当前内存剩余量 System.out.println("当前内存剩余量: " + mem.getFree() / 1024L + "K free"); Swap swap = sigar.getSwap(); // 交换区总量 System.out.println("交换区总量: " + swap.getTotal() / 1024L + "K av"); // 当前交换区使用量 System.out.println("当前交换区使用量: " + swap.getUsed() / 1024L + "K used"); // 当前交换区剩余量 System.out.println("当前交换区剩余量: " + swap.getFree() / 1024L + "K free"); &#125; private static void cpu() throws SigarException &#123; Sigar sigar = new Sigar(); CpuInfo infos[] = sigar.getCpuInfoList(); CpuPerc cpuList[] = null; cpuList = sigar.getCpuPercList(); for (int i = 0; i &lt; infos.length; i++) &#123;// 不管是单块CPU还是多CPU都适用 CpuInfo info = infos[i]; System.out.println("第" + (i + 1) + "块CPU信息"); System.out.println("CPU的总量MHz: " + info.getMhz());// CPU的总量MHz System.out.println("CPU生产商: " + info.getVendor());// 获得CPU的卖主，如：Intel System.out.println("CPU类别: " + info.getModel());// 获得CPU的类别，如：Celeron System.out.println("CPU缓存数量: " + info.getCacheSize());// 缓冲存储器数量 printCpuPerc(cpuList[i]); &#125; &#125; private static void printCpuPerc(CpuPerc cpu) &#123; System.out.println("CPU用户使用率: " + CpuPerc.format(cpu.getUser()));// 用户使用率 System.out.println("CPU系统使用率: " + CpuPerc.format(cpu.getSys()));// 系统使用率 System.out.println("CPU当前等待率: " + CpuPerc.format(cpu.getWait()));// 当前等待率 System.out.println("CPU当前错误率: " + CpuPerc.format(cpu.getNice()));// System.out.println("CPU当前空闲率: " + CpuPerc.format(cpu.getIdle()));// 当前空闲率 System.out.println("CPU总的使用率: " + CpuPerc.format(cpu.getCombined()));// 总的使用率 &#125; private static void os() &#123; OperatingSystem OS = OperatingSystem.getInstance(); // 操作系统内核类型如： 386、486、586等x86 System.out.println("操作系统: " + OS.getArch()); System.out.println("操作系统CpuEndian(): " + OS.getCpuEndian());// System.out.println("操作系统DataModel(): " + OS.getDataModel());// // 系统描述 System.out.println("操作系统的描述: " + OS.getDescription()); // 操作系统类型 // System.out.println("OS.getName(): " + OS.getName()); // System.out.println("OS.getPatchLevel(): " + OS.getPatchLevel());// // 操作系统的卖主 System.out.println("操作系统的卖主: " + OS.getVendor()); // 卖主名称 System.out.println("操作系统的卖主名: " + OS.getVendorCodeName()); // 操作系统名称 System.out.println("操作系统名称: " + OS.getVendorName()); // 操作系统卖主类型 System.out.println("操作系统卖主类型: " + OS.getVendorVersion()); // 操作系统的版本号 System.out.println("操作系统的版本号: " + OS.getVersion()); &#125; private static void who() throws SigarException &#123; Sigar sigar = new Sigar(); Who who[] = sigar.getWhoList(); if (who != null &amp;&amp; who.length &gt; 0) &#123; for (int i = 0; i &lt; who.length; i++) &#123; // System.out.println("当前系统进程表中的用户名" + String.valueOf(i)); Who _who = who[i]; System.out.println("用户控制台: " + _who.getDevice()); System.out.println("用户host: " + _who.getHost()); // System.out.println("getTime(): " + _who.getTime()); // 当前系统进程表中的用户名 System.out.println("当前系统进程表中的用户名: " + _who.getUser()); &#125; &#125; &#125; private static void file() throws Exception &#123; Sigar sigar = new Sigar(); FileSystem fslist[] = sigar.getFileSystemList(); try &#123; for (int i = 0; i &lt; fslist.length; i++) &#123; System.out.println("分区的盘符名称" + i); FileSystem fs = fslist[i]; // 分区的盘符名称 System.out.println("盘符名称: " + fs.getDevName()); // 分区的盘符名称 System.out.println("盘符路径: " + fs.getDirName()); System.out.println("盘符标志: " + fs.getFlags());// // 文件系统类型，比如 FAT32、NTFS System.out.println("盘符类型: " + fs.getSysTypeName()); // 文件系统类型名，比如本地硬盘、光驱、网络文件系统等 System.out.println("盘符类型名: " + fs.getTypeName()); // 文件系统类型 System.out.println("盘符文件系统类型: " + fs.getType()); FileSystemUsage usage = null; usage = sigar.getFileSystemUsage(fs.getDirName()); switch (fs.getType()) &#123; case 0: // TYPE_UNKNOWN ：未知 break; case 1: // TYPE_NONE break; case 2: // TYPE_LOCAL_DISK : 本地硬盘 // 文件系统总大小 System.out.println(fs.getDevName() + "总大小: " + usage.getTotal() + "KB"); // 文件系统剩余大小 System.out.println(fs.getDevName() + "剩余大小: " + usage.getFree() + "KB"); // 文件系统可用大小 System.out.println(fs.getDevName() + "可用大小: " + usage.getAvail() + "KB"); // 文件系统已经使用量 System.out.println(fs.getDevName() + "已经使用量: " + usage.getUsed() + "KB"); double usePercent = usage.getUsePercent() * 100D; // 文件系统资源的利用率 System.out.println(fs.getDevName() + "资源的利用率: " + usePercent + "%"); break; case 3:// TYPE_NETWORK ：网络 break; case 4:// TYPE_RAM_DISK ：闪存 break; case 5:// TYPE_CDROM ：光驱 break; case 6:// TYPE_SWAP ：页面交换 break; &#125; System.out.println(fs.getDevName() + "读出： " + usage.getDiskReads()); System.out.println(fs.getDevName() + "写入： " + usage.getDiskWrites()); &#125; &#125; catch (Exception e) &#123; // TODO: handle exception e.printStackTrace(); &#125; return; &#125; private static void net() throws Exception &#123; Sigar sigar = new Sigar(); String ifNames[] = sigar.getNetInterfaceList(); for (int i = 0; i &lt; ifNames.length; i++) &#123; String name = ifNames[i]; NetInterfaceConfig ifconfig = sigar.getNetInterfaceConfig(name); System.out.println("网络设备名: " + name);// 网络设备名 System.out.println("IP地址: " + ifconfig.getAddress());// IP地址 System.out.println("子网掩码: " + ifconfig.getNetmask());// 子网掩码 if ((ifconfig.getFlags() &amp; 1L) &lt;= 0L) &#123; System.out.println("!IFF_UP...skipping getNetInterfaceStat"); continue; &#125; NetInterfaceStat ifstat = sigar.getNetInterfaceStat(name); System.out.println(name + "接收的总包裹数:" + ifstat.getRxPackets());// 接收的总包裹数 System.out.println(name + "发送的总包裹数:" + ifstat.getTxPackets());// 发送的总包裹数 System.out.println(name + "接收到的总字节数:" + ifstat.getRxBytes());// 接收到的总字节数 System.out.println(name + "发送的总字节数:" + ifstat.getTxBytes());// 发送的总字节数 System.out.println(name + "接收到的错误包数:" + ifstat.getRxErrors());// 接收到的错误包数 System.out.println(name + "发送数据包时的错误数:" + ifstat.getTxErrors());// 发送数据包时的错误数 System.out.println(name + "接收时丢弃的包数:" + ifstat.getRxDropped());// 接收时丢弃的包数 System.out.println(name + "发送时丢弃的包数:" + ifstat.getTxDropped());// 发送时丢弃的包数 &#125; &#125; private static void ethernet() throws SigarException &#123; Sigar sigar = null; sigar = new Sigar(); String[] ifaces = sigar.getNetInterfaceList(); for (int i = 0; i &lt; ifaces.length; i++) &#123; NetInterfaceConfig cfg = sigar.getNetInterfaceConfig(ifaces[i]); if (NetFlags.LOOPBACK_ADDRESS.equals(cfg.getAddress()) || (cfg.getFlags() &amp; NetFlags.IFF_LOOPBACK) != 0 || NetFlags.NULL_HWADDR.equals(cfg.getHwaddr())) &#123; continue; &#125; System.out.println(cfg.getName() + "IP地址:" + cfg.getAddress());// IP地址 System.out.println(cfg.getName() + "网关广播地址:" + cfg.getBroadcast());// 网关广播地址 System.out.println(cfg.getName() + "网卡MAC地址:" + cfg.getHwaddr());// 网卡MAC地址 System.out.println(cfg.getName() + "子网掩码:" + cfg.getNetmask());// 子网掩码 System.out.println(cfg.getName() + "网卡描述信息:" + cfg.getDescription());// 网卡描述信息 System.out.println(cfg.getName() + "网卡类型" + cfg.getType());// &#125; &#125;&#125; SerializableFactory4Marshalling 123456789101112131415161718192021222324252627282930313233343536public class SerializableFactory4Marshalling &#123; /** * 创建Jboss Marshalling解码器MarshallingDecoder * @return MarshallingDecoder */ public static MarshallingDecoder buildMarshallingDecoder() &#123; //首先通过Marshalling工具类的精通方法获取Marshalling实例对象 参数serial标识创建的是java序列化工厂对象。 //jboss-marshalling-serial 包提供 final MarshallerFactory marshallerFactory = Marshalling.getProvidedMarshallerFactory("serial"); //创建了MarshallingConfiguration对象，配置了版本号为5 final MarshallingConfiguration configuration = new MarshallingConfiguration(); // 序列化版本。只要使用JDK5以上版本，version只能定义为5。 configuration.setVersion(5); //根据marshallerFactory和configuration创建provider UnmarshallerProvider provider = new DefaultUnmarshallerProvider(marshallerFactory, configuration); //构建Netty的MarshallingDecoder对象，俩个参数分别为provider和单个消息序列化后的最大长度 MarshallingDecoder decoder = new MarshallingDecoder(provider, 1024 * 1024 * 1); return decoder; &#125; /** * 创建Jboss Marshalling编码器MarshallingEncoder * @return MarshallingEncoder */ public static MarshallingEncoder buildMarshallingEncoder() &#123; final MarshallerFactory marshallerFactory = Marshalling.getProvidedMarshallerFactory("serial"); final MarshallingConfiguration configuration = new MarshallingConfiguration(); configuration.setVersion(5); MarshallerProvider provider = new DefaultMarshallerProvider(marshallerFactory, configuration); //构建Netty的MarshallingEncoder对象，MarshallingEncoder用于实现序列化接口的POJO对象序列化为二进制数组 MarshallingEncoder encoder = new MarshallingEncoder(provider); return encoder; &#125;&#125; HeatbeatMessage 1234567891011121314151617181920212223242526272829303132333435363738394041public class HeatbeatMessage implements Serializable &#123; private static final long serialVersionUID = 2827219147304706826L; private String ip; private Map&lt;String, Object&gt; cpuMsgMap; private Map&lt;String, Object&gt; memMsgMap; private Map&lt;String, Object&gt; fileSysMsgMap; @Override public String toString() &#123; return "HeatbeatMessage [\nip=" + ip + ", \ncpuMsgMap=" + cpuMsgMap + ", \nmemMsgMap=" + memMsgMap + ", \nfileSysMsgMap=" + fileSysMsgMap + "]"; &#125; public String getIp() &#123; return ip; &#125; public void setIp(String ip) &#123; this.ip = ip; &#125; public Map&lt;String, Object&gt; getCpuMsgMap() &#123; return cpuMsgMap; &#125; public void setCpuMsgMap(Map&lt;String, Object&gt; cpuMsgMap) &#123; this.cpuMsgMap = cpuMsgMap; &#125; public Map&lt;String, Object&gt; getMemMsgMap() &#123; return memMsgMap; &#125; public void setMemMsgMap(Map&lt;String, Object&gt; memMsgMap) &#123; this.memMsgMap = memMsgMap; &#125; public Map&lt;String, Object&gt; getFileSysMsgMap() &#123; return fileSysMsgMap; &#125; public void setFileSysMsgMap(Map&lt;String, Object&gt; fileSysMsgMap) &#123; this.fileSysMsgMap = fileSysMsgMap; &#125;&#125; RequestMessage 12345678910111213141516171819202122232425262728293031323334353637public class RequestMessage implements Serializable &#123; private static final long serialVersionUID = 7084843947860990140L; private Long id; private String message; private byte[] attachment; @Override public String toString() &#123; return "RequestMessage [id=" + id + ", message=" + message + "]"; &#125; public RequestMessage() &#123; super(); &#125; public RequestMessage(Long id, String message, byte[] attachment) &#123; super(); this.id = id; this.message = message; this.attachment = attachment; &#125; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125; public byte[] getAttachment() &#123; return attachment; &#125; public void setAttachment(byte[] attachment) &#123; this.attachment = attachment; &#125;&#125; ResponseMessage 1234567891011121314151617181920212223242526272829public class ResponseMessage implements Serializable &#123; private static final long serialVersionUID = -8134313953478922076L; private Long id; private String message; @Override public String toString() &#123; return "ResponseMessage [id=" + id + ", message=" + message + "]"; &#125; public ResponseMessage() &#123; super(); &#125; public ResponseMessage(Long id, String message) &#123; super(); this.id = id; this.message = message; &#125; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125;&#125; server端开发： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * 1. 双线程组 * 2. Bootstrap配置启动信息 * 3. 注册业务处理Handler * 4. 绑定服务监听端口并启动服务 */public class Server4HelloWorld &#123; // 监听线程组，监听客户端请求 private EventLoopGroup acceptorGroup = null; // 处理客户端相关操作线程组，负责处理与客户端的数据通讯 private EventLoopGroup clientGroup = null; // 服务端启动相关配置信息 private ServerBootstrap bootstrap = null; public Server4HelloWorld()&#123; init(); &#125; private void init()&#123; // 初始化线程组,构建线程组的时候，如果不传递参数，则默认构建的线程组线程数是CPU核心数量。 acceptorGroup = new NioEventLoopGroup(); //监听线程组 clientGroup = new NioEventLoopGroup(); //处理客户端线程组 // 初始化服务的配置 bootstrap = new ServerBootstrap(); // 绑定线程组 bootstrap.group(acceptorGroup, clientGroup); // 设定通讯模式为NIO， 同步非阻塞 bootstrap.channel(NioServerSocketChannel.class); // 设定缓冲区大小， 缓存区的单位是字节。 bootstrap.option(ChannelOption.SO_BACKLOG, 1024); // SO_SNDBUF发送缓冲区，SO_RCVBUF接收缓冲区，SO_KEEPALIVE开启心跳监测（保证连接有效） bootstrap.option(ChannelOption.SO_SNDBUF, 16*1024) .option(ChannelOption.SO_RCVBUF, 16*1024) .option(ChannelOption.SO_KEEPALIVE, true); &#125; /** * 监听处理逻辑。 * @param port 监听端口。 * @param acceptorHandlers 处理器， 如何处理客户端请求。 * @return * @throws InterruptedException */ public ChannelFuture doAccept(int port, final ChannelHandler... acceptorHandlers) throws InterruptedException&#123; /* * childHandler是服务的Bootstrap独有的方法。是用于提供处理对象的。 * 可以一次性增加若干个处理逻辑。是类似责任链模式的处理方式。 * 增加A，B两个处理逻辑，在处理客户端请求数据的时候，根据A-》B顺序依次处理。 * * ChannelInitializer - 用于提供处理器的一个模型对象。 * 其中定义了一个方法，initChannel方法。 * 方法是用于初始化处理逻辑责任链条的。 * 可以保证服务端的Bootstrap只初始化一次处理器，尽量提供处理逻辑的重用。 * 避免反复的创建处理器对象。节约资源开销。 */ bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(acceptorHandlers); &#125; &#125;); // bind方法 - 绑定监听端口的。ServerBootstrap可以绑定多个监听端口。 多次调用bind方法即可 // sync - 开始监听逻辑。 返回一个ChannelFuture。 返回结果代表的是监听成功后的一个对应的未来结果 // 可以使用ChannelFuture实现后续的服务器和客户端的交互。 ChannelFuture future = bootstrap.bind(port).sync(); return future; &#125; /** * shutdownGracefully - 方法是一个安全关闭的方法。可以保证不放弃任何一个已接收的客户端请求。 */ public void release()&#123; this.acceptorGroup.shutdownGracefully(); this.clientGroup.shutdownGracefully(); &#125; public static void main(String[] args)&#123; ChannelFuture future = null; Server4HelloWorld server = null; try&#123; server = new Server4HelloWorld(); future = server.doAccept(9999,new Server4HelloWorldHandler()); System.out.println("server started."); // 关闭连接的。 future.channel().closeFuture().sync(); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //回收线程组资源 if(null != server)&#123; server.release(); &#125; &#125; &#125;&#125; serverHandler 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * @Sharable注解 - * 代表当前Handler是一个可以分享的处理器。也就意味着，服务器注册此Handler后，可以分享给多个客户端同时使用。 * 如果不使用注解描述类型，则每次客户端请求时，必须为客户端重新创建一个新的Handler对象。 * 如果handler是一个Sharable的，一定避免定义可写的实例变量。修改会发生混乱 * bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new XxxHandler()); &#125; &#125;); */package com.sxt.netty.first;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelFutureListener;import io.netty.channel.ChannelHandler.Sharable;import io.netty.channel.socket.SocketChannel;import io.netty.channel.ChannelHandlerAdapter;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInitializer;@Sharablepublic class Server4HelloWorldHandler extends ChannelHandlerAdapter &#123; /** * 业务处理逻辑 * 用于处理读取数据请求的逻辑。 * ctx - 上下文对象。其中包含于客户端建立连接的所有资源。 如： 对应的Channel * msg - 读取到的数据。 默认类型是ByteBuf，是Netty自定义的。是对ByteBuffer的封装。 不需要考虑复位问题。 */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; // 获取读取的数据， 是一个缓冲。 ByteBuf readBuffer = (ByteBuf) msg; // 创建一个字节数组，用于保存缓存中的数据。 byte[] tempDatas = new byte[readBuffer.readableBytes()]; // 将缓存中的数据读取到字节数组中。 readBuffer.readBytes(tempDatas); String message = new String(tempDatas, "UTF-8"); System.out.println("from client : " + message); if("exit".equals(message))&#123; ctx.close(); return; &#125; String line = "server message to client!"; // 写操作自动释放缓存，避免内存溢出问题。 ctx.writeAndFlush(Unpooled.copiedBuffer(line.getBytes("UTF-8"))); // 注意，如果调用的是write方法。不会刷新缓存，缓存中的数据不会发送到客户端，必须再次调用flush方法才行。 // ctx.write(Unpooled.copiedBuffer(line.getBytes("UTF-8"))); // ctx.flush() &#125; /** * 异常处理逻辑， 当客户端异常退出的时候，也会运行。 * ChannelHandlerContext关闭，也代表当前与客户端连接的资源关闭。 */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("server exceptionCaught method run..."); // cause.printStackTrace(); ctx.close(); &#125;&#125; client 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * 因为客户端是请求的发起者，不需要监听。 * 只需要定义唯一的一个线程组即可。 */public class Client4HelloWorld &#123; // 处理请求和处理服务端响应的线程组 private EventLoopGroup group = null; // 客户端启动相关配置信息 private Bootstrap bootstrap = null; public Client4HelloWorld()&#123; init(); &#125; private void init()&#123; group = new NioEventLoopGroup(); bootstrap = new Bootstrap(); // 绑定线程组 bootstrap.group(group); // 设定通讯模式为NIO bootstrap.channel(NioSocketChannel.class); &#125; public ChannelFuture doRequest(String host, int port, final ChannelHandler... handlers) throws InterruptedException&#123; /* * 客户端的Bootstrap没有childHandler方法。只有handler方法。 * 方法含义等同ServerBootstrap中的childHandler * 在客户端必须绑定处理器，也就是必须调用handler方法。 * 服务器必须绑定处理器，必须调用childHandler方法。 */ this.bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(handlers); &#125; &#125;); // 建立连接。 ChannelFuture future = this.bootstrap.connect(host, port).sync(); return future; &#125; public void release()&#123; this.group.shutdownGracefully(); &#125; public static void main(String[] args) &#123; Client4HelloWorld client = null; ChannelFuture future = null; try&#123; client = new Client4HelloWorld(); future = client.doRequest("localhost", 9999, new Client4HelloWorldHandler()); Scanner s = null; while(true)&#123; s = new Scanner(System.in); System.out.print("enter message send to server (enter 'exit' for close client) &gt; "); String line = s.nextLine(); if("exit".equals(line))&#123; // addListener - 增加监听，当某条件满足的时候，触发监听器。 // ChannelFutureListener.CLOSE - 关闭监听器，代表ChannelFuture执行返回后，关闭连接。 future.channel().writeAndFlush(Unpooled.copiedBuffer(line.getBytes("UTF-8"))) .addListener(ChannelFutureListener.CLOSE); break; &#125; future.channel().writeAndFlush(Unpooled.copiedBuffer(line.getBytes("UTF-8"))); TimeUnit.SECONDS.sleep(1); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; if(null != client)&#123; client.release(); &#125; &#125; &#125;&#125; clientHandler 1234567891011121314151617181920212223242526272829303132333435363738public class Client4HelloWorldHandler extends ChannelHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; try&#123; ByteBuf readBuffer = (ByteBuf) msg; byte[] tempDatas = new byte[readBuffer.readableBytes()]; readBuffer.readBytes(tempDatas); System.out.println("from server : " + new String(tempDatas, "UTF-8")); &#125;finally&#123; // 用于释放缓存。避免内存溢出 ReferenceCountUtil.release(msg); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("client exceptionCaught method run..."); // cause.printStackTrace(); ctx.close(); &#125; /*@Override // 断开连接时执行 public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channelInactive method run..."); &#125; @Override // 连接通道建立成功时执行 public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channelActive method run..."); &#125; @Override // 每次读取完成时执行 public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channelReadComplete method run..."); &#125;*/&#125; 合体圆满（基础代码演示）拆包粘包问题解决 ： ​ netty 使用 tcp/ip 协议传输数据。而 tcp/ip 协议是类似水流一样的数据传输方式。多次 访问的时候有可能出现数据粘包的问题。（Netty是NIO的模型，是同步非阻塞的，一定执行read/write时候就继续向下执行了，让底层的代码给我们处理执行数据的准备，怎么去读写操作，如果我们客户端发起多个数据，read方法到底是读几条数据？不知道客户端发送过来到底是几套数据，每一条间到底有什么间隔）解决这种问题的方式如下： 定长数据流 ：客户端和服务器，提前协调好，每个消息长度固定。（如：长度 10）。如果客户端或服 务器写出的数据不足 10，则使用空白字符补足（如：使用空格）。 server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * 1. 双线程组 * 2. Bootstrap配置启动信息 * 3. 注册业务处理Handler * 4. 绑定服务监听端口并启动服务 */public class Server4FixedLength &#123; // 监听线程组，监听客户端请求 private EventLoopGroup acceptorGroup = null; // 处理客户端相关操作线程组，负责处理与客户端的数据通讯 private EventLoopGroup clientGroup = null; // 服务启动相关配置信息 private ServerBootstrap bootstrap = null; public Server4FixedLength()&#123; init(); &#125; private void init()&#123; acceptorGroup = new NioEventLoopGroup(); clientGroup = new NioEventLoopGroup(); bootstrap = new ServerBootstrap(); // 绑定线程组 bootstrap.group(acceptorGroup, clientGroup); // 设定通讯模式为NIO bootstrap.channel(NioServerSocketChannel.class); // 设定缓冲区大小 bootstrap.option(ChannelOption.SO_BACKLOG, 1024); // SO_SNDBUF发送缓冲区，SO_RCVBUF接收缓冲区，SO_KEEPALIVE开启心跳监测（保证连接有效） bootstrap.option(ChannelOption.SO_SNDBUF, 16*1024) .option(ChannelOption.SO_RCVBUF, 16*1024) .option(ChannelOption.SO_KEEPALIVE, true); &#125; public ChannelFuture doAccept(int port) throws InterruptedException&#123; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelHandler[] acceptorHandlers = new ChannelHandler[3]; // 定长Handler。通过构造参数设置消息长度（单位是字节）。发送的消息长度不足可以使用空格补全。 acceptorHandlers[0] = new FixedLengthFrameDecoder(3); // 字符串解码器Handler，会自动处理channelRead方法的msg参数，将ByteBuf类型的数据转换为字符串对象 acceptorHandlers[1] = new StringDecoder(Charset.forName("UTF-8")); acceptorHandlers[2] = new Server4FixedLengthHandler(); ch.pipeline().addLast(acceptorHandlers); &#125; &#125;); ChannelFuture future = bootstrap.bind(port).sync(); return future; &#125; public void release()&#123; this.acceptorGroup.shutdownGracefully(); this.clientGroup.shutdownGracefully(); &#125; public static void main(String[] args)&#123; ChannelFuture future = null; Server4FixedLength server = null; try&#123; server = new Server4FixedLength(); future = server.doAccept(9999); System.out.println("server started."); future.channel().closeFuture().sync(); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; if(null != server)&#123; server.release(); &#125; &#125; &#125;&#125; serverHandler 123456789101112131415161718192021public class Server4FixedLengthHandler extends ChannelHandlerAdapter &#123; // 业务处理逻辑 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String message = msg.toString(); System.out.println("from client : " + message.trim()); String line = "ok "; ctx.writeAndFlush(Unpooled.copiedBuffer(line.getBytes("UTF-8"))); &#125; // 异常处理逻辑 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("server exceptionCaught method run..."); // cause.printStackTrace(); ctx.close(); &#125;&#125; client 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788/** * 1. 单线程组 * 2. Bootstrap配置启动信息 * 3. 注册业务处理Handler * 4. connect连接服务，并发起请求 */public class Client4FixedLength &#123; // 处理请求和处理服务端响应的线程组 private EventLoopGroup group = null; // 服务启动相关配置信息 private Bootstrap bootstrap = null; public Client4FixedLength()&#123; init(); &#125; private void init()&#123; group = new NioEventLoopGroup(); bootstrap = new Bootstrap(); // 绑定线程组 bootstrap.group(group); // 设定通讯模式为NIO bootstrap.channel(NioSocketChannel.class); &#125; public ChannelFuture doRequest(String host, int port) throws InterruptedException&#123; this.bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelHandler[] handlers = new ChannelHandler[3]; handlers[0] = new FixedLengthFrameDecoder(3); // 字符串解码器Handler，会自动处理channelRead方法的msg参数，将ByteBuf类型的数据转换为字符串对象 handlers[1] = new StringDecoder(Charset.forName("UTF-8")); handlers[2] = new Client4FixedLengthHandler(); ch.pipeline().addLast(handlers); &#125; &#125;); ChannelFuture future = this.bootstrap.connect(host, port).sync(); return future; &#125; public void release()&#123; this.group.shutdownGracefully(); &#125; public static void main(String[] args) &#123; Client4FixedLength client = null; ChannelFuture future = null; try&#123; client = new Client4FixedLength(); future = client.doRequest("localhost", 9999); Scanner s = null; while(true)&#123; s = new Scanner(System.in); System.out.print("enter message send to server &gt; "); String line = s.nextLine(); byte[] bs = new byte[5]; byte[] temp = line.getBytes("UTF-8"); if(temp.length &lt;= 5)&#123; for(int i = 0; i &lt; temp.length; i++)&#123; bs[i] = temp[i]; &#125; &#125; future.channel().writeAndFlush(Unpooled.copiedBuffer(bs)); TimeUnit.SECONDS.sleep(1); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; if(null != client)&#123; client.release(); &#125; &#125; &#125;&#125; clientHandler 12345678910111213141516171819public class Client4FixedLengthHandler extends ChannelHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; try&#123; String message = msg.toString(); System.out.println("from server : " + message); &#125;finally&#123; // 用于释放缓存。避免内存溢出 ReferenceCountUtil.release(msg); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("client exceptionCaught method run..."); // cause.printStackTrace(); ctx.close(); &#125; 注意：此时客户端只要不能满足的数据是3的长度，要可以进行多次发送。服务端才能收到数据，并且是连起来的。当客户端发的是大3的长度是，如发送abc123def4，服务器会把它当成三条数据，4不会出来，客户端在发送2个长度数据，服务端才可以接收到。 ​ 中文问题，unicode中，一个汉字可能长度是2也可能是3。如果在上面代码中的客户端发送”中国”二个字就有意思了。服务端会接收到2次，一个字一次。 特殊结束符 ：客户端和服务器，协商定义一个特殊的分隔符号，分隔符号长度自定义。如：‘#’、‘$_$’、 ‘AA@’。在通讯的时候，只要没有发送分隔符号，则代表一条数据没有结束。 server 1234567891011121314151617181920public ChannelFuture doAccept(int port) throws InterruptedException&#123; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; // 数据分隔符, 定义的数据分隔符一定是一个ByteBuf类型的数据对象。 ByteBuf delimiter = Unpooled.copiedBuffer("$E$".getBytes()); ChannelHandler[] acceptorHandlers = new ChannelHandler[3]; // 处理固定结束标记符号的Handler。这个Handler没有@Sharable注解修饰， // 必须每次初始化通道时创建一个新对象 // 使用特殊符号分隔处理数据粘包问题，也要定义每个数据包最大长度。netty建议数据有最大长度。 acceptorHandlers[0] = new DelimiterBasedFrameDecoder(1024, delimiter); // 字符串解码器Handler，会自动处理channelRead方法的msg参数，将ByteBuf类型的数据转换为字符串对象 acceptorHandlers[1] = new StringDecoder(Charset.forName("UTF-8")); acceptorHandlers[2] = new Server4DelimiterHandler(); ch.pipeline().addLast(acceptorHandlers); &#125; &#125;); ChannelFuture future = bootstrap.bind(port).sync(); serverHandler 1234567// 业务处理逻辑@Overridepublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String message = msg.toString(); System.out.println("from client : " + message); //会放回三条字符串 String line = "server message $E$ test delimiter handler!! $E$ second message $E$"; ctx.writeAndFlush(Unpooled.copiedBuffer(line.getBytes("UTF-8")));&#125; client和server改动基本一样。此时如果客户端不输入分隔符，可以进行不断输入。 协议 ：相对最成熟的数据传递方式。有服务器的开发者提供一个固定格式的协议标准。客户端 和服务器发送数据和接受数据的时候，都依据协议制定和解析消息。 1协议格式：HEADcontent-length:xxxxHEADBODYxxxxxxBODY 12345678910111213public ChannelFuture doAccept(int port, final ChannelHandler... acceptorHandlers) throws InterruptedException&#123; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new StringDecoder(Charset.forName("UTF-8"))); ch.pipeline().addLast(acceptorHandlers); &#125; &#125;); ChannelFuture future = bootstrap.bind(port).sync(); return future;&#125; serverHandler 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Sharablepublic class Server4ProtocolHandler extends ChannelHandlerAdapter &#123; // 业务处理逻辑 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String message = msg.toString(); System.out.println("server receive protocol content : " + message); message = ProtocolParser.parse(message); if(null == message)&#123; System.out.println("error request from client"); return ; &#125; System.out.println("from client : " + message); String line = "server message"; line = ProtocolParser.transferTo(line); System.out.println("server send protocol content : " + line); ctx.writeAndFlush(Unpooled.copiedBuffer(line.getBytes("UTF-8"))); &#125; // 异常处理逻辑 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("server exceptionCaught method run..."); cause.printStackTrace(); ctx.close(); &#125; static class ProtocolParser&#123; public static String parse(String message)&#123; String[] temp = message.split("HEADBODY"); temp[0] = temp[0].substring(4); temp[1] = temp[1].substring(0, (temp[1].length()-4)); int length = Integer.parseInt(temp[0].substring(temp[0].indexOf(":")+1)); if(length != temp[1].length())&#123; return null; &#125; return temp[1]; &#125; public static String transferTo(String message)&#123; message = "HEADcontent-length:" + message.length() + "HEADBODY" + message + "BODY"; return message; &#125; &#125;&#125; 协议限定拆分的格式。解决粘包。 序列化对象 ：JBoss Marshalling 序列化 ，Java 是面向对象的开发语言。传递的数据如果是 Java 对象，应该是最方便且可靠。 1234567891011121314public ChannelFuture doAccept(int port, final ChannelHandler... acceptorHandlers) throws InterruptedException&#123; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingDecoder()); ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingEncoder()); ch.pipeline().addLast(acceptorHandlers); &#125; &#125;); ChannelFuture future = bootstrap.bind(port).sync(); return future;&#125; serverHandler 123456789101112131415161718192021222324252627@Sharablepublic class Server4SerializableHandler extends ChannelHandlerAdapter &#123; // 业务处理逻辑 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("from client : ClassName - " + msg.getClass().getName() + " ; message : " + msg.toString()); if(msg instanceof RequestMessage)&#123; RequestMessage request = (RequestMessage)msg; // 解压缩 // byte[] attachment = GzipUtils.unzip(request.getAttachment()); // System.out.println(new String(attachment)); &#125; ResponseMessage response = new ResponseMessage(0L, "test response"); ctx.writeAndFlush(response); &#125; // 异常处理逻辑 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("server exceptionCaught method run..."); cause.printStackTrace(); ctx.close(); &#125;&#125; client 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public ChannelFuture doRequest(String host, int port, final ChannelHandler... handlers) throws InterruptedException&#123; this.bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingDecoder()); ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingEncoder()); ch.pipeline().addLast(handlers); &#125; &#125;); ChannelFuture future = this.bootstrap.connect(host, port).sync(); return future;&#125;public void release()&#123; this.group.shutdownGracefully();&#125;public static void main(String[] args) &#123; Client4Serializable client = null; ChannelFuture future = null; try&#123; client = new Client4Serializable(); future = client.doRequest("localhost", 9999, new Client4SerializableHandler()); String attachment = "test attachment"; byte[] attBuf = attachment.getBytes(); // attBuf = GzipUtils.zip(attBuf); // RequestMessage msg = new RequestMessage(new Random().nextLong(), // "test", new byte[0]); // 压缩，有效减少网络中传递的数据 attBuf = GzipUtils.zip(attBuf); RequestMessage msg = new RequestMessage(new Random().nextLong(), "test", attBuf); future.channel().writeAndFlush(msg); TimeUnit.SECONDS.sleep(1); future.addListener(ChannelFutureListener.CLOSE); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; if(null != client)&#123; client.release(); &#125; &#125;&#125; clientHandler 12345678910111213141516public class Client4SerializableHandler extends ChannelHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("from server : ClassName - " + msg.getClass().getName() + " ; message : " + msg.toString()); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("client exceptionCaught method run..."); cause.printStackTrace(); ctx.close(); &#125;&#125; 定时断线重连 ：客户端断线重连机制。 客户端数量多，且需要传递的数据量级较大。可以周期性的发送数据的时候，使用。要 求对数据的即时性不高的时候，才可使用。 优点： 可以使用数据缓存。不是每条数据进行一次数据交互。可以定时回收资源，对 资源利用率高。相对来说，即时性可以通过其他方式保证。如： 120 秒自动断线。数据变 化 1000 次请求服务器一次。300 秒中自动发送不足 1000 次的变化数据（Timer）。 server 12345678910111213141516171819public ChannelFuture doAccept(int port) throws InterruptedException&#123; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingDecoder()); ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingEncoder()); // 定义一个定时断线处理器，当多长时间内，没有任何的可读取数据，自动断开连接。 // 没有@Sharable的 // 构造参数，就是间隔时长。 默认的单位是秒。 // 自定义间隔时长单位。 new ReadTimeoutHandler(long times, TimeUnit unit); ch.pipeline().addLast(new ReadTimeoutHandler(3)); ch.pipeline().addLast(new Server4TimerHandler()); &#125; &#125;); ChannelFuture future = bootstrap.bind(port).sync(); return future;&#125; client 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public void setHandlers() throws InterruptedException&#123; this.bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingDecoder()); ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingEncoder()); // 写操作自定断线。 在指定时间内，没有写操作，自动断线。 ch.pipeline().addLast(new WriteTimeoutHandler(3)); ch.pipeline().addLast(new Client4TimerHandler()); &#125; &#125;);&#125;public ChannelFuture getChannelFuture(String host, int port) throws InterruptedException&#123; // future是null，我们就要建立连接 if(future == null)&#123; future = this.bootstrap.connect(host, port).sync(); &#125; // 如果非空，看些连接未来状态中的通道是否有效。 // 如果future非空，但是有个channel，证明已经和服务器断开了，但是future没有被回收 // 重连操作 if(!future.channel().isActive())&#123; future = this.bootstrap.connect(host, port).sync(); &#125; return future;&#125;public void release()&#123; this.group.shutdownGracefully();&#125;public static void main(String[] args) &#123; Client4Timer client = null; ChannelFuture future = null; try&#123; client = new Client4Timer(); client.setHandlers(); future = client.getChannelFuture("localhost", 9999); // 循环3从。睡眠2秒，没有超出时长 for(int i = 0; i &lt; 3; i++)&#123; RequestMessage msg = new RequestMessage(new Random().nextLong(), "test"+i, new byte[0]); future.channel().writeAndFlush(msg); TimeUnit.SECONDS.sleep(2); &#125; // 有超出时长 TimeUnit.SECONDS.sleep(5); future = client.getChannelFuture("localhost", 9999); RequestMessage msg = new RequestMessage(new Random().nextLong(), "test", new byte[0]); future.channel().writeAndFlush(msg); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; if(null != client)&#123; client.release(); &#125; &#125;&#125; clientHandler 123456789101112131415161718192021222324public class Client4TimerHandler extends ChannelHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("from server : ClassName - " + msg.getClass().getName() + " ; message : " + msg.toString()); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("client exceptionCaught method run..."); cause.printStackTrace(); ctx.close(); &#125; /** * 当连接建立成功后，出发的代码逻辑。 * 在一次连接中只运行唯一一次。 * 通常用于实现连接确认和资源初始化的。 */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("client channel active"); &#125; server端运行图。有断线有异常， .jpg) .jpg) 心跳监测 ：使用定时发送消息的方式，实现硬件检测，达到心态检测的目的。 心跳监测是用于检测电脑硬件和软件信息的一种技术。如：CPU 使用率，磁盘使用率， 内存使用率，进程情况，线程情况等。 sigar ：需要下载一个 zip 压缩包。内部包含若干 sigar 需要的操作系统文件。sigar 插件是通过 JVM 访问操作系统，读取计算机硬件的一个插件库。读取计算机硬件过程中，必须由操作系统提供硬件信息。硬件信息是通过操作系统提供的。zip 压缩包中是 sigar 编写的操作系统文 件，如：windows 中的动态链接库文件。 解压需要的操作系统文件，将操作系统文件赋值到${Java_home}/bin 目录中。 server 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class Server4Heatbeat &#123; // 监听线程组，监听客户端请求 private EventLoopGroup acceptorGroup = null; // 处理客户端相关操作线程组，负责处理与客户端的数据通讯 private EventLoopGroup clientGroup = null; // 服务启动相关配置信息 private ServerBootstrap bootstrap = null; public Server4Heatbeat()&#123; init(); &#125; private void init()&#123; acceptorGroup = new NioEventLoopGroup(); clientGroup = new NioEventLoopGroup(); bootstrap = new ServerBootstrap(); // 绑定线程组 bootstrap.group(acceptorGroup, clientGroup); // 设定通讯模式为NIO bootstrap.channel(NioServerSocketChannel.class); // 设定缓冲区大小 bootstrap.option(ChannelOption.SO_BACKLOG, 1024); // SO_SNDBUF发送缓冲区，SO_RCVBUF接收缓冲区，SO_KEEPALIVE开启心跳监测（保证连接有效） bootstrap.option(ChannelOption.SO_SNDBUF, 16*1024) .option(ChannelOption.SO_RCVBUF, 16*1024) .option(ChannelOption.SO_KEEPALIVE, true); &#125; public ChannelFuture doAccept(int port) throws InterruptedException&#123; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingDecoder()); ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingEncoder()); ch.pipeline().addLast(new Server4HeatbeatHandler()); &#125; &#125;); ChannelFuture future = bootstrap.bind(port).sync(); return future; &#125; public void release()&#123; this.acceptorGroup.shutdownGracefully(); this.clientGroup.shutdownGracefully(); &#125; public static void main(String[] args)&#123; ChannelFuture future = null; Server4Heatbeat server = null; try&#123; server = new Server4Heatbeat(); future = server.doAccept(9999); System.out.println("server started."); future.channel().closeFuture().sync(); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; if(null != server)&#123; server.release(); &#125; &#125; &#125;&#125; serverHandler 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Sharablepublic class Server4HeatbeatHandler extends ChannelHandlerAdapter &#123; private static List&lt;String&gt; credentials = new ArrayList&lt;&gt;(); private static final String HEATBEAT_SUCCESS = "SERVER_RETURN_HEATBEAT_SUCCESS"; public Server4HeatbeatHandler()&#123; // 初始化客户端列表信息。一般通过配置文件读取或数据库读取。 credentials.add("192.168.199.222_WIN-QIUB2JF5TDP"); &#125; // 业务处理逻辑 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if(msg instanceof String)&#123; this.checkCredential(ctx, msg.toString()); &#125; else if (msg instanceof HeatbeatMessage)&#123; this.readHeatbeatMessage(ctx, msg); &#125; else &#123; // 服务器发送的信息是错误的，或者对外暴露的ip和端口号被与系统无关的人访问到了，自动断开连接 ctx.writeAndFlush("wrong message").addListener(ChannelFutureListener.CLOSE); &#125; &#125; private void readHeatbeatMessage(ChannelHandlerContext ctx, Object msg)&#123; HeatbeatMessage message = (HeatbeatMessage) msg; System.out.println(message); System.out.println("======================================="); ctx.writeAndFlush("receive heatbeat message"); &#125; /** * 身份检查。检查客户端身份是否有效。 * 客户端身份信息应该是通过数据库或数据文件定制的。 * 身份通过 - 返回确认消息。 * 身份无效 - 断开连接 * @param ctx * @param credential */ private void checkCredential(ChannelHandlerContext ctx, String credential)&#123; System.out.println(credential); System.out.println(credentials); if(credentials.contains(credential))&#123; ctx.writeAndFlush(HEATBEAT_SUCCESS); &#125;else&#123; ctx.writeAndFlush("no credential contains").addListener(ChannelFutureListener.CLOSE); &#125; &#125; // 异常处理逻辑 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("server exceptionCaught method run..."); // cause.printStackTrace(); ctx.close(); &#125;&#125; clientHandler 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public class Client4HeatbeatHandler extends ChannelHandlerAdapter &#123; private ScheduledExecutorService executorService = Executors.newScheduledThreadPool(1); private ScheduledFuture heatbeat; private InetAddress remoteAddr; private static final String HEATBEAT_SUCCESS = "SERVER_RETURN_HEATBEAT_SUCCESS"; //只运行一次 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; // 获取本地INET信息 this.remoteAddr = InetAddress.getLocalHost(); // 获取本地计算机名 String computerName = System.getenv().get("COMPUTERNAME"); String credentials = this.remoteAddr.getHostAddress() + "_" + computerName; System.out.println(credentials); // 发送到服务器，作为信息比对证书 ctx.writeAndFlush(credentials); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; try&#123; if(msg instanceof String)&#123; if(HEATBEAT_SUCCESS.equals(msg))&#123; this.heatbeat = this.executorService.scheduleWithFixedDelay(new HeatbeatTask(ctx), 0L, 2L, TimeUnit.SECONDS); System.out.println("client receive - " + msg); &#125;else&#123; System.out.println("client receive - " + msg); &#125; &#125; &#125;finally&#123; ReferenceCountUtil.release(msg); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("client exceptionCaught method run..."); // cause.printStackTrace(); // 回收资源 if(this.heatbeat != null)&#123; this.heatbeat.cancel(true); this.heatbeat = null; &#125; ctx.close(); &#125; class HeatbeatTask implements Runnable&#123; private ChannelHandlerContext ctx; public HeatbeatTask()&#123; &#125; public HeatbeatTask(ChannelHandlerContext ctx)&#123; this.ctx = ctx; &#125; public void run()&#123; try &#123; HeatbeatMessage msg = new HeatbeatMessage(); msg.setIp(remoteAddr.getHostAddress()); Sigar sigar = new Sigar(); // CPU信息 CpuPerc cpuPerc = sigar.getCpuPerc(); Map&lt;String, Object&gt; cpuMsgMap = new HashMap&lt;&gt;(); cpuMsgMap.put("Combined", cpuPerc.getCombined()); cpuMsgMap.put("User", cpuPerc.getUser()); cpuMsgMap.put("Sys", cpuPerc.getSys()); cpuMsgMap.put("Wait", cpuPerc.getWait()); cpuMsgMap.put("Idle", cpuPerc.getIdle()); // 内存信息 Map&lt;String, Object&gt; memMsgMap = new HashMap&lt;&gt;(); Mem mem = sigar.getMem(); memMsgMap.put("Total", mem.getTotal()); memMsgMap.put("Used", mem.getUsed()); memMsgMap.put("Free", mem.getFree()); // 文件系统 Map&lt;String, Object&gt; fileSysMsgMap = new HashMap&lt;&gt;(); FileSystem[] list = sigar.getFileSystemList(); fileSysMsgMap.put("FileSysCount", list.length); List&lt;String&gt; msgList = null; for(FileSystem fs : list)&#123; msgList = new ArrayList&lt;&gt;(); msgList.add(fs.getDevName() + "总大小: " + sigar.getFileSystemUsage(fs.getDirName()).getTotal() + "KB"); msgList.add(fs.getDevName() + "剩余大小: " + sigar.getFileSystemUsage(fs.getDirName()).getFree() + "KB"); fileSysMsgMap.put(fs.getDevName(), msgList); &#125; msg.setCpuMsgMap(cpuMsgMap); msg.setMemMsgMap(memMsgMap); msg.setFileSysMsgMap(fileSysMsgMap); ctx.writeAndFlush(msg); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; HTTP 协议处理 ：使用 Netty 服务开发。实现 HTTP 协议处理逻辑。 没有客户端，做一个http协议文件传输到netty服务器 server 12345678910111213141516171819202122232425262728293031323334353637383940/** * http协议文件传输 * @author Qixuan.Chen * 创建时间：2015年5月4日 */ public class HttpStaticFileServer &#123; private final int port;//端口 public HttpStaticFileServer(int port) &#123; this.port = port; &#125; public void run() throws Exception &#123; EventLoopGroup bossGroup = new NioEventLoopGroup();//线程一 //这个是用于serversocketchannel的event EventLoopGroup workerGroup = new NioEventLoopGroup();//线程二//这个是用于处理accept到的channel try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new HttpStaticFileServerInitializer()); b.bind(port).sync().channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) throws Exception &#123; int port = 8089; if (args.length &gt; 0) &#123; port = Integer.parseInt(args[0]); &#125; else &#123; port = 8089; &#125; new HttpStaticFileServer(port).run();//启动服务 &#125; &#125; 12345678910111213141516171819202122232425262728293031323334public class HttpStaticFileServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; // Create a default pipeline implementation. // 通道的连接点 ChannelPipeline pipeline = ch.pipeline(); // Uncomment the following line if you want HTTPS //SSLEngine engine = SecureChatSslContextFactory.getServerContext().createSSLEngine(); //engine.setUseClientMode(false); //pipeline.addLast("ssl", new SslHandler(engine)); /** * （1）ReadTimeoutHandler，用于控制读取数据的时候的超时，10表示如果10秒钟都没有数据读取了，那么就引发超时，然后关闭当前的channel （2）WriteTimeoutHandler，用于控制数据输出的时候的超时，构造参数1表示如果持续1秒钟都没有数据写了，那么就超时。 （3）HttpRequestrianDecoder，这个handler用于从读取的数据中将http报文信息解析出来，无非就是什么requestline，header，body什么的。。。 （4）然后HttpObjectAggregator则是用于将上卖解析出来的http报文的数据组装成为封装好的httprequest对象。。 （5）HttpresponseEncoder，用于将用户返回的httpresponse编码成为http报文格式的数据 （6）HttpHandler，自定义的handler，用于处理接收到的http请求。 */ pipeline.addLast("decoder", new HttpRequestDecoder());// http-request解码器,http服务器端对request解码 pipeline.addLast("aggregator", new HttpObjectAggregator(65536));//对传输文件大少进行限制 pipeline.addLast("encoder", new HttpResponseEncoder());//http-response解码器,http服务器端对response编码 // 向客户端发送数据的一个Handler pipeline.addLast("chunkedWriter", new ChunkedWriteHandler()); pipeline.addLast("handler", new HttpStaticFileServerHandler(true)); // Specify false if SSL.(如果是ssl,就指定为false) &#125; &#125; serverHandler 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310public class HttpStaticFileServerHandler extends SimpleChannelInboundHandler&lt;FullHttpRequest&gt; &#123; public static final String HTTP_DATE_FORMAT = "EEE, dd MMM yyyy HH:mm:ss zzz"; public static final String HTTP_DATE_GMT_TIMEZONE = "GMT"; public static final int HTTP_CACHE_SECONDS = 60; private final boolean useSendFile; public HttpStaticFileServerHandler(boolean useSendFile) &#123; this.useSendFile = useSendFile; &#125; /** * 类似channelRead方法。 */ @Override public void messageReceived( ChannelHandlerContext ctx, FullHttpRequest request) throws Exception &#123; // 请求头信息是否正确的 if (!request.decoderResult().isSuccess()) &#123; sendError(ctx, BAD_REQUEST); return; &#125; if (request.method() != GET) &#123; sendError(ctx, METHOD_NOT_ALLOWED); return; &#125; final String uri = request.uri(); System.out.println("-----uri----"+uri); final String path = sanitizeUri(uri); System.out.println("-----path----"+path); if (path == null) &#123; sendError(ctx, FORBIDDEN); return; &#125; File file = new File(path); if (file.isHidden() || !file.exists()) &#123; sendError(ctx, NOT_FOUND); return; &#125; if (file.isDirectory()) &#123; if (uri.endsWith("/")) &#123; sendListing(ctx, file); &#125; else &#123; sendRedirect(ctx, uri + '/'); &#125; return; &#125; if (!file.isFile()) &#123; sendError(ctx, FORBIDDEN); return; &#125; // Cache Validation String ifModifiedSince = (String) request.headers().get(IF_MODIFIED_SINCE); if (ifModifiedSince != null &amp;&amp; !ifModifiedSince.isEmpty()) &#123; SimpleDateFormat dateFormatter = new SimpleDateFormat(HTTP_DATE_FORMAT, Locale.US); Date ifModifiedSinceDate = dateFormatter.parse(ifModifiedSince); // Only compare up to the second because the datetime format we send to the client // does not have milliseconds long ifModifiedSinceDateSeconds = ifModifiedSinceDate.getTime() / 1000; long fileLastModifiedSeconds = file.lastModified() / 1000; if (ifModifiedSinceDateSeconds == fileLastModifiedSeconds) &#123; sendNotModified(ctx); return; &#125; &#125; // 文件下载 RandomAccessFile raf; try &#123; raf = new RandomAccessFile(file, "r"); &#125; catch (FileNotFoundException fnfe) &#123; sendError(ctx, NOT_FOUND); return; &#125; long fileLength = raf.length(); HttpResponse response = new DefaultHttpResponse(HTTP_1_1, OK); //setContentLength(response, fileLength); HttpHeaderUtil.setContentLength(response, fileLength); setContentTypeHeader(response, file); setDateAndCacheHeaders(response, file); if (HttpHeaderUtil.isKeepAlive(request)) &#123; response.headers().set(CONNECTION, HttpHeaderValues.KEEP_ALIVE); &#125; // Write the initial line and the header. ctx.write(response); // Write the content. ChannelFuture sendFileFuture; if (useSendFile) &#123; sendFileFuture = ctx.write(new DefaultFileRegion(raf.getChannel(), 0, fileLength), ctx.newProgressivePromise()); &#125; else &#123; sendFileFuture = ctx.write(new ChunkedFile(raf, 0, fileLength, 8192), ctx.newProgressivePromise()); &#125; sendFileFuture.addListener(new ChannelProgressiveFutureListener() &#123; @Override public void operationProgressed(ChannelProgressiveFuture future, long progress, long total) &#123; if (total &lt; 0) &#123; // total unknown System.err.println("Transfer progress: " + progress); &#125; else &#123; System.err.println("Transfer progress: " + progress + " / " + total); &#125; &#125; @Override public void operationComplete(ChannelProgressiveFuture future) throws Exception &#123; System.err.println("Transfer complete."); &#125; &#125;); // Write the end marker ChannelFuture lastContentFuture = ctx.writeAndFlush(LastHttpContent.EMPTY_LAST_CONTENT); // Decide whether to close the connection or not. if (!HttpHeaderUtil.isKeepAlive(request)) &#123; // Close the connection when the whole content is written out. lastContentFuture.addListener(ChannelFutureListener.CLOSE); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); if (ctx.channel().isActive()) &#123; sendError(ctx, INTERNAL_SERVER_ERROR); &#125; &#125; private static final Pattern INSECURE_URI = Pattern.compile(".*[&lt;&gt;&amp;\"].*"); /** * 路径解码 * @param uri * @return */ private static String sanitizeUri(String uri) &#123; // Decode the path. try &#123; uri = URLDecoder.decode(uri, "UTF-8"); &#125; catch (UnsupportedEncodingException e) &#123; try &#123; uri = URLDecoder.decode(uri, "ISO-8859-1"); &#125; catch (UnsupportedEncodingException e1) &#123; throw new Error(); &#125; &#125; if (!uri.startsWith("/")) &#123; return null; &#125; // Convert file separators. uri = uri.replace('/', File.separatorChar); // Simplistic dumb security check. // You will have to do something serious in the production environment. if (uri.contains(File.separator + '.') || uri.contains('.' + File.separator) || uri.startsWith(".") || uri.endsWith(".") || INSECURE_URI.matcher(uri).matches()) &#123; return null; &#125; // Convert to absolute path. return System.getProperty("user.dir") + File.separator + uri; &#125; private static final Pattern ALLOWED_FILE_NAME = Pattern.compile("[A-Za-z0-9][-_A-Za-z0-9\\.]*"); private static void sendListing(ChannelHandlerContext ctx, File dir) &#123; FullHttpResponse response = new DefaultFullHttpResponse(HTTP_1_1, OK); response.headers().set(CONTENT_TYPE, "text/html; charset=UTF-8"); StringBuilder buf = new StringBuilder(); String dirPath = dir.getPath(); buf.append("&lt;!DOCTYPE html&gt;\r\n"); buf.append("&lt;html&gt;&lt;head&gt;&lt;title&gt;"); buf.append("Listing of: "); buf.append(dirPath); buf.append("&lt;/title&gt;&lt;/head&gt;&lt;body&gt;\r\n"); buf.append("&lt;h3&gt;Listing of: "); buf.append(dirPath); buf.append("&lt;/h3&gt;\r\n"); buf.append("&lt;ul&gt;"); buf.append("&lt;li&gt;&lt;a href=\"../\"&gt;..&lt;/a&gt;&lt;/li&gt;\r\n"); for (File f: dir.listFiles()) &#123; if (f.isHidden() || !f.canRead()) &#123; continue; &#125; String name = f.getName(); if (!ALLOWED_FILE_NAME.matcher(name).matches()) &#123; continue; &#125; buf.append("&lt;li&gt;&lt;a href=\""); buf.append(name); buf.append("\"&gt;"); buf.append(name); buf.append("&lt;/a&gt;&lt;/li&gt;\r\n"); &#125; buf.append("&lt;/ul&gt;&lt;/body&gt;&lt;/html&gt;\r\n"); ByteBuf buffer = Unpooled.copiedBuffer(buf, CharsetUtil.UTF_8); response.content().writeBytes(buffer); buffer.release(); // Close the connection as soon as the error message is sent. ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); &#125; private static void sendRedirect(ChannelHandlerContext ctx, String newUri) &#123; FullHttpResponse response = new DefaultFullHttpResponse(HTTP_1_1, FOUND); response.headers().set(LOCATION, newUri); // Close the connection as soon as the error message is sent. ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); &#125; private static void sendError(ChannelHandlerContext ctx, HttpResponseStatus status) &#123; FullHttpResponse response = new DefaultFullHttpResponse( HTTP_1_1, status, Unpooled.copiedBuffer("Failure: " + status.toString() + "\r\n", CharsetUtil.UTF_8)); response.headers().set(CONTENT_TYPE, "text/plain; charset=UTF-8"); // Close the connection as soon as the error message is sent. ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); &#125; /** * When file timestamp is the same as what the browser is sending up, send a "304 Not Modified" * * @param ctx * Context */ private static void sendNotModified(ChannelHandlerContext ctx) &#123; FullHttpResponse response = new DefaultFullHttpResponse(HTTP_1_1, NOT_MODIFIED); setDateHeader(response); // Close the connection as soon as the error message is sent. ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); &#125; /** * Sets the Date header for the HTTP response * * @param response * HTTP response */ private static void setDateHeader(FullHttpResponse response) &#123; SimpleDateFormat dateFormatter = new SimpleDateFormat(HTTP_DATE_FORMAT, Locale.US); dateFormatter.setTimeZone(TimeZone.getTimeZone(HTTP_DATE_GMT_TIMEZONE)); Calendar time = new GregorianCalendar(); response.headers().set(DATE, dateFormatter.format(time.getTime())); &#125; /** * Sets the Date and Cache headers for the HTTP Response * * @param response * HTTP response * @param fileToCache * file to extract content type */ private static void setDateAndCacheHeaders(HttpResponse response, File fileToCache) &#123; SimpleDateFormat dateFormatter = new SimpleDateFormat(HTTP_DATE_FORMAT, Locale.US); dateFormatter.setTimeZone(TimeZone.getTimeZone(HTTP_DATE_GMT_TIMEZONE)); // Date header Calendar time = new GregorianCalendar(); response.headers().set(DATE, dateFormatter.format(time.getTime())); // Add cache headers time.add(Calendar.SECOND, HTTP_CACHE_SECONDS); response.headers().set(EXPIRES, dateFormatter.format(time.getTime())); response.headers().set(CACHE_CONTROL, "private, max-age=" + HTTP_CACHE_SECONDS); response.headers().set( LAST_MODIFIED, dateFormatter.format(new Date(fileToCache.lastModified()))); &#125; /** * Sets the content type header for the HTTP Response * * @param response * HTTP response * @param file * file to extract content type */ private static void setContentTypeHeader(HttpResponse response, File file) &#123; MimetypesFileTypeMap mimeTypesMap = new MimetypesFileTypeMap(); response.headers().set(CONTENT_TYPE, mimeTypesMap.getContentType(file.getPath())); &#125; &#125; 流数据的传输处理 ​ 在基于流的传输里比如 TCP/IP，接收到的数据会先被存储到一个 socket 接收缓冲里。不 幸的是，基于流的传输并不是一个数据包队列，而是一个字节队列。即使你发送了 2 个独立 的数据包，操作系统也不会作为 2 个消息处理而仅仅是作为一连串的字节而言。因此这是不 能保证你远程写入的数据就会准确地读取。所以一个接收方不管他是客户端还是服务端，都 应该把接收到的数据整理成一个或者多个更有意思并且能够让程序的业务逻辑更好理解的 数据。 在处理流数据粘包拆包时，可以使用下述处理方式： 使用定长数据处理，如：每个完整请求数据长度为 8 字节等。（FixedLengthFrameDecoder） 使用特殊分隔符的方式处理，如：每个完整请求数据末尾使用’\0’作为数据结束标记。（DelimiterBasedFrameDecoder） 使用自定义协议方式处理，如：http 协议格式等。 使用 POJO 来替代传递的流数据，如：每个完整的请求数据都是一个 RequestMessage 对象，在 Java 语言中，使用 POJO 更符合语种特性，推荐使用。 转载至微信公众号《Java患者》。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>并发和多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于fastjson解析后的属性排序问题解决方案]]></title>
    <url>%2F%E5%85%B3%E4%BA%8Efastjson%E8%A7%A3%E6%9E%90%E5%90%8E%E7%9A%84%E5%B1%9E%E6%80%A7%E6%8E%92%E5%BA%8F%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.html</url>
    <content type="text"><![CDATA[问题起因：123String testJson = "&#123;\"lh_u\":32,\"ps\":7,\"qt\":24,\"ns\":87,\"gm\":11,\"dp\":2,\"lh_ua\":34,\"ft\":82,\"jzsj\":\"24时\",\"lg\":71,\"yt\":6,\"jzrq\":\"3月4日\",\"ba\":62&#125;";JSONObject jsonObject = JSON.parseObject(jsonString); ----- ①System.out.println(jsonObject.toJSONString()); 上述方法得到的jsonObject属性排序和字符串的字段排序是一样的，如果想对jsonObject的属性进行排序，可以做此处理：12JSONObject jsonObjectNew1 = JSON.parseObject(JSON.toJSONString(jsonObject,SerializerFeature.MapSortField));JSONObject jsonObjectNew2 = JSON.parseObject(JSON.toJSONString(jsonObject,SerializerFeature.MapSortField),Feature.OrderedField); 将jsonObject转成按字母排序的字符串，然后再转成jsonObject对象，再次转成jsonObject时如果不加Feature.OrderedField，则jsonObject和①中的一样，加上则保持属性顺序不变。参考文章]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>fastjson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hashcode详解]]></title>
    <url>%2Fhashcode%E8%AF%A6%E8%A7%A3.html</url>
    <content type="text"><![CDATA[序言写这篇文章是因为在看hashMap源码时遇到有什么hashcode值，然后就去查，脑袋里面是有映像的，不就是在Object中有equals和hashcode方法嘛，这在学java基础的时候就遇到过，不过那时候无所谓，不懂就不懂，就一笔带过去了，然后到现在，又回过头来补本应该以前就搞清楚的问题，所以知道了一个道理，学习不是一个追求速度的事情，不懂就要去查清楚，弄明白，一步一个脚印，虽然刚开始可能会很慢，不过慢慢的，学习的多了，理解的多了，会越来越快的。越来越轻松，不至于现在还在补原来的知识。后悔也无事于补了，起码现在知道了这个道理，学习永远都不会迟，只要突然一天的醒悟，一切都会慢慢好起来的。 弄懂这知识点，查了很多博文，发现很多类似，我不知道哪个才是原著，所以复制一个我觉得比较好的，不过看了也有些迷惑，我会写下来。引用博文 一、hashcode是什么？1、hash和hash表是什么？ 想要知道这个hashcode，首先得知道hash，通过百度百科看一下 hash是一个函数，该函数中的实现就是一种算法，就是通过一系列的算法来得到一个hash值，这个时候，我们就需要知道另一个东西，hash表，通过hash算法得到的hash值就在这张hash表中，也就是说，hash表就是所有的hash值组成的，有很多种hash函数，也就代表着有很多种算法得到hash值，如上面截图的三种，等会我们就拿第一种来说。 2、hashcode 有了前面的基础，这里讲解就简单了，hashcode就是通过hash函数得来的，通俗的说，就是通过某一种算法得到的，hashcode就是在hash表中有对应的位置。每个对象都有hashcode，对象的hashcode怎么得来的呢？ 首先一个对象肯定有物理地址，在别的博文中会hashcode说成是代表对象的地址，这里肯定会让读者形成误区，对象的物理地址跟这个hashcode地址不一样，hashcode代表对象的地址说的是对象在hash表中的位置，物理地址说的对象存放在内存中的地址，那么对象如何得到hashcode呢？通过对象的内部地址(也就是物理地址)转换成一个整数，然后该整数通过hash函数的算法就得到了hashcode，所以，hashcode是什么呢？就是在hash表中对应的位置。这里如果还不是很清楚的话，举个例子，hash表中有 hashcode为1、hashcode为2、(…)3、4、5、6、7、8这样八个位置，有一个对象A，A的物理地址转换为一个整数17(这是假如)，就通过直接取余算法，17%8=1，那么A的hashcode就为1，且A就在hash表中1的位置。肯定会有其他疑问，接着看下面，这里只是举个例子来让你们知道什么是hashcode的意义。 二、hashcode有什么作用呢？前面说了这么多关于hash函数，和hashcode是怎么得来的，还有hashcode对应的是hash表中的位置，可能大家就有疑问，为什么hashcode不直接写物理地址呢，还要另外用一张hash表来代表对象的地址？接下来就告诉你hashcode的作用， 1、HashCode的存在主要是为了查找的快捷性，HashCode是用来在散列存储结构中确定对象的存储地址的(后半句说的用hashcode来代表对象就是在hash表中的位置) 为什么hashcode就查找的更快，比如：我们有一个能存放1000个数这样大的内存中，在其中要存放1000个不一样的数字，用最笨的方法，就是存一个数字，就遍历一遍，看有没有相同得数，当存了900个数字，开始存901个数字的时候，就需要跟900个数字进行对比，这样就很麻烦，很是消耗时间，用hashcode来记录对象的位置，来看一下。hash表中有1、2、3、4、5、6、7、8个位置，存第一个数，hashcode为1，该数就放在hash表中1的位置，存到100个数字，hash表中8个位置会有很多数字了，1中可能有20个数字，存101个数字时，他先查hashcode值对应的位置，假设为1，那么就有20个数字和他的hashcode相同，他只需要跟这20个数字相比较(equals)，如果每一个相同，那么就放在1这个位置，这样比较的次数就少了很多，实际上hash表中有很多位置，这里只是举例只有8个，所以比较的次数会让你觉得也挺多的，实际上，如果hash表很大，那么比较的次数就很少很少了。 通过对原始方法和使用hashcode方法进行对比，我们就知道了hashcode的作用，并且为什么要使用hashcode了 三、equals方法和hashcode的关系？通过前面这个例子，大概可以知道，先通过hashcode来比较，如果hashcode相等，那么就用equals方法来比较两个对象是否相等，用个例子说明：上面说的hash表中的8个位置，就好比8个桶，每个桶里能装很多的对象，对象A通过hash函数算法得到将它放到1号桶中，当然肯定有别的对象也会放到1号桶中，如果对象B也通过算法分到了1号桶，那么它如何识别桶中其他对象是否和它一样呢，这时候就需要equals方法来进行筛选了。1、如果两个对象equals相等，那么这两个对象的HashCode一定也相同 2、如果两个对象的HashCode相同，不代表两个对象就相同，只能说明这两个对象在散列存储结构中，存放于同一个位置这两条你们就能够理解了。 四、为什么equals方法重写的话，建议也一起重写hashcode方法？（如果对象的equals方法被重写，那么对象的HashCode方法也尽量重写）举个例子，其实就明白了这个道理，比如：有个A类重写了equals方法，但是没有重写hashCode方法，看输出结果，对象a1和对象a2使用equals方法相等，按照上面的hashcode的用法，那么他们两个的hashcode肯定相等，但是这里由于没重写hashcode方法，他们两个hashcode并不一样，所以，我们在重写了equals方法后，尽量也重写了hashcode方法，通过一定的算法，使他们在equals相等时，也会有相同的hashcode值。 实例：现在来看一下String的源码中的equals方法和hashcode方法。这个类就重写了这两个方法，现在为什么需要重写这两个方法了吧？equals方法：其实跟我上面写的那个例子是一样的原理，所以通过源码又知道了String的equals方法验证的是两个字符串的值是否一样。还有Double类也重写了这些方法。很多类有比较这类的，都重写了这两个方法，因为在所有类的父类Object中。equals的功能就是 “==”号的功能。你们还可以比较String对象的equals和==的区别啦。这里不再说明。 转载至]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[greenplum安装与部署（详细）]]></title>
    <url>%2Fgreenplum%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2%EF%BC%88%E8%AF%A6%E7%BB%86%EF%BC%89.html</url>
    <content type="text"><![CDATA[修改系统配置1.1 gp服务器列表192.168.7.11 hadoop01 主节点192.168.7.12 hadoop02 数据节点1192.168.7.13 hadoop03 数据节点2192.168.7.14 hadoop04 主节点切换备份节点 1.2 修改系统配置项关闭SELINUXvi /etc/selinux/configSELINUX=disabled 1.3 关闭防火墙systemctl status firewalldsystemctl stop firewalld.servicesystemctl disable firewalld.service 1.4 修改内核配置参数,并执行 sysctl -p使之生效123456789101112131415161718192021vi /etc/sysctl.confkernel.shmmax = 500000000kernel.shmmni = 4096kernel.shmall = 4000000000kernel.sem = 2000 4096000 2000 2048kernel.sysrq = 1kernel.core_uses_pid = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.msgmni = 2048net.ipv4.tcp_syncookies = 1net.ipv4.ip_forward = 0net.ipv4.conf.default.accept_source_route = 0net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_max_syn_backlog = 4096net.ipv4.conf.all.arp_filter = 1net.ipv4.ip_local_port_range = 1025 65535net.core.netdev_max_backlog = 10000net.core.rmem_max = 2097152net.core.wmem_max = 2097152vm.overcommit_memory = 2 (服务器核数，根据实际情况修改参数) 12345vi /etc/security/limits.conf* soft nofile 65536* hard nofile 65536* soft nproc 131072* hard nproc 131072 1.5 配置集群中各节点hosts信息12345vi /etc/hosts192.168.7.11 hadoop01 192.168.7.12 hadoop02 192.168.7.13 hadoop03 192.168.7.14 hadoop04 安装greenplum-db2.1 下载greenplum下载地址执行安装包：1rpm -ivh greenplum-db-6.1.0-rhel7-x86_64.rpm --nodeps --force 2.2 创建gpadmin用户解压完成后以root身份创建gpadmin用户和组，用来管理greenplum-db123456groupadd gpadmin # 创建分组 useradd gpadmin -g gpadmin # 创建用户并分配组 passwd gpadmin # 为gpadmin分配密码 cd /usr/local/greenplum-db chown -R gpadmin:gpadmin greenplum-db/ chown -R gpadmin:gpadmin greenplum-cc-web/ 2.3 创建配置文件切换到gpadmin用户下123su - gpadmin mkdir -p /usr/local/greenplum-db/gpconfigs cd /usr/local/greenplum-db/gpconfigs 创建配置文件12345678910vim hostfile_exkeys输入hadoop01hadoop02hadoop03hadoop04vi hostfilehadoop02hadoop03hadoop04 2.4 在gpadmin和root用户下添加环境变量12345678vi ~/.bashrc##添加以下内容 export LD_LIBRARY_PATH=$GPHOME/lib export MASTER_DATA_DIRECTORY=/home/gpadmin/gpdata/master/gpseg-1 source /usr/local/greenplum-db/greenplum_path.sh source /usr/local/greenplum-cc-web/gpcc_path.sh保存退出，执行下面语句使其生效source ~/.bashrc 2.5 切换到root用户123gpssh-exkeys -f /usr/local/greenplum-db/gpconfigs/hostfile_exkeys #拷贝mster节点公钥至各segment节点 gpseginstall -f /usr/local/greenplum-db/gpconfigs/hostfile_exkeys -p gpadmin 说明gpssh-exkeys -f hostfile_exkeys 将会在master节点生成公私钥，并拷贝至hostfile_exkeys各segment节点，实现后续无密钥登陆gpseginstall -f /usr/local/greenplum-db/gpconfigs/hostfile_exkeys -p gpadmin 使用默认用户名(gpadmin) 密码：gpadmin 在各segment节点安装Greenplum-db 2.6 在master及各segment节点创建数据存储目录1234567mkdir /home/gpadmin/gpdata/mastercd /home/gpadmin/chown -R gpadmin:gpadmin gpdata/mastergpssh -f /usr/local/greenplum-db/gpconfigs/hostfile -e "mkdir -p /home/gpadmin/gpdata/data1/primary;mkdir -p /home/gpadmin/gpdata/data2/primary”gpssh -f /usr/local/greenplum-db/gpconfigs/hostfile -e "mkdir -p /home/gpadmin/gpdata/data1/mirror;mkdir -p /home/gpadmin/gpdata/data2/mirror"gpssh -f /usr/local/greenplum-db/gpconfigs/hostfile -e "chown -R gpadmin:gpadmin /home/gpadmin/gpdata"gpssh -f /usr/local/greenplum-db/gpconfigs/hostfile_exkeys -v -e 'ntpd' 切换到gpadmin用户下,初始化数据库集群12345678910111213141516su - gpadmin cd /usr/local/greenplum-db/ cp /usr/local/greenplum-db/docs/cli_help/gpconfigs/gpinitsystem_config /usr/local/greenplum-db/gpconfigs/gpinitsystem_config vi /usr/local/greenplum-db/gpconfigs/gpinitsystem_config###修改以下内容MASTER_HOSTNAME=mdwPORT_BASE=40000declare -a DATA_DIRECTORY=(/home/gpadmin/gpdata/data1/primary /home/gpadmin/gpdata/data1/primary /home/gpadmin/gpdata/data2/primary /home/gpadmin/gpdata/data2/primary)MASTER_DIRECTORY=/home/gpadmin/gpdata/masterMASTER_PORT=5432MIRROR_PORT_BASE=50000REPLICATION_PORT_BASE=41000MIRROR_REPLICATION_PORT_BASE=51000declare -a MIRROR_DATA_DIRECTORY=(/home/gpadmin/gpdata/data1/mirror /home/gpadmin/gpdata/data1/mirror /home/gpadmin/gpdata/data2/mirror /home/gpadmin/gpdata/data2/mirror)DATABASE_NAME=gpdbMACHINE_LIST_FILE=/usr/local/greenplum-db/gpconfigs/hostfile 2.7 配置修改完成之后，执行以下命令初始化数据库1gpinitsystem -c /usr/local/greenplum-db/gpconfigs/gpinitsystem_config -h /usr/local/greenplum-db/gpconfigs/hostfile 2.8 psql修改数据库密码123456psql -d gpdb gpdb=# alter user gpadmin with password 'gpadmin' gpdb=# \q 然后，重启greenplumdb集群gpstop -agpstart -a 2.9 使用pgadmin，navicat 等工具连接连接时，如果登录不成功，一般报错如下样子： psql: FATAL: no pg_hba.conf entry for host “192.168.xxx.xxx”,表示访问权限不够修改文件/home/gpadmin/gpdata/master/gpseg-1/pg_hba.conf]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>greenplum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git配置多个SSH-Key]]></title>
    <url>%2Fgit%E9%85%8D%E7%BD%AE%E5%A4%9A%E4%B8%AASSH-Key.html</url>
    <content type="text"><![CDATA[1、生成一个公司用的SSH-Key1$ ssh-keygen -t rsa -C "youremail@yourcompany.com” -f ~/.ssh/id-rsa 在~/.ssh/目录会生成id-rsa和id-rsa.pub私钥和公钥。 我们将id-rsa.pub中的内容粘帖到公司gitlab服务器的SSH-key的配置中。 2、 生成一个github用的SSH-Key1$ ssh-keygen -t rsa -C "youremail@your.com” -f ~/.ssh/github-rsa 在~/.ssh/目录会生成github-rsa和github-rsa.pub私钥和公钥。 我们将github-rsa.pub中的内容粘帖到github服务器的SSH-key的配置中。 3、添加私钥1$ ssh-add ~/.ssh/id_rsa $ ssh-add ~/.ssh/github_rsa 如果执行ssh-add时提示”Could not open a connection to your authentication agent”，可以现执行命令：1$ ssh-agent bash 然后再运行ssh-add命令。可以通过 ssh-add -l 来确私钥列表1$ ssh-add -l 可以通过 ssh-add -D 来清空私钥列表1$ ssh-add -D 4、修改配置文件在 ~/.ssh 目录下新建一个config文件1touch config 添加内容：12345678910# gitlabHost gitlab.com HostName gitlab.com PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa# githubHost github.com HostName github.com PreferredAuthentications publickey IdentityFile ~/.ssh/github_rsa 5、测试1$ ssh -T git@github.com 输出Hi stefzhlg! You’ve successfully authenticated, but GitHub does not provide shell access.]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows添加公钥后，无法克隆的问题解决]]></title>
    <url>%2FPermission.html</url>
    <content type="text"><![CDATA[背景：windows下添加公钥后，无法克隆github 上的文件，报错Permission denied (publickey). fatal: Could not read from remote repository. 博主在github上下载tiny face的的源代码的时候，遇到git clone命令为：git clone –recursive git@github.com:peiyunh/tiny.git 而当我在ternimal下执行这条语句的时候，出现错误： Permissiondenied (publickey). fatal:Could not read from remote repository. Pleasemake sure you have the correct access rights and the repository exists. 但是其实执行命令：git clone git@github.com:peiyunh/tiny.git 是没有问题的（不加–recursive参数），于是百度了一番，我理解的是原因是由于你在本地（或者服务器上）没有生成ssh key，你可以在ternimal下执行： cd ~/.ssh ls来查看是否有文件id_rsa以及文件id_rsa.pub，如下图所示：（我的已经生成了，所以我ls后会显示。） 下面记录下解决办法： 1.首先，如果你没有ssh key的话，在ternimal下输入命令：ssh-keygen -t rsa -C “youremail@example.com”， youremail@example.com改为自己的邮箱即可，途中会让你输入密码啥的，不需要管，一路回车即可，会生成你的ssh key。（如果重新生成的话会覆盖之前的ssh key。） 2.然后再ternimal下执行命令： ssh -v git@github.com 最后两句会出现： No more authentication methods to try. Permission denied (publickey). 3.这时候再在ternimal下输入： ssh-agent -s 然后会提示类似的信息： SSH_AUTH_SOCK=/tmp/ssh-GTpABX1a05qH/agent.404; export SSH_AUTH_SOCK; SSH_AGENT_PID=13144; export SSH_AGENT_PID; echo Agent pid 13144; 4.接着再输入： ssh-add ~/.ssh/id_rsa 这时候应该会提示： Identity added: …（这里是一些ssh key文件路径的信息） （注意）如果出现错误提示： Could not open a connection to your authentication agent. 请执行命令：eval ssh-agent -s后继续执行命令 ssh-add ~/.ssh/id_rsa，这时候一般没问题啦。 5.打开你刚刚生成的id_rsa.pub，将里面的内容复制，进入你的github账号，在settings下，SSH and GPG keys下new SSH key，title随便取一个名字，然后将id_rsa.pub里的内容复制到Key中，完成后Add SSH Key。 6.最后一步，验证Key 在ternimal下输入命令： ssh -T git@github.com 提示：Hi xxx! You’ve successfully authenticated, but GitHub does not provide shell access. 这时候你的问题就解决啦，可以使用命令 git clone –recursive git@github.com:peiyunh/tiny.git 去下载你的代码啦。]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka基本操作]]></title>
    <url>%2Fkafka%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C.html</url>
    <content type="text"><![CDATA[kafka基本操作：查看topic主题：kafka-topics.sh –list –zookeeper node1:2181,node2:2181,node3:2181 查看分区：kafka-topics.sh –zookeeper node2:2181,node3:2181,node4:2181 –describe –topic MotorVehiclekafka-topics.sh –zookeeper node2:2181,node3:2181,node4:2181 –describe –topic WifiRecordkafka-topics.sh –zookeeper node2:2181,node3:2181,node4:2181 –describe –topic ImsiRecord 创建topic主题：kafka-topics.sh –create –zookeeper node1:2181,node2:2181,node3:2181 –replication-factor 1 –partitions 1 –topic MotorImsi 删除主题：kafka-topics.sh –delete –zookeeper node1:2181,node2:2181,node3:2181 –topic Test004 开启生产者：（注意kafka所在节点）目前都是单节点kafka-console-producer.sh –broker-list node3:6667 –topic MotorWifi 开启消费者：kafka-console-consumer.sh –bootstrap-server node1:6667,node2:6667,node3:6667 –topic Test003 –from-beginning kafka中默认消息的保留时间是7天，若想更改，需在配置文件server.properties里更改选项：log.retention.hours=168但是有的时候我们需要对某一个主题的消息存留的时间进行变更，而不影响其他主题。 可以使用命令：kafka-configs.sh –zookeeper localhost:2181 –entity-type topics –entity-name topicName –alter –add-config log.retention.hours=120使得主题的留存时间保存为5天 如果报错的话，可以将时间单位更改成毫秒：kafka-configs.sh –zookeeper localhost:2181 –entity-type topics –entity-name test –alter –add-config retention.ms=43200000 将jar包放入后台运行：nohup java -jar xinyi_kafka_consumer-0.0.1-SNAPSHOT-imsi.jar &gt;&gt; xinyi_kafka_consumer-0.0.1-SNAPSHOT-imsi.out 2&gt;&amp;1 &amp; nohup java -jar xinfo-spark-scheduler.jar &gt;&gt; xinfo-spark-scheduler.out 2&gt;&amp;1 &amp; nohup java -jar xinyi_kafka_consumer_video-0.0.1-SNAPSHOT.jar &gt;&gt; xinyi_kafka_consumer_video-0.0.1-SNAPSHOT.out 2&gt;&amp;1 &amp;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafKa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka如何彻底删除topic及数据]]></title>
    <url>%2Fkafka%E5%A6%82%E4%BD%95%E5%BD%BB%E5%BA%95%E5%88%A0%E9%99%A4topic%E5%8F%8A%E6%95%B0%E6%8D%AE.html</url>
    <content type="text"><![CDATA[前言：删除kafka topic及其数据，严格来说并不是很难的操作。但是，往往给kafka 使用者带来诸多问题。项目组之前接触过多个开发者，发现都会偶然出现无法彻底删除kafka的情况。本文总结多个删除kafka topic的应用场景，总结一套删除kafka topic的标准操作方法。 step1：如果需要被删除topic 此时正在被程序 produce和consume，则这些生产和消费程序需要停止。因为如果有程序正在生产或者消费该topic，则该topic的offset信息一致会在broker更新。调用kafka delete命令则无法删除该topic。同时，需要设置 auto.create.topics.enable = false，默认设置为true。如果设置为true，则produce或者fetch 不存在的topic也会自动创建这个topic。这样会给删除topic带来很多意向不到的问题。所以，这一步很重要，必须设置auto.create.topics.enable = false，并认真把生产和消费程序彻底全部停止。 step2：server.properties 设置 delete.topic.enable=true如果没有设置 delete.topic.enable=true，则调用kafka 的delete命令无法真正将topic删除，而是显示（marked for deletion） step3：调用命令删除topic：./bin/kafka-topics –delete –zookeeper 【zookeeper server:port】 –topic 【topic name】 step4：删除kafka存储目录（server.properties文件log.dirs配置，默认为”/data/kafka-logs”）相关topic的数据目录。注意：如果kafka 有多个 broker，且每个broker 配置了多个数据盘（比如 /data/kafka-logs,/data1/kafka-logs …），且topic也有多个分区和replica，则需要对所有broker的所有数据盘进行扫描，删除该topic的所有分区数据。 一般而言，经过上面4步就可以正常删除掉topic和topic的数据。但是，如果经过上面四步，还是无法正常删除topic，则需要对kafka在zookeeer的存储信息进行删除。具体操作如下：（注意：以下步骤里面，kafka在zk里面的节点信息是采用默认值，如果你的系统修改过kafka在zk里面的节点信息，则需要根据系统的实际情况找到准确位置进行操作） step5：找一台部署了zk的服务器，使用命令：bin/zkCli.sh -server 【zookeeper server:port】登录到zk shell，然后找到topic所在的目录：ls /brokers/topics，找到要删除的topic，然后执行命令：rmr /brokers/topics/【topic name】即可，此时topic被彻底删除。如果topic 是被标记为 marked for deletion，则通过命令 ls /admin/delete_topics，找到要删除的topic，然后执行命令：rmr /admin/delete_topics/【topic name】备注：网络上很多其它文章还说明，需要删除topic在zk上面的消费节点记录、配置节点记录，比如：rmr /consumers/【consumer-group】rmr /config/topics/【topic name】其实正常情况是不需要进行这两个操作的，如果需要，那都是由于操作不当导致的。比如step1停止生产和消费程序没有做，step2没有正确配置。也就是说，正常情况下严格按照step1 – step5 的步骤，是一定能够正常删除topic的。step6：完成之后，调用命令：./bin/kafka-topics.sh –list –zookeeper 【zookeeper server:port】查看现在kafka的topic信息。正常情况下删除的topic就不会再显示。但是，如果还能够查询到删除的topic，则重启zk和kafka即可。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改kafka保留天数对数据入库的影响]]></title>
    <url>%2F%E4%BF%AE%E6%94%B9kafka%E4%BF%9D%E7%95%99%E5%A4%A9%E6%95%B0%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%85%A5%E5%BA%93%E7%9A%84%E5%BD%B1%E5%93%8D.html</url>
    <content type="text"><![CDATA[场景介绍：Kafka集群三个broker，同时有一个生产者和一个消费者，生产者producer 已生产2个多小时约10万条数据，同时消费者将数据插入hbase中。 测试：1、打开ambari操作界面，在kafka配置页下修改保留天数的参数log.retention.hours 为1小时，同时修改文件大小segment为10000。（此处数值的设置，为了方便测试）https://hexoblog-1254111960.cos.ap-guangzhou.myqcloud.com/%E4%BF%AE%E6%94%B9kafka%E4%BF%9D%E7%95%99%E5%A4%A9%E6%95%B0%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%85%A5%E5%BA%93%E7%9A%84%E5%BD%B1%E5%93%8D.png2、修改完参数之后，逐一重启broker，在重启的过程中，在消费者控制台可以看到如下错误：重启完之后，错误消失。 3、测试结果：当把保留天数修改为1小时，则kafka中的数据只保留最近1小时的数据，早于1小时之前的数据将被删除。注意：如果被删除的数据没有被消费，则该被删除的数据不会被插入库中。所以，修改参数之前，确保数据已经消费过，并入库。]]></content>
  </entry>
  <entry>
    <title><![CDATA[创建分布式图数据库JanusGraph对象的两种方法]]></title>
    <url>%2F%E5%88%9B%E5%BB%BA%E5%88%86%E5%B8%83%E5%BC%8F%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93JanusGraph%E5%AF%B9%E8%B1%A1%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95.html</url>
    <content type="text"><![CDATA[JanusGraph 是一个分布式图数据库，相对于neo4j可进行横向扩展，且存储和图引擎分离，架构优美，本文将介绍JanusGraph的两种创建方式。1、添加Maven依赖1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.janusgraph&lt;/groupId&gt; &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.janusgraph&lt;/groupId&gt; &lt;artifactId&gt;janusgraph-cassandra&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.janusgraph&lt;/groupId&gt; &lt;artifactId&gt;janusgraph-es&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt;&lt;/dependency&gt; 有以下两种方式构建JanusGraph对象1、通过配置文件构建图对象12JanusGraph graph = JanusGraphFactory.open("janusgraph/conf/janusgraph-cassandra-es.properties");graph.close(); 2、通过Configuration构建图对象123456789101112131415161718192021222324252627282930313233import org.apache.commons.configuration.BaseConfiguration;import org.apache.tinkerpop.gremlin.process.traversal.dsl.graph.GraphTraversalSource;import org.janusgraph.core.JanusGraph;import org.janusgraph.core.JanusGraphFactory; public class Test &#123; public static void main(String[] args) &#123; BaseConfiguration config = new BaseConfiguration(); ////////////使用内存作为存储端 //config.setProperty("storage.backend", "inmemory"); //////////使用cassandra+es作为存储端 config.setProperty("storage.backend", "cassandrathrift"); config.setProperty("storage.cassandra.keyspace", "janus"); config.setProperty("storage.hostname", "127.0.0.1"); config.setProperty("index.search.backend", "elasticsearch"); config.setProperty("index.search.hostname", "127.0.0.1"); config.setProperty("cache.db-cache", "true"); config.setProperty("cache.db-cache-time", "300000"); config.setProperty("cache.db-cache-size", "0.5"); ; JanusGraph graph = JanusGraphFactory.open(config); GraphTraversalSource g = graph.traversal(); //其它逻辑代码 g.tx().rollback(); graph.close(); &#125;&#125;]]></content>
      <categories>
        <category>图数据库</category>
      </categories>
      <tags>
        <tag>JanusGraph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JanusGraph系统架构]]></title>
    <url>%2FJanusGraph%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84.html</url>
    <content type="text"><![CDATA[JanusGraph是一个图形数据库引擎。 JanusGraph本身专注于压缩图序列化、丰富图数据建模、高效的查询执行。 此外，JanusGraph利用Hadoop进行图分析和批处理。JanusGraph为数据持久化，数据索引和客户端访问实现了强大的模块化接口。 JanusGraph的模块化架构使其能够与各种存储，索引和客户端技术进行互操作; 这也使得JanusGraph升级对应的组件过程变得更加简单。在JanusGraph和磁盘之间有一个或多个存储和索引适配器。 JanusGraph标配以下适配器，但JanusGraph的模块化架构支持第三方适配器。 数据存储:Apache CassandraApache HBaseOracle Berkeley DB Java企业版 索引，用于加快访问速度并支持更复杂的查询语句:ElasticsearchApache SolrApache Lucene总体来讲，应用程序可以通过两种方式与JanusGraph进行交互：嵌在应用程序中的JanusGraph在同一个JVM中执行Gremlin语句。 查询任务、JanusGraph缓存和事务处理都在同一个JVM中，而后端数据检索可能是在本地或远程。通过向服务器提交Gremlin查询语句来与本地或远程JanusGraph实例交互。 JanusGraph本身支持Apache TinkerPop栈的Gremlin Server组件。 图 2.1. 高层JanusGraph架构和上下文]]></content>
      <categories>
        <category>图数据库</category>
      </categories>
      <tags>
        <tag>JanusGraph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多节点时间同步]]></title>
    <url>%2F%E5%A4%9A%E8%8A%82%E7%82%B9%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5.html</url>
    <content type="text"><![CDATA[linux 系统有两个时钟：一个是硬件时钟，即BIOS时间；另一个是系统时钟，是linux系统Kernel（内核）时间。系统开启时，系统会读取硬件时间，设置系统时间。因此，设置了系统时间，重启时会失效。要想永久更改时间，可以先同步系统时间（基于网络时间，准确性较高），再同步系统时间。 第一种情况 ：有网以网络时间为准校验 （1）查看时区date – 查看系统时间[root@develop Asia]# date -RFri, 22 Mar 2019 14:11:10 +0800 – +0800 代表是东八区，如果不是，自行更改到东八区 （2）安装ntpdate工具yum install ntpdate （3）同步时间，用的是阿里云的服务器systemctl stop ntpd – 停掉ntpd 服务，使 ntpdate 可以运行ntpdate ntp1.aliyun.com或者 ntpdate time.windows.comntpdate asia.pool.ntp.orgntpdate time.nuri.net （4）同步硬件时间hwclock 查看硬件时间hwclock –systohc –localtime – 同步硬件时间 （5）永久生效hwclock -wsystemctl start ntpd – 结束完之后 ，开启ntpd 第二种情况 ：没有网络这种同步时间的方法，很适合在无网的情况下，同步机器集群时间下面一起操作一遍。如果 有两台机器，选择其中一台机器A，作为服务端，机器B，作为客户端 （1）修改服务端A① 修改配置文件[root@hanadevelop Asia]# vi /etc/ntp.conf ②重启ntpdsystemctl restart ntpd – 要保证ntpd 服务开启，不然其它机器不能同步该机器的时间 （2）修改客户端机器B①修改配置[root@develop Asia]# vi /etc/ntp.conf ②[root@demo sysconfig]# systemctl restart ntpd[root@demo sysconfig]# systemctl enable ntpd[root@demo sysconfig]# systemctl stop ntpd （3）测试①修改机器A的时间[root@hanadevelop Asia]# date -s ‘2019-3-22 17:00:11’2019年 03月 22日 星期五 17:00:11 CST ②同步机器B的时间查看同步完成 ③ 同步硬件时间hwclock –systohc –localtime – 同步硬件时间hwclock -w – 永久生效]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FastJson反序列化为什么用TypeReference]]></title>
    <url>%2FFastJson%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8TypeReference.html</url>
    <content type="text"><![CDATA[泛型序列化非TypeReference code：1234567891011public static void main(String[] args) &#123; Map&lt;String, Person&gt; map = new HashMap&lt;&gt;(16); map.put("one", new Person("zhangsan")); map.put("two", new Person("lisi")); String jsonStr = JSON.toJSONString(map); byte[] bytes = jsonStr.getBytes(); String json = new String(bytes); Map&lt;String, Person&gt; res = JSON.parseObject(json, Map.class); System.out.println(res.get("one")); System.out.println(res.get("one").getName());&#125; 输出：12345678910Exception in thread "main" java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.CommandLineWrapper.main(CommandLineWrapper.java:67)Caused by: java.lang.ClassCastException: com.alibaba.fastjson.JSONObject cannot be cast to subtitle.io.Person at subtitle.io.MyMain.main(MyMain.java:48) ... 5 more&#123;"name":"zhangsan"&#125; 泛型序列化TypeReference code：1234567891011public static void main(String[] args) &#123; Map&lt;String, Person&gt; map = new HashMap&lt;&gt;(16); map.put("one", new Person("zhangsan")); map.put("two", new Person("lisi")); String jsonStr = JSON.toJSONString(map); byte[] bytes = jsonStr.getBytes(); String json = new String(bytes); Map&lt;String, Person&gt; res = JSON.parseObject(json, new TypeReference&lt;Map&lt;String, Person&gt;&gt;()&#123;&#125;); System.out.println(res.get("one")); System.out.println(res.get("one").getName());&#125; 输出：12Person&#123;name='zhangsan'&#125;zhangsan 报错原因： 反序列化时候，虽然添加Map.class，但是没有办法指定Person类型，导致反序列化后的对象为Map]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>fastjson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark项目在IDEA运行正常，spark-submit提示没有合适驱动]]></title>
    <url>%2Fdriver.html</url>
    <content type="text"><![CDATA[java.sql.SQLException: No suitable driver 解决方案报错代码：1Exception in thread "main" java.sql.SQLException: No suitable driver at java.sql.DriverManager.getDriver(DriverManager.java:315) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:85) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:85) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:84) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:35) at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:60) at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80) at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656) at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77) at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656) at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273) at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267) at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:501) at com.xinyi.multiCollision.service.JobService$.dealWithMoreData(JobService.scala:406) at com.xinyi.multiCollision.sparkJob.MultiCollisionApp$.main(MultiCollisionApp.scala:70) at com.xinyi.multiCollision.sparkJob.MultiCollisionApp.main(MultiCollisionApp.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 一般情况下，此报错原因是因为缺少连接数据库的驱动。正常情况下，连接参数，驱动都是能考虑到的。但是在连接参数，驱动已添加的情况下也会报错。本人spark项目在IDEA正常运行，但是spark-submit的时候出现此错误（之前是可以正常运行的，后面报错，怀疑跟集群环境有关）。提交脚本已经配置了参数–conf spark.yarn.jars （虽然已经配置，驱动，jar包有可能不加载进去）。此时的解决方案：将mysql-connector-java-5.1.24-bin.jar的jar包加入集群JAVA_HOME\jre\lib\ext文件夹下，再次运行，问题得以解决。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark2.4.2编译（mac系统下）]]></title>
    <url>%2Fspark2-4-2%E7%BC%96%E8%AF%91.html</url>
    <content type="text"><![CDATA[编译前所注意事项：首先，尽可能阅读官网编译文档 Building Apache Spark源码下载推荐git clone 或者 wget 。编译前确保网络良好。 下载所需要的软件（注意版本）· Spark-2.4.2.tgz· Hadoop-2.7.6· Scala-2.11.12· jdk1.8.0_191· apache-maven-3.6.x· git注意：其中spark是源码，其他是可运行包 解压安装并配置环境变量（过程略）配置完，注意测试。其中，maven配置本地库，镜像地址设置为阿里云地址。1234# 创建本地仓库文件夹mkdir ~/maven_repo# 修改settings.xml文件vim $MAVEN_HOME/conf/settings.xml 部分代码：1234567891011121314151617&lt;!-- localRepository | The path to the local repository maven will use to store artifacts. | | Default: $&#123;user.home&#125;/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt;&lt;localRepository&gt;/home/max/maven_repo&lt;/localRepository&gt;&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*,!cloudera&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt; http://maven.aliyun.com/nexus/content/groups/public &lt;/url&gt;&lt;/mirror&gt; 修改脚本make-distribution.sh编译不使用mvn这个命令,直接用make-distribution.sh脚本，但是需要修改该脚本1234567891011121314151617181920212223242526272829#spark-2.4.2文件夹下vim ./dev/make-distribution.sh#将这些行注释掉 此处为最佳实践，为的是通过指定版本号减少编译时间#VERSION=$("$MVN" help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null\# | grep -v "INFO"\# | grep -v "WARNING"\# | tail -n 1)#SCALA_VERSION=$("$MVN" help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\# | grep -v "INFO"\# | grep -v "WARNING"\# | tail -n 1)#SPARK_HADOOP_VERSION=$("$MVN" help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\# | grep -v "INFO"\# | grep -v "WARNING"\# | tail -n 1)#SPARK_HIVE=$("$MVN" help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\# | grep -v "INFO"\# | grep -v "WARNING"\# | fgrep --count "&lt;id&gt;hive&lt;/id&gt;";\# # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\# # because we use "set -o pipefail"# echo -n)##添加一下参数，注意，版本号要对应自己想要的生产环境VERSION=2.4.2SCALA_VERSION=2.11SPARK_HADOOP_VERSION=hadoop-2.6.0-cdh5.14.0SPARK_HIVE=1 修改源码包spark-2.4.2下的pom.xml123456789101112131415161718192021222324252627282930313233&lt;repositories&gt; &lt;!--&lt;repositories&gt; This should be at top, it makes maven try the central repo first and then othersand hence faster dep resolution &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Maven Repository&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;--&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public//&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 开始编译12345678./dev/make-distribution.sh \--name hadoop-2.6.0-cdh5.14.0 \--tgz \-Phadoop-2.6 \-Dhadoop.version=2.6.0-cdh5.14.0 \-Phive -Phive-thriftserver \-Pyarn \-Pkubernetes 编译大概需要半小时以上，耐心等待就行。编译过程中如果报错，一般有error字样。出现以下字样，代表编译完成：编译后包所在位置，源码包spark-2.4.2根目录下：至此，编译完！]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop分布式文件系统：架构和设计]]></title>
    <url>%2FHadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%EF%BC%9A%E6%9E%B6%E6%9E%84%E5%92%8C%E8%AE%BE%E8%AE%A1.html</url>
    <content type="text"><![CDATA[引言Hadoop分布式文件系统(HDFS)被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统。它和现有的分布式文件系统有很多共同点。但同时，它和其他的分布式文件系统的区别也是很明显的。HDFS是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS放宽了一部分POSIX约束，来实现流式读取文件系统数据的目的。HDFS在最开始是作为Apache Nutch搜索引擎项目的基础架构而开发的。HDFS是Apache Hadoop Core项目的一部分。这个项目的地址是http://hadoop.apache.org/core/。 前提和设计目标硬件错误硬件错误是常态而不是异常。HDFS可能由成百上千的服务器所构成，每个服务器上存储着文件系统的部分数据。我们面对的现实是构成系统的组件数目是巨大的，而且任一组件都有可能失效，这意味着总是有一部分HDFS的组件是不工作的。因此错误检测和快速、自动的恢复是HDFS最核心的架构目标。 流式数据访问运行在HDFS上的应用和普通的应用不同，需要流式访问它们的数据集。HDFS的设计中更多的考虑到了数据批处理，而不是用户交互处理。比之数据访问的低延迟问题，更关键的在于数据访问的高吞吐量。POSIX标准设置的很多硬性约束对HDFS应用系统不是必需的。为了提高数据的吞吐量，在一些关键方面对POSIX的语义做了一些修改。 大规模数据集运行在HDFS上的应用具有很大的数据集。HDFS上的一个典型文件大小一般都在G字节至T字节。因此，HDFS被调节以支持大文件存储。它应该能提供整体上高的数据传输带宽，能在一个集群里扩展到数百个节点。一个单一的HDFS实例应该能支撑数以千万计的文件。 简单的一致性模型HDFS应用需要一个“一次写入多次读取”的文件访问模型。一个文件经过创建、写入和关闭之后就不需要改变。这一假设简化了数据一致性问题，并且使高吞吐量的数据访问成为可能。Map/Reduce应用或者网络爬虫应用都非常适合这个模型。目前还有计划在将来扩充这个模型，使之支持文件的附加写操作。 “移动计算比移动数据更划算”一个应用请求的计算，离它操作的数据越近就越高效，在数据达到海量级别的时候更是如此。因为这样就能降低网络阻塞的影响，提高系统数据的吞吐量。将计算移动到数据附近，比之将数据移动到应用所在显然更好。HDFS为应用提供了将它们自己移动到数据附近的接口。 异构软硬件平台间的可移植性HDFS在设计的时候就考虑到平台的可移植性。这种特性方便了HDFS作为大规模数据应用平台的推广。 Namenode 和 DatanodeHDFS采用master/slave架构。一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。集群中的Datanode一般是一个节点一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组Datanode上。Namenode执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体Datanode节点的映射。Datanode负责处理文件系统客户端的读写请求。在Namenode的统一调度下进行数据块的创建、删除和复制。 HDFS 架构Namenode和Datanode被设计成可以在普通的商用机器上运行。这些机器一般运行着GNU/Linux操作系统(OS)。HDFS采用Java语言开发，因此任何支持Java的机器都可以部署Namenode或Datanode。由于采用了可移植性极强的Java语言，使得HDFS可以部署到多种类型的机器上。一个典型的部署场景是一台机器上只运行一个Namenode实例，而集群中的其它机器分别运行一个Datanode实例。这种架构并不排斥在一台机器上运行多个Datanode，只不过这样的情况比较少见。 集群中单一Namenode的结构大大简化了系统的架构。Namenode是所有HDFS元数据的仲裁者和管理者，这样，用户数据永远不会流过Namenode。 文件系统的名字空间 (namespace)HDFS支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。当前，HDFS不支持用户磁盘配额和访问权限控制，也不支持硬链接和软链接。但是HDFS架构并不妨碍实现这些特性。 Namenode负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被Namenode记录下来。应用程序可以设置HDFS保存的文件的副本数目。文件副本的数目称为文件的副本系数，这个信息也是由Namenode保存的。 数据复制HDFS被设计成能够在一个大集群中跨机器可靠地存储超大文件。它将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。为了容错，文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。HDFS中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。 Namenode全权管理数据块的复制，它周期性地从集群中的每个Datanode接收心跳信号和块状态报告(Blockreport)。接收到心跳信号意味着该Datanode节点工作正常。块状态报告包含了一个该Datanode上所有数据块的列表。 HDFS Datanodes副本存放: 最最开始的一步副本的存放是HDFS可靠性和性能的关键。优化的副本存放策略是HDFS区分于其他大部分分布式文件系统的重要特性。这种特性需要做大量的调优，并需要经验的积累。HDFS采用一种称为机架感知(rack-aware)的策略来改进数据的可靠性、可用性和网络带宽的利用率。目前实现的副本存放策略只是在这个方向上的第一步。实现这个策略的短期目标是验证它在生产环境下的有效性，观察它的行为，为实现更先进的策略打下测试和研究的基础。 大型HDFS实例一般运行在跨越多个机架的计算机组成的集群上，不同机架上的两台机器之间的通讯需要经过交换机。在大多数情况下，同一个机架内的两台机器间的带宽会比不同机架的两台机器间的带宽大。 通过一个机架感知的过程，Namenode可以确定每个Datanode所属的机架id。一个简单但没有优化的策略就是将副本存放在不同的机架上。这样可以有效防止当整个机架失效时数据的丢失，并且允许读数据的时候充分利用多个机架的带宽。这种策略设置可以将副本均匀分布在集群中，有利于当组件失效情况下的负载均衡。但是，因为这种策略的一个写操作需要传输数据块到多个机架，这增加了写的代价。 在大多数情况下，副本系数是3，HDFS的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。这种策略减少了机架间的数据传输，这就提高了写操作的效率。机架的错误远远比节点的错误少，所以这个策略不会影响到数据的可靠性和可用性。于此同时，因为数据块只放在两个（不是三个）不同的机架上，所以此策略减少了读取数据时需要的网络传输总带宽。在这种策略下，副本并不是均匀分布在不同的机架上。三分之一的副本在一个节点上，三分之二的副本在一个机架上，其他副本均匀分布在剩下的机架中，这一策略在不损害数据可靠性和读取性能的情况下改进了写的性能。 当前，这里介绍的默认副本存放策略正在开发的过程中。 副本选择为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。如果在读取程序的同一个机架上有一个副本，那么就读取该副本。如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。 安全模式Namenode启动后会进入一个称为安全模式的特殊状态。处于安全模式的Namenode是不会进行数据块的复制的。Namenode从所有的 Datanode接收心跳信号和块状态报告。块状态报告包括了某个Datanode所有的数据块列表。每个数据块都有一个指定的最小副本数。当Namenode检测确认某个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全(safely replicated)的；在一定百分比（这个参数可配置）的数据块被Namenode检测确认是安全之后（加上一个额外的30秒等待时间），Namenode将退出安全模式状态。接下来它会确定还有哪些数据块的副本没有达到指定数目，并将这些数据块复制到其他Datanode上。 文件系统元数据的持久化Namenode上保存着HDFS的名字空间。对于任何对文件系统元数据产生修改的操作，Namenode都会使用一种称为EditLog的事务日志记录下来。例如，在HDFS中创建一个文件，Namenode就会在Editlog中插入一条记录来表示；同样地，修改文件的副本系数也将往Editlog插入一条记录。Namenode在本地操作系统的文件系统中存储这个Editlog。整个文件系统的名字空间，包括数据块到文件的映射、文件的属性等，都存储在一个称为FsImage的文件中，这个文件也是放在Namenode所在的本地文件系统上。 Namenode在内存中保存着整个文件系统的名字空间和文件数据块映射(Blockmap)的映像。这个关键的元数据结构设计得很紧凑，因而一个有4G内存的Namenode足够支撑大量的文件和目录。当Namenode启动时，它从硬盘中读取Editlog和FsImage，将所有Editlog中的事务作用在内存中的FsImage上，并将这个新版本的FsImage从内存中保存到本地磁盘上，然后删除旧的Editlog，因为这个旧的Editlog的事务都已经作用在FsImage上了。这个过程称为一个检查点(checkpoint)。在当前实现中，检查点只发生在Namenode启动时，在不久的将来将实现支持周期性的检查点。 Datanode将HDFS数据以文件的形式存储在本地的文件系统中，它并不知道有关HDFS文件的信息。它把每个HDFS数据块存储在本地文件系统的一个单独的文件中。Datanode并不在同一个目录创建所有的文件，实际上，它用试探的方法来确定每个目录的最佳文件数目，并且在适当的时候创建子目录。在同一个目录中创建所有的本地文件并不是最优的选择，这是因为本地文件系统可能无法高效地在单个目录中支持大量的文件。当一个Datanode启动时，它会扫描本地文件系统，产生一个这些本地文件对应的所有HDFS数据块的列表，然后作为报告发送到Namenode，这个报告就是块状态报告。 通讯协议所有的HDFS通讯协议都是建立在TCP/IP协议之上。客户端通过一个可配置的TCP端口连接到Namenode，通过ClientProtocol协议与Namenode交互。而Datanode使用DatanodeProtocol协议与Namenode交互。一个远程过程调用(RPC)模型被抽象出来封装ClientProtocol和Datanodeprotocol协议。在设计上，Namenode不会主动发起RPC，而是响应来自客户端或 Datanode 的RPC请求。 健壮性HDFS的主要目标就是即使在出错的情况下也要保证数据存储的可靠性。常见的三种出错情况是：Namenode出错, Datanode出错和网络割裂(network partitions)。 磁盘数据错误，心跳检测和重新复制每个Datanode节点周期性地向Namenode发送心跳信号。网络割裂可能导致一部分Datanode跟Namenode失去联系。Namenode通过心跳信号的缺失来检测这一情况，并将这些近期不再发送心跳信号Datanode标记为宕机，不会再将新的IO请求发给它们。任何存储在宕机Datanode上的数据将不再有效。Datanode的宕机可能会引起一些数据块的副本系数低于指定值，Namenode不断地检测这些需要复制的数据块，一旦发现就启动复制操作。在下列情况下，可能需要重新复制：某个Datanode节点失效，某个副本遭到损坏，Datanode上的硬盘错误，或者文件的副本系数增大。 集群均衡HDFS的架构支持数据均衡策略。如果某个Datanode节点上的空闲空间低于特定的临界点，按照均衡策略系统就会自动地将数据从这个Datanode移动到其他空闲的Datanode。当对某个文件的请求突然增加，那么也可能启动一个计划创建该文件新的副本，并且同时重新平衡集群中的其他数据。这些均衡策略目前还没有实现。 数据完整性从某个Datanode获取的数据块有可能是损坏的，损坏可能是由Datanode的存储设备错误、网络错误或者软件bug造成的。HDFS客户端软件实现了对HDFS文件内容的校验和(checksum)检查。当客户端创建一个新的HDFS文件，会计算这个文件每个数据块的校验和，并将校验和作为一个单独的隐藏文件保存在同一个HDFS名字空间下。当客户端获取文件内容后，它会检验从Datanode获取的数据跟相应的校验和文件中的校验和是否匹配，如果不匹配，客户端可以选择从其他Datanode获取该数据块的副本。 元数据磁盘错误FsImage和Editlog是HDFS的核心数据结构。如果这些文件损坏了，整个HDFS实例都将失效。因而，Namenode可以配置成支持维护多个FsImage和Editlog的副本。任何对FsImage或者Editlog的修改，都将同步到它们的副本上。这种多副本的同步操作可能会降低Namenode每秒处理的名字空间事务数量。然而这个代价是可以接受的，因为即使HDFS的应用是数据密集的，它们也非元数据密集的。当Namenode重启的时候，它会选取最近的完整的FsImage和Editlog来使用。 Namenode是HDFS集群中的单点故障(single point of failure)所在。如果Namenode机器故障，是需要手工干预的。目前，自动重启或在另一台机器上做Namenode故障转移的功能还没实现。 快照快照支持某一特定时刻的数据的复制备份。利用快照，可以让HDFS在数据损坏时恢复到过去一个已知正确的时间点。HDFS目前还不支持快照功能，但计划在将来的版本进行支持。 数据组织数据块HDFS被设计成支持大文件，适用HDFS的是那些需要处理大规模的数据集的应用。这些应用都是只写入数据一次，但却读取一次或多次，并且读取速度应能满足流式读取的需要。HDFS支持文件的“一次写入多次读取”语义。一个典型的数据块大小是64MB。因而，HDFS中的文件总是按照64M被切分成不同的块，每个块尽可能地存储于不同的Datanode中。 Staging客户端创建文件的请求其实并没有立即发送给Namenode，事实上，在刚开始阶段HDFS客户端会先将文件数据缓存到本地的一个临时文件。应用程序的写操作被透明地重定向到这个临时文件。当这个临时文件累积的数据量超过一个数据块的大小，客户端才会联系Namenode。Namenode将文件名插入文件系统的层次结构中，并且分配一个数据块给它。然后返回Datanode的标识符和目标数据块给客户端。接着客户端将这块数据从本地临时文件上传到指定的Datanode上。当文件关闭时，在临时文件中剩余的没有上传的数据也会传输到指定的Datanode上。然后客户端告诉Namenode文件已经关闭。此时Namenode才将文件创建操作提交到日志里进行存储。如果Namenode在文件关闭前宕机了，则该文件将丢失。 上述方法是对在HDFS上运行的目标应用进行认真考虑后得到的结果。这些应用需要进行文件的流式写入。如果不采用客户端缓存，由于网络速度和网络堵塞会对吞估量造成比较大的影响。这种方法并不是没有先例的，早期的文件系统，比如AFS，就用客户端缓存来提高性能。为了达到更高的数据上传效率，已经放松了POSIX标准的要求。 流水线复制当客户端向HDFS文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副本系数设置为3，当本地临时文件累积到一个数据块的大小时，客户端会从Namenode获取一个Datanode列表用于存放副本。然后客户端开始向第一个Datanode传输数据，第一个Datanode一小部分一小部分(4 KB)地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中第二个Datanode节点。第二个Datanode也是这样，一小部分一小部分地接收数据，写入本地仓库，并同时传给第三个Datanode。最后，第三个Datanode接收数据并存储在本地。因此，Datanode能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的方式从前一个Datanode复制到下一个。 可访问性HDFS给应用提供了多种访问方式。用户可以通过Java API接口访问，也可以通过C语言的封装API访问，还可以通过浏览器的方式访问HDFS中的文件。通过WebDAV协议访问的方式正在开发中。 DFSShellHDFS以文件和目录的形式组织用户数据。它提供了一个命令行的接口(DFSShell)让用户与HDFS中的数据进行交互。命令的语法和用户熟悉的其他shell(例如 bash, csh)工具类似。下面是一些动作/命令的示例： 动作命令创建一个名为 /foodir 的目录 bin/hadoop dfs -mkdir /foodir创建一个名为 /foodir 的目录 bin/hadoop dfs -mkdir /foodir查看名为 /foodir/myfile.txt 的文件内容 bin/hadoop dfs -cat /foodir/myfile.txtDFSShell 可以用在那些通过脚本语言和文件系统进行交互的应用程序上。 DFSAdminDFSAdmin 命令用来管理HDFS集群。这些命令只有HDSF的管理员才能使用。下面是一些动作/命令的示例： 动作命令将集群置于安全模式 bin/hadoop dfsadmin -safemode enter显示Datanode列表 bin/hadoop dfsadmin -report使Datanode节点 datanodename退役 bin/hadoop dfsadmin -decommission datanodename 浏览器接口一个典型的HDFS安装会在一个可配置的TCP端口开启一个Web服务器用于暴露HDFS的名字空间。用户可以用浏览器来浏览HDFS的名字空间和查看文件的内容。 存储空间回收文件的删除和恢复当用户或应用程序删除某个文件时，这个文件并没有立刻从HDFS中删除。实际上，HDFS会将这个文件重命名转移到/trash目录。只要文件还在/trash目录中，该文件就可以被迅速地恢复。文件在/trash中保存的时间是可配置的，当超过这个时间时，Namenode就会将该文件从名字空间中删除。删除文件会使得该文件相关的数据块被释放。注意，从用户删除文件到HDFS空闲空间的增加之间会有一定时间的延迟。 只要被删除的文件还在/trash目录中，用户就可以恢复这个文件。如果用户想恢复被删除的文件，他/她可以浏览/trash目录找回该文件。/trash目录仅仅保存被删除文件的最后副本。/trash目录与其他的目录没有什么区别，除了一点：在该目录上HDFS会应用一个特殊策略来自动删除文件。目前的默认策略是删除/trash中保留时间超过6小时的文件。将来，这个策略可以通过一个被良好定义的接口配置。 减少副本系数当一个文件的副本系数被减小后，Namenode会选择过剩的副本删除。下次心跳检测时会将该信息传递给Datanode。Datanode遂即移除相应的数据块，集群中的空闲空间加大。同样，在调用setReplication API结束和集群中空闲空间增加间会有一定的延迟。 参考资料HDFS Java APIHDFS 源代码]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop2.7.5HA集群搭建]]></title>
    <url>%2FHadoop2-7-5HA%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA.html</url>
    <content type="text"><![CDATA[hadoop HA原理概述为什么会有 hadoop HA 机制呢？ HA：High Available，高可用在Hadoop 2.0之前,在HDFS 集群中NameNode 存在单点故障 (SPOF：A Single Point of Failure)。对于只有一个 NameNode 的集群，如果 NameNode 机器出现故障(比如宕机或是软件、硬件升级)，那么整个集群将无法使用，直到 NameNode 重新启动。 那如何解决呢？HDFS 的 HA 功能通过配置 Active/Standby 两个 NameNodes 实现在集群中对 NameNode 的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将 NameNode 很快的切换到另外一台机器。 在一个典型的 HDFS(HA) 集群中，使用两台单独的机器配置为 NameNodes 。在任何时间点，确保 NameNodes 中只有一个处于 Active 状态，其他的处在 Standby 状态。其中ActiveNameNode 负责集群中的所有客户端操作，StandbyNameNode 仅仅充当备机，保证一旦 ActiveNameNode 出现问题能够快速切换。 为了能够实时同步 Active 和 Standby 两个 NameNode 的元数据信息（实际上 editlog），需提供一个共享存储系统，可以是 NFS、QJM（Quorum Journal Manager）或者 Zookeeper，ActiveNamenode 将数据写入共享存储系统，而 Standby 监听该系统，一旦发现有新数据写入，则读取这些数据，并加载到自己内存中，以保证自己内存状态与 Active NameNode 保持基本一致，如此这般，在紧急情况下 standby 便可快速切为 active namenode。为了实现快速切换，Standby 节点获取集群的最新文件块信息也是很有必要的。为了实现这一目标，DataNode 需要配置 NameNodes 的位置，并同时给他们发送文件块信息以及心跳检测。 SecondaryNameNode 和 Standby Namenode 的区别？在1.x版本中，SecondaryNameNode将fsimage跟edits进行合并，生成新的fsimage文件用http post传回NameNode节点。SecondaryNameNode不能做NameNode的备份。在hadoop 2.x版本中才引入StandbyNameNode，从journalNode上拷贝的。StandbyNameNode是可以做namenode的备份。 Hadoop2的高可用并取代SecondaryNamenode在hadoop2实际生产环境中，为什么还需要SecondeNamenodesecondary namenode和namenode的区别 集群规划描述：hadoop HA 集群的搭建依赖于 zookeeper，所以选取三台当做 zookeeper 集群我总共准备了四台主机，分别是 hadoop02，hadoop03，hadoop04，hadoop05其中 hadoop02 和 hadoop03 做 namenode 的主备切换，hadoop04 和 hadoop05 做resourcemanager 的主备切换 集群服务器准备1、 修改主机名2、 修改 IP 地址3、 添加主机名和 IP 映射4、 添加普通用户 hadoop 用户并配置 sudoer 权限5、 设置系统启动级别6、 关闭防火墙/关闭 Selinux7、 安装 JDK两种准备方式：1、 每个节点都单独设置，这样比较麻烦。线上环境可以编写脚本实现2、 虚拟机环境可是在做完以上 7 步之后，就进行克隆3、 然后接着再给你的集群配置 SSH 免密登陆和搭建时间同步服务8、 配置 SSH 免密登录9、 同步服务器时间 集群安装1、安装 Zookeeper 集群（略）2、 安装 hadoop 集群 修改配置文件：core-site.xml :1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;!-- 指定hdfs的nameservice为myha01 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://myha01/&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop临时目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data1/hadoopdata/&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop01:2181,hadoop02:2181,hadoop03:2181,hadoop04:2181,hadoop05:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- hadoop链接zookeeper的超时时长设置 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;1000&lt;/value&gt; &lt;description&gt;ms&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118&lt;configuration&gt; &lt;!-- 指定副本数--&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置namenode和datanode的工作目录-数据存储目录--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data1/hadoopdata/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data1/hadoopdata/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;!-- 启用webhdfs--&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--指定hdfs的nameservice为myha01，需要和core-site.xml中的保持一致 dfs.ha.namenodes.[nameservice id]为在nameservice中的每一个NameNode设置唯一标示符。 配置一个逗号分隔的NameNode ID列表。这将是被DataNode识别为所有的NameNode。 例如，如果使用&quot;myha01&quot;作为nameservice ID，并且使用&quot;nn1&quot;和&quot;nn2&quot;作为NameNodes标示符 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;myha01&lt;/value&gt; &lt;/property&gt; &lt;!-- myha01下面有两个NameNode,分别是nn1,nn2--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.myha01&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.myha01.nn1&lt;/name&gt; &lt;value&gt;hadoop01:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.myha01.nn1&lt;/name&gt; &lt;value&gt;hadoop01:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.myha01.nn2&lt;/name&gt; &lt;value&gt;hadoop02:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.myha01.nn2&lt;/name&gt; &lt;value&gt;hadoop02:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode的edits元数据的共享存储位置。也就是JournalNode列表 该url的配置格式：qjournal://host1:port1;host2:port2;host3:port3/journalId journalId推荐使用nameservice，默认端口号是：8485 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop02:8485;hadoop03:8485;hadoop04:8485/myha01&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定JournalNode在本地磁盘存放数据的位置--&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data1/journaldata&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启NameNode失败自动切换--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置失败自动切换实现方式--&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.myha01&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;!-- 使用sshfence隔离机制时需要ssh免密登陆--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置sshfence隔离机制赶超时间--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.failover-controller.cli-check.rpc-timeout.ms&lt;/name&gt; &lt;value&gt;60000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml: 12345678910111213141516171819&lt;configuration&gt; &lt;!--指定mr框架为yarn方式--&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!--指定mapreduce jobhistory--&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop01:10020&lt;/value&gt; &lt;/property&gt; &lt;!--任务历史服务器的web地址--&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop01:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;configuration&gt; &lt;!-- 开启RM高可用--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的cluster id--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yrc&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的名字--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!--分别指定RM的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop03&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop04&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk集群地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop01:2181,hadoop02:2181,hadoop03:2181,hadoop04:2181,hadoop05:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;86400&lt;/value&gt; &lt;/property&gt; &lt;!--启用自动恢复--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 制定resourcemanager的状态信息存储在zookeeper集群上 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hadoop-env.sh: 修改JAVA_HOMEslaves文件：修改映射 #注意：配置文件是最容易出错的地方，建议复制，不建议手写，容易出错。格式化namenode之后，如果出错了，建议删除临时文件日志目录，每台机器都要删除，重新格式化，注意顺序步骤。 如果DFSZKFailoverController自动死掉，则有可能是因为以下配置问题：错误配置，会导致DFSZKFailoverController死掉：正确写法，注意sshfence不能换行： 分发安装包到其他机器1234scp -r hadoop-2.7.5 hadoop@hadoop02:$PWDscp -r hadoop-2.7.5 hadoop@hadoop03:$PWDscp -r hadoop-2.7.5 hadoop@hadoop04:$PWDscp -r hadoop-2.7.5 hadoop@hadoop05:$PWD 并分别配置环境变量vi ~/.bashrc添加两行：export HADOOP_HOME=/home/hadoop/apps/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin保存退出 集群初始化操作：（注意：严格按照以下步骤执行）1、 先启动 zookeeper 集群启动：zkServer.sh start检查启动是否正常：zkServer.sh status2、 分别在每个 zookeeper（也就是规划的三个 journalnode 节点，不一定跟 zookeeper节点一样）节点上启动 journalnode 进程 12345[hadoop@hadoop01 ~]$ hadoop-daemon.sh start journalnode[hadoop@hadoop02 ~]$ hadoop-daemon.sh start journalnode[hadoop@hadoop03 ~]$ hadoop-daemon.sh start journalnode[hadoop@hadoop04 ~]$ hadoop-daemon.sh start journalnode[hadoop@hadoop05 ~]$ hadoop-daemon.sh start journalnode 然后用 jps 命令查看是否各个 datanode 节点上都启动了 journalnode 进程如果报错，根据错误提示改进3、在第一个 namenode 上执行格式化操作然后会在 core-site.xml 中配置的临时目录中生成一些集群的信息把他拷贝的第二个 namenode 的相同目录下12&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/data/hadoopdata/&lt;/value&gt; 这个目录下，千万记住：两个 namenode 节点该目录中的数据结构是一致的[hadoop@hadoop02 ~]$ scp -r ~/data/hadoopdata/ hadoop03:~/data或者也可以在另一个 namenode 上执行：hadoop namenode -bootstrapStandby 1[hadoop@hadoop02 ~]$ hadoop namenode -format 4、格式化 ZKFC 1[hadoop@hadoop02 ~]$ hdfs zkfc -formatZK 在第一台机器上即可5、启动 HDFS 1[hadoop@hadoop02 ~]$ start-dfs.sh 查看各节点进程是否启动正常：依次为 2345 四台机器的进程 最终效果：访问 web 页面 http://hadoop01:50070访问 web 页面 http://hadoop02:50070 启动 YARN[hadoop@hadoop04 ~]$ start-yarn.sh在主备 resourcemanager 中随便选择一台进行启动，正常启动之后，检查各节点的进程：若备用节点的 resourcemanager 没有启动起来，则手动启动起来1[hadoop@hadoop04 ~]$ yarn-daemon.sh start resourcemanager 访问页面：http://hadoop03:8088访问页面：http://hadoop04:8088 自动跳转至hadoop03机器 查看各主节点的状态HDFS:hdfs haadmin -getServiceState nn1hdfs haadmin -getServiceState nn2YARN:yarn rmadmin -getServiceState rm1yarn rmadmin -getServiceState rm2 启动 mapreduce 任务历史服务器1[hadoop@hadoop01 ~]$ mr-jobhistory-daemon.sh start historyserver 按照配置文件配置的历史服务器的 web 访问地址去访问：http://hadoop01:19888 集群启动测试1、干掉 active namenode， 看看集群有什么变化 干掉active namenode，standby namenode瞬间转为active状态；重新启动刚才那台干掉的节点后，该节点变为standby 状态。2、在上传文件的时候干掉 active namenode， 看看有什么变化 会报错，但是可以上传成功。3、干掉 active resourcemanager， 看看集群有什么变化 hadoop03上的yarn节点就不能访问，hadoop05上的yarn节点可以正常访问。4、在执行任务的时候干掉 active resourcemanager，看看集群 执行wordcount程序，执行的时候，在hadoop03节点上杀死ResourceManager，最终能够成功执行。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在大数据环境中执行情感分析]]></title>
    <url>%2F%E5%9C%A8%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E4%B8%AD%E6%89%A7%E8%A1%8C%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[情感分析情感分析是利用文本分析来挖掘各种观点的数据来源的过程。通常情况下，情感分析是在从互联网和各种社交媒体平台收集的数据上执行的。政治家和政府经常利用情感分析来了解人们如何看待他们和他们的政策。随着社交媒体的出现，人们可以从各种不同来源（比如移动设备和 Web 浏览器）捕获数据，并用不同的数据格式存储这些数据。由于社交媒体内容对于传统存储系统（比如 RDBMS、关系数据库管理系统）是非结构化的，所以我们需要一些可以处理和分析各种不同数据的工具。不过，大数据技术旨在处理不同来源、不同格式的结构化和非结构化数据。在本文中，我将介绍如何利用大数据工具来捕获数据，以便存储和处理用于情感分析的数据。 处理大数据无论何时从采用多种格式（结构化、半结构化或非结构化的）的多个来源收集数据，都需要考虑建立一个 Hadoop 集群和一个 Hadoop 分布式文件系统（HDFS）来存储数据。HDFS 提供了一种管理大数据的灵活方式： 可以将您的一些分析数据移动到现有的关系数据库管理系统（RDBMS）中，比如 Oracle 或 MySQL，这样您就可以利用现有的 BI 和报告工具。 可以将数据存储在 HDFS 中，供将来分析使用，例如，通过执行像 ANOVA.T 这样的测试来比较旧数据与新数据。 如果只需要分析数据的影响，那么可以删除这些数据。要了解如何设置 Hadoop 集群，请将数据导入 HDFS，然后在您的 Hadoop 环境中分析这些数据，请参阅我的其他 developerWorks 文章， “将 Hadoop 与现有的 RDBMS 相集成“。检索数据并将数据存储在 HDFS 中最好的情感分析包括来自多个来源的数据。在本文中，我将介绍如何从这些来源中检索数据： Twitter 提要 RSS 提要 移动应用程序我还将解释如何将来自不同来源的数据存储在 HDFS 中（存储在您的 Hadoop 集群中）。从 Twitter 提要中检索数据Twitter（一种流行的微博网站）有一组 API，它们使得我们能够检索和操作 tweet。但是首先，我们需要实现 Twitter 的 OAuth 框架。简单地讲，有了这个框架，应用程序就可以代表您登录到 Twitter，无需您登录到 Twitter 网站。查看 Twitter 开发人员站点的设置过程，其中解释了如何指派实现此操作的应用程序。在这个过程中，会为您分配一个密钥和一个密钥令牌，您的应用程序将使用它们来代表您执行身份验证。在您的应用程序完成身份验证后，您就可以使用 Twitter API 来获取 tweet。您可以通过使用 R 或通过使用 Jaql 获取来自 Twitter 提要的数据。因为 Jaql 被设计用于处理 JSON 数据，所以它是适用于 tweet 的默认数据格式，使用 Jaql 可能更简单一些。有人可能会决定使用 R，这样做可能纯粹是因为他们自己的 R 技能。通过使用 Jaql 检索来自 Twitter 的数据在您的应用程序完成身份验证后，我们就可以使用 Twitter API 来获取 tweet。因为我们想要在流化模式下，所以我们的 Twitter URL 是：url = “https://stream.twitter.com/1.1/statuses/filter.json?track=governmentTopic“;使用我们正在挖掘的政府主题的名称来替换 governmentTopic。通过使用与以下代码类似的代码，我们可以用一个变量来获取 tweet：12jsonResultTweets = read(http(url));jsonResultTweets; 在运行 Jaql 脚本时，它会提取与政府主题相关的 tweet。这些 tweet 是以 JSON 格式返回的。如果我们想通过位置知道关于我们的政府主题的讨论范围，可以使用下面的代码片段来获取 tweet：123governmentTopicDiscussionByLocation = jsonResultTweets -&gt; transform&#123;location: $.location,user_id: $.from_user_id_str,date_created:$.created_at,comment:$text&#125; -&gt; group by key = $.location 然后，我们可以使用下面的代码片段将此信息存储到您的 HDFS 中：123governmentTopicDiscussionByLocation Cnt -&gt;write(del(&quot;/user/governmentTopics/governmentTopic_1Tweets.del&quot;, schema =schema &#123; list_of_comma_seperated_json_fields&#125; 其中的 list_of_comma_seperated_json_fields 是一些逗号分隔的字段：location、from_user_id_str 和 created_at。这样就可以通过 Oozie 工作流来运行整个 Jaql 脚本，代码可能类似于以下代码示例：12345678910url = &quot;https://stream.twitter.com/1.1/statuses/filter.json?track=governmentTopic&quot;; jsonResultTweets = read(http(url));jsonResultTweets;governmentTopicDiscussionByLocation = jsonResultTweets -&gt; transform &#123;location: $.location,user_id: $.from_user_id_str,user_name: $.user.name,user_location: $.user.location,date_created: $.created_at,comment: $.text&#125; -&gt; group by key = $.location governmentTopicDiscussionByLocation -&gt; write(del(&quot;/user/governmentTopics/governmentTopic_1Tweets.del&quot;, schema = schema &#123;location,user_id,user_name,user_location,date_created,comment&#125; transform 方法将会清除数据，而 write 方法会将数据保存到 HDFS。要处理流数据或动态数据，需要将此脚本与 Flume 整合，Flume 是 Apache Hadoop 生态系统中的另一个大数据工具。（您可以通过阅读了解有关此 developerWorks 文章中的 Flume 的更多信息，”使用 Flume 部署和管理可扩展的 Web 服务”。）通过使用 R 从 Twitter 中检索数据要使用 R 检索 tweet，需要在您的系统上安装某些软件包。虽然我们可以使用 RStudio，但下面这些步骤显示了如何设置和使用 R 控制台。在 Ubuntu 电脑上，我完成了下面这些步骤来安装必要的 R 软件包：安装这些软件包：12345libcurl4-gnutls-dev libcurl4-nss-dev libcurl4-openssl-dev r-base r-base-devr-cran-rjson 打开 R 控制台，并运行这些命令来安装这些包来访问 Twitter：123install.packages(“twitteR”)install.packages(“ROAuth”)install.packages(“RCurl”) 将这些库加载到您的 R 工作区中：1234rm(list=ls())library(twitteR)library(ROAuth)library(RCurl) 现在，我们可以用下面的 R 脚本对 Twitter 进行身份验证：12345678910111213141516download.file(url=&quot;http://curl.haxx.se/ca/cacert.pem&quot;,destfile=&quot;cacert.pem&quot;)requestURL &lt;- &quot;https://api.twitter.com/oauth/request_token&quot;accessURL &lt;- &quot;https://api.twitter.com/oauth/access_token&quot;authURL &lt;- &quot;https://api.twitter.com/oauth/authorize&quot;consumerKey &lt;- myConsumerKeyFromTwitterconsumerSecret &lt;- myConsumerSeccretFromTwittermyCred &lt;- OAuthFactory$new(consumerKey=consumerKey, consumerSecret=consumerSecret, requestURL=requestURL, accessURL=accessURL, authURL=authURL) accessToken &lt;- myAccessTokenFromTwitteraccessSecret &lt;- myAccessSecretFromTwitter setup_twitter_oauth(consumerKey,consumerSecret,accessToken,accessSecret) 然后，我们可以使用下面的代码片段来获取 tweet：govt_sentiment_data &lt;- searchTwitter(“#keyWord”,since={last_date_pulled}keyWord 是您要分析的政府主题，last_date_pulled 是您最后一次获取 tweet 的日期。如果您想要按固定时间间隔自动流化 Twitter 数据和拉取数据，可以使用以下代码片段替换前面的代码：12govt_sentiment_data &lt;- filterStream( file=&quot;tweets_rstats.json&quot;,track=&quot;#keyWord&quot;, timeout=3600, oauth=myCred) 我们可以用下面的 R 脚本来清理数据：123456789101112131415govt_sentiment_data_txt = govt_sentiment_data$text# remove retweet entitiesgovt_sentiment_data_txt = gsub(“(RT|via)((?:\\b\\W*@\\w+)+)”, “”, tweet_txt)# remove at peoplegovt_sentiment_data_txt = gsub(“@\\w+”, “”, tweet_txt)# remove punctuationgovt_sentiment_data_txt = gsub(“[[:punct:]]”, “”, tweet_txt)# remove numbersgovt_sentiment_data_txt = gsub(“[[:digit:]]”, “”, tweet_txt)# remove html linksgovt_sentiment_data_txt = gsub(“http\\w+”, “”, tweet_txt)# remove unnecessary spacesgovt_sentiment_data_txt = gsub(“[ \t]&#123;2,&#125;”, “”, tweet_txt)govt_sentiment_data_txt = gsub(“^\\s+|\\s+$”, “”, tweet_txt)govt_sentiment_data_txt=gsub(“[^0-9a-zA-Z ,./?&gt;&lt;:;’~`!@#&amp;*’]”,””, tweet_txt) 最后，要将已清理的数据保存到您的 HDFS，可以使用下面的代码片段：1234hdfsFile &lt;- hdfs.file(&quot;/tmp/govt_sentiment_data.txt&quot;, &quot;w&quot;)hdfs.write(govt_sentiment_data_txt, hdfsFile)hdfs.close(hdfsFile)write(govt_sentiment_data, &quot;govt_sentiment_data.txt&quot;) 从 RSS 提要检索数据除了 tweet 之外，我们还想从新闻文章中收集个人意见或观点。对于这种类型的数据，建议您组合使用 Java 和 Rome 工具从 RSS 提要中获取数据。Rome 是一个 Java 库，用于访问和操纵网络上的新闻提要。在本示例中，我们获得了有关新闻文章的以下信息：标题、链接和描述。然后，我们从这些数据点提取我们所需的信息。要确定将要使用的新闻提要，需要使用某种形式的网页排名 技术。该技术被用在搜索算法中，用于确定某一事项在其引用和普及方面的相关性。基本原理是，被外部实体点击或引用的几率越高，优先级就越高，因此就会出现在搜索结果的顶部。下面的 Java 代码标识了一些新闻提要和使用网页排名，以确定它们与我们的数据相关：1234567891011121314151617181920212223242526272829303132333435363738private static void getFeeds(String newsFeedUrlLink)&#123; File f = new File(“newsFeeds.txt”); boolean ok = false; try &#123; URL feedUrl = new URL(newsFeedUrlLink); SyndFeedInput input = new SyndFeedInput(); InputSource source = new InputSource(feedUrl.openStream()); SyndFeed feed = input.build(source); for (Iterator i = feed.getEntries().iterator(); i.hasNext();) &#123; SyndEntry entry = (SyndEntry) i.next(); writeToFile(f,entry); &#125; ok = true; &#125; catch (Exception ex) &#123; ex.printStackTrace(); System.out.println(&quot;ERROR: &quot;+ex.getMessage()); &#125; if (!ok) &#123; System.out.println(); System.out.println(&quot;FeedReader reads and prints any RSS/Atom feed type.&quot;); System.out.println(&quot;The first parameter must be the URL of the feed to read.&quot;); System.out.println(); &#125; &#125; private static void writeToFile(File f, SyndEntry entry) throws IOException &#123; FileWriter fw = new FileWriter(f.getName(),true); BufferedWriter bw = new BufferedWriter(fw); bw.write(entry.getTitle()+”\n”); bw.close(); &#125; 接下来，我们可以使用下面的代码片段将数据存储在我们使用 Twitter 数据创建的 HDFS 文件中。要将此数据添加到我们使用 Twitter 数据创建的 HDFS 文件中，必须修改 hdfs-site.xml 文件中的 dfs.support.append 属性值，因为 HDFS 默认情况下不允许将数据添加到文件。123456789mydata &lt;- readLines(&quot;newsFeeds.txt&quot;)myfile &lt;- hdfs.file(&quot;/tmp/govt_sentiment_data.txt&quot;, &quot;r&quot;)dfserialized &lt;- hdfs.read(myfile)df &lt;- unserialize(dfserialized)hdfs.close(myfile) //write(mydata, file = &quot;/tmp/govt_sentiment_data.txt&quot;,append = TRUE)hdfs.write(mydata, file = &quot;/tmp/govt_sentiment_data.txt&quot;,append = TRUE)government_sentiment_data &lt;- read.hdfs(“/tmp/govt_sentiment_data.txt”) 从移动应用程序中检索数据除了 Twitter 数据和 RSS 提要数据之外，我们还可以从包含个人意见和观点的移动应用程序中收集数据。在本示例中，我假设您创建了一个简单的移动应用程序，该应用程序已安装在允许用户提供关于政府主题或政策的意见的移动设备上。可以将 J2ME 应用程序上传到某个 WAP 服务器，移动设备（甚至是像诺基亚 3310 这样的老款设备）可以从该服务器下载和安装应用程序。用户提供的信息被发送回一个 RDBMS 并进行储存，以供将来分析使用。您可以使用 Sqoop 将数据从 RDBMS 服务器移动到我们的 Hadoop 集群。在 Hadoop 集群上运行 sqoop 脚本的以下行：12sqoop import --options-file dbCredentials.txt --connectjdbc:mysql://217.8.156.117/govt_policy_app --table opinions –-target-dir /tmp \ --append –append 标记告诉 Sqoop 将导入的数据添加到我们已经从以前的数据来源获得的数据集中，该数据集通过 –target-dir 标记来指示。将已收集的数据合并成一个数据源在收集了来自 Twitter 的数据（通过使用 Jaql 或 R）、来自 RSS 提要的数据（通过使用 Java）和来自移动应用程序的数据（通过使用 Sqoop）后，我们会将数据添加到单个 HDFS 文件中。可以通过实现了 Oozie 工作流引擎来自动化这些脚本，并设置命令来按照某个时间间隔运行脚本，或者作为触发事件发生的结果。有关如何设置 Sqoop 和 Oozie 的更多信息，请参阅我的其他 developerWorks 文章，”将 Hadoop 与现有的 RDBMS 相集成”。您可以增强您的 Oozie 工作流程，以便实现减少重复数据的限制，重复数据是整合来自不同来源的数据所导致的。例如，您可能会限制每个话题一个 Twitter 句柄，在您的数据集中，每个观点一个移动号码。在组合数据上执行情感分析在组合数据之后，我们就可以在单个数据源上完成情感分析，这使我们可以获得分析的统一性、一致性和准确性。您可以使用 R、Jaql、Pig 或 Hive 来执行这些分析。Pig 和 Hive 是具有类似 SQL 的语法的语言，运行在 Hadoop 平台上。本例中，我决定用 R 来分析检索数据，因为 R 具有用于图形表示的丰富的内置模型函数和库，比如 ggplot2。要完成情感分析，需要有一个词典或单词列表。字典包括一组描述某一范围内的积极词和消极词的标准单词。词典确定了社交媒体中常常使用的嘲讽词、影射词、俚语、新词汇、字符和表情。这些词汇列表可从互联网上获得，定期更新，并整合到我们的情感分析逻辑中。以下代码利用了检索到的数据，并将它们与我们的单词列表相匹配，以获得积极词和消极词的数量。积极词和消极词的总数差距为我们提供了一个得分，该得分指示了我们的数据对于我们要分析的政府主题是积极的还是消极的。1234sentiment.pos=scan(&apos;/Users/charles/Downloads/r/positive-words.txt&apos;,what=&apos;character&apos;,comment.char=&apos;;&apos;)sentiment.neg=scan(&apos;/Users/charles/Downloads/r/negative-words.txt&apos;,what=&apos;character&apos;,comment.char=&apos;;&apos;)pos.words=c(sentiment.pos,&apos;good&apos;,&apos;reelect&apos;,&apos;accountable&apos;,&apos;stable&apos;)neg.words=c(sentiment.neg,&apos;bad&apos;,&apos;corrupt&apos;,&apos;greedy&apos;,&apos;unstable&apos;) 此外，以下代码表示了情感评分算法：1234567891011121314151617require(plyr)require(stringr)score.sentiment = function(sentences, pos.words, neg.words, .progress=&apos;none&apos;)&#123;sentence = tolower(sentence)word.list = str_split(sentence, &apos;\\s+&apos;)words = unlist(word.list)pos.matches = match(words, pos.words)neg.matches = match(words, neg.words)pos.matches = !is.na(pos.matches)neg.matches = !is.na(neg.matches)score = sum(pos.matches) - sum(neg.matches)return(score)&#125;, pos.words, neg.words, .progress=.progress )scores.df = data.frame(score=scores, text=sentences)return(scores.df)&#125; 然后，我们可以通过使用下面的代码片段，调用情感得分算法函数来计算数据的得分：12require(plyr)opinion.score &lt;- score.sentiment(opinion.txt,pos.words,neg.words,progress=&apos;text&apos;) 最后，我们可以通过使用 R 的内置图表和图形功能，对得分数据执行进一步分析，并通过使用下面的代码片段，绘制一幅图表来显示分数条：123library(&quot;ggplot2&quot;)hist(opinion.scores$score)qplot(opinion.scores$score) 您可以通过使用 BigSheets 进一步地分析数据，BigSheets 由 IBM InfoSphere BigInsights 提供。该工具使得非技术用户可以进行各种分析，并用图表查看数据。有关如何使用 BigSheets 工具的更多信息，请阅读 developerWorks 文章 “适用于普通人的 BigSheets”。结束语大数据工具可以根据来自任何来源或空间的数据，提供不带偏见的洞察，从而制定正确的、准确的决策，并实施这些决策。通过采用大数据工具，比如本文中所描述的那些工具，您可以轻松地实现自己的投资回报。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>情感数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[金融行业大数据用户画像实践]]></title>
    <url>%2F%E9%87%91%E8%9E%8D%E8%A1%8C%E4%B8%9A%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E5%AE%9E%E8%B7%B5.html</url>
    <content type="text"><![CDATA[金融消费者逐渐年轻化，80、90后成为客户主力，他们的消费意识和金融意识正在增强。金融服务正在从以产品为中心，转向以消费者为中心。所有金融行业面对的最大挑战是消费者的消费行为和消费需求的转变，金融企业迫切需要为产品寻找目标客户和为客户定制产品。进入移动互联网时代之后，金融业务地域限制被打破。金融企业没有固定业务区域，金融服务面对所有用户是平的。金融消费者逐渐年轻化，80、90后成为客户主力，他们的消费意识和金融意识正在增强。金融服务正在从以产品为中心，转向以消费者为中心。所有金融行业面对的最大挑战是消费者的消费行为和消费需求的转变，金融企业迫切需要为产品寻找目标客户和为客户定制产品。 一、用户画像背后的原因1、金融消费行为的改变，企业无法接触到客户80后、90后总计共有3.4亿人口，并日益成为金融企业主要的消费者，但是他们的金融消费习惯正在改变，他们不愿意到金融网点办理业务，不喜欢被动接受金融产品和服务。年轻人将主要的时间都消费在移动互联网，消费在智能手机上。平均每个人，每天使用智能手机的时间超过了3小时，年轻人可能会超过4个小时。浏览手机已经成为工作和睡觉之后的，人类第三大生活习惯，移动APP也成为所有金融企业的客户入口、服务入口、消费入口、数据入口。金融企业越来越难面对面接触到年轻人，无法像过去一样，从对话中了解年轻人的想法，了解年轻人金融产品的需求。 2、消费者需求出现分化，需要寻找目标客户客户群体正在出现分化，市场上很少有一种产品和一种金融服务可以满足所有用户的需求。金融产品也需要进行细化，为不同客户提供不同产品。金融企业面对的客户群体基数很大，有的客户高风险偏好高，希望高风险高收益;有的客户风险偏好低，希望稳健收益;有的客户金融理财意识低，只需服务较好即可;有的客户完全没有主意，你说是啥就是啥;有的客户注重体验，有的客户注重实惠，有的客户注重品牌，有的客户注重风险等等。不同年龄，不同收入，不同职业，不同资产的客户对金融产品的需求都不尽相同。金融企业需要为不同的客户定制产品，满足不同客户的需要。对于金融企业，理财和消费是主要的业务需求。客户消费习惯的改变，企业无法接触到客户，无法了解客户需求;客户需求的分化，企业需要细分客户，为目标客户开发设计产品。金融企业需要借助于户画像，来了解客户，找到目标客户，触达客户。 二、用户画像的目的用户画像是在解客户需求和消费能力，以及客户信用额度的基础上，寻找潜在产品的目标客户，并利用画像信息为客户开发产品。提到用户画像，很多厂商都会提到360度用户画像，其实经常360度客户画像是一个广告宣传用语，根本不存数据可以全面描述客户，透彻了解客户。人是非常复杂的动物，信息纬度非常复杂，仅仅依靠外部信息来刻画客户内心需要根本不可能。用户画像一词具有很重的场景因素，不同企业对于用户画像有着不同对理解和需求。举个例子，金融行业和汽车行业对于用户画像需求的信息完全不一样，信息纬度也不同，对画像结果要求也不同。每个行业都有一套适合自己行业的用户画像方法，但是其核心都是为客户服务，为业务场景服务。用户画像本质就是从业务角度出发对用户进行分析，了解用户需求，寻找目标客户。另外一个方面就是，金融企业利用统计的信息，开发出适合目标客户的产品。从商业角度出发的用户画像对企业具有很大的价值，用户画像目的有两个。一个是业务场景出发，寻找目标客户。另外一个就是，参考用户画像的信息，为用户设计产品或开展营销活动。 三、用户画像工作坚持的原则市场上用户画像的方法很多，许多企业也提供用户画像服务，将用户画像提升到很有逼格一件事。金融企业是最早开始用户画像的行业，由于拥有丰富的数据，金融企业在进行用户画像时，对众多纬度的数据无从下手，总是认为用户画像数据纬度越多越好，画像数据越丰富越好，某些输入的数据还设定了权重甚至建立了模型，搞的用户画像是一个巨大而负责的工程。但是费力很大力气进行了画像之后，却发现只剩下了用户画像，和业务相聚甚远，没有办法直接支持业务运营，投入精力巨大但是回报微小，可以说是得不偿失，无法向领导交代。事实上，用户画像涉及数据的纬度需要业务场景结合，既要简单干练又要和业务强相关，既要筛选便捷又要方便进一步操作。用户画像需要坚持三个原则，分别是人口属性和信用信息为主，强相关信息为主，定性数据为主。下面就分别展开进行解释和分析。 1、信用信息和人口属性为主描述一个用户的信息很多，信用信息是用户画像中重要的信息，信用信息是描述一个人在社会中的消费能力信息。任何企业进行用户画像的目的是寻找目标客户，其必须是具有潜在消费能力的用户。信用信息可以直接证明客户的消费能力，是用户画像中最重要和基础的信息。一句戏言，所有的信息都是信用信息就是这个道理。其包含消费者工作、收入、学历、财产等信息。定位完目标客户之后，金融企业需要触达客户，人口属性信息就是起到触达客户的作用，人口属性信息包含姓名、性别，电话号码，邮件地址，家庭住址等信息。这些信息可以帮助金融企业联系客户，将产品和服务推销给客户。 2、采用强相关信息，忽略弱相关信息我们需要介绍一下强相关信息和弱相关信息。强相关信息就是同场景需求直接相关的信息，其可以是因果信息，也可以是相关程度很高的信息。如果定义采用0到1作为相关系数取值范围的化，0.6以上的相关系数就应该定义为强相关信息。例如在其他条件相同的前提下，35岁左右人的平均工资高于平均年龄为30岁的人，计算机专业毕业的学生平均工资高于哲学专业学生，从事金融行业工作的平均工资高于从事纺织行业的平均工资，上海的平均工资超过海南省平均工资。从这些信息可以看出来人的年龄、学历、职业、地点对收入的影响较大，同收入高低是强相关关系。简单的将，对信用信息影响较大的信息就是强相关信息，反之则是弱相关信息。用户其他的信息，例如用户的身高、体重、姓名、星座等信息，很难从概率上分析出其对消费能力的影响，这些弱相关信息，这些信息就不应该放到用户画像中进行分析，对用户的信用消费能力影响很小，不具有较大的商业价值。用户画像和用户分析时，需要考虑强相关信息，不要考虑弱相关信息，这是用户画像的一个原则。 3、将定量的信息归类为定性的信息用户画像的目的是为产品筛选出目标客户，定量的信息不利于对客户进行筛选，需要将定量信息转化为定性信息，通过信息类别来筛选人群。例如可以将年龄段对客户进行划分，18岁-25岁定义为年轻人，25岁-35岁定义为中青年，36-45定义为中年人等。可以参考个人收入信息，将人群定义为高收入人群，中等收入人群，低收入人群。参考资产信息也可以将客户定义为高、中、低级别。定性信息的类别和方式方法，金融可以从自身业务出发，没有固定的模式。将金融企业各类定量信息，集中在一起，对定性信息进行分类，并进行定性化，有利与对用户进行筛选，快速定位目标客户，是用户画像的另外一个原则。 四、用户画像的方法介绍，不要太复杂金融企业需要结合业务需求进行用户画像，从实用角度出发，我们可以将用户画像信息分成五类信息。分别是人口属性，信用属性，消费特征，兴趣爱好，社交属性。它们基本覆盖了业务需求所需要的强相关信息，结合外部场景数据将会产生巨大的商业价值。我们先了解下用户画像的五大类信息的作用，以及涉及的强相关信息。特别复杂的用户画像纬度例如八个纬度，十个纬度信息都不利于商业应用，不建议金融企业进行采用，其他具有价值的信息，基本上都可以归纳到这五个纬度。金融企业达到其商业需求，从这五个纬度信息进行应用就可以了，不需要过于复杂用户画像这个工作，同时商业意义也不太大。 1、人口属性：用于描述一个人基本特征的信息，主要作用是帮助金融企业知道客户是谁，如何触达用户。姓名，性别，年龄，电话号码，邮箱，家庭住址都属于人口属性信息。 2、信用属性：用于描述用户收入潜力和收入情况，支付能力。帮助企业了解客户资产情况和信用情况，有利于定位目标客户。客户职业、收入、资产、负债、学历、信用评分等都属于信用信息。 3、消费特征：用于描述客户主要消费习惯和消费偏好，用于寻找高频和高价值客户。帮助企业依据客户消费特点推荐相关金融产品和服务，转化率将非常高。为了便于筛选客户，可以参考客户的消费记录将客户直接定性为某些消费特征人群，例如差旅人群，境外游人群，旅游人群，餐饮用户，汽车用户，母婴用户，理财人群等。 4、兴趣爱好：用于描述客户具有哪方面的兴趣爱好，在这些兴趣方面可能消费偏好比较高。帮助企业了解客户兴趣和消费倾向，定向进行活动营销。兴趣爱好的信息可能会和消费特征中部分信息有重复，区别在于数据来源不同。消费特征来源于已有的消费记录，但是购买的物品和服务不一定是自己享用，但是兴趣爱好代表本人的真实兴趣。例如户外运动爱好者，旅游爱好者，电影爱好者，科技发烧友，健身爱好者，奢侈品爱好者等。兴趣爱好的信息可能来源于社交信息和客户位置信息。 5、社交信息：用于描述用户在社交媒体的评论，这些信息往往代表用户内心的想法和需求，具有实时性高，转化率高的特点。例如客户询问上海哪里好玩?澳大利亚墨尔本的交通?房屋贷款哪家优惠多?那个理财产品好?这些社交信息都是代表客户多需求，如果企业可以及时了解到，将会有助于产品推广。这些用户画像信息归类基本覆盖了业务需求和产品开发所需要的信息，需要对这些信息进行进行整理和处理。根据业务场景，将定量的数据转化为定性的数据，并将强相关数据进行整理。 五、金融企业用户画像的基本步骤如下参考金融企业的数据类型和业务需求，可以将金融企业用户画像工作进行细化。基本上从数据集中到数据处理，从强相关数据到定性分类数据，从引入外部数据到依据业务场景进行筛选目标用户。 1)画像相关数据的整理和集中金融企业内部的信息分布在不同的系统中，一般情况下，人口属性信息主要集中在客户关系管理系统，信用信息主要集中在交易系统和产品系统之中，也集中在客户关系管理系统中，消费特征主要集中在渠道和产品系统中。兴趣爱好和社交信息需要从外部引入，例如客户的行为轨迹可以代表其兴趣爱好和品牌爱好，移动设备到位置信息可以提供较为准确的兴趣爱好信息。社交信息，可以借助于金融行业自身的文本挖掘能力进行采集和分析，也是可以借助于厂商的技术能力在社交网站上直接获得。社交信息往往是实时信息，商业价值较高，转化率也较高，是大数据预测方面的主要信息来源。例如用用户在社交网站上提出罗马哪里好玩的问题，就代表用户未来可能有出国旅游的需求;如果客户在对比两款汽车的优良，客户购买汽车的可能性就较大。金融企业可以及时介入，为客户提供金融服务。客户画像数据主要分为五类，人口属性、信用信息、消费特征、兴趣爱好、社交信息。这些数据都分布在不同的信息系统，金融企业都上线了数据仓库(DW)，所有画像相关的强相关信息都可以从数据仓库里面整理和集中，并且依据画像商业需求，利用跑批作业，加工数据，生成用户画像的原始数据。数据仓库成为用户画像数据的主要处理工具，依据业务场景和画像需求将原始数据进行分类、筛选、归纳、加工等，生成用户画像需要的原始数据。用户画像的纬度信息不是越多越好，只需要找到可五大类画像信息强相关信息，同业务场景强相关信息，同产品和目标客户强相关信息即可。根本不存在360度的用户画像信息，也不存在丰富的信息可以完全了解客户，另外数据的实效性也要重点考虑。 2)找到同业务场景强相关数据依据用户画像的原则，所有画像信息应该是5大分类的强相关信息。强相关信息是指同业务场景强相关信息，可以帮助金融行业定位目标客户，了解客户潜在需求，开发需求产品。只有强相关信息才能帮助金融企业有效结合业务需求，创造商业价值。例如姓名、手机号、家庭地址就是能够触达客户的强人口属性信息，收入、学历、职业、资产就是客户信用信息的强相关信息。差旅人群、境外游人群、汽车用户、旅游人群、母婴人群就是消费特征的强相关信息。摄影爱好者、游戏爱好者、健身爱好者、电影人群、户外爱好者就是客户兴趣爱好的强相关信息。社交媒体上发表的旅游需求，旅游攻略，理财咨询，汽车需求，房产需求等信息代表了用户的内心需求，是社交信息场景应用的强相关信息。金融企业内部信息较多，在用户画像阶段不需要对所有信息都采用，只需要采用同业务场景和目标客户强相关的信息即可，这样有助于提高产品转化率，降低ROI，有利于简单找到业务应用场景，在数据变现过程中也容易实现。千万不要将用户画像工作搞的过于复杂，同业务场景关系不大，这样就让很多金融企业特别是领导失去用户画像的兴趣，看不到用户画像的商业，不愿意在大数据领域投资。为企业带来商业价值才是用户画像工作的主要动力和主要目的。 3)对数据进行分类和标签化(定量to定性)金融企业集中了所有信息之后，依据业务需求，对信息进行加工整理，需要对定量的信息进行定性，方便信息分类和筛选。这部分工作建议在数据仓库进行，不建议在大数据管理平台(DMP)里进行加工。定性信息进行定量分类是用户画像的一个重要工作环节，具有较高的业务场景要求，考验用户画像商业需求的转化。其主要目的是帮助企业将复杂数据简单化，将交易数据定性进行归类，并且融入商业分析的要求，对数据进行商业加工。例如可以将客户按照年龄区间分为学生，青年，中青年，中年，中老年，老年等人生阶段。源于各人生阶段的金融服务需求不同，在寻找目标客户时，可以通过人生阶段进行目标客户定位。企业可以利用客户的收入、学历、资产等情况将客户分为低、中、高端客户，并依据其金融服务需求，提供不同的金融服务。可以参考其金融消费记录和资产信息，以及交易产品，购买的产品，将客户消费特征进行定性描述，区分出电商客户，理财客户，保险客户，稳健投资客户，激进投资客户，餐饮客户，旅游客户，高端客户，公务员客户等。利用外部的数据可以将定性客户的兴趣爱好，例如户外爱好者，奢侈品爱好者，科技产品发烧友，摄影爱好者，高端汽车需求者等信息。将定量信息归纳为定性信息，并依据业务需求进行标签化，有助于金融企业找到目标客户，并且了解客户的潜在需求，为金融行业的产品找到目标客户，进行精准营销，降低营销成本，提高产品转化率。另外金融企业还可以依据客户的消费特征、兴趣爱好、社交信息及时为客户推荐产品，设计产品，优化产品流程。提高产品销售的活跃率，帮助金融企业更好地为客户设计产品。 4)依据业务需求引入外部数据利用数据进行画像目的主要时为业务场景提供数据支持，包括寻找到产品的目标客户和触达客户。金融企业自身的数据不足以了解客户的消费特征、兴趣爱好、社交信息。金融企业可以引入外部信息来丰富客户画像信息，例如引入银联和电商的信息来丰富消费特征信息，引入移动大数据的位置信息来丰富客户的兴趣爱好信息，引入外部厂商的数据来丰富社交信息等。外部信息的纬度较多，内容也很丰富，但是如何引入外部信息是一项具有挑战的工作。外部信息在引入时需要考虑几个问题，分别是外部数据的覆盖里，如何和内部数据打通，和内部信息的匹配率，以及信息的相关程度，还有数据的鲜活度，这些都是引入外部信息的主要考虑纬度。外部数据鱼龙混杂，数据的合规性也是金融企业在引入外部数据时的一个重要考虑，敏感的信息例如手机号、家庭住址、身份证号在引入或匹配时都应该注意隐私问题，基本的原则是不进行数据交换，可以进行数据匹配和验证。外部数据不会集中在某一家，需要金融企业花费大量时间进行寻找。外部数据和内部数据的打通是个很复杂的问题，手机号/设备号/身份证号的MD5数值匹配是一种好的方法，不涉及隐私数据的交换，可以进行唯一匹配。依据行业内部的经验，没有一家企业外部数据可以满足企业要求，外部数据的引入需要多方面数据。一般情况下，数据覆盖率达到70%以上，就是一个非常高的覆盖率。覆盖率达到20%以上就可以进行商业应用了。金融行业外部数据源较好合作方有银联、芝麻信用、运营商、中航信、腾云天下、腾讯、微博、前海征信，各大电商平台等。市场上数据提供商已经很多，并且数据质量都不错，需要金融行业一家一家去挖掘，或者委托一个厂商代理引入也可以。独立第三方帮助金融行业引入外部数据可以降低数据交易成本，同时也可以降低数据合规风险，是一个不错得尝试。另外各大城市和区域的大数据交易平台，也是一个较好的外部数据引入方式。 5)按照业务需求进行筛选客户(DMP的作用)用户画像主要目的是让金融企业挖掘已有的数据价值，利用数据画像技术寻找到目标客户和客户到潜在需求，进行产品推销和设计改良产品。用户画像从业务场景出发，实现数据商业变现重要方式。用户画像是数据思维运营过程中到一个重要闭环，帮助金融企业利用数据进行精细化运营和市场营销，以及产品设计。用户画像就是一切以数据商业化运营为中心，以商业场景为中，帮助金融企业深度分析客户，找到目标客户。DMP(大数据管理平台)在整个用户画像过程中起到了一个数据变现的作用。从技术角度来讲，DMP将画像数据进行标签化，利用机器学习算法来找到相似人群，同业务场景深度结合，筛选出具有价值的数据和客户，定位目标客户，触达客户，对营销效果进行记录和反馈。大数据管理平台DMP过去主要应用在广告行业，在金融行业应用不多，未来会成为数据商业应用的主要平台。DMP可以帮助信用卡公司筛选出未来一个月可能进行分期付款的客户，电子产品重度购买客户，筛选出金融理财客户，筛选出高端客户(在本行资产很少，但是在他行资产很多)，筛选出保障险种，寿险，教育险，车险等客户，筛选出稳健投资人，激进投资人，财富管理等方面等客户，并且可以触达这些客户，提高产品转化率，利用数据进行价值变现。DMP还可以了解客户的消费习惯、兴趣爱好、以及近期需求，为客户定制金融产品和服务，进行跨界营销。利用客户的消费偏好，提高产品转化率，提高用户黏度。DMP还作为引入外部数据的平台，将外部具有价值的数据引入到金融企业内部，补充用户画像数据，创建不同业务应用场景和商业需求，特别是移动大数据、电商数据、社交数据的应用，可以帮助金融企业来进行数据价值变现，让用户画像离商业应用更加近一些，体现用户画像的商业价值。用户画像的关键不是360度分析客户，而是为企业带来商业价值，离开了商业价值谈用户画像就是耍流氓。金融企业用户画像项目出发点一定要从业务需求出发，从强相关数据出发，从业务场景应用出发。用户画像的本质就是深度分析客户，掌握具有价值数据，找到目标客户，按照客户需求来定制产品，利用数据实现价值变现。 六、金融行业用户画像实践1)银行用户画像实践介绍银行具有丰富的交易数据、个人属性数据、消费数据、信用数据和客户数据，用户画像的需求较大。但是缺少社交信息和兴趣爱好信息。到银行网点来办业务的人年纪偏大，未来消费者主要在网上进行业务办理。银行接触不到客户，无法了解客户需求，缺少触达客户的手段。分析客户、了解客户、找到目标客户、为客户设计其需要的产品，成了银行进行用户画像的主要目的。银行的主要业务需求集中在消费金融、财富管理、融资服务，用户画像要从这几个角度出发，寻找目标客户。银行的客户数据很丰富，数据类型和总量较多，系统也很多。可以严格遵循用户画像的五大步骤。先利用数据仓库进行数据集中，筛选出强相关信息，对定量信息定性化，生成DMP需要的数据。利用DMP进行基础标签和应用定制，结合业务场景需求，进行目标客户筛选或对用户进行深度分析。同时利用DMP引入外部数据，完善数据场景设计，提高目标客户精准度。找到触达客户的方式，对客户进行营销，并对营销效果进行反馈，衡量数据产品的商业价值。利用反馈数据来修正营销活动和提高ROI。形成市场营销的闭环，实现数据商业价值变现的闭环。另外DMP还可以深度分析客户，依据客户的消费特征、兴趣爱好、社交需求、信用信息来开发设计产品，为金融企业的产品开发提供数据支撑，并为产品销售方式提供场景数据。简单介绍一些DMP可以做到的数据场景变现。A：寻找分期客户利用银联数据+自身数据+信用卡数据，发现信用卡消费超过其月收入的用户，推荐其进行消费分期。B：寻找高端资产客户利用银联数据+移动位置数据(别墅/高档小区)+物业费代扣数据+银行自身数据+汽车型号数据，发现在银行资产较少，在其他行资产较多的用户，为其提供高端资产管理服务C：需找理财客户利用自身数据(交易+工资)+移动端理财客户端/电商活跃数据。发现客户将工资/资产转到外部，但是电商消费不活跃客户，其互联网理财可能性较大，可以为其提供理财服务，将资金留在本行。D：寻找境外游客户利用自身卡消费数据+移动设备位置信息+社交好境外强相关数据(攻略，航线，景点，费用)，寻找境外游客户为其提供金融服务。E：寻找贷款客户：利用自身数据(人口属性+信用信息)+移动设备位置信息+社交购房/消费强相关信息，寻找即将购车/购房的目标客户，为其提供金融服务(抵押贷款/消费贷款)。 2)保险行业用户画像实践保险行业的产品是一个长周期产品，保险客户再次购买保险产品的转化率很高，经营好老客户是保险公司一项重要任务。保险公司内部的交易系统不多，交易方式不是很复杂，数据主要集中在产品系统和交易系统之中，客户关系管理系统中也包含丰富了信息，但是数据集中在很多保险公司还没有完成，数据仓库建设可能需要在用户画像建设前完成。保险公司主要数据有人口属性信息，信用信息，产品销售信息，客户家人信息。缺少兴趣爱好、消费特征、社交信息等信息。保险产品主要有寿险，车险，保障，财产险，意外险，养老险，旅游险。保险行业DMP用户画像的业务场景都是围绕保险产品进行的，简单的应用场景可以是。A：依据自身数据(个人属性)+外部养车App活跃情况，为保险公司找到车险客户B：依据自身数据(个人属性)+移动设备位置信息—户外运动人群，为保险企业找到商旅人群，推销意外险和保障险。C：依据自身数据(家人数据)+人生阶段信息，为用户推荐理财保险，寿险，保障保险，养老险，教育险D：依据自身数据+外部数据，为高端人士提供财产险和寿险 3)证券行业用户画像2015年4月13日，一码通实施之后，证券行业面临了互联网证券平台的强力竞争，依据TalkingData发布的金融App排行榜，移动互联网证券App，排名前5位的证券类App，只有一家传统券商华泰证券。排名第一的互联网券商同化顺覆装机量是排名第一传统券商的6倍，前三名的互联券商总体覆盖用户接近6000万用户。用户总数还在不断增加。传统证券行业现在面临的主要挑战是用户交易账户的争夺，证券行业如何增加新用户?如何留住用户?如何提高证券行业用户的活跃?如何提高单个客户的收入?是证券行业主要的业务需求。证券行业拥有的数据类型有个人属性信息例如用户名称，手机号码，家庭地址，邮件地址等。证券公司还拥有交易用户的资产和交易纪录，同时还拥有用户收益数据，利用这些数据和外部数据，证券公司可以利用数据建立业务场景，筛选目标客户，为用户提供适合的产品，同时提高单个客户收入。证券公司可以利用用户画像数据来进行产品设计，下面举几个例子，看看用户画像和用户分析来帮助证券公司创造商业价值。 七、外部数据介绍金融企业内部数据主要集中在个人属性，信用属性和消费特征上，缺少社交属性和兴趣偏好等信息，这些信息可以通过第三方获得。社交数据就是客户在社交媒体上发表的言论和行为，可以是评论，文章，图片，甚至可以是表情符号，音频和视频。社交数据可以依靠第三方平台，在社交网站上利用爬虫技术进行获得(Spider)。社交数据的打通是一个挑战，如果能够客户的授权最好，金融企业就可以将社交数据纳入到用户画像之中。社交数据具有实时和反映内心需要的特点，富国银行已经将社交数据作为分析客户需求的一个重要数据纬度。例如如果某一个客户在社交媒体上发表了一个问题，罗马有哪些好玩的地方，金融企业就会推测客户可能近期会有出境游的计划，就会向客户推销一些旅游相关产品。社交媒体数据正在成为金融企业积极争取获得的数据，除了利用网络爬虫技术到微博上进行数据采集之外，金融企业自身网站上到文本数据采集和呼叫中心(callcenter)纪录的信息都可以进行文本挖掘。通过客户编号，进行打通，将其补充到客户画像之中。社交数据需要通过数据挖掘将其定义为结构化数据，并且同业务场景、客户需求向结合，清晰进行分类。例如将母婴论坛发言活跃的用户定义为潜在教育需求客户，将学生论坛活跃的客户定义为学区房需要客户，将境外自助游论坛上活跃的客户定义为境外旅游客户，将雪球上活跃的客户定义为理财客户等。金融企业完全可以从社交数据中挖掘出客户近期的消费需求，及时进行市场营销和定制产品。兴趣爱好数据可以借助于移动大数据位置信息获得，客户手机设备的位置轨迹信息可以揭示客户喜欢何种品牌，喜欢吃辣还是吃火锅，客户喜欢旅游还是喜欢宅在家里，客户喜欢看电影还是喜欢运动。客户喜欢中档品牌还是高档品牌，客户喜欢喝茶还是喝咖啡。移动手机上App的安装情况和活动频次一样可以揭示客户的兴趣和爱好。同时移动大数据进行加工之后还可以告诉金融企业，客户近期的需求是买车还是买房。外部数据引入过程中，金融企业面临的巨大挑战是外部数据的覆盖率，如何打通内外部数据，外部数据同内部客户的匹配率，外部数据同业务的相关度，外部数据的活跃程度等。用户画像平台(DMP)可以通过技术手段将外部数据引入到金融企业内部，建立标准的标签体系，提供灵活的用户画像方式，按照业务场景进行筛选客户。 八、移动大数据的商业价值移动互联网时代，移动大数据具有较高的商业价值。如果一个用户不喜欢一个App，其不会装在手机上。客户经常使用的App可以推测用户的兴趣爱好和消费偏好。另外移动设备的位置信息可以帮助金融企业了解客户行为轨迹、兴趣爱好、品牌偏好和消费需求。 1)移动App提供一切服务，App可以反映用户喜好智能手机上安装的App正在代替PC互联网为所有客户提供服务，清晨起床可以看看墨迹天气，了解一下今天的天气情况。出门时可以通过嘀嘀打车来预定出租车，安排出行。或者通过百度地图来了解路况信息，决定进行从哪条路到公司。快到中午时，可以通过饿了吗或者百度外卖预定午餐，如果想出去吃饭可以利用大众点评订餐和买单。中午可以利用携程App预定家庭旅行机票和酒店，还可以将通过App看看理财产品。如果需要看电影，可以通过格瓦拉来预定要电影票，如果需要看医生，可以通过微医网预约医生。晚上可以通过淘宝来购物，通过学习宝来监督子女教育等。可以看出移动App已经可以满足人们大部分生活需要，提供了人们的衣食住行、教育、医疗、旅游、金融等服务。移动App包围了人们的日常生活，成为人们消费的主要场所。智能手机上App使用的频率，可以代表用户的喜好。例如喜欢理财的客户，其智能手机上一定会安装理财App，并经常使用;母婴人群也会安装和母婴相关的App，频繁使用;商旅人群使用商旅App的频率一定会高于其他移动用户。80后、90后的消费行为将会以移动互联网为主，App的安装和活跃数据更加能够反应出年轻人的消费偏好。 2)智能设备的位置信息，商业价值广大智能手机设备的位置信息代表了消费者的位置轨迹，这个轨迹可以推测出消费者的消费偏好和习惯。在美国，移动设备位置信息的商业化较为成熟，GPS数据正在帮助很多企业进行数据变现，提高社会运营效率。在中国，移动大数据的商业应用刚刚开始，在房地产业、零售行业、金融行业、市场分析等领域取得了一些效果。移动大数据中的位置信息代表了用户轨迹，商业应用较早。2014年，美国移动设备位置信息的市场规模接近1000亿美金。但中国移动设备位置信息的商业应用才刚刚开始。目前主要的应用在互联网金融的反欺诈领域。线上的欺诈行为具有较高的隐蔽性，很难识别和侦测。P2P贷款用户很大一部分来源于线上，因此恶意欺诈事件发生在线上的风险远远大于线下。中国的很多数据处于封闭状态，P2P公司在客户真实信息验证方面面临较大的挑战。移动大数据可以验证P2P客户的居住地点，例如某个客户在利用手机申请贷款时，填写自己居住地是上海。但是P2P企业依据其提供的手机设备信息，发现其过去三个月从来没有居住在上海，这个人提交的信息可能是假信息，发生恶意欺诈的风险较高。移动设备的位置信息可以辨识出设备持有人的居住地点，帮助P2P公司验证贷款申请人的居住地。借款用户的工作单位是用户还款能力的强相关信息，具有高薪工作的用户，其贷款信用违约率较低。这些客户成为很多贷款平台积极争取的客户，也是恶意欺诈团伙主要假冒的客户。某个用户在申请贷款时，如果声明自己是工作在上海陆家嘴金融企业的高薪人士，其贷款审批会很快并且额度也会较高。但是P2P公司利用移动大数据，发现这个用户在过去的三个月里面，从来没有出现在陆家嘴，大多数时间在城乡结合处活动，那么这个用户恶意欺诈的可能性就较大。移动大数据可以帮助P2P公司在一定程度上来验证贷款用户真实工作地点，降低犯罪分子利用高薪工作进行恶意欺诈的风险。P2P企业可以利用移动设备的位置信息，了解过去3个月用户的行为轨迹。如果某个用户经常在半夜2点出现在酒吧等危险区域，并且经常有飙车行为，这个客户定义成高风险客户的概率就较高。移动App的使用习惯和某些高风险App也可以帮助P2P企业识别出用户的高风险行为。如果用户经常在半夜2点频繁使用App，其成为高风险客户的概率就较大。移动大数据在预防互联网恶意欺诈和高风险客户识别方面，已经有了成熟的应用场景。前海征信、宜信、聚信立、闪银已经开始利用TalkingData的数据，预防互联网恶意欺诈和识别高风险客户，并取得了较好的效果。移动大数据应用场景正在被逐步挖掘出来，未来移动大数商业应用将更加广阔。用户画像是大数据商业应用的重要领域，其实并没有多么复杂，只要掌握用户画像的原则和方法，以及实施步骤。结合金融企业的业务场景，用户画像可以帮助金融企业创造商业价值，实现大数据直接变现。文章来源]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>用户画像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD转DataFrame的一道面试题]]></title>
    <url>%2FRDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%80%E9%81%93%E9%9D%A2%E8%AF%95%E9%A2%98.html</url>
    <content type="text"><![CDATA[题目现在在我们HDFS文件系统上面 存了一个文件，该文件格式是 .txt文件格式，要求把这个文件格式转换成为parquet文件格式 :解题思路:1）先读取文件生成一个RDD2）把RDD转换成为一个DataFrame，RDD[Person].toDF3) 写数据，指定文件格式就可以了！！代码实现 :12345678910val conf = new SparkConf().setMaster("local").setAppName("DataFrameReflection") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) import sqlContext.implicits._ val peopelRDD: RDD[People] = sc.textFile("hdfs://hadoop01:9000/resources/people.txt") .map(line =&gt; People(line.split(",")(0),line.split(",")(1).trim.toInt)) val df = peopelRDD.toDF() df.write.format("parquet").save("hdfs://hadoop01:9000/test/")]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>面试题</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD转DataFrame的两种方法]]></title>
    <url>%2FRDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95.html</url>
    <content type="text"><![CDATA[介绍一下Spark将RDD转换成DataFrame的两种方式。 通过是使用case class的方式，不过在scala 2.10中最大支持22个字段的case class,这点需要注意 是通过spark内部的StructType方式，将普通的RDD转换成DataFrame 装换成DataFrame后，就可以使用SparkSQL来进行数据筛选过滤等操作 方法一123456789101112131415161718192021import org.apache.spark.rdd.RDDimport org.apache.spark.sql.SQLContextimport org.apache.spark.&#123;SparkConf, SparkContext&#125;//需要提前知道列名及类型case class People(var name: String,var age:Int)object DataFrameReflection &#123; def main(args:Array[String]):Unit = &#123; val conf = new SparkConf().setMaster("local").setAppName("DataFrameReflection") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) import sqlContext.implicits._ val peopelRDD: RDD[People] = sc.textFile("people.txt") .map(line =&gt; People(line.split(",")(0),line.split(",")(1).trim.toInt)) val df = peopelRDD.toDF() df.createOrReplaceTempView("people") sqlContext.sql("select * from people").show() &#125;&#125; 方法二12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import org.apache.spark.rdd.RDDimport org.apache.spark.sql.types.&#123;IntegerType, StringType, StructField, StructType&#125;import org.apache.spark.sql.&#123;DataFrame, Row, SQLContext&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;object DataFrameProgrammatically &#123; def main(args:Array[String]): Unit = &#123; val conf = new SparkConf().setMaster("local").setAppName("DataFrameProgrammatically") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) //读取文件 val rdd: RDD[String] = sc.textFile("people.txt") /** * 得到 rowRDD */ val rowRDD: RDD[Row] = rdd.map(line =&gt; &#123; val fields = line.split(",") Row(fields(0), fields(1).trim.toInt) &#125;) /** * 得到structType */ val structType = StructType( StructField("name",StringType,true) :: StructField("age",IntegerType,true) :: Nil ) /** * rowRDD:RDD[Row] * schema: StructType */ val df: DataFrame = sqlContext.createDataFrame(rowRDD,structType) df.createOrReplaceTempView("people") sqlContext.sql("select * from people").show()//// /**// * 官网schema实现方法// */// val schemaString = "name age"// val fields = schemaString.split(" ")// .map(fieldName =&gt; StructField(// fieldName,StringType,nullable = true// ))// val schema = StructType(fields) &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>RDD</tag>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java单例模式]]></title>
    <url>%2FJava%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[饿汉式1234567891011//饿汉式单例类，在类初始化时，已经自行实例化public class Singleton1 &#123; //私有的默认构造方法 private Singleton1()&#123;&#125; //已经自行实例化 private static final Singleton1 single = new Singleton1() ; //静态工厂方法 private static Singleton1 getInstance()&#123; return single ; &#125;&#125; 懒汉式单例类 1234567891011121314//懒汉式单例类，在第一次调用的时候实例化public class Singleton2 &#123; //私有的默认构造方法 private Singleton2()&#123;&#125; //注意，这里没有final private static Singleton2 single = null ; //静态工厂方法，如果不加synchronized线程不安全 public synchronized static Singleton2 getInstance() &#123; if(single == null )&#123; single = new Singleton2() ; &#125; return single ; &#125;&#125; 登记式单例类 123456789101112131415161718192021222324252627282930313233343536public class RegistSingleton &#123; //用ConcurrentHashMap来维护映射关系，这是线程安全的 public static final Map&lt;String,Object&gt; REGIST=new ConcurrentHashMap&lt;String, Object&gt;(); static &#123; //把RegistSingleton自己也纳入容器管理 RegistSingleton registSingleton=new RegistSingleton(); REGIST.put(registSingleton.getClass().getName(),registSingleton); &#125; private RegistSingleton()&#123;&#125; public static Object getInstance(String className)&#123; //如果传入的类名为空，就返回RegistSingleton实例 if(className==null) className=RegistSingleton.class.getName(); //如果没有登记就用反射new一个 if (!REGIST.containsKey(className))&#123; //没有登记就进入同步块 synchronized (RegistSingleton.class)&#123; //再次检测是否登记 if (!REGIST.containsKey(className))&#123; try &#123; //实例化对象 REGIST.put(className,Class.forName(className).newInstance()); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; //返回单例 return REGIST.get(className); &#125;&#125; 总结java中单例模式是一种常见的设计模式，单例模式分三种：懒汉式单例、饿汉式单例、登记式单例三种。单例模式有一下特点：1、单例类只能有一个实例。2、单例类必须自己自己创建自己的唯一实例。3、单例类必须给所有其他对象提供这一实例。单例模式确保某个类只有一个实例，而且自行实例化并向整个系统提供这个实例。在计算机系统中，线程池、缓存、日志对象、对话框、打印机、显卡的驱动程序对象常被设计成单例。这些应用都或多或少具有资源管理器的功能。每台计算机可以有若干个打印机，但只能有一个Printer Spooler，以避免两个打印作业同时输出到打印机中。每台计算机可以有若干通信端口，系统应当集中管理这些通信端口，以避免一个通信端口同时被两个请求同时调用。总之，选择单例模式就是为了避免不一致状态，避免政出多头。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经验之谈]]></title>
    <url>%2F%E7%BB%8F%E9%AA%8C%E4%B9%8B%E8%B0%88.html</url>
    <content type="text"><![CDATA[1、重构是程序员的主力技能。2、工作日志能提升脑容量。3、先用profiler调查，才有脸谈优化。4、注释贵精不贵多。杜绝大姨妈般的“例注”。漫山遍野的碎碎念注释，实际就是背景噪音。5、普通程序员+google=超级程序员。6、单元测试总是合算的。7、不要先写框架再写实现。最好反过来，从原型中提炼框架。8、代码结构清晰，其它问题都不算事儿。9、好的项目作风硬派，一键测试，一键发布，一键部署；烂的项目生性猥琐，口口相传，不立文字，神神秘秘。10、编码不要畏惧变化，要拥抱变化。11、常充电。程序员只有一种死法：土死的。12、编程之事，隔离是方向，起名是关键，测试是主角，调试是补充，版本控制是后悔药。13、一行代码一个兵。形成建制才能有战斗力。单位规模不宜过大，千人班，万人排易成万人坑。14、重构/优化/修复Bug，同时只能做一件。15、简单模块注意封装，复杂模块注意分层。16、人脑性能有限，整洁胜于杂乱。读不懂的代码，尝试整理下格式；不好用的接口，尝试重新封装下。17、迭代速度决定工作强度。想多快好省，就从简化开发流程，加快迭代速度开始。18、忘掉优化写代码。过早优化等同恶意破坏；忘掉代码做优化。优化要基于性能测试，而不是纠结于字里行间。19、最好的工具是纸笔；其次好的是markdown。20、Leader问任务时间，若答不上来，可能是任务拆分还不够细。21、宁可多算一周，不可少估一天。过于“乐观”容易让boss受惊吓。22、最有用的语言是English。其次的可能是Python。23、百闻不如一见。画出结果，一目了然。调试耗时将大大缩短。24、资源、代码应一道受版本管理。资源匹配错误远比代码匹配错误更难排查。25、不要基于想象开发， 要基于原型开发。原型的价值是快速验证想法，帮大家节省时间。26、序列化首选明文文本 。诸如二进制、混淆、加密、压缩等等有需要时再加。27、编译器永远比你懂微观优化。只能向它不擅长的方向努力。28、不要定过大、过远、过细的计划。即使定了也没有用。29、至少半数时间将花在集成上。时间，时间，时间总是不够。30、与主流意见/方法/风格/习惯相悖时，先检讨自己最可靠。31、出现bug主动查，不管是不是你的。这能让你业务能力猛涨、个人形象飙升；如果你的bug被别人揪出来…..呵呵，那你会很被动～≧﹏≦32、不知怎么选技术书时就挑薄的。起码不会太贵，且你能看完。33、git是最棒的。简单，可靠，免费。34、仅对“可预测的非理性”抛断言。35、Log要写时间与分类。并且要能重定向输出。36、注释是稍差的文档。更好的是清晰的命名。让代码讲自己的故事。37、造轮子是很好的锻炼方法。前提是你见过别的轮子。38、code review最好以小组/结对的形式。对业务有一定了解，建议会更有价值（但不绝对）。而且不会成为负担。管理员个人review则很容易成team的瓶颈。39、提问前先做调研。问不到点上既被鄙视，又浪费自己的时间。原文地址]]></content>
      <categories>
        <category>干货</category>
      </categories>
      <tags>
        <tag>经验之谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM常见面试题]]></title>
    <url>%2FJVM%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98.html</url>
    <content type="text"><![CDATA[简单说说JVM的架构运行时区 堆 栈 方法区 程序计数器 本地方法 描述一下垃圾回收的过程(常见)垃圾回收主要发生在堆中 年轻代 eden区 s1 s2 年老代常见的垃圾回收的算法 标记清除法 复制算法 标记整理算法说说常见的垃圾回收器 串行—Serial 并行—Parallel 并发—CMS写一段代码，让它报堆内存溢出 不断的创建对象 你要去用这些对象写一段代码，让它报栈内存溢出 递归一直调用即可设置堆或栈的参数 -Xms -Xmx -Xss364k]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解java虚拟机精华总结]]></title>
    <url>%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3java%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%B2%BE%E5%8D%8E%E6%80%BB%E7%BB%93%EF%BC%88%E9%9D%A2%E8%AF%95%EF%BC%89.html</url>
    <content type="text"><![CDATA[运行时数据区域Java虚拟机管理的内存包括几个运行时数据内存：方法区、虚拟机栈、本地方法栈、堆、程序计数器，其中方法区和堆是由线程共享的数据区，其他几个是线程隔离的数据区 程序计数器程序计数器是一块较小的内存，他可以看做是当前线程所执行的行号指示器。 字节码解释器工作的时候就是通过改变这个计数器的值来选取下一条需要执行的字节码的指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Native方法，这个计数器则为空。此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemotyError情况的区域 Java虚拟机栈虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧用于储存局部变量表、操作数栈、动态链接、方法出口等信息。每个方法从调用直至完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 栈内存就是虚拟机栈，或者说是虚拟机栈中局部变量表的部分 局部变量表存放了编辑期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（refrence）类型和returnAddress类型（指向了一条字节码指令的地址） 其中64位长度的long和double类型的数据会占用两个局部变量空间，其余的数据类型只占用1个。 Java虚拟机规范对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常。如果虚拟机扩展时无法申请到足够的内存，就会跑出OutOfMemoryError异常 本地方法栈本地方法栈和虚拟机栈发挥的作用是非常类似的，他们的区别是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务本地方法栈区域也会抛出StackOverflowError和OutOfMemoryErroy异常 Java堆 堆是Java虚拟机所管理的内存中最大的一块。 Java堆是被所有线程共享的一块内存区域，在虚拟机启动的时候创建，此内存区域的唯一目的是存放对象实例，几乎所有的对象实例都在这里分配内存。所有的对象实例和数组都在堆上分配。 Java堆是垃圾收集器管理的主要区域。Java堆细分为新生代和老年代。 不管怎样，划分的目的都是为了更好的回收内存，或者更快地分配内存。Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。如果在堆中没有完成实例分配，并且堆也无法在扩展时将会抛出OutOfMemoryError异常 方法区方法区它用于储存已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据，除了Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载。当方法区无法满足内存分配需求时，将抛出OutOfMemoryErroy异常。 运行时常量池它是方法区的一部分。Class文件中除了有关的版本、字段、方法、接口等描述信息外、还有一项信息是常量池，用于存放编辑期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放Java语言并不要求常量一定只有编辑期才能产生，也就是可能将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法当常量池无法再申请到内存时会抛出OutOfMemoryError异常 hotspot虚拟机对象对象的创建 检查虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已经被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程 分配内存接下来将为新生对象分配内存，为对象分配内存空间的任务等同于把一块确定的大小的内存从Java堆中划分出来。假设Java堆中内存是绝对规整的，所有用过的内存放在一遍，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针指向空闲空间那边挪动一段与对象大小相等的距离，这个分配方式叫做“指针碰撞” 如果Java堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式成为“空闲列表”选择那种分配方式由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 Init执行new指令之后会接着执行Init方法，进行初始化，这样一个对象才算产生出来 对象的内存布局在HotSpot虚拟机中，对象在内存中储存的布局可以分为3块区域：对象头、实例数据和对齐填充 对象头包括两部分：a) 储存对象自身的运行时数据，如哈希码、GC分带年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳b) 另一部分是指类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例 对象的访问定位 使用句柄访问Java堆中将会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址优势:reference中存储的是稳点的句柄地址,在对象被移动(垃圾收集时移动对象是非常普遍的行为)时只会改变句柄中的实例数据指针，而reference本身不需要修改 使用直接指针访问Java堆对象的布局就必须考虑如何访问类型数据的相关信息,而refreence中存储的直接就是对象的地址 优势：速度更快，节省了一次指针定位的时间开销，由于对象的访问在Java中非常频繁，因此这类开销积少成多后也是一项非常可观的执行成本 OutOfMemoryError 异常Java堆溢出Java堆用于存储对象实例，只要不断的创建对象，并且保证GCRoots到对象之间有可达路径来避免垃圾回收机制清除这些对象，那么在数量到达最大堆的容量限制后就会产生内存溢出异常，如果是内存泄漏，可进一步通过工具查看泄漏对象到GC Roots的引用链。于是就能找到泄露对象是通过怎样的路径与GC Roots相关联并导致垃圾收集器无法自动回收它们的。掌握了泄漏对象的类型信息及GC Roots引用链的信息，就可以比较准确地定位出泄漏代码的位置如果不存在泄露，换句话说，就是内存中的对象确实都还必须存活着，那就应当检查虚拟机的堆参数（-Xmx与-Xms），与机器物理内存对比看是否还可以调大，从代码上检查是否存在某些对象生命周期过长、持有状态时间过长的情况，尝试减少程序运行期的内存消耗 虚拟机栈和本地方法栈溢出对于HotSpot来说，虽然-Xoss参数（设置本地方法栈大小）存在，但实际上是无效的，栈容量只由-Xss参数设定。关于虚拟机栈和本地方法栈，在Java虚拟机规范中描述了两种异常： 如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出StackOverflowError如果虚拟机在扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError异常在单线程下，无论由于栈帧太大还是虚拟机栈容量太小，当内存无法分配的时候，虚拟机抛出的都是StackOverflowError异常如果是多线程导致的内存溢出，与栈空间是否足够大并不存在任何联系，这个时候每个线程的栈分配的内存越大，反而越容易产生内存溢出异常。解决的时候是在不能减少线程数或更换64为的虚拟机的情况下，就只能通过减少最大堆和减少栈容量来换取更多的线程 方法区和运行时常量池溢出String.intern()是一个Native方法，它的作用是：如果字符串常量池中已经包含一个等于此String对象的字符串，则返回代表池中这个字符串的String对象；否则，将此String对象包含的字符串添加到常量池中，并且返回此String对象的引用 由于常量池分配在永久代中，可以通过-XX:PermSize和-XX:MaxPermSize限制方法区大小，从而间接限制其中常量池的容量。 Intern(): JDK1.6 intern方法会把首次遇到的字符串实例复制到永久代，返回的也是永久代中这个字符串实例的引用，而由StringBuilder创建的字符串实例在Java堆上，所以必然不是一个引用 JDK1.7 intern()方法的实现不会再复制实例，只是在常量池中记录首次出现的实例引用，因此intern()返回的引用和由StringBuilder创建的那个字符串实例是同一个 垃圾收集程序计数器、虚拟机栈、本地方法栈3个区域随线程而生，随线程而灭，在这几个区域内就不需要过多考虑回收的问题，因为方法结束或者线程结束时，内存自然就跟随着回收了 判断对象存活引用计数器法给对象添加一个引用计数器，每当由一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的 可达性分析算法通过一系列的成为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径成为引用链，当一个对象到GCROOTS没有任何引用链相连时，则证明此对象时不可用的 Java语言中GC Roots的对象包括下面几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈JNI（Native方法）引用的对象 引用强引用就是在程序代码之中普遍存在的，类似Object obj = new Object() 这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象软引用用来描述一些还有用但并非必须的元素。对于它在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存才会抛出内存溢出异常 弱引用用来描述非必须对象的，但是它的强度比软引用更弱一些，被引用关联的对象只能生存到下一次垃圾收集发生之前，当垃圾收集器工作时，无论当前内存是否足够都会回收掉只被弱引用关联的对象 虚引用的唯一目的就是能在这个对象被收集器回收时收到一个系统通知 Finalize方法任何一个对象的finalize()方法都只会被系统自动调用一次，如果对象面临下一次回收，它的finalize()方法不会被再次执行，因此第二段代码的自救行动失败了 回收方法区永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类 废弃常量：假如一个字符串abc已经进入了常量池中，如果当前系统没有任何一个String对象abc，也就是没有任何Stirng对象引用常量池的abc常量，也没有其他地方引用的这个字面量，这个时候发生内存回收这个常量就会被清理出常量池 无用的类： 该类所有的实例都已经被回收，就是Java堆中不存在该类的任何实例 加载该类的ClassLoader已经被回收 该类对用的java.lang.Class对象没有在任何地方被引用，无法再任何地方通过反射访问该类的方法 垃圾收集算法 标记—清除算法 算法分为标记和清除两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象、不足:一个是效率问题，标记和清除两个过程的效率都不高；另一个是空间问题，标记清楚之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后再程序运行过程中需要分配较大的对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作 复制算法 他将可用内存按照容量划分为大小相等的两块，每次只使用其中的一块。当这块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可 不足：将内存缩小为了原来的一半 实际中我们并不需要按照1:1比例来划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor 当另一个Survivor空间没有足够空间存放上一次新生代收集下来的存活对象时，这些对象将直接通过分配担保机制进入老年代 标记整理算法 让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存 分代收集算法 只是根据对象存活周期的不同将内存划分为几块。一般是把java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用标记清理或者标记整理算法来进行回收 垃圾收集器 Serial收集器： 这个收集器是一个单线程的收集器，但它的单线程的意义不仅仅说明它会只使用一个COU或一条收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集时，必须暂停其他所有的工作线程，直到它手机结束 ParNew 收集器： Serial收集器的多线程版本，除了使用了多线程进行收集之外，其余行为和Serial收集器一样 并行：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态 并发：指用户线程与垃圾收集线程同时执行（不一定是并行的，可能会交替执行），用户程序在继续执行，而垃圾收集程序运行于另一个CPU上 Parallel Scavenge 收集器是一个新生代收集器，它是使用复制算法的收集器，又是并行的多线程收集器。 吞吐量：就是CPU用于运行用户代码的时间与CPU总消耗时间的比值。即吞吐量=运行用户代码时间/（运行用户代码时间+垃圾收集时间） Serial Old 收集器： 是Serial收集器的老年代版本,是一个单线程收集器，使用标记整理算法 Parallel Old 收集器： Parallel Old是Paraller Seavenge收集器的老年代版本，使用多线程和标记整理算法 CMS收集器： CMS收集器是基于标记清除算法实现的，整个过程分为4个步骤： 1.初始标记2.并发标记3.重新标记4.并发清除 优点：并发收集、低停顿 缺点： CMS收集器对CPU资源非常敏感，CMS默认启动的回收线程数是（CPU数量+3）/4， CMS收集器无法处理浮动垃圾，可能出现Failure失败而导致一次Full G场地产生 CMS是基于标记清除算法实现的 G1收集器： 它是一款面向服务器应用的垃圾收集器 并行与并发：利用多CPU缩短STOP-The-World停顿的时间 分代收集 空间整合：不会产生内存碎片 可预测的停顿 运作方式：初始标记，并发标记，最终标记，筛选回收6.内存分配与回收策略对象优先在Eden分配：大多数情况对象在新生代Eden区分配，当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC 大对象直接进入老年代：所谓大对象就是指需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组。这样做的目的是避免Eden区及两个Servivor之间发生大量的内存复制 长期存活的对象将进入老年代如果对象在Eden区出生并且尽力过一次Minor GC后仍然存活，并且能够被Servivor容纳，将被移动到Servivor空间中，并且把对象年龄设置成为1.对象在Servivor区中每熬过一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度（默认15岁），就将会被晋级到老年代中 动态对象年龄判定为了更好地适应不同程序的内存状况，虚拟机并不是永远地要求对象的年龄必须达到了MaxTenuringThreshold才能晋级到老年代，如果在Servivor空间中相同年龄所有对象的大小总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入到老年代，无须登到MaxTenuringThreshold中要求的年龄 空间分配担保：在发生Minor GC 之前，虚拟机会检查老年代最大可 用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor DC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许那么会继续检查老年代最大可用的连续空间是否大于晋级到老年代对象的平均大小，如果大于，将尝试进行一次Minor GC，尽管这次MinorGC 是有风险的：如果小于，或者HandlePromotionFailure设置不允许冒险，那这时也要改为进行一次Full GC 虚拟机类加载机制虚拟机吧描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制在Java语言里面，类型的加载。连接和初始化过程都是在程序运行期间完成的 类加载的时机类被加载到虚拟机内存中开始，到卸载为止，整个生命周期包括：加载、验证、准备、解析、初始化、使用和卸载7个阶段 加载、验证、准备、初始化和卸载这5个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始，而解析阶段则不一定：它在某些情况下可以再初始化阶段之后再开始，这个是为了支持Java语言运行时绑定（也成为动态绑定或晚期绑定） 虚拟机规范规定有且只有5种情况必须立即对类进行初始化： 遇到new、getstatic、putstatic或invokestatic这4条字节码指令时，如果类没有进行过初始化，则需要触发其初始化。生成这4条指令的最常见的Java代码场景是：使用new关键字实例化对象的时候、读取或设置一个类的静态字段（被final修饰、已在编译期把结果放入常量池的静态字段除外）的时候，以及调用一个类的静态方法的时候 使用java.lang.reflect包的方法对类进行反射调用的时候，如果类没有进行过初始化，则需要先触发其初始化 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化 当虚拟机启动时候，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类 当使用JDK1.7的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化被动引用： 通过子类引用父类的静态字段，不会导致子类初始化 通过数组定义来引用类，不会触发此类的初始化 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化接口的初始化：接口在初始化时，并不要求其父接口全部完成类初始化，只有在正整使用到父接口的时候（如引用接口中定义的常量）才会初始化类加载的过程加载 通过一个类的全限定名类获取定义此类的二进制字节流 将这字节流所代表的静态存储结构转化为方法区运行时数据结构 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口 怎么获取二进制字节流？ 从ZIP包中读取，这很常见，最终成为日后JAR、EAR、WAR格式的基础 从网络中获取，这种场景最典型的应用就是Applet 运行时计算生成，这种常见使用得最多的就是动态代理技术 由其他文件生成，典型场景就是JSP应用 从数据库中读取，这种场景相对少一些（中间件服务器）数组类本身不通过类加载器创建，它是由Java虚拟机直接创建的数组类的创建过程遵循以下规则： 如果数组的组件类型(指的是数组去掉一个维度的类型)是引用类型，那就递归采用上面的加载过程去加载这个组件类型，数组C将在加载该组件类型的类加载器的类名称空间上被标识 如果数组的组件类型不是引用类型(列如int[]组数)，Java虚拟机将会把数组C标识为与引导类加载器关联 数组类的可见性与它的组件类型的可见性一致，如果组件类型不是引用类型，那数组类的可见性将默认为public 验证验证阶段会完成下面4个阶段的检验动作：文件格式验证，元数据验证，字节码验证，符号引用验证 文件格式验证第一阶段要验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理。这一阶段可能包括： 是否以魔数oxCAFEBABE开头 主、次版本号是否在当前虚拟机处理范围之内 常量池的常量中是否有不被支持的常量类型(检查常量tag标志) 指向常量的各种索引值中是否有指向不存在的常量或不符合类型的常量 CONSTANT_Itf8_info 型的常量中是否有不符合UTF8编码的数据 Class文件中各个部分及文件本身是否有被删除的或附加的其他信息这个阶段的验证时基于二进制字节流进行的，只有通过类这个阶段的验证后，字节流才会进入内存的方法区进行存储，所以后面的3个验证阶段全部是基于方法区的存储结构进行的，不会再直接操作字节流元数据验证 这个类是否有父类(除了java.lang.Object之外,所有的类都应当有父类) 这个类的父类是否继承了不允许被继承的类（被final修饰的类） 如果这个类不是抽象类，是否实现类其父类或接口之中要求实现的所有方法 类中的字段、方法是否与父类产生矛盾(列如覆盖类父类的final字段,或者出现不符合规则的方法重载，列如方法参数都一致，但返回值类型却不同等)第二阶段的主要目的是对类元数据信息进行语义校验，保证不存在不符合Java语言规范的元数据信息字节码验证第三阶段是整个验证过程中最复杂的一个阶段，主要目的似乎通过数据流和控制流分析，确定程序语言是合法的、符合逻辑的。在第二阶段对元数据信息中的数据类型做完校验后，这个阶段将对类的方法体进行校验分析，保证被校验类的方法在运行时不会做出危害虚拟机安全的事件。 保证任意时刻操作数栈的数据类型与指令代码序列都能配合工作，列如，列如在操作数栈放置类一个int类型的数据，使用时却按long类型来加载入本地变量表中 保证跳转指令不会跳转到方法体以外的字节码指令上 保证方法体中的类型转换时有效的，列如可以把一个子类对象赋值给父类数据类型，这个是安全的，但是吧父类对象赋值给子类数据类型，甚至把对象赋值给与它毫无继承关系、完全不相干的一个数据类型，则是危险和不合法的符号引用验证发生在虚拟机将符号引用转化为直接引用的时候，这个转化动作将在连接的第三阶段——解析阶段中发生。 符号引用中通过字符串描述的全限定名是否能找到相对应的类 在指定类中是否存在符合方法的字段描述符以及简单名称所描述的方法和字段 符号引用中的类、字段、方法的访问性是否可被当前类访问 对于虚拟机的类加载机制来说，验证阶段是非常重要的，但是不一定必要（因为对程序运行期没有影响）的阶段。如果全部代码都已经被反复使用和验证过，那么在实施阶段就可以考虑使用Xverify：none参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间 准备准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量都在方法区中进行分配。这个时候进行内存分配的仅包括类变量(被static修饰的变量)，而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在Java堆中。其次，这里说的初始值通常下是数据类型的零值。假设public static int value = 123；那变量value在准备阶段过后的初始值为0而不是123，因为这时候尚未开始执行任何Java方法，而把value赋值为123的putstatic指令是程序被编译后，存放于类构造器()方法之中，所以把value赋值为123的动作将在初始化阶段才会执行，但是如果使用final修饰，则在这个阶段其初始值设置为123 解析解析阶段是虚拟机将常量池内符号引用替换为直接引用的过 初始化类的初始化阶段是类加载过程的最后一步，前面的类加载过程中，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才正真开始执行类中定义的Java程序代码(或者说是字节码) 类的加载器双亲委派模型：只存在两种不同的类加载器：启动类加载器（Bootstrap ClassLoader），使用C++实现，是虚拟机自身的一部分。另一种是所有其他的类加载器，使用JAVA实现，独立于JVM，并且全部继承自抽象类java.lang.ClassLoader. 启动类加载器（Bootstrap ClassLoader），负责将存放在\lib目录中的，或者被-Xbootclasspath参数所制定的路径中的，并且是JVM识别的（仅按照文件名识别，如rt.jar，如果名字不符合，即使放在lib目录中也不会被加载），加载到虚拟机内存中，启动类加载器无法被JAVA程序直接引用。 扩展类加载器，由sun.misc.Launcher$ExtClassLoader实现，负责加载\lib\ext目录中的，或者被java.ext.dirs系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader），由sun.misc.Launcher$AppClassLoader来实现。由于这个类加载器是ClassLoader中的getSystemClassLoader()方法的返回值，所以一般称它为系统类加载器。负责加载用户类路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 这张图表示类加载器的双亲委派模型（Parents Delegation model）. 双亲委派模型要求除了顶层的启动加载类外，其余的类加载器都应当有自己的父类加载器。，这里类加载器之间的父子关系一般不会以继承的关系来实现，而是使用组合关系来复用父类加载器的代码。 双亲委派模型的工作过程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都是应该传送到顶层的启动类加载器中，只有当父类加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载。 这样做的好处就是：Java类随着它的类加载器一起具备了一种带有优先级的层次关系。例如类java.lang.Object,它存放在rt.jar中，无论哪一个类加载器要加载这个类，最终都是委派给处于模型最顶端的启动类加载器进行加载，因此Object类在程序的各种类加载器环境中都是同一个类。相反，如果没有使用双亲委派模型，由各个类加载器自行去加载的话，如果用户自己编写了一个称为java.lang.object的类，并放在程序的ClassPath中，那系统中将会出现多个不同的Object类，Java类型体系中最基础的行为也就无法保证，应用程序也将会变得一片混乱就是保证某个范围的类一定是被某个类加载器所加载的，这就保证在程序中同 一个类不会被不同的类加载器加载。这样做的一个主要的考量，就是从安全层 面上，杜绝通过使用和JRE相同的类名冒充现有JRE的类达到替换的攻击方式 Java内存模型与线程内存间的交互操作关于主内存与工作内存之间的具体交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步到主内存之间的实现细节，Java内存模型定义了以下八种操作来完成： lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。 unlock（解锁）：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read（读取）：作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。 write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。 如果要把一个变量从主内存中复制到工作内存，就需要按顺寻地执行read和load操作， 如果把变量从工作内存中同步回主内存中，就要按顺序地执行store和write操作。Java内存 模型只要求上述操作必须按顺序执行，而没有保证必须是连续执行。也就是read和load之间， store和write之间是可以插入其他指令的，如对主内存中的变量a、b进行访问时，可能的顺 序是read a，read b，load b， load a。 Java内存模型还规定了在执行上述八种基本操作时，必须满足如下规则： 不允许read和load、store和write操作之一单独出现 不允许一个线程丢弃它的最近assign的操作，即变量在工作内存中改变了之后必须同步到主内存中。 不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步回主内存中。 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量。即就是对一个变量实施use和store操作之前，必须先执行过了assign和load操作。 一个变量在同一时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。lock和unlock必须成对出现如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值如果一个变量事先没有被lock操作锁定，则不允许对它执行unlock操作；也不允许去unlock一个被其他线程锁定的变量。对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作）。 重排序在执行程序时为了提高性能，编译器和处理器经常会对指令进行重排序。重排序分成三种类型： 编译器优化的重排序。编译器在不改变单线程程序语义放入前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从Java源代码到最终实际执行的指令序列，会经过下面三种重排序： 为了保证内存的可见性，Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。Java内存模型把内存屏障分为LoadLoad、LoadStore、StoreLoad和StoreStore四种： 对于volatile型变量的特殊规则当一个变量定义为volatile之后，它将具备两种特性： 第一：保证此变量对所有线程的可见性，这里的可见性是指当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。普通变量的值在线程间传递需要通过主内存来完成由于valatile只能保证可见性，在不符合一下两条规则的运算场景中，我们仍要通过加锁来保证原子性 运算结果并不依赖变量的当前值，或者能够确保只有单一的线程修改变量的值。 变量不需要与其他的状态变量共同参与不变约束 第二：禁止指令重排序，普通的变量仅仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中执行顺序一致，这个就是所谓的线程内表现为串行的语义 Java内存模型中对volatile变量定义的特殊规则。假定T表示一个线程，V和W分别表示两个volatile变量，那么在进行read、load、use、assign、store、write操作时需要满足如下的规则： 只有当线程T对变量V执行的前一个动作是load的时候，线程T才能对变量V执行use动作；并且，只有当线程T对变量V执行的后一个动作是use的时候，线程T才能对变量V执行load操作。线程T对变量V的use操作可以认为是与线程T对变量V的load和read操作相关联的，必须一起连续出现。这条规则要求在工作内存中，每次使用变量V之前都必须先从主内存刷新最新值，用于保证能看到其它线程对变量V所作的修改后的值。 只有当线程T对变量V执行的前一个动是assign的时候，线程T才能对变量V执行store操作；并且，只有当线程T对变量V执行的后一个动作是store操作的时候，线程T才能对变量V执行assign操作。线程T对变量V的assign操作可以认为是与线程T对变量V的store和write操作相关联的，必须一起连续出现。这一条规则要求在工作内存中，每次修改V后都必须立即同步回主内存中，用于保证其它线程可以看到自己对变量V的修改。 假定操作A是线程T对变量V实施的use或assign动作，假定操作F是操作A相关联的load或store操作，假定操作P是与操作F相应的对变量V的read或write操作；类型地，假定动作B是线程T对变量W实施的use或assign动作，假定操作G是操作B相关联的load或store操作，假定操作Q是与操作G相应的对变量V的read或write操作。如果A先于B，那么P先于Q。这条规则要求valitile修改的变量不会被指令重排序优化，保证代码的执行顺序与程序的顺序相同。对于long和double型变量的特殊规则Java模型要求lock、unlock、read、load、assign、use、store、write这8个操作都具有原子性，但是对于64为的数据类型（long和double），在模型中特别定义了一条相对宽松的规定：允许虚拟机将没有被volatile修饰的64位数据的读写操作分为两次32为的操作来进行，即允许虚拟机实现选择可以不保证64位数据类型的load、store、read和write这4个操作的原子性原子性、可见性和有序性原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。Java内存模型是通过在变量修改后将新值同步会主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方式来实现可见性，valatile特殊规则保障新值可以立即同步到祝内存中。Synchronized是在对一个变量执行unlock之前，必须把变量同步回主内存中（执行store、write操作）。被final修饰的字段在构造器中一旦初始化完成，并且构造器没有吧this的引用传递出去，那在其他线程中就能看见final字段的值 可见性：可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 有序性：即程序执行的顺序按照代码的先后顺序执行。 先行发生原则这些先行发生关系无须任何同步就已经存在，如果不再此列就不能保障顺序性，虚拟机就可以对它们任意地进行重排序 程序次序规则：在一个线程内，按照程序代码顺序，书写在前面的操作先行发生于书写在后面的操作。准确的说，应该是控制顺序而不是程序代码顺序，因为要考虑分支。循环等结构 管程锁定规则：一个unlock操作先行发生于后面对同一个锁的lock操作。这里必须强调的是同一个锁，而后面的是指时间上的先后顺序 Volatile变量规则：对一个volatile变量的写操作先行发生于后面对这个变量的读操作，这里的后面同样是指时间上的先后顺序 线程启动规则：Thread对象的start()方法先行发生于此线程的每一个动作 线程终止规则：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread.joke()方法结束、ThradisAlive()的返回值等手段检测到线程已经终止执行 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断时间的发生，可以通过Thread.interrupted()方法检测到是否有中断发生 对象终结规则：一个对象的初始化完成(构造函数执行结束)先行发生于它的finalize()方法的开始 传递性：如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论Java线程调度 协同式调度：线程的执行时间由线程本身控制 抢占式调度：线程的执行时间由系统来分配 状态转换 新建 运行：可能正在执行。可能正在等待CPU为它分配执行时间 无限期等待：不会被分配CUP执行时间，它们要等待被其他线程显式唤醒 限期等待：不会被分配CUP执行时间，它们无须等待被其他线程显式唤醒，一定时间会由系统自动唤醒 阻塞：阻塞状态在等待这获取到一个排他锁，这个时间将在另一个线程放弃这个锁的时候发生；等待状态就是在等待一段时间，或者唤醒动作的发生 结束：已终止线程的线程状态，线程已经结束执行 线程安全 不可变：不可变的对象一定是线程安全的、无论是对象的方法实现还是方法的调用者，都不需要再采取任何的线程安全保障。例如：把对象中带有状态的变量都声明为final，这样在构造函数结束之后，它就是不可变的。 绝对线程安全 相对线程安全：相对的线程安全就是我们通常意义上所讲的线程安全，它需要保证对这个对象单独的操作是线程安全的，我们在调用的时候不需要做额外的保障措施，但是对于一些特定顺序的连续调用，就可能需要在调用端使用额外的同步手段来保证调用的正确性 线程兼容：对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全使用 线程对立：是指无论调用端是否采取了同步措施，都无法在多线程环境中并发使用的代码线程安全的实现方法 互斥同步：同步是指在多个线程并发访问共享数据时，保证共享数据在同一个时刻只被一个（或者是一些，使用信号量的时候）线程使用。而互斥是实现同步的一种手段，临界区、互斥量和信号量都是主要的互斥实现方式。互斥是因，同步是果：互斥是方法，同步是目的. 在Java中，最基本的互斥同步手段就是synchronized关键字，它经过编译之后，会在同步块的前后分别形成monitorenter和monitorexit这两个字节码指令，这两个字节码都需要一个reference类型的参数来指明要锁定和解锁的对象。如果Java程序中的synchronized明确指定了对象参数，那就是这个对象的reference；如果没有指明，那就根据synchronized修饰的是实例方法还是类方法，去取对应的对象实例或Class对象来作为锁对象。在执行monitorenter指令时，首先要尝试获取对象的锁。如果这个对象没有被锁定，或者当前线程已经拥有了那个对象的锁，把锁的计数器加1，对应的在执行monitorexit指令时会将锁计数器减1，当计数器为0时，锁就被释放。如果获取对象锁失败，哪当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止 Synchronized，ReentrantLock增加了一些高级功能 等待可中断：是指当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情，可中断特性对处理执行时间非常长的同步块很有帮助 公平锁：是指多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁；非公平锁则不能保证这一点，在锁被释放时，任何一个等待锁的线程都有机会获得锁。Synchronized中的锁是非公平的，ReentrantLock默认情况下也是非公平的，但可以通过带布尔值的构造函数要求使用公平锁 锁绑定多个条件是指一个ReentrantLock对象可以同时绑定多个Condition对象，而在synchronized中，锁对象的wait()和notify()或notifyAll()方法可以实现一个隐含的条件，如果要和多余一个的条件关联的时候，就不得不额外地添加一个锁，而ReentrantLock则无须这样做，只需要多次调用newCondition方法即可 非阻塞同步 无同步方案可重入代码：也叫纯代码，可以在代码执行的任何时刻中断它，转而去执行另外一段代码（包括递归调用它本身）而在控制权返回后，原来的程序不会出现任何错误。所有的可重入代码都是线程安全的，但是并非所有的线程安全的代码都是可重入的。 判断一个代码是否具备可重入性：如果一个方法，它的返回结果是可预测的，只要输入了相同的数据，就都能返回相同的结果，那它就满足可重入性的要求，当然也就是线程安全的 线程本地存储：如果一段代码中所需要的数据必须与其他代码共享，那就看看这些共享数据的代码是否能保证在同一个线程中执行？如果能保障，我们就可以把共享数据的可见范围限制在同一个线程之内，这样，无须同步也能保证线程之间不出现数据争用的问题 锁优化适应性自旋、锁消除、锁粗化、轻量级锁和偏向锁 自旋锁与自适应自旋自旋锁：如果物理机器上有一个以上的处理器，能让两个或以上的线程同时并行执行，我们就可以让后面请求锁的那个线程稍等一下，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁。为了让线程等待，我们只需让线程执行一个忙循环（自旋），这项技术就是所谓的自旋锁 自适应自旋转：是由前一次在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后要获取这个锁时将可能省略掉自过程，以避免浪费处理器资源。 锁消除锁消除是指虚拟机即时编辑器在运行时，对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行消除。如果在一段代码中。推上的所有数据都不会逃逸出去从而被其他线程访问到，那就可以把它们当作栈上数据对待，认为它们是线程私有的，同步加锁自然就无须进行 锁粗化如果虚拟机检测到有一串零碎的操作都是对同一对象的加锁，将会把加锁同步的范围扩展（粗化）到整个操作序列的外部 轻量级锁偏向锁它的目的是消除无竞争情况下的同步原语，进一步提高程序的运行性能。如果轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把这个同步都消除掉，CAS操作都不做了如果在接下俩的执行过程中，该锁没有被其他线程获取，则持有偏向锁的线程将永远不需要在进行同步 逃逸分析逃逸分析的基本行为就是分析对象动态作用域：当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他方法中，成为方法逃逸。甚至还可能被外部线程访问到，比如赋值给类变量或可以在其他线程中访问的实例变量，称为线程逃逸 如果一个对象不会逃逸到方法或线程之外，也就是别的方法或线程无法通过任何途径访问到这个对象，则可能为这个变量进行一些高效的优化栈上分配：如果确定一个对象不会逃逸出方法外，那让这个对象在栈上分配内存将会是一个不错的注意，对象所占用的内存空间就可以随栈帧出栈而销毁。如果能使用栈上分配，那大量的对象就随着方法的结束而销毁了，垃圾收集系统的压力将会小很多 同步消除：如果确定一个变量不会逃逸出线程，无法被其他线程访问，那这个变量的读写肯定就不会有竞争，对这个变量实施的同步措施也就可以消除掉 标量替换：标量就是指一个数据无法在分解成更小的数据表示了，int、long等及refrence类型等都不能在进一步分解，它们称为标量。 如果一个数据可以继续分解，就称为聚合量，Java中的对象就是最典型的聚合量 如果一个对象不会被外部访问，并且这个对象可以被拆散的化，那程序正整执行的时候将可能不创建这个对象，而改为直接创建它的若干个被这个方法使用到的成员变量来代替]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark架构及原理]]></title>
    <url>%2FSpark%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%8E%9F%E7%90%86.html</url>
    <content type="text"><![CDATA[Spark架构及原理]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD原理]]></title>
    <url>%2FRDD%E5%8E%9F%E7%90%86.html</url>
    <content type="text"><![CDATA[RDD概念RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。RDD是Spark的最基本抽象,是对分布式内存的抽象使用，实现了以操作本地集合的方式来操作分布式数据集的抽象实现。RDD是Spark最核心的东西，它表示已被分区，不可变的并能够被并行操作的数据集合，不同的数据集格式对应不同的RDD实现。RDD必须是可序列化的。RDD可以cache到内存中，每次对RDD数据集的操作之后的结果，都可以存放到内存中，下一个操作可以直接从内存中输入，省去了MapReduce大量的磁盘IO操作 RDD可以横向多分区，当计算过程中内存不足时，将数据刷到磁盘等外部存储上，从而实现数据在内存和外存的灵活切换。可以说，RDD是有虚拟数据结构组成，并不包含真实数据体。 RDD的内部属性通过RDD的内部属性，用户可以获取相应的元数据信息。通过这些信息可以支持更复杂的算法或优化。 一组分片（Partition），即数据集的基本组成单位对于RDD来说，每个分片都会被一个task计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配的CPU Core的数据。 计算每个分片的函数Spark中RDD的计算是以分片为单位的，通过函数可以对每个数据块进行RDD需要进行的用户自定义函数运算。函数会对迭代器进行复合，不需要保存每次计算的结果。 RDD之间的依赖关系对父RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 一个Partitioner，即RDD的分片函数当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 分区列表，存储存取每个Partition的优先位置（preferred location）对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 可选属性key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。 可选属性每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”) RDD的特点 创建：只能通过转换 ( transformation ，如map/filter/groupBy/join 等，区别于动作 action) 从两种数据源中创建 RDD 1 ）稳定存储中的数据； 2 ）其他 RDD。 只读：状态不可变，不能修改。 分区：支持使 RDD 中的元素根据那个 key 来分区 ( partitioning ) ，保存到多个结点上。还原时只会重新计算丢失分区的数据，而不会影响整个系统。 路径：在 RDD 中叫世族或血统 ( lineage ) ，即 RDD 有充足的信息关于它是如何从其他 RDD 产生而来的。 持久化：支持将会被重用的 RDD 缓存 ( 如 in-memory 或溢出到磁盘 )。 延迟计算：Spark 也会延迟计算 RDD ，使其能够将转换管道化 (pipeline transformation)。 操作：丰富的转换（transformation）和动作 ( action ) ， count/reduce/collect/save 等。 执行了多少次transformation操作，RDD都不会真正执行运算（记录lineage），只有当action操作被执行时，运算才会触发。 RDD的优点 RDD只能从持久存储或通过Transformations操作产生，相比于分布式共享内存(DSM)可以更高效实现容错，对于丢失部分数据分区只需根据它的lineage就可重新计算出来，而不需要做特定的Checkpoint。 RDD的不变性，可以实现类Hadoop MapReduce的推测式执行。 RDD的数据分区特性，可以通过数据的本地性来提高性能，这不Hadoop MapReduce是一样的。 RDD都是可序列化的，在内存不足时可自动降级为磁盘存储，把RDD存储于磁盘上，这时性能会有大的下降但不会差于现在的MapReduce。 批量操作：任务能够根据数据本地性 (data locality) 被分配，从而提高性能。 RDD的存储与分区 用户可以选择不同的存储级别存储RDD以便重用。 当前RDD默认是存储于内存，但当内存不足时，RDD会spill到disk。 RDD在需要进行分区把数据分布于集群中时会根据每条记录Key进行分区（如Hash 分区），以此保证两个数据集在Join时能高效。 RDD根据useDisk、useMemory、useOffHeap、deserialized、replication参数的组合定义了以下存储级别：12345678910111213//存储等级定义： val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(false, false, true, false) RDD的容错机制RDD的容错机制实现分布式数据集容错方法有两种：数据检查点和记录更新 RDD采用记录更新的方式：记录所有更新点的成本很高。所以，RDD只支持粗颗粒变换，即只记录单个块（分区）上执行的单个操作，然后创建某个RDD的变换序列（血统 lineage）存储下来； 变换序列指，每个RDD都包含了它是如何由其他RDD变换过来的以及如何重建某一块数据的信息。因此RDD的容错机制又称“血统”容错。 要实现这种“血统”容错机制，最大的难题就是如何表达父RDD和子RDD之间的依赖关系。实际上依赖关系可以分两种，窄依赖和宽依赖。 窄依赖：子RDD中的每个数据块只依赖于父RDD中对应的有限个固定的数据块 宽依赖：子RDD中的一个数据块可以依赖于父RDD中的所有数据块。例如：map变换，子RDD中的数据块只依赖于父RDD中对应的一个数据块；groupByKey变换，子RDD中的数据块会依赖于多块父RDD中的数据块，因为一个key可能分布于父RDD的任何一个数据块中 将依赖关系分类的两个特性： 窄依赖可以在某个计算节点上直接通过计算父RDD的某块数据计算得到子RDD对应的某块数据；宽依赖则要等到父RDD所有数据都计算完成之后，并且父RDD的计算结果进行hash并传到对应节点上之后才能计算子RDD。 数据丢失时，对于窄依赖只需要重新计算丢失的那一块数据来恢复；对于宽依赖则要将祖先RDD中的所有数据块全部重新计算来恢复。 所以在“血统”链特别是有宽依赖的时候，需要在适当的时机设置数据检查点。也是这两个特性要求对于不同依赖关系要采取不同的任务调度机制和容错恢复机制。 Spark计算工作流 输入：在Spark程序运行中，数据从外部数据空间（例如，HDFS、Scala集合或数据）输入到Spark，数据就进入了Spark运行时数据空间，会转化为Spark中的数据块，通过BlockManager进行管理。 运行：在Spark数据输入形成RDD后，便可以通过变换算子fliter等，对数据操作并将RDD转化为新的RDD，通过行动（Action）算子，触发Spark提交作业。如果数据需要复用，可以通过Cache算子，将数据缓存到内存。 输出：程序运行结束数据会输出Spark运行时空间，存储到分布式存储中（如saveAsTextFile输出到HDFS）或Scala数据或集合中（collect输出到Scala集合，count返回Scala Int型数据）。 Spark的核心数据模型是RDD，但RDD是个抽象类，具体由各子类实现，如MappedRDD、ShuffledRDD等子类。Spark将常用的大数据操作都转化成为RDD的子类。 RDD编程模型textFile算子从HDFS读取日志文件，返回“file”（RDD）；filter算子筛出带“ERROR”的行，赋给 “errors”（新RDD）；cache算子把它缓存下来以备未来使用；count算子返回“errors”的行数。RDD看起来与Scala集合类型 没有太大差别，但它们的数据和运行模型大相迥异。1234val file = sc.textFile("hdfs://...")val errors = file.filter(_.contains("ERROR"))errors.cache()errors.count() 上面代码给出了RDD数据模型，并将上例中用到的四个算子映射到四种算子类型。Spark程序工作在两个空间中：Spark RDD空间和Scala原生数据空间。在原生数据空间里，数据表现为标量（scalar，即Scala基本类型，用橘色小方块表示）、集合类型（蓝色虚线 框）和持久存储（红色圆柱）。 下图描述了Spark运行过程中通过算子对RDD进行转换， 算子是RDD中定义的函数，可以对RDD中的数据进行转换和操作。 输入算子（橘色箭头）将Scala集合类型或存储中的数据吸入RDD空间，转为RDD（蓝色实线框）。输入算子的输入大致有两类：一类针对 Scala集合类型，如parallelize；另一类针对存储数据，如上例中的textFile。输入算子的输出就是Spark空间的RDD。 因为函数语义，RDD经过变换（transformation）算子（蓝色箭头）生成新的RDD。变换算子的输入和输出都是RDD。RDD会被划分 成很多的分区 （partition）分布到集群的多个节点中，图1用蓝色小方块代表分区。注意，分区是个逻辑概念，变换前后的新旧分区在物理上可能是同一块内存或存 储。这是很重要的优化，以防止函数式不变性导致的内存需求无限扩张。有些RDD是计算的中间结果，其分区并不一定有相应的内存或存储与之对应，如果需要 （如以备未来使用），可以调用缓存算子（例子中的cache算子，灰色箭头表示）将分区物化（materialize）存下来（灰色方块）。 一部分变换算子视RDD的元素为简单元素，分为如下几类： 输入输出一对一（element-wise）的算子，且结果RDD的分区结构不变，主要是map、flatMap（map后展平为一维RDD）； 输入输出一对一，但结果RDD的分区结构发生了变化，如union（两个RDD合为一个）、coalesce（分区减少）； 从输入中选择部分元素的算子，如filter、distinct（去除冗余元素）、subtract（本RDD有、它RDD无的元素留下来）和sample（采样）。 另一部分变换算子针对Key-Value集合，又分为： 对单个RDD做element-wise运算，如mapValues（保持源RDD的分区方式，这与map不同）； 对单个RDD重排，如sort、partitionBy（实现一致性的分区划分，这个对数据本地性优化很重要，后面会讲）； 对单个RDD基于key进行重组和reduce，如groupByKey、reduceByKey； 对两个RDD基于key进行join和重组，如join、cogroup。 后三类操作都涉及重排，称为shuffle类操作。 从RDD到RDD的变换算子序列，一直在RDD空间发生。这里很重要的设计是lazy evaluation：计算并不实际发生，只是不断地记录到元数据。元数据的结构是DAG（有向无环图），其中每一个“顶点”是RDD（包括生产该RDD 的算子），从父RDD到子RDD有“边”，表示RDD间的依赖性。Spark给元数据DAG取了个很酷的名字，Lineage（世系）。这个 Lineage也是前面容错设计中所说的日志更新。 Lineage一直增长，直到遇上行动（action）算子（图1中的绿色箭头），这时 就要evaluate了，把刚才累积的所有算子一次性执行。行动算子的输入是RDD（以及该RDD在Lineage上依赖的所有RDD），输出是执行后生 成的原生数据，可能是Scala标量、集合类型的数据或存储。当一个算子的输出是上述类型时，该算子必然是行动算子，其效果则是从RDD空间返回原生数据空间。 RDD的运行逻辑如图所示，在Spark应用中，整个执行流程在逻辑上运算之间会形成有向无环图。Action算子触发之后会将所有累积的算子形成一个有向无环图，然后由调度器调度该图上的任务进行运算。Spark的调度方式与MapReduce有所不同。Spark根据RDD之间不同的依赖关系切分形成不同的阶段（Stage），一个阶段包含一系列函数进行流水线执行。图中的A、B、C、D、E、F、G，分别代表不同的RDD，RDD内的一个方框代表一个数据块。数据从HDFS输入Spark，形成RDD A和RDD C，RDD C上执行map操作，转换为RDD D，RDD B和RDD F进行join操作转换为G，而在B到G的过程中又会进行Shuffle。最后RDD G通过函数saveAsSequenceFile输出保存到HDFS中。 RDD依赖关系RDD依赖关系如下图所示： 窄依赖 (narrowdependencies) 和宽依赖 (widedependencies) 。 窄依赖是指 父 RDD 的每个分区都只被子 RDD 的一个分区所使用，例如map、filter。 宽依赖就是指父 RDD 的分区被多个子 RDD 的分区所依赖，例如groupByKey、reduceByKey等操作。如果父RDD的一个Partition被一个子RDD的Partition所使用就是窄依赖，否则的话就是宽依赖。 这种划分有两个用处。首先，窄依赖支持在一个结点上管道化执行。例如基于一对一的关系，可以在 filter 之后执行 map 。其次，窄依赖支持更高效的故障还原。因为对于窄依赖，只有丢失的父 RDD 的分区需要重新计算。而对于宽依赖，一个结点的故障可能导致来自所有父 RDD 的分区丢失，因此就需要完全重新执行。因此对于宽依赖，Spark 会在持有各个父分区的结点上，将中间数据持久化来简化故障还原，就像 MapReduce 会持久化 map 的输出一样。 特别说明：对于join操作有两种情况，如果join操作的使用每个partition仅仅和已知的Partition进行join，此时的join操作就是窄依赖；其他情况的join操作就是宽依赖；因为是确定的Partition数量的依赖关系，所以就是窄依赖，得出一个推论，窄依赖不仅包含一对一的窄依赖，还包含一对固定个数的窄依赖（也就是说对父RDD的依赖的Partition的数量不会随着RDD数据规模的改变而改变） Stage的划分： Stage划分的依据就是宽依赖，什么时候产生宽依赖呢？例如reduceByKey，groupByKey等Action。 从后往前推理，遇到宽依赖就断开，遇到窄依赖就把当前的RDD加入到Stage中； 每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition数量决定的； 最后一个Stage里面的任务的类型是ResultTask，前面所有其他Stage里面的任务类型都是ShuffleMapTask； 代表当前Stage的算子一定是该Stage的最后一个计算步骤。 补充：Hadoop中的MapReduce操作中的Mapper和Reducer在Spark中基本等量算子是：map、reduceByKey；在一个Stage内部，首先是算子合并，也就是所谓的函数式编程的执行的时候最终进行函数的展开从而把一个Stage内部的多个算子合并成为一个大算子（其内部包含了当前Stage中所有算子对数据的计算逻辑）；其次是由于Transformation操作的Lazy特性！！在具体算子交给集群的Executor计算之前，首先会通过Spark Framework（DAGScheduler）进行算子的优化。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>RDD</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkCore调优]]></title>
    <url>%2FSparkCore%E8%B0%83%E4%BC%98.html</url>
    <content type="text"><![CDATA[开发角度 原则一：避免创建重复的RDD 原则二：尽可能用同一个RDD 原则三：对多次使用的RDD进行持久化如何选择一种最合适的持久化策略 MEMORY_ONLY MEMORY_ONLY_SER MEMORY_AND_DISK_SER 不考虑：DISK_ONLY和_2后缀 原则四：尽量避免使用shuffle类算子 能不用就不用 能不能用非shuffle类的算子去替代非shuffle类的join -》 map操作替代 原则五：使用map-side预聚合的shuffle操作：groupBykey 和 reduceBykey 原则六：使用高性能的算子： 使用reduceBykey//aggregateBykey替代groupBykey 使用mapPartitions替代普通map 使用foreachPartitions替代foreach 使用filter之后进行coalesce操作 使用repartitionAndSortWithhinPartitions替代repartition与sort类操作 原则七：广播大变量 原则八：使用Kryo优化序列化性能 conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”) conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) 优化数据结构 原则十：Data Locality PROCESS_LOCAL data is in the same JVM as the running code. This is the best locality possible NODE_LOCAL data is on the same node. Examples might be in HDFS on the same node, or in another executor on the same node. This is a little slower than PROCESS_LOCAL because the data has to travel between processes NO_PREF data is accessed equally quickly from anywhere and has no locality preference RACK_LOCAL data is on the same rack of servers. Data is on a different server on the same rack so needs to be sent over the network, typically through a single switch ANY data is elsewhere on the network and not in the same rack 默认值-spark.locality.wait-3s spark.locality.wait.process-建议60s park.locality.wait.node-建议30s spark.locality.wait.rack-建议20s 数据倾斜（面试的重点） 美团技术博客数据倾斜：数据倾斜发生时的现象： 绝大多数task执行的都非常快，但个别task执行极慢 原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常数据倾斜发生的最根本原因如何定位导致数据倾斜的代码： shuffle（找代码里面发生shuffle的算子） stage划分 界面观察就可以定位到是哪个算子导致的数据倾斜如何定位到哪个key导致的数据倾斜： 方式一： countBykey 有可能出来结果，但是会遇到数据倾斜 方式二：sample countBykey 方案解决：解决方案一：使用Hive ETL预处理数据 方案实现思路—-Hive实现预处理 方案实现原理—-数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已 方案优点—–实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升 方案缺点—–治标不治本，Hive ETL中还是会发生数据倾斜 解决方案二：过滤少量导致倾斜的key 方案实现原理-将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。 方案优点—-实现简单，而且效果也很好，可以完全规避掉数据倾斜 方案缺点—-key对于我们来说，没有实际意义才行 解决方案三：提高shuffle操作的并行度（没多大用） 方案优点—-实现起来比较简单，可以有效缓解和减轻数据倾斜的影响 方案缺点—-只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 解决方案四：两阶段聚合（局部聚合+全局聚合） 方案实现原理—-将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。 方案优点—-对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 方案缺点—-仅仅适用于聚合类的shuffle操作—-groupBykey，join类的shuffle操作]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive环境搭建]]></title>
    <url>%2FHive%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.html</url>
    <content type="text"><![CDATA[Hive安装内嵌Dervy版本 上传安装包 apache-hive-2.3.2-bin.tar.gz 解压安装包 tar -zxvf apache-hive-2.3.2-bin.tar.gz -C /home/hadoop/apps/ 进入到 bin 目录，运行 hive 脚本：[hadoop@hadoop02 bin]$ ./hive注意： 这时候一般会报错：Terminal initialization failed; falling back to unsupported，是因为 hadoop（/root/apps/hadoop-2.6.5/share/hadoop/yarn/lib）集群的 jline-0.9.94.jar 包版本 过低，替换成 hive/lib 中的 jline-2.12.jar 包即可。记住：所有 hdfs 节点都得替换 hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.4.jar 替换成 jline-2.12.jar 如果报错就按照此方式解决，没有报错就不用管，在使用新的 hadoop-2.7.5 版本中已经不 存在这个问题。所以不用关注。 外置 MySQL 版本 准备好 MySQL（请参考以下文档，或者自行安装 MySQL，或者一个可用的 MySQL） 上传安装包 apache-hive-2.3.2-bin.tar.gz 解压安装包 tar -zxvf apache-hive-2.3.2-bin.tar.gz -c ~/apps/ 修改配置文件 123[hadoop@hadoop02 conf]# touch hive-site.xml [hadoop@hadoop02 conf]# vi hive-site.xml ` 123456789101112131415161718192021222324&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive_metastore_232?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;!-- 如果 mysql 和 hive 在同一个服务器节点，那么请更改 hadoop02 为 localhost --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 可选配置，该配置信息用来指定 Hive 数据仓库的数据存储在 HDFS 上的目录12345&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/myhive/warehouse&lt;/value&gt; &lt;description&gt;hive default warehouse, if nessecory, change it&lt;/description&gt; &lt;/property&gt; 一定要记得加入 MySQL 驱动包（mysql-connector-java-5.1.40-bin.jar） 该 jar 包放置在 hive 的根路径下的 lib 目录 安装完成，配置环境变量 vi ~/.bashrc 添加以下两行内容： export HIVE_HOME=/home/hadoop/apps/apache-hive-2.3.2-bin export PATH=$PATH:$HIVE_HOME/bin 保存退出。 最后不要忘记：[hadoop@hadoop02 bin]$ source ~/.bashrc 验证 Hive 安装，执行命令hive –help 初始化元数据库注意：当使用的 hive 是 2.x 之前的版本，不做初始化也是 OK 的，当 hive 第一次启动的 时候会自动进行初始化，只不过会不会生成足够多的元数据库中的表。在使用过程中会 慢慢生成。但最后进行初始化。如果使用的 2.x 版本的 Hive，那么就必须手动初始化元 数据库。使用命令： 1[hadoop@hadoop02 bin]$ schematool -dbType mysql -initSchema 启动 Hive 客户端 1[hadoop@hadoop02 bin]$ hive --service cli 退出 Hive Linux RPM 方式安装 MySQL（记得使用 root 账户进行操作，若使用普通用户，那么请修改相应文件夹权限） 检查以前是否装过 MySQL 1rpm -qa|grep -i mysql 发现有的话就都卸载 1rpm -e --nodeps ........ 删除老版本 mysql 的开发头文件和库 123456rm -fr /usr/lib/mysql #数据库目录 rm -fr /usr/include/mysql rm -f /etc/my.cnf rm -fr /var/lib/mysql 注意：卸载后/var/lib/mysql 中的数据及/etc/my.cnf 不会删除，确定没用后就手工删除 准备安装包 MySQL-5.6.26-1.linux_glibc2.5.x86_64.rpm-bundle.tar， 上传，解压 命令：tar -zxvf MySQL-5.6.26-1.linux_glibc2.5.x86_64.rpm-bundle.tar 开始安装 安装 server rpm -ivh MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm 安装客户端 rpm -ivh MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpm 登陆 MYSQL（登录之前千万记得一定要启动 mysql 服务） 启动命令： [hadoop@hadoop01 ~]$ service mysql start然后登陆，初始密码在 /root/.mysql_secret 这个文件里 修改密码 set PASSWORD=PASSWORD(‘root’); 退出登陆验证，看是否改密码成功 增加远程登陆权限，执行以下两个命令： 12mysql&gt;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;root&apos; WITH GRANT OPTION; mysql&gt;FLUSH PRIVILEGES; 命令释义：grant 权限 1,权限 2,„权限 n on 数据库名称.表名称 to 用户名@用户地址 identified by ‘密码’; PS：1,权限 2,„权限 n 代表 select，insert，update，delete，create，drop，index，alter，grant， references，reload，shutdown，process，file 等 14 个权限。 当权限 1,权限 2,„权限 n 被 all privileges 或者 all 代替，表示赋予用户全部权限。 当数据库名称.表名称被.代替，表示赋予用户操作服务器上所有数据库所有表的权限。 用户地址可以是 localhost，也可以是 ip 地址、机器名字、域名。也可以用’%’地址连接。 至此 mysql 安装成功 更改数据库的默认编码为 UTF-8]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hive数据存储]]></title>
    <url>%2FHive%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8.html</url>
    <content type="text"><![CDATA[、Hive 的存储结构包括数据库、表、视图、分区和表数据等。数据库，表，分区等等都对 应 HDFS 上的一个目录。表数据对应 HDFS 对应目录下的文件。 Hive 中所有的数据都存储在 HDFS 中，没有专门的数据存储格式，因为 Hive 是读模式 （Schema On Read） ，可支持 TextFile，SequenceFile，RCFile 或者自定义格式等 只需要在创建表的时候告诉 Hive 数据中的列分隔符和行分隔符，Hive 就可以解析数据 Hive 的默认列分隔符：控制符 Ctrl + A，\x01 Hive 的默认行分隔符：换行符 \n 、Hive 中包含以下数据模型： database：在 HDFS 中表现为${hive.metastore.warehouse.dir}目录下一个文件夹 table：在 HDFS 中表现所属 database 目录下一个文件夹 external table：与 table 类似，不过其数据存放位置可以指定任意 HDFS 目录路径 partition：在 HDFS 中表现为 table 目录下的子目录 bucket：在 HDFS 中表现为同一个表目录或者分区目录下根据某个字段的值进行 hash 散 列之后的多个文件 view：与传统数据库类似，只读，基于基本表创建 Hive 的元数据存储在 RDBMS 中，除元数据外的其它所有数据都基于 HDFS 存储。默认情 况下，Hive 元数据保存在内嵌的 Derby 数据库中，只能允许一个会话连接，只适合简单的 测试。实际生产环境中不适用，为了支持多用户会话，则需要一个独立的元数据库，使用 MySQL 作为元数据库，Hive 内部对 MySQL 提供了很好的支持 Hive 中的表分为 内部表、 外部表、 分区表 和 Bucket表内部表和外部表的区别：删除内部表，删除表元数据和数据 删除外部表，删除元数据，不删除数据 内部表和外部表的使用选择：大多数情况，他们的区别不明显，如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表，但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中使用外部表的场景是针对一个数据集有多个不同的 Schema通过外部表和内部表的区别和使用选择的对比可以看出来，hive 其实仅仅只是对存储在 HDFS 上的数据提供了一种新的抽象。而不是管理存储在 HDFS 上的数据。所以不管创建内部 表还是外部表，都可以对 hive 表的数据存储目录中的数据进行增删操作。 分区表和分桶表的区别：Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。同 时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所 以对添加进分区的数据不做模式校验，分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive架构]]></title>
    <url>%2FHive%E6%9E%B6%E6%9E%84.html</url>
    <content type="text"><![CDATA[基本组成用户接口 CLI，Shell 终端命令行（Command Line Interface），采用交互形式使用 Hive 命令行与 Hive 进行交互，最常用（学习，调试，生产） JDBC/ODBC，是 Hive 的基于 JDBC 操作提供的客户端，用户（开发员，运维人员）通过 这连接至 Hive server 服务 Web UI，通过浏览器访问 Hive Thrift ServerThrift 是 Facebook 开发的一个软件框架，可以用来进行可扩展且跨语言的服务的开发， Hive 集成了该服务，能让不同的编程语言调用 Hive 的接口 元数据存储 元数据，通俗的讲，就是存储在 Hive 中的数据的描述信息。 Hive 中的元数据通常包括：表的名字，表的列和分区及其属性，表的属性（内部表和 外部表），表的数据所在目录 Metastore 默认存在自带的 Derby 数据库中。缺点就是不适合多用户操作，并且数据存 储目录不固定。数据库跟着 Hive 走，极度不方便管理 解决方案：通常存我们自己创建的 MySQL 库（本地 或 远程） Hive 和 MySQL 之间通过 MetaStore 服务交互 Driver：编译器（Compiler），优化器（Optimizer），执行器（Executor） Driver 组件完成 HQL 查询语句从词法分析，语法分析，编译，优化，以及生成逻辑执行 计划的生成。生成的逻辑执行计划存储在 HDFS 中，并随后由 MapReduce 调用执行 Hive 的核心是驱动引擎， 驱动引擎由四部分组成： (1) 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST） (2) 编译器：编译器是将语法树编译为逻辑执行计划 (3) 优化器：优化器是对逻辑执行计划进行优化 (4) 执行器：执行器是调用底层的运行框架执行逻辑执行计划 执行流程HiveQL 通过命令行或者客户端提交，经过 Compiler 编译器，运用 MetaStore 中的元数 据进行类型检测和语法分析，生成一个逻辑方案(Logical Plan)，然后通过的优化处理，产生 一个 MapReduce 任务。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive基本概念]]></title>
    <url>%2FHive%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.html</url>
    <content type="text"><![CDATA[Hive的基本概念 开发者: Facebook实现并开源 作用: 基于Hadoop的一个数据仓库工具，可以将结构化的数据映射为一张数据库表，并提供HQL(Hive SQL)查询功能，底层数据是存储在HDFS上。 本质: 将SQL语句转换为MapReduce任务运行，使不熟悉Mapreduce的用户很方便地利用HQL处理和计算HDFS上的结构化的数据，使用于离线的批量数据计算。 Hive 依赖于 HDFS 存储数据，Hive 将 HQL 转换成 MapReduce 执行 所以说 Hive 是基于 Hadoop 的一个数据仓库工具，实质就是一款基于 HDFS 的 MapReduce 计算框架，对存储在 HDFS 中的数据进行分析和管理 hbase 和 hive 的区别 hbase： 数据库 hive : 数据仓库 区别：1、数据库，对于数据会做精细化的管理，具有事务的概念 数据仓库，存储数据的格式就类似打包，没有事务的概念 2、操作方式的区别： 数据库：NoSQL语法 put get scan delele 数据仓库: SQL方言 Hive的SQL === HQL hibernate:HQL 3、用途的区别： 数据库：OLTP 联机事务处理 增删改 数据仓库： OLAP 联机分析处理 查询 hive是数据仓库，它根本就不支持 update和delete 但是支持insert 4、模式上的区别 数据库： 写模式 hbase无严格模式 ： 仅有的模式校验只有 表名和列簇的名称 数据仓库： 读模式 Hive的特点优点 可扩展性,横向扩展，Hive 可以自由的扩展集群的规模，一般情况下不需要重启服务 横向扩展：通过分担压力的方式扩展集群的规模 纵向扩展： 一台服务器cpu i7-6700k 4核心8线程， 8核心16线程，内存64G =&gt; 128G 延展性，Hive 支持自定义函数，用户可以根据自己的需求来实现自己的函数 良好的容错性，可以保障即使有节点出现问题，SQL 语句仍可完成执行 缺点 Hive 不支持记录级别的增删改操作，但是用户可以通过查询生成新表或者将查询结 果导入到文件中（当前选择的 hive-2.3.2 的版本支持记录级别的插入操作） Hive 的查询延时很严重，因为 MapReduce Job 的启动过程消耗很长时间，所以不能 用在交互查询系统中。 Hive 不支持事务（因为不没有增删改，所以主要用来做 OLAP（联机分析处理），而 不是 OLTP（联机事务处理），这就是数据处理的两大级别） Hive 只适合用来做海量离线数 据统计分析，也就是数据仓库]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase底层原理]]></title>
    <url>%2Fhbase%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86.html</url>
    <content type="text"><![CDATA[1、系统架构 client职责 HBase有两张特殊表 .METE.：记录了用户所有表拆分出来的Region映射信息，.META.可以有多个Region -ROOT-:记录了.METE.表的Region信息，-ROOT-只有一个Region，无论如何不会分裂 Client访问用户数据过程： 1、首先访问zookeeper，找到-root-表的region所在的位置 2、然后访问-ROOT-表，接着访问.META.表 3、最后才能找到用户数据的位置去访问中间需要多次网络操作，不过Client端会做cache缓存 ZooKeeper职责 ZooKeeper 为 HBase 提供 Failover 机制，选举 Master，避免单点 Master 单点故障问题 存储所有 Region 的寻址入口，即-ROOT-表的位置信息 （在哪台服务器上） 实时监控 RegionServer 的状态，将 RegionServer 的上线和下线信息实时通知给 Master 存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family Master职责 为RegionServer分配Region 负责RegionServer的负载均衡 发现失效的 RegionServer 并重新分配其上的 Region HDFS 上的垃圾文件（HBase）回收 处理 Schema 更新请求（表的创建，删除，修改，列簇的增加等等） RegionServer职责 RegionServer 维护 Master 分配给它的 Region，处理对这些 Region 的 IO 请求 RegionServer 负责 Split 在运行过程中变得过大的 Region，负责 Compact 操作 （溢出到磁盘的文件有可能会有很多，会进行合并，把rowkey相同的所有keyvalue对象收集到一起，进行合并） 注意：1、可以看到，client 访问 HBase 上数据的过程并不需要 master 参与（寻址访问 zookeeper 和RegioneServer，数据读写访问 RegioneServer）， Master 仅仅维护者 Table 和 Region 的元数据Stay hungry Stay foolish – http://blog.csdn.net/zhongqi2513 信息，负载很低。 2、.META. 存的是所有的 Region 的位置信息，那么 RegioneServer 当中 Region 在进行分裂之后 的新产生的 Region，是由 Master 来决定发到哪个 RegioneServer，这就意味着，只有 Master 知道 new Region 的位置信息，所以，由 Master 来管理.META.这个表当中的数据的 CRUD所以结合以上两点表明，在没有 Region 分裂的情况，Master 宕机一段时间是可以忍受的 2、物理存储整体物理结构 Table 中的所有行都按照 RowKsey 的字典序排列。 Table 在行的方向上分割为多个 HRegion。 HRegion 按大小分割的(默认 10G)，每个表一开始只有一个 HRegion，随着数据不断插入 表，HRegion 不断增大，当增大到一个阀值的时候，HRegion 就会等分会两个新的 HRegion。 当表中的行不断增多，就会有越来越多的 HRegion。 HRegion 是 Hbase 中分布式存储和负载均衡的最小单元。最小单元就表示不同的 HRegion 可以分布在不同的 HRegionserver 上。但一个 HRegion 是不会拆分到多个 server 上的。 HRegion 虽然是负载均衡的最小单元，但并不是物理存储的最小单元。事实上，HRegion 由一个或者多个 Store 组成，每个 Store 保存一个 Column Family。每个 Strore 又由一个 memStore 和 0 至多个 StoreFile 组成 StoreFile和HFile结构 MemStore和StoreFile 一个 Hregion 由多个 Store 组成，每个 Store 包含一个列族的所有数据 Store 包括位于内存的一个 memstore 和位于硬盘的多个 storefile 组成 写操作先写入 memstore，当 memstore 中的数据量达到某个阈值，HRegionServer 启动 flushcache 进程写入 storefile，每次写入形成单独一个 Hfile 当总 storefile 大小超过一定阈值后，会把当前的 region 分割成两个，并由 HMaster 分配给相 应的 region 服务器，实现负载均衡 客户端检索数据时，先在 memstore 找，找不到再找 storefile HLog(WAL) WAL 意为 Write ahead log(http://en.wikipedia.org/wiki/Write-ahead_logging)，类似 mysql 中的 binlog，用来做灾难恢复之用，Hlog 记录数据的所有变更，一旦数据修改，就可以从 log 中 进行恢复。 每个 Region Server 维护一个 Hlog,而不是每个 Region 一个。这样不同 region(来自不同 table) 的日志会混在一起，这样做的目的是不断追加单个文件相对于同时写多个文件而言，可以减 少磁盘寻址次数，因此可以提高对 table 的写性能。带来的麻烦是，如果一台 region server 下线，为了恢复其上的 region，需要将 region server 上的 log 进行拆分，然后分发到其它 region server 上进行恢复。 HLog 文件就是一个普通的 Hadoop Sequence File： 1、HLog Sequence File 的 Key 是 HLogKey 对象，HLogKey 中记录了写入数据的归属信息，除 了 table 和 region 名字外，同时还包括 sequence number 和 timestamp，timestamp 是”写入 时间”，sequence number 的起始值为 0，或者是最近一次存入文件系统中 sequence number。 HLog Sequece File 的 Value 是 HBase 的 KeyValue 对象，即对应 HFile 中的 KeyValue 3、寻址机制既然读写都在 RegionServer 上发生，我们前面有讲到，每个 RegionSever 为一定数量的 Region 服务，那么 Client 要对某一行数据做读写的时候如何能知道具体要去访问哪个 RegionServer 呢？那就是接下来我们要讨论的问题 老的Region寻址方式在 HBase-0.96 版本以前，HBase有两个特殊的表，分别是-ROOT-表和.META.表，其中-ROOT的位置存储在 ZooKeeper 中， -ROOT-本身存储了.META. Table 的 RegionInfo 信息，并且-ROOT不会分裂，只有一个 Region。而 .META.表可以被切分成多个 Region。读取的流程如下图所示：详细步骤： 第 1 步：Client 请求 ZooKeeper 获得-ROOT-所在的 RegionServer 地址 第 2 步：Client 请求-ROOT-所在的 RS 地址，获取.META.表的地址，Client 会将-ROOT-的相关 信息 cache 下来，以便下一次快速访问 第 3 步：Client 请求.META.表的 RegionServer 地址，获取访问数据所在 RegionServer 的地址， Client 会将.META.的相关信息 cache 下来，以便下一次快速访问 第 4 步：Client 请求访问数据所在 RegionServer 的地址，获取对应的数据 从上面的路径我们可以看出，用户需要 3 次请求才能直到用户 Table 真正的位置，这在一定 程序带来了性能的下降。在 0.96 之前使用 3 层设计的主要原因是考虑到元数据可能需要很 大。但是真正集群运行，元数据的大小其实很容易计算出来。在 BigTable 的论文中，每行 METADATA 数据存储大小为 1KB 左右，如果按照一个 Region 为 128M 的计算，3 层设计可以支持的 Region 个数为 2^34 个，采用 2 层设计可以支持 2^17（131072）。那么 2 层设计的情 况下一个集群可以存储 4P 的数据。这仅仅是一个 Region 只有 128M 的情况下。如果是 10G 呢? 因此，通过计算，其实 2 层设计就可以满足集群的需求。因此在 0.96 版本以后就去掉 了-ROOT-表了。 4、读写过程读请求过程1、客户端通过 ZooKeeper 以及-ROOT-表和.META.表找到目标数据所在的 RegionServer(就是 数据所在的 Region 的主机地址) 2、联系 RegionServer 查询目标数据 3、RegionServer 定位到目标数据所在的 Region，发出查询请求 4、Region 先在 Memstore 中查找，命中则返回 5、如果在 Memstore 中找不到，则在 Storefile 中扫描 为了能快速的判断要查询的数据在不在这个 StoreFile 中，应用了 BloomFilter （BloomFilter，布隆过滤器：迅速判断一个元素是不是在一个庞大的集合内，但是他有一个 弱点：它有一定的误判率） （误判率：原本不存在与该集合的元素，布隆过滤器有可能会判断说它存在，但是，如果 布隆过滤器，判断说某一个元素不存在该集合，那么该元素就一定不在该集合内） 写请求过程 Client 先根据 RowKey 找到对应的 Region 所在的 RegionServer Client 向 RegionServer 提交写请求 RegionServer 找到目标 Region Region 检查数据是否与 Schema 一致 如果客户端没有指定版本，则获取当前系统时间作为数据版本 将更新写入 WAL Log 将更新写入 Memstore 判断 Memstore 的是否需要 flush 为 StoreFile 文件。 Hbase 在做数据插入操作时，首先要找到 RowKey 所对应的的 Region，怎么找到的？其实这 个简单，因为.META.表存储了每张表每个 Region 的起始 RowKey 了。 建议：在做海量数据的插入操作，避免出现递增 rowkey 的 put 操作 如果 put 操作的所有 RowKey 都是递增的，那么试想，当插入一部分数据的时候刚好进行分 裂，那么之后的所有数据都开始往分裂后的第二个 Region 插入，就造成了数据热点现象。 细节描述： HBase 使用 MemStore 和 StoreFile 存储对表的更新。 数据在更新时首先写入 HLog(WAL Log)，再写入内存(MemStore)中，MemStore 中的数据是排 序的，当 MemStore 累计到一定阈值时，就会创建一个新的 MemStore，并且将老的 MemStore 添加到 flush 队列，由单独的线程 flush 到磁盘上，成为一个 StoreFile。于此同时，系统会在 ZooKeeper 中记录一个 redo point，表示这个时刻之前的变更已经持久化了。当系统出现意 Stay hungry Stay foolish – http://blog.csdn.net/zhongqi2513外时，可能导致内存(MemStore)中的数据丢失，此时使用 HLog(WAL Log)来恢复 checkpoint 之后的数据。 StoreFile 是只读的，一旦创建后就不可以再修改。因此 HBase 的更新/修改其实是不断追加 的操作。当一个 Store 中的 StoreFile 达到一定的阈值后，就会进行一次合并(minor_compact, major_compact)，将对同一个 key 的修改合并到一起，形成一个大的 StoreFile，当 StoreFile 的大小达到一定阈值后，又会对 StoreFile 进行 split，等分为两个 StoreFile。由于对表的更 新是不断追加的，compact 时，需要访问 Store 中全部的 StoreFile 和 MemStore，将他们按 rowkey 进行合并，由于 StoreFile 和 MemStore 都是经过排序的，并且 StoreFile 带有内存中 索引，合并的过程还是比较快。 major_compact 和 minor_compact 的区别： minor_compact 仅仅合并小文件（HFile） major_compact 合并一个 region 内的所有文件 Client 写入 -&gt; 存入 MemStore，一直到 MemStore 满 -&gt; Flush 成一个 StoreFile，直至增长到 一定阈值 -&gt; 触发 Compact 合并操作 -&gt; 多个 StoreFile 合并成一个 StoreFile，同时进行版本 合并和数据删除 -&gt; 当StoreFiles Compact后，逐步形成越来越大的StoreFile -&gt; 单个StoreFile 大小超过一定阈值后，触发 Split 操作，把当前 Region Split 成 2 个 Region，Region 会下线， 新 Split 出的 2 个孩子 Region 会被 HMaster 分配到相应的 HRegionServer 上，使得原先 1 个 Region 的压力得以分流到 2 个 Region 上由此过程可知，HBase 只是增加数据，有所得更新 和删除操作，都是在 Compact 阶段做的，所以，用户写操作只需要进入到内存即可立即返 回，从而保证 I/O 高性能。 写入数据的过程补充： 工作机制：每个 HRegionServer 中都会有一个 HLog 对象，HLog 是一个实现 Write Ahead Log 的类，每次用户操作写入 Memstore 的同时，也会写一份数据到 HLog 文件，HLog 文件定期 会滚动出新，并删除旧的文件(已持久化到 StoreFile 中的数据)。当 HRegionServer 意外终止 后，HMaster 会通过 ZooKeeper 感知，HMaster 首先处理遗留的 HLog 文件，将不同 Region 的log数据拆分，分别放到相应Region目录下，然后再将失效的Region（带有刚刚拆分的log） 重新分配，领取到这些 Region 的 HRegionServer 在 load Region 的过程中，会发现有历史 HLog 需要处理，因此会 Replay HLog 中的数据到 MemStore 中，然后 flush 到 StoreFiles，完成数据 恢复。 5、RegionServer工作机制 Region 分配 任何时刻，一个 Region 只能分配给一个 RegionServer。master 记录了当前有哪些可用的 RegionServer。以及当前哪些 Region 分配给了哪些 RegionServer，哪些 Region 还没有分配。 当需要分配的新的 Region，并且有一个 RegionServer 上有可用空间时，Master 就给这个 RegionServer 发送一个装载请求，把 Region 分配给这个 RegionServer。RegionServer 得到请 求后，就开始对此 Region 提供服务。 RegionServer 上线Master 使用 zookeeper 来跟踪 RegionServer 状态。当某个 RegionServer 启动时，会首先在 ZooKeeper 上的 server 目录下建立代表自己的 znode。由于 Master 订阅了 server 目录上的变 更消息，当 server 目录下的文件出现新增或删除操作时，Master 可以得到来自 ZooKeeper 的实时通知。因此一旦 RegionServer 上线，Master 能马上得到消息。 RegionServer 下线 当 RegionServer 下线时，它和 zookeeper 的会话断开，ZooKeeper 而自动释放代表这台 server 的文件上的独占锁。Master 就可以确定： 1、RegionServer 和 ZooKeeper 之间的网络断开了。 2、RegionServer 挂了。 无论哪种情况， RegionServer都无法继续为它的Region提供服务了，此时Master会删除server 目录下代表这台 RegionServer 的 znode 数据，并将这台 RegionServer 的 Region 分配给其它还 活着的同志。 6、Master工作机制Master 上线 Master 启动进行以下步骤: 从ZooKeeper上获取唯一一个代表Active Master的锁，用来阻止其它Master成为Master。 扫描 ZooKeeper 上的 server 父节点，获得当前可用的 RegionServer 列表。 和每个 RegionServer 通信，获得当前已分配的 Region 和 RegionServer 的对应关系。 扫描.META. Region 的集合，计算得到当前还未分配的 Region，将他们放入待分配 Region 列表。 ### Master 下线 由于 Master 只维护表和 Region 的元数据，而不参与表数据 IO 的过程，Master 下线仅 导致所有元数据的修改被冻结(无法创建删除表，无法修改表的 schema，无法进行 Region 的负载均衡，无法处理 Region 上下线，无法进行 Region 的合并，唯一例外的是 Region 的 split 可以正常进行，因为只有 RegionServer 参与)，表的数据读写还可以正常进行。因此 Master 下线短时间内对整个 hbase 集群没有影响。 从上线过程可以看到，Master 保存的信息全是可以冗余信息（都可以从系统其它地方 收集到或者计算出来） 因此，一般 HBase 集群中总是有一个 Master 在提供服务，还有一个以上的 Master 在等 待时机抢占它的位置。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount求共同好友代码实现]]></title>
    <url>%2Fwordcount%E6%B1%82%E5%85%B1%E5%90%8C%E5%A5%BD%E5%8F%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html</url>
    <content type="text"><![CDATA[12345678910package com.Practice.SameFriend;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;import org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.Arrays;/** * 求共同好友合并版 * 主要思路： 第一步：求出每一个好友所对应的所有用户 * 第二步：将第一步中所有用户进行排序，两两组合，最后求出两用户间的共同好友 */public class SameFriendMerge1 &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); //第一个job信息 Job job = Job.getInstance(conf); job.setJar(&quot;wordcountJar/wordcount.jar&quot;); job.setMapperClass(SFMerge1Mapper1.class); job.setReducerClass(SFMerge1Reducer1.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); Path inputPath = new Path(&quot;input/sameFriend&quot;); Path outputPath = new Path(&quot;output/sameFriend&quot;); if(fs.isDirectory(outputPath))&#123; fs.delete(outputPath,true); &#125; FileInputFormat.setInputPaths(job,inputPath); FileOutputFormat.setOutputPath(job,outputPath); //第二个job信息 Job job1 = Job.getInstance(conf); job1.setJar(&quot;wordcountJar/wordcount.jar&quot;); job1.setMapperClass(SFMerge1Mapper2.class); job1.setReducerClass(SFMerge1Reducer2.class); job1.setOutputKeyClass(Text.class); job1.setOutputValueClass(Text.class); Path inputPath1 = new Path(&quot;output/sameFriend&quot;); Path outputPath1 = new Path(&quot;output/sameFriend1&quot;); if(fs.isDirectory(outputPath1))&#123; fs.delete(outputPath1,true); &#125; FileInputFormat.setInputPaths(job1,inputPath1); FileOutputFormat.setOutputPath(job1,outputPath1); ControlledJob ctlJob1 = new ControlledJob(job.getConfiguration()); ControlledJob ctlJob2 = new ControlledJob(job.getConfiguration()); ctlJob1.setJob(job); ctlJob2.setJob(job1); ctlJob2.addDependingJob(ctlJob1); JobControl jobControl = new JobControl(&quot;SameFriends&quot;); jobControl.addJob(ctlJob1); jobControl.addJob(ctlJob2); Thread jobThread = new Thread(jobControl); jobThread.start(); // 每隔一段时间来判断一下该jc线程的任务是否执行完成 while (!jobControl.allFinished())&#123; Thread.sleep(500); &#125; jobControl.stop(); &#125; public static class SFMerge1Mapper1 extends Mapper&lt;LongWritable,Text,Text,Text&gt;&#123; private Text outKey = new Text(); private Text outValue = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] split = value.toString().split(&quot;:&quot;); outValue.set(split[0]); String[] friends = split[1].split(&quot;,&quot;); for (String str : friends) &#123; outKey.set(str); context.write(outKey,outValue); &#125; &#125; &#125; /** * 第一次reducer输出结果： A F,I,O,K,G,D,C,H,B B E,J,F,A C B,E,K,A,H,G,F D H,C,G,F,E,A,K,L E A,B,L,G,M,F,D,H F C,M,L,A,D,G G M H O I O,C J O K O,B L D,E M E,F O A,H,I,J,F */ public static class SFMerge1Reducer1 extends Reducer&lt;Text,Text,Text,Text&gt;&#123; private Text outValue = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); for (Text str : values) &#123; if( sb.length()!= 0 )&#123; sb.append(&quot;,&quot;); &#125; sb.append(str); &#125; outValue.set(sb.toString()); context.write(key,outValue); &#125; &#125; public static class SFMerge1Mapper2 extends Mapper&lt;LongWritable,Text,Text,Text&gt;&#123; private Text outValue = new Text(); private Text outKey = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] splits = value.toString().split(&quot;\t&quot;); String[] strings = splits[1].split(&quot;,&quot;); outValue.set(splits[0]); Arrays.sort(strings); for (int i = 0; i &lt; strings.length - 1; i++) &#123; for (int j = i+1; j &lt; strings.length; j++) &#123; outKey.set(strings[i]+&quot;-&quot;+strings[j]); context.write(outKey,outValue); &#125; &#125; &#125; &#125; /** * 第二次reducer输出结果： A-B E,C A-C D,F A-D E,F A-E B,C,D A-F C,E,O,D,B A-G E,F,C,D A-H C,D,E,O A-I O A-J O,B A-K C,D A-L F,D,E A-M F,E B-C A B-D A,E B-E C B-F C,A,E B-G E,C,A B-H E,C,A B-I A B-K A,C B-L E B-M E B-O A,K C-D A,F C-E D C-F A,D C-G A,D,F C-H D,A C-I A C-K A,D C-L D,F C-M F C-O I,A D-E L D-F A,E D-G E,A,F D-H A,E D-I A D-K A D-L E,F D-M F,E D-O A E-F D,M,C,B E-G C,D E-H C,D E-J B E-K C,D E-L D F-G D,C,A,E F-H A,D,O,E,C F-I O,A F-J B,O F-K D,C,A F-L E,D F-M E F-O A G-H D,C,E,A G-I A G-K D,A,C G-L D,F,E G-M E,F G-O A H-I O,A H-J O H-K A,C,D H-L D,E H-M E H-O A I-J O I-K A I-O A K-L D K-O A L-M E,F */ public static class SFMerge1Reducer2 extends Reducer&lt;Text,Text,Text,Text&gt;&#123; private Text outValue = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); for (Text str : values) &#123; if(sb.length()!= 0)&#123; sb.append(&quot;,&quot;); &#125; sb.append(str); outValue.set(sb.toString()); &#125; context.write(key,outValue); &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount学生成绩普通版案例]]></title>
    <url>%2Fwordcount%E5%AD%A6%E7%94%9F%E6%88%90%E7%BB%A9%E6%99%AE%E9%80%9A%E7%89%88%E6%A1%88%E4%BE%8B.html</url>
    <content type="text"><![CDATA[12345678910111213package com.Practice.StudentScores;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193import java.io.IOException;import java.util.ArrayList;import java.util.Collections;import java.util.List;/** * 题目：学生成绩普通版 * * computer,huangxiaoming,85 computer,xuzheng,54 computer,huangbo,86 computer,liutao,85 computer,huanglei,99 computer,liujialing,85 computer,liuyifei,75 computer,huangdatou,48 computer,huangjiaju,88 computer,huangzitao,85 english,zhaobenshan,57 english,liuyifei,85 english,liuyifei,76 english,huangdatou,48 english,zhouqi,85 english,huangbo,85 english,huangxiaoming,96 english,huanglei,85 english,liujialing,75 algorithm,liuyifei,75 algorithm,huanglei,76 algorithm,huangjiaju,85 algorithm,liutao,85 algorithm,huangdou,42 algorithm,huangzitao,81 math,wangbaoqiang,85 math,huanglei,76 math,huangjiaju,85 math,liutao,48 math,xuzheng,54 math,huangxiaoming,85 math,liujialing,85 * * 1、每一个course的最高分，最低分，平均分 返回结果格式： course max=95 min=22 avg=55 例子： computer max=99 min=48 avg=75 解题思路：在map以course作为key值，其余部分作为value，在reduce中设置变量max，min，avg，通过累计求出，并设置格式 2、求该成绩表当中出现了相同分数的分数，还有次数，以及该分数的人 返回结果的格式： 科目 分数 次数 该分数的人 例子： computer 85 3 huangzitao,liujialing,huangxiaoming 解题思路：求某科目中出现系统分数的人数以及分数，map以科目和分数作为key值，进行分组，在reduce中进行计数，当计数结果大于1时，输出分数，人数和人名 */public class StudentScores1 &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); Job job = Job.getInstance(conf); job.setJar(&quot;wordcountJar/wordcount.jar&quot;);// job.setMapperClass(StudentScoresMapper.class);// job.setReducerClass(StudentScoresReducer.class); job.setMapperClass(StudentScoresMapper2.class); job.setReducerClass(StudentScoresReducer2.class);// job.setOutputKeyClass(Text.class);// job.setOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); Path inputPath = new Path(&quot;input/studentScores&quot;); Path outputPath = new Path(&quot;output/studentScores&quot;); if(fs.isDirectory(outputPath))&#123; fs.delete(outputPath,true); &#125; FileInputFormat.setInputPaths(job,inputPath); FileOutputFormat.setOutputPath(job,outputPath); Boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion ? 0 : 1); &#125; /** * 第一题 */ public static class StudentScoresMapper extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt; &#123; private IntWritable outValue = new IntWritable(); private Text outKey = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] splits = value.toString().split(&quot;,&quot;); outKey.set(splits[0]); outValue.set(Integer.parseInt(splits[2])); context.write(outKey,outValue); &#125; &#125; /** * 第一题 */ public static class StudentScoresReducer extends Reducer&lt;Text,IntWritable,Text,Text&gt; &#123; private Text outValue = new Text(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int min = 1000 ; int max = 0 ; int avg ; int sum = 0 ; int count = 0 ; //方法一：最大最小通过逐一比较得到// for (IntWritable val:// values) &#123;// int score = val.get();// if(max &lt; score)&#123;// max = score;// &#125;// if(min &gt; score)&#123;// min = score ;// &#125;// sum += score ;// count++ ;// &#125;// avg = sum /count ;// String outStr = &quot;max=&quot;+max+&quot; min=&quot;+min+&quot; avg=&quot;+avg;// outValue.set(outStr); //方法二：最大最小值通过集合数组得到 List&lt;Integer&gt; scores = new ArrayList&lt;&gt;(); for (IntWritable val : values) &#123; scores.add(val.get()); sum += val.get(); count++; &#125; Collections.sort(scores); min = scores.get(0); max = scores.get(scores.size()-1); avg = sum / count ; String outStr = &quot;max=&quot;+max+&quot; min=&quot;+min+&quot; avg=&quot;+avg; outValue.set(outStr); context.write(key,outValue); &#125; &#125; /** * 第二题 */ public static class StudentScoresMapper2 extends Mapper&lt;LongWritable,Text,Text,Text&gt;&#123; private Text outKey = new Text(); private Text outValue = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] splits = value.toString().split(&quot;,&quot;); outKey.set(splits[0]+&quot;\t&quot;+splits[2]); outValue.set(splits[1]); context.write(outKey,outValue); &#125; &#125; /** * 第二题 */ public static class StudentScoresReducer2 extends Reducer&lt;Text,Text,Text,Text&gt;&#123; private Text outValue = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); int count = 0 ; for (Text text : values) &#123; if(sb.length()!=0)&#123; sb.append(&quot;,&quot;); &#125; sb.append(text); count++; &#125; //如果count大于等于2，说明有分数重合的 if(count &gt;= 2 )&#123; outValue.set(count+&quot; &quot;+sb.toString()); context.write(key,outValue); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount求学生平均成绩案例]]></title>
    <url>%2Fwordcount%E6%B1%82%E5%AD%A6%E7%94%9F%E5%B9%B3%E5%9D%87%E6%88%90%E7%BB%A9%E6%A1%88%E4%BE%8B.html</url>
    <content type="text"><![CDATA[1234567891011121314package com.Practice.AverageScores;import com.Practice.SameFriend.SameFriend;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import java.io.IOException;import java.util.StringTokenizer;/** * 求学生平均成绩 * 计算学生考试平均成绩 源数据： 张三 98 李四 96 王五 95 张三 90 李四 92 王五 99 张三 80 李四 90 王五 94 张三 82 李四 92 */public class AverageScores &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); Job job = Job.getInstance(conf); job.setJar(&quot;wordcountJar/wordcount.jar&quot;); job.setMapperClass(AverageScoresMapper.class); job.setReducerClass(AverageScoresReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); Path inputPath = new Path(&quot;input/AverageScore&quot;); Path outputPath = new Path(&quot;output/AverageScore&quot;); if(fs.isDirectory(outputPath))&#123; fs.delete(outputPath,true); &#125; FileInputFormat.setInputPaths(job,inputPath); FileOutputFormat.setOutputPath(job,outputPath); boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion ? 0 : 1 ); &#125; public static class AverageScoresMapper extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt; &#123; private Text outKey = new Text(); private IntWritable outValue = new IntWritable(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] splits = value.toString().split(&quot;\t&quot;); outKey.set(splits[0]); outValue.set(Integer.parseInt(splits[1])); context.write(outKey,outValue); &#125; &#125; public static class AverageScoresReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123; private IntWritable outValue = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int avg = 0 ; int sum = 0 ; int count = 0 ; for (IntWritable val : values) &#123; int score = val.get(); sum += score ; count ++ ; &#125; avg = sum /count ; outValue.set(avg); context.write(key,outValue); &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount数据去重案例]]></title>
    <url>%2Fwordcount%E6%95%B0%E6%8D%AE%E5%8E%BB%E9%87%8D%E6%A1%88%E4%BE%8B.html</url>
    <content type="text"><![CDATA[12345678910111213package com.Practice.RemoveDupData;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import java.io.IOException;/** * 题目：数据去重 * 解题思路：将每行数据作为key值，value值为空 * 2012-3-1 a 2012-3-2 b 2012-3-3 c 2012-3-4 d 2012-3-5 a 2012-3-6 b 2012-3-7 c 2012-3-3 c 2012-3-1 b 2012-3-2 a 2012-3-3 b 2012-3-4 d 2012-3-5 a 2012-3-6 c 2012-3-7 d 2012-3-3 c */public class RemoveDupData &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); Job job = Job.getInstance(conf); job.setJar(&quot;wordcountJar/wordcount.jar&quot;); job.setMapperClass(RemoveDupDataMapper.class); job.setReducerClass(RemoveDupDataReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); Path inputPath = new Path(&quot;input/RemoveDupData&quot;); Path outputPath = new Path(&quot;output/RemoveDupData&quot;); if(fs.isDirectory(outputPath))&#123; fs.delete(outputPath,true); &#125; FileInputFormat.setInputPaths(job,inputPath); FileOutputFormat.setOutputPath(job,outputPath); boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion ? 0 : 1 ); &#125; public static class RemoveDupDataMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; context.write(value,NullWritable.get()); &#125; &#125; public static class RemoveDupDataReducer extends Reducer&lt;Text,NullWritable,Text,NullWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key,NullWritable.get()); &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount 互为好友对案例]]></title>
    <url>%2Fwordcount%E4%BA%92%E4%B8%BA%E5%A5%BD%E5%8F%8B%E5%AF%B9%E6%A1%88%E4%BE%8B.html</url>
    <content type="text"><![CDATA[1234567891011121314package com.Practice.SameFriend2;import com.Practice.SameFriend.SameFriend;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import java.io.IOException;/** * 求互粉好友对 比如：如果A和B互为好友，那么A-B即为互粉好友对。 * A:B,C,D,F,E,O B:A,C,E,K C:F,A,D,I D:A,E,F,L E:B,C,D,M,L F:A,B,C,D,E,O,M G:A,C,D,E,F H:A,C,D,E,O I:A,O J:B,O K:A,C,D L:D,E,F M:E,F,G O:A,H,I,J,K */public class SameFriend2 &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); Job job = Job.getInstance(conf); job.setJar(&quot;wordcountJar/wordcount.jar&quot;); job.setMapperClass(SameFriend2Mapper.class); job.setReducerClass(SameFriend2Reducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); Path inputPath = new Path(&quot;input/SameFriend2&quot;); Path outputPath = new Path(&quot;output/SameFriend2&quot;); if(fs.isDirectory(outputPath))&#123; fs.delete(outputPath,true); &#125; FileInputFormat.setInputPaths(job,inputPath); FileOutputFormat.setOutputPath(job,outputPath); boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion ? 0 : 1 ); &#125; public static class SameFriend2Mapper extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt; &#123; private Text outKey = new Text(); private IntWritable outValue = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] splits = value.toString().split(&quot;:&quot;); String[] strings = splits[1].split(&quot;,&quot;); for (String str: strings) &#123; //关键代码：如果A和B互为好友，则必定存在A-B和B-A，此处的作用就是将A-B和B-A 都编程为A-B，或者都编程为B-A if(splits[0].compareTo(str)&gt;0)&#123; outKey.set(str+&quot;-&quot;+splits[0]); &#125;else&#123; outKey.set(splits[0]+&quot;-&quot;+str); &#125; context.write(outKey,outValue); &#125; &#125; &#125; /** * 如果互为好友对，count必定为2 */ public static class SameFriend2Reducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123; private IntWritable outValue = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0 ; for (IntWritable val : values) &#123; count += val.get(); &#125; //当count为2时，即可求出好友对 if(count == 2)&#123; outValue.set(count); context.write(key,outValue); &#125; &#125; &#125;&#125; ```]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA windows本地运行wordcount程序]]></title>
    <url>%2FIDEAwindows%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8Cwordcount%E7%A8%8B%E5%BA%8F.html</url>
    <content type="text"><![CDATA[第一步创建maven项目第二步创建WordCountDemo类12345678910111213141516package com.wordcountModel;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.StringTokenizer; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128/** * 本地运行wordcount程序 */public class WordCountDemo &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; //指定hdfs的相关参数 Configuration conf = new Configuration() ; FileSystem fs = FileSystem.get(conf); //通过Configuration对象获取job对象，该job对象会组织所有的该mapreduce的所有各种组件 Job job = Job.getInstance(conf); //指定jar包所在路径，本地模式需要这样指定，如果不是本地，则使用setJarByClass指定所在class文件即可 //job.setJarByClass(&quot;wordcountJar/wordcount.jar&quot;) job.setJar(&quot;wordcountJar/wordcount.jar&quot;); //指定mapper类和reducer类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); //Mapper的输入key-value类型，由mapreduce框架决定 //指定maptask的输出类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 假如 mapTask的输出key-value类型，跟reduceTask的输出key-value类型一致，那么，以上两句代码可以不用设置 // reduceTask的输入key-value类型 就是 mapTask的输出key-value类型。所以不需要指定 // 指定reducetask的输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 为job指定输入数据的组件和输出数据的组件，以下两个参数是默认的，所以不指定也是OK的 // job.setInputFormatClass(TextInputFormat.class); // job.setOutputFormatClass(TextOutputFormat.class); // 为该mapreduce程序制定默认的数据分区组件。默认是 HashPartitioner.class // job.setPartitionerClass(HashPartitioner.class); // 指定该mapreduce程序数据的输入和输出路径 Path inputPath = new Path(&quot;input/wordcount/&quot;); Path outputPath = new Path(&quot;output/wordcount/&quot;); // 设置该MapReduce程序的ReduceTask的个数,默认为1 // job.setNumReduceTasks(3); // 该段代码是用来判断输出路径存在不存在，存在就删除，虽然方便操作，但请谨慎 if(fs.isDirectory(outputPath))&#123; fs.delete(outputPath,true); &#125; //设置wordcount程序的输入路径 FileInputFormat.setInputPaths(job,inputPath); //设置wordcount程序的输出路径 FileOutputFormat.setOutputPath(job,outputPath); // job.submit(); // 最后提交任务(verbose布尔值 决定要不要将运行进度信息输出给用户) boolean waitForCompletion = job.waitForCompletion(true); System.exit( waitForCompletion ? 0 : 1); &#125; /** * Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; * * KEYIN 是指框架读取到的数据的key的类型，在默认的InputFormat下，读到的key是一行文本的起始偏移量，所以key的类型是Long * VALUEIN 是指框架读取到的数据的value的类型,在默认的InputFormat下，读到的value是一行文本的内容，所以value的类型是String * KEYOUT 是指用户自定义逻辑方法返回的数据中key的类型，由用户业务逻辑决定，在此wordcount程序中，我们输出的key是单词，所以是String * VALUEOUT 是指用户自定义逻辑方法返回的数据中value的类型，由用户业务逻辑决定,在此wordcount程序中，我们输出的value是单词的数量，所以是Integer * * 但是，String ，Long等jdk中自带的数据类型，在序列化时，效率比较低，hadoop为了提高序列化效率，自定义了一套序列化框架 * 所以，在hadoop的程序中，如果该数据需要进行序列化（写磁盘，或者网络传输），就一定要用实现了hadoop序列化框架的数据类型 * * Long ----&gt; LongWritable * String ----&gt; Text * Integer ----&gt; IntWritable * Null ----&gt; NullWritable */ public static class WordcountMapper extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt; &#123; /** * LongWritable key : 该key就是value该行文本的在文件当中的起始偏移量 * Text value ： 就是MapReduce框架默认的数据读取组件TextInputFormat读取文件当中的一行文本 */ private Text word = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 切分单词 StringTokenizer tokenizer = new StringTokenizer(value.toString()); while (tokenizer.hasMoreTokens())&#123; word.set(tokenizer.nextToken()); // 每个单词计数一次，也就是把单词组织成&lt;hello,1&gt;这样的key-value对往外写出 context.write(word,new IntWritable(1)); &#125; &#125; &#125; /** * 首先，和前面一样，Reducer类也有输入和输出，输入就是Map阶段的处理结果，输出就是Reduce最后的输出 * reducetask在调我们写的reduce方法,reducetask应该收到了前一阶段（map阶段）中所有maptask输出的数据中的一部分 * （数据的key.hashcode%reducetask数==本reductask号），所以reducetaks的输入类型必须和maptask的输出类型一样 * * reducetask将这些收到kv数据拿来处理时，是这样调用我们的reduce方法的： 先将自己收到的所有的kv对按照k分组（根据k是否相同） * 将某一组kv中的第一个kv中的k传给reduce方法的key变量，把这一组kv中所有的v用一个迭代器传给reduce方法的变量values */ public static class WordcountReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123; /** * Text key : mapTask输出的key值 * Iterable&lt;IntWritable&gt; values ： key对应的value的集合（该key只是相同的一个key） * * reduce方法接收key值相同的一组key-value进行汇总计算 */ private IntWritable result = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; //结果汇总 int sum = 0 ; for (IntWritable val: values ) &#123; sum += val.get(); &#125; //汇总的结果往外输出 result.set(sum); context.write(key,result); &#125; &#125;&#125; 注意事项：wordcount程序加载配置文件的顺序为: 1、conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://hadoop01:9000&quot;); 2、通过加载配置文件： conf.addResource(&quot;hdfs_config/core-site.xml&quot;); conf.addResource(&quot;hdfs_config/hdfs-site.xml&quot;); 3、加载本地hadoop的jar包中的配置文件 所以，如果要进行本地运行wordcount程序，则使用第二种，即不需要手动配置，程序会自动加载。如果配置文件夹中已经存在已经配置好的文件，程序会优先加载配置文件夹中的配置文件。 第三步 导入jar包 第四步 运行程序注：部分注意事项写在程序注释中。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell实现九九乘法表]]></title>
    <url>%2Fshell%E5%AE%9E%E7%8E%B0%E4%B9%9D%E4%B9%9D%E4%B9%98%E6%B3%95%E8%A1%A8.html</url>
    <content type="text"><![CDATA[12345678for (( i=1;i&lt;=9;i++ ))do for (( j=1;j&lt;=i;j++ )) do echo -n "$j*$i=$[$j*$i] " done echo -e '\n'done]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置163云源repo]]></title>
    <url>%2F%E9%85%8D%E7%BD%AE163%E4%BA%91%E6%BA%90repo.html</url>
    <content type="text"><![CDATA[进入yum配置文件目录cd /etc/yum.repos.d/ 备份配置文件 mv CentOS-Base.repo CentOS-Base.repo.bak 下载163的配置 wget http://mirrors.163.com/.help/CentOS6- Base-163.repo，下载下来的文件名为 CentOS6- Base-163.repo 改名 mv CentOS6-Base-163.repo CentOS-Base.repo 更新数据库 yum update]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>rpm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring侵入式和非侵入式的区别]]></title>
    <url>%2FSpring%E4%BE%B5%E5%85%A5%E5%BC%8F%E5%92%8C%E9%9D%9E%E4%BE%B5%E5%85%A5%E5%BC%8F%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content type="text"><![CDATA[简单解释：侵入式：使用者编写代码时，需要继承或者实现框架的类或接口，需要依赖框架。非侵入式：使用者编写代码时，无需继承或者实现框架的类或接口，察觉不到框架的存在。 Spring框架是一种非侵入式的轻量级框架###1.非侵入式的技术体现允许在应用系统中自由选择和组装Spring框架的各个功能模块，并且不强制要求应用系统的类必须从Spring框架的系统API的某个类来继承或者实现某个接口。 2.如何实现非侵入式的设计目标的 1）应用反射机制，通过动态调用的方式来提供各方面的功能，建立核心组间BeanFactory 2）配合使用Spring框架中的BeanWrapper和BeanFactory组件类最终达到对象的实例创建和属性注入 3）优点：允许所开发出来的应用系统能够在不用的环境中自由移植，不需要修改应用系统中的核心功能实现的代码]]></content>
      <categories>
        <category>框架</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见SQL语句]]></title>
    <url>%2F%E5%B8%B8%E8%A7%81SQL%E8%AF%AD%E5%8F%A5.html</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147create database employeeselect * from emp;delete from emp where empno=9999;select ename,sal from emp ;select * from emp where sal &gt; 2000;select * from emp where deptno = 20 and sal &gt; 2000 ;select * from emp where deptno = 20 or sal &gt; 2000 ;select * from emp where sal between 1000 and 3000 ;select * from emp where empno = 7788 or empno = 7369 or empno=7521;&lt;!---more---&gt;-- inselect * from emp where empno in(7788,7369,7521);-- DISTINCT 去重select job from emp ;select DISTINCT job from emp ;-- 别名（字段，表）select empno 员工编号,ename 员工姓名 from emp ;select ename,sal,sal*1.05 新工资 from emp;select ename,sal from emp e ;select e.ename,e.sal from emp e ;-- null 的判断select * from emp where comm is not null ;--模糊查询--查询所有S打头的员工信息（模糊查询） % 代表0到多个字符select * from emp where ename like 'S%';--查询所有N结尾的select * from emp where ename like '%N';--查询所有包含S的select * from emp where ename like '%S%';--查询所有第二个字符为L的员工信息select * from emp where ename like '__L%';--排序 (默认升序 order by字段 [asc] | desc)--默认 升序select * from emp order by sal ;--降序select * from emp order by sal desc ;--按照工资的降序排序，如果工资一样的，则按照empno的升序排序(用逗号)select * from emp order by sal desc , empno ASC;--限制结果查询（limit m,n) m代表起始索引，n代表记录的数目,limit 仅适用于MySQLselect * from emp limit 5,5;--查询20号部门工资最高的员工的信息select * from emp where deptno = 20 order by sal desc limit 0,1 ;--计算3的15次方--数学函数select ABS(10); --绝对值select CEIL(-12.3); --向上取整select FLOOR(12.5); --向下取整select ROUND(12.6); --四舍五入select ROUNT(12.49，1); select POW(3,3); --幂运算select RAND(); --随机数[0,1)--字符串函数desc emp ;select LENGTH(ename) from emp ; --获取字符串长度select length('this is a apple');select LOWER(ename) from emp ; --转换为小写select UPPER('this is a page'); -- 转换大小写select substr('aabbcc',1,2); -- 从1开始select LPAD('smith',10,'*'); -- 开始字符串，总长度，padstr填充的字符select RPAD('smith',10,'*'); -- 右填充select TRIM(' smi th'); --去空格-- 日期select NOW();select sysdate();select CURRENT_DATE(); --当前日期select current_time(); --当前时间select YEAR('1998-09-09');select MONTH('1998-09-09');select DAY('1998-09-09');select DATE_ADD('1998-09-09',INTERVAL 2 YEAR);select * from student ;alter table student add birthday date AFTER age;-- 聚合函数--count /sum/avg/max/min-- 员工数（统计记录数）select count(*) from emp ;select count(1) from emp ;-- 统计非空字段数目select count(comm) from emp;-- SUMselect sum(sal) from emp ;-- AVG select avg(sal) from emp ;-- MINselect min(sal) from emp ;-- MAX select max(sal) from emp ;-- 分组函数 GROUP BY deptno-- 每个部门的平均工资-- group by 根据条件字段的值返回相应的记录数；但是在select字句中，只能出现聚合函数或者分组的条件字段。select deptno, avg(sal) from emp GROUP BY deptno ;-- 各个职位员工数？ jobselect job, count(*) from emp group by job;-- 平均工资大于2000的部门的部门编号和平均工资？ where 不能放group by之后，having可以，group by 和having 经常配套使用 -- 1.求出每个部门的平均工资 -- 2.平均工资&gt;2000select deptno,avg(sal) from emp GROUP BY deptno having avg(sal) &gt; 2000;-- where 和 having 的区别-- 查询工资大于1500的每个部门的平均工资select deptno, avg(sal) from emp where sal &gt; 1500 GROUP BY deptno ;-- 查询平均工资大于1500的部门编号和平均工资select deptno,avg(sal) from emp GROUP BY deptno having avg(sal) &gt; 1500 ;-- 加密函数select MD5('root');select SHA('root');select PASSWORD('root');-- 外键约束： foreign keycreate table classroom( cid int PRIMARY key, cname varchar(20));alter table student add CONSTRAINT FK_CID FOREIGN KEY(cid) REFERENCES classroom(cid);select * from emp ;-- 高级查询-- 多表查询-- 查询的结果集分布于多张表-- 查询员工编号为7788的员工姓名和部门名称。-- 内连接 ，注意：1 内连接的结果集与连接顺序无关 2 在多张表中都出现的数据才会出现在内连接的结果集中select e.ename,d.dname from emp e ,dept d where e.empno = 7788 and e.deptno = d.deptno;-- inner join ...on....select ename,dname from emp inner join dept on emp.deptno = dept.deptno ;-- inner join ...using... (局限,当deptno不同时)select ename,dname from emp inner join dept using(deptno)]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合常见面试题]]></title>
    <url>%2FJava%E9%9B%86%E5%90%88%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98.html</url>
    <content type="text"><![CDATA[Java集合框架是最常被问到的Java面试问题，要理解Java技术强大特性就有必要掌握集合框架。这里有一些实用问题，常在核心Java面试中问到。 什么是Java集合API Java集合框架API是用来表示和操作集合的统一框架，它包含接口、实现类、以及帮助程序员完成一些编程的算法。简言之，API在上层完成以下几件事： ● 编程更加省力，提高城程序速度和代码质量 ● 非关联的API提高互操作性 ● 节省学习使用新API成本 ● 节省设计新API的时间 ● 鼓励、促进软件重用 具体来说，有6个集合接口，最基本的是Collection接口，由三个接口Set、List、SortedSet继承，另外两个接口是Map、SortedMap，这两个接口不继承Collection，表示映射而不是真正的集合。 什么是Iterator 一些集合类提供了内容遍历的功能，通过java.util.Iterator接口。这些接口允许遍历对象的集合。依次操作每个元素对象。当使用 Iterators时，在获得Iterator的时候包含一个集合快照。通常在遍历一个Iterator的时候不建议修改集合本省。 Iterator与ListIterator有什么区别？ Iterator：只能正向遍历集合，适用于获取移除元素。ListIerator：继承Iterator，可以双向列表的遍历，同样支持元素的修改。 什么是HaspMap和Map？ Map是接口，Java 集合框架中一部分，用于存储键值对，HashMap是用哈希算法实现Map的类。 HashMap与HashTable有什么区别？对比Hashtable VS HashMap 两者都是用key-value方式获取数据。Hashtable是原始集合类之一（也称作遗留类）。HashMap作为新集合框架的一部分在Java2的1.2版本中加入。它们之间有一下区别： ● HashMap和Hashtable大致是等同的，除了非同步和空值（HashMap允许null值作为key和value，而Hashtable不可以）。 ● HashMap没法保证映射的顺序一直不变，但是作为HashMap的子类LinkedHashMap，如果想要预知的顺序迭代（默认按照插入顺序），你可以很轻易的置换为HashMap，如果使用Hashtable就没那么容易了。 ● HashMap不是同步的，而Hashtable是同步的。 ● 迭代HashMap采用快速失败机制，而Hashtable不是，所以这是设计的考虑点。 在Hashtable上下文中同步是什么意思？ 同步意味着在一个时间点只能有一个线程可以修改哈希表，任何线程在执行hashtable的更新操作前需要获取对象锁，其他线程等待锁的释放。 什么叫做快速失败特性 从高级别层次来说快速失败是一个系统或软件对于其故障做出的响应。一个快速失败系统设计用来即时报告可能会导致失败的任何故障情况，它通常用来停止正常的操作而不是尝试继续做可能有缺陷的工作。当有问题发生时，快速失败系统即时可见地发错错误告警。在Java中，快速失败与iterators有关。如果一个iterator在集合对象上创建了，其它线程欲“结构化”的修改该集合对象，并发修改异常 （ConcurrentModificationException） 抛出。 怎样使Hashmap同步？ HashMap可以通过Map m = Collections.synchronizedMap（hashMap）来达到同步的效果。 什么时候使用Hashtable，什么时候使用HashMap 基本的不同点是Hashtable同步HashMap不是的，所以无论什么时候有多个线程访问相同实例的可能时，就应该使用Hashtable，反之使用HashMap。非线程安全的数据结构能带来更好的性能。 如果在将来有一种可能—你需要按顺序获得键值对的方案时，HashMap是一个很好的选择，因为有HashMap的一个子类 LinkedHashMap。所以如果你想可预测的按顺序迭代（默认按插入的顺序），你可以很方便用LinkedHashMap替换HashMap。反观要是使用的Hashtable就没那么简单了。同时如果有多个线程访问HashMap，Collections.synchronizedMap（）可以代替，总的来说HashMap更灵活。 为什么Vector类认为是废弃的或者是非官方地不推荐使用？或者说为什么我们应该一直使用ArrayList而不是Vector 你应该使用ArrayList而不是Vector是因为默认情况下你是非同步访问的，Vector同步了每个方法，你几乎从不要那样做，通常有想要同步的是整个操作序列。同步单个的操作也不安全（如果你迭代一个Vector，你还是要加锁，以避免其它线程在同一时刻改变集合）.而且效率更慢。当然同样有锁的开销即使你不需要，这是个很糟糕的方法在默认情况下同步访问。你可以一直使用Collections.sychronizedList来装饰一个集合。 事实上Vector结合了“可变数组”的集合和同步每个操作的实现。这是另外一个设计上的缺陷。Vector还有些遗留的方法在枚举和元素获取的方法，这些方法不同于List接口，如果这些方法在代码中程序员更趋向于想用它。尽管枚举速度更快，但是他们不能检查如果集合在迭代的时候修改了，这样将导致问题。尽管以上诸多原因，oracle也从没宣称过要废弃Vector。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合体系结构以及集合和数组的区别]]></title>
    <url>%2FJava%E9%9B%86%E5%90%88%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E4%BB%A5%E5%8F%8A%E9%9B%86%E5%90%88%E5%92%8C%E6%95%B0%E7%BB%84%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content type="text"><![CDATA[数组和集合的定义一、数组 数组是java语言内置的数据类型，他是一个线性的序列，所有可以快速访问其他的元素，数组和其他语言不同，当你创建了一个数组时，他的容量是不变的，而且在生命周期也是不能改变的，还有JAVA数组会做边界检查，如果发现有越界现象，会报RuntimeException异常错误，当然检查边界会以效率为代价。二、集合 JAVA还提供其他集合，list，map，set，他们处理对象的时候就好像这些对象没有自己的类型一样，而是直接归根于Object，这样只需要创建一个集合，把对象放进去，取出时转换成自己的类型就行了。三、数组和集合的区别 一、数组声明了它容纳的元素的类型，而集合不声明。二、数组是静态的，一个数组实例具有固定的大小，一旦创建了就无法改变容量了。而集合是可以动态扩展容量，可以根据需要动态改变大小，集合提供更多的成员方法，能满足更多的需求。三、数组的存放的类型只能是一种（基本类型/引用类型）,集合存放的类型可以不是一种(不加泛型时添加的类型是Object)。四、数组是java语言中内置的数据类型,是线性排列的,执行效率或者类型检查都是最快的。集合体系结构 Collection├List （有序集合，允许相同元素和null）│├LinkedList （非同步，允许相同元素和null，遍历效率低插入和删除效率高）│├ArrayList （非同步，允许相同元素和null，实现了动态大小的数组，遍历效率高，用的多）│└Vector（同步，允许相同元素和null，效率低）│ └Stack（继承自Vector，实现一个后进先出的堆栈）└Set （无序集合，不允许相同元素，最多有一个null元素） |-HashSet(无序集合，不允许相同元素，最多有一个null元素) Map （没有实现collection接口，key不能重复，value可以重复，一个key映射一个value）├Hashtable （实现Map接口，同步，不允许null作为key和value，用自定义的类当作key的话要复写hashCode和eques方法，）├HashMap （实现Map接口，非同步，允许null作为key和value，用的多）└WeakHashMap（实现Map接口） Collection接口 Collection是最基本的集合接口，一个Collection代表一组Object，即Collection的元素（Elements）。一些Collection允许相同的元素而另一些不行。一些能排序而另一些不行。Java SDK不提供直接继承自Collection的类，Java SDK提供的类都是继承自Collection的“子接口”如List和Set。 所有实现Collection接口的类都必须提供两个标准的构造函数：无参数的构造函数用于创建一个空的Collection，有一个Collection参数的构造函数用于创建一个新的Collection，这个新的Collection与传入的Collection有相同的元素。后一个构造函数允许用户复制一个Collection。 如何遍历Collection中的每一个元素？不论Collection的实际类型如何，它都支持一个iterator()的方法，该方法返回一个迭代子，使用该迭代子即可逐一访问Collection中每一个元素。典型的用法如下： Iterator it = collection.iterator(); // 获得一个迭代子 while(it.hasNext()) { Object obj = it.next(); // 得到下一个元素 } 由Collection接口派生的两个接口是List和Set。一、List接口 List是有序的Collection，使用此接口能够精确的控制每个元素插入的位置。用户能够使用索引（元素在List中的位置，类似于数组下标）来访问List中的元素，这类似于Java的数组。和下面要提到的Set不同，List允许有相同的元素。 除了具有Collection接口必备的iterator()方法外，List还提供一个listIterator()方法，返回一个ListIterator接口，和标准的Iterator接口相比，ListIterator多了一些add()之类的方法，允许添加，删除，设定元素，还能向前或向后遍历。 实现List接口的常用类有LinkedList，ArrayList，Vector和Stack。LinkedList类 LinkedList实现了List接口，允许null元素。LinkenList底层采用了双向链表来存储数据，每个节点都存储着上一个节点和下一个节点的地址以及本节点的数据。此外LinkedList提供额外的get，remove，insert方法在LinkedList的首部或尾部。这些操作使LinkedList可被用作堆栈（stack），队列（queue）或双向队列（deque）。 注意：LinkedList没有同步方法。如果多个线程同时访问一个List，则必须自己实现访问同步。一种解决方法是在创建List时构造一个同步的List： List list = Collections.synchronizedList(new LinkedList(…));ArrayList类 ArrayList实现了可变大小的数组。它允许所有元素，包括null。ArrayList底层采用动态数组的存储方式，便利效率非常高，ArrayList是线程不安全的。size，isEmpty，get，set方法运行时间为常数。但是add方法开销为分摊的常数，添加n个元素需要O(n)的时间。其他的方法运行时间为线性。 每个ArrayList实例都有一个容量（Capacity），即用于存储元素的数组的大小。这个容量可随着不断添加新元素而自动增加，但是增长算法并没有定义。当需要插入大量元素时，在插入前可以调用ensureCapacity方法来增加ArrayList的容量以提高插入效率。ArrayList和LindedList的区别： 1.ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。 2.对于随机访问get和set，ArrayList觉得优于LinkedList，因为LinkedList要移动指针。 3.对于新增和删除操作add和remove，LinedList比较占优势，因为ArrayList要移动数据。Vector类 Vector非常类似ArrayList，但是Vector是同步的。由Vector创建的Iterator，虽然和ArrayList创建的Iterator是同一接口，但是，因为Vector是同步的，当一个Iterator被创建而且正在被使用，另一个线程改变了Vector的状态（例如，添加或删除了一些元素），这时调用Iterator的方法时将抛出ConcurrentModificationException，因此必须捕获该异常。Stack 类 Stack继承自Vector，实现一个后进先出的堆栈。Stack提供5个额外的方法使得Vector得以被当作堆栈使用。基本的push和pop方法，还有peek方法得到栈顶的元素，empty方法测试堆栈是否为空，search方法检测一个元素在堆栈中的位置。Stack刚创建后是空栈。二、Set接口Set是一种无序的并且不包含重复的元素的Collection，即任意的两个元素e1和e2都有e1.equals(e2)=false，Set最多有一个null元素。 很明显，Set的构造函数有一个约束条件，传入的Collection参数不能包含重复的元素。HashSet 类是哈希表实现的,HashSet中的数据是无序的，可以放入null，但只能放入一个null，两者中的值都不能重复，因为HashSet的底层实现是HashMap，但是HashSet只使用了HashMap的key来存取数据所以HashSet存的数据不能重复。 HashSet要求放入的对象必须实现HashCode()方法，放入的对象，是以hashcode码作为标识的，而具有相同内容的 String对象，hashcode是一样，所以放入的内容不能重复。但是同一个类的对象可以放入不同的实例 。 三、Map接口Map没有继承Collection接口，Map提供key到value的映射。一个Map中不能包含相同的key，每个key只能映射一个value。Map接口提供3种集合的视图，Map的内容可以被当作一组key集合，一组value集合，或者一组key-value映射。 Hashtable类Hashtable继承Map接口，实现一个key-value映射的哈希表。任何非空（non-null）的对象都可作为key或者value。 添加数据使用put(key, value)，取出数据使用get(key)，这两个基本操作的时间开销为常数。使用Hashtable的简单示例如下，将1，2，3放到Hashtable中，他们的key分别是”one”，”two”，”three”：Hashtable numbers = new Hashtable();numbers.put(“one”, new Integer(1));numbers.put(“two”, new Integer(2));numbers.put(“three”, new Integer(3)); 要取出一个数，比如2，用相应的key：Integer n = (Integer)numbers.get(“two”);System.out.println(“two = ” + n); 由于作为key的对象将通过计算其散列函数来确定与之对应的value的位置，因此任何作为key的对象都必须实现hashCode和equals方法。hashCode和equals方法继承自根类Object，如果你用自定义的类当作key的话，要相当小心，按照散列函数的定义，如果两个对象相同，即obj1.equals(obj2)=true，则它们的hashCode必须相同，但如果两个对象不同，则它们的hashCode不一定不同，如果两个不同对象的hashCode相同，这种现象称为冲突，冲突会导致操作哈希表的时间开销增大，所以尽量定义好的hashCode()方法，能加快哈希表的操作。 如果相同的对象有不同的hashCode，对哈希表的操作会出现意想不到的结果（期待的get方法返回null），要避免这种问题，只需要牢记一条：要同时复写equals方法和hashCode方法，而不要只写其中一个。 Hashtable是同步的。 HashMap类HashMap和Hashtable类似，不同之处在于HashMap是非同步的，并且允许null，即null value和null key，但是将HashMap视为Collection时（values()方法可返回Collection）。WeakHashMap类WeakHashMap是一种改进的HashMap，它对key实行“弱引用”，如果一个key不再被外部所引用，那么该key可以被GC回收。 总结1.如果涉及到堆栈，队列等操作，应该考虑用List，对于需要快速插入，删除元素，应该使用LinkedList，如果需要快速随机访问元素，应该使用ArrayList。2.如果程序在单线程环境中，或者访问仅仅在一个线程中进行，考虑非同步的类，其效率较高，如果多个线程可能同时操作一个类，应该使用同步的类。3.要特别注意对哈希表的操作，作为key的对象要正确复写equals和hashCode方法。4.尽量返回接口而非实际的类型，如返回List而非ArrayList，这样如果以后需要将ArrayList换成LinkedList时，客户端代码不用改变。这就是针对抽象编程。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据抽象]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E6%8A%BD%E8%B1%A1.html</url>
    <content type="text"><![CDATA[概念数据抽象结构：对现实世界的一种抽象从实际的人、物、事和概念中抽取所关心的共同特性，忽略非本质的细节，把这些特性用各种概念精确地加以描述这些概念组成了某种模型。 三种常用抽象分类（Classification)定义某一类概念作为现实世界中一组对象的类型，这些对象具有某些共同的特性和行为，它抽象了对象值和型之间的“is member of”的语义，在E-R模型中，实体型就是这种抽象。 聚集（Aggregation）定义某一类型的组成成分，它抽象了对象内部类型和成分之间“is part of”的语义，在E-R模型中若干属性的聚集组成了实体型，就是这种抽象。 概括（Generalization）定义类型之间的一种子集联系，它抽象了类型之间的“is subset of”的语义，概括有一个很重要的性质：继承性。子类继承超类上定义的所有抽象。注：原E-R模型不具有概括，本书对E-R模型作了扩充，允许定义超类实体型和子类实体型。用双竖边的矩形框表示子类，用直线加小圆圈表示超类-子类的联系 数据抽象的用途对需求分析阶段收集到的数据进行分类、组织（聚集），形成实体实体的属性，标识实体的码确定实体之间的联系类型(1:1，1:n，m:n) 类背后蕴含的基本思想是数据抽象和封装数据抽象是一种依赖于接口和实现分离的编程和设计技术。类的设计者必须关心类是如何实现的，但是使用该类的程序员不必了解这些细节，仅需抽象地考虑该类型能做什么。封装是一项将低层次的元素组合起来形成新的、高层次实体的技术。函数和类都是封装的形式。被封装的元素隐藏了他们的实现细节，其主要优点在于：避免类内部出现无意的、可能破坏对象状态的用户级错误；使得修改类的实现时只要保持接口不变，就无需改变用户级代码。]]></content>
      <categories>
        <category>干货</category>
      </categories>
      <tags>
        <tag>概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基本数据类型取值范围]]></title>
    <url>%2FJava%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%8F%96%E5%80%BC%E8%8C%83%E5%9B%B4.html</url>
    <content type="text"><![CDATA[在JAVA中一共有八种基本数据类型，他们分别是byte、short、int、long、float、double、char、boolean 整型其中byte、short、int、long都是表示整数的，只不过他们的取值范围不一样byte的取值范围为-128~127，占用1个字节（-2的7次方到2的7次方-1）short的取值范围为-32768~32767，占用2个字节（-2的15次方到2的15次方-1）int的取值范围为（-2147483648~2147483647），占用4个字节（-2的31次方到2的31次方-1）long的取值范围为（-9223372036854774808~9223372036854774807），占用8个字节（-2的63次方到2的63次方-1）可以看到byte和short的取值范围比较小，而long的取值范围太大，占用的空间多，基本上int可以满足我们的日常的计算了，而且int也是使用的最多的整型类型了。在通常情况下，如果JAVA中出现了一个整数数字比如35，那么这个数字就是int型的，如果我们希望它是byte型的，可以在数据后加上大写的 B：35B，表示它是byte型的，同样的35S表示short型，35L表示long型的，表示int我们可以什么都不用加，但是如果要表示long型的，就一定要在数据后面加“L”。 浮点型float和double是表示浮点型的数据类型，他们之间的区别在于他们的精确度不同float 3.402823e+38 ~ 1.401298e-45（e+38表示是乘以10的38次方，同样，e-45表示乘以10的负45次方）占用4个字节double 1.797693e+308~ 4.9000000e-324 占用8个字节double型比float型存储范围更大，精度更高，所以通常的浮点型的数据在不声明的情况下都是double型的，如果要表示一个数据是float型的，可以在数据后面加上“F”。浮点型的数据是不能完全精确的，所以有的时候在计算的时候可能会在小数点最后几位出现浮动，这是正常的。 boolean型（布尔型）这个类型只有两个值，true和false（真和非真）boolean t = true；boolean f = false；char型（文本型）用于存放字符的数据类型，占用2个字节，采用unicode编码，它的前128字节编码与ASCII兼容字符的存储范围在\u0000~\uFFFF，在定义字符型的数据时候要注意加’ ‘，比如 ‘1’表示字符’1’而不是数值1，char c = ‘ 1 ‘;我们试着输出c看看，System.out.println(c);结果就是1，而如果我们这样输出呢System.out.println(c+0);结果却变成了49。如果我们这样定义c看看char c = ‘ \u0031 ‘;输出的结果仍然是1，这是因为字符’1’对应着unicode编码就是\u0031char c1 = ‘h’,c2 = ‘e’,c3=’l’,c4=’l’,c5 = ‘o’;System.out.print(c1);System.out.print(c2);System.out.print(c3);System.out.print(c4);Sytem.out.print(c5);]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础编程题]]></title>
    <url>%2FJava%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B%E9%A2%98.html</url>
    <content type="text"><![CDATA[把一个数组arr[n]进行反转12345678910111213141516171819202122232425//方法一，思路：利用for循环，只循环n/2-1次，在同一个数组里进行值的交换 int[] arr = &#123;1,2,3,4,5,6,7,8,9&#125;; for (int i = 0,j = arr.length-1; i &lt; arr.length; i++,j--) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp ; if(i==3) break; &#125; System.out.println(Arrays.toString(arr)); &lt;!---more---&gt;//方法二，思路：重新new一个数组，将原数组逆向赋值给新数组int[] arrTemp = new int[arr.length]; for (int i = 0; i &lt; arr.length; i++) &#123; arrTemp[i] = arr[arr.length-i-1]; &#125; arr = arrTemp ; System.out.println(Arrays.toString(arr));//方法三，思路：通过Collections.reverse(list); ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); for (int i = 0; i &lt; arr.length; i++) &#123; list.add(arr[i]); &#125; Collections.reverse(list); System.out.println( list);]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动装箱的陷阱]]></title>
    <url>%2F%E8%87%AA%E5%8A%A8%E8%A3%85%E7%AE%B1%E7%9A%84%E9%99%B7%E9%98%B1.html</url>
    <content type="text"><![CDATA[12345678910111213Integer a = 1;Integer b = 2;Integer c = 3;Integer d = 3;Integer e = 321;Integer f = 321;Long g = 3L;System.out.println(c == d);System.out.println(e == f);System.out.println(c == (a+b));System.out.println(c.equals(a+b));System.out.println(g == (a+b));System.out.println(g.equals(a+b)); 输出结果：123456truefalsetruetruetruefalse 包装类“==”运算会发生自动拆装箱，在没有算术运算的情况下不会自动拆箱，以及它们的equals()方法不处理数据转型的关系。 自动装箱过程调用了valueOf()的方法，查看valueOf()方法的源码为：1234567891011121314151617181920212223242526272829303132333435363738public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high"); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125; &#125; 通过阅读源代码，可以发现，java内部为了节省内存，IntegerCache类中有一个数组缓存了值从-128到127的Integer对象。当我们调用Integer.valueOf（int i）的时候，如果i的值时结余-128到127之间的，会直接从这个缓存中返回一个对象，否则就new一个新的Integer对象。321&gt;127,所以重新new了一个对象。 典型题目：请提供一个对i和j的声明，将下面的循环转变成为一个无限循环：123while(j &lt;= i &amp;&amp; i &lt;= j &amp;&amp; i != j)&#123; &#125; 答案：12345Integer i = 200 ;Integer j = 200 ; while(j&lt;=i &amp;&amp; i&lt;=j &amp;&amp; i!= j)&#123; &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二进制、十进制快速转换]]></title>
    <url>%2F%E4%BA%8C%E8%BF%9B%E5%88%B6%E3%80%81%E5%8D%81%E8%BF%9B%E5%88%B6%E5%BF%AB%E9%80%9F%E8%BD%AC%E6%8D%A2.html</url>
    <content type="text"><![CDATA[方法一以8位 来演示: 第一种:00000001 100000010 200000100 400001000 800010000 1600100000 3201000000 6410000000 128 第二种：00000001 100000011 300000111 700001111 1500011111 3100111111 6301111111 1273.第三种：10000000 12811000000 19211100000 22411110000 24011111000 24811111100 25211111110 254举个例子： 11101011 可分为： 11100000（上面第三种类型） 224 00001000（上面第一种类型） 8 00000011（上面第二种类型） 3 我们通过记住上面三种类型的转换，再用加 法（加法口算你会吧）立即得到结果：235方法二：熟记以下排列，其实很Easy了，从右往 左，依次是前一个数的2倍：2^8 2^7 2^6 2^5 2^4 2^3 2^2 2^1 2^0256 128 64 32 16 8 4 2 1随便写个数字比如4848 = 32 + 16,所以在32 和 16所在的位置为1，其余为0，转为2进制就是256 128 64 32 16 8 4 2 10 0 0 1 1 0 0 0 0 二进制转十进制就更简单了，比如随便写的一串 01111101先写上 ： 0 1 1 1 1 1 0 1然后填充 128 64 32 16 8 4 2 1 十进制为 64+32+16+8+4+1=125]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>进制转换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DOS下创建文件和文件夹]]></title>
    <url>%2FDOS%E4%B8%8B%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E5%92%8C%E6%96%87%E4%BB%B6%E5%A4%B9.html</url>
    <content type="text"><![CDATA[DOS下创建文件的三种方法第一种： echo 内容 &gt; a.txt 重定向输出，此时创建文本文件a.txt echo 内容 &gt;&gt; a.txt 向a.txt文件中追加信息第二种：copy con a.txt 创建空文本文件a.txt输入完成后，按ctrl+z退出第三种：type nul&gt;filename 可以创建一个名为filename的空文件，在批处理中经常使用copy nul a.txt 可创建一个空文件，如果a.txt已经存在，且有内容，会被清空，在批文件中经常用。 DOS下创建目录的命令 创建：md 目录名 或者 mkdir 目录名 删除：rd 目录名 或者rmdir 目录名 删除非空目录： rd /s /q 目录名 /s 除目录本身外，还将删除指定目录下的所有子目录和文件,用于删除目录树。 /q 安静模式 /s 删除目录树时不要求确认其他一些命令 cmd命令 附件命令： 切换 盘符d: e: f: 查看文件目录清单dirdir /s 查看所有目录和子目录下的文件目录清单dir /p分屏显示 改变当前目录cd 目录cd.. 回退到上一级目录cd/ 回退到根目录 新建文件夹（目录）md 目录名 新建文件copy con 文件名.扩展名内容ctrl + z 显示文件内容type 文件名.扩展名 复制copy 源文件目录 目标文件目录 重命名ren 原文件名 新文件名 移动move 原文件 目标路径 删除目录rd 目录名但是只能删除 空的目录 删除文件del 文件.扩展名del . 清屏cls 退出exit]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>DOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intellij导出JavaDoc编码异常]]></title>
    <url>%2FIntellij%E5%AF%BC%E5%87%BAJavaDoc%E7%BC%96%E7%A0%81%E5%BC%82%E5%B8%B8.html</url>
    <content type="text"><![CDATA[Intellij IDEA 导出JavaDoc时编码异常之解决方案。 javac编译提示错误需要为 class、interface 或 enumHelloWorld.java:1: 需要为 class、interface 或 enum锘缝ublic class HelloWorld{^D:\IDE-workspace\src\com\dudefu\www\Test2.java:1: 错误: 非法字符: ‘\ufeff’?package com.dudefu.www;^D:\IDE-workspace\src\com\dudefu\www\Test2.java:1: 错误: 需要class, interface或enum?package com.dudefu.www; ^3 错误 原因这个错误出现的原因主要是在中文操作系统中，使用一贯的“javac HelloWorld.java”方式编译UTF-8（带BOM）编码的.java源文件，在没有指定编码参数（encoding）的情况下，默认是使用GBK编码。当编译器用GBK编码来编译UTF-8文件时，就会把UTF-8（带BOM）编码文件的文件头的占3个字节的头信息，按照GBK中汉字占两个字节、英文占1个字节的特性解码成了“乱码”的两个汉字。这个源文件应该是用记事本另存存为UTF-8编码造成的。 对于非GBK及其子集编码（GB2312）的正确的源文件，编译方式为“javac -encoding “UTF-8” HelloWord.java”，这样代码错误的指定代码里就不会出现乱码的中文。 但是依然会有错误，提示“HelloWorld.java:1: 非法字符: \65279。这是因为.java对于UTF-8编码，只识别UTF-8（不带BOM）那种。而记事本只支持保存文件为带签名的UTF-8，那有没有办法解决呢？当然是有的，那就是使用EmEditor、EditPlus、UltraEdit或Notepad++之类的工具另存为UTF（不带BOM）（区别于带UTF + BOM）的编码文件。这时候使用“javac -encoding “UTF-8” HelloWorld.java”，就没有上述编码问题了。也许有人会说，“我干脆都用GBK不就行了吗，为什么还要用UTF-8呢？”这是因为UTF-8支持世界多种语言的文字，被世界多数国家接受，是国际通用编码，也是Java推荐使用的编码。Java集成开发环境Eclipse中默认编码就是UTF-8。如果使用GBK，尤其是做网站，在非汉语国家，将无法正常浏览。在信息化时代，国际交往日益频繁；做软件和网站，不能只着眼当前，也要为日后维护做优化、降低维护成本。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Intellij</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java内存区域与内存溢出异常]]></title>
    <url>%2FJava%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E4%B8%8E%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E5%BC%82%E5%B8%B8.html</url>
    <content type="text"><![CDATA[程序运行时，存储数据的五个地方：1）寄存器 这是最快的存储区，位于处理器内部。但是，寄存器数量极其有限，所以寄存器根据需求进行分配。你不能直接控制，也不能在程序中感觉到寄存器存在的任何迹象（C和C++允许向编译器建议寄存器的分配方式）。寄存器是存在在cpu上的。而内存是挂在数据总线的，数据总线就是用来决定传输数据的大小。而就是通过在寄存器上的地址来寻找相应内存。总的来说，寄存器和内存是两个东西，程序是无法来控制寄存器，所以这里了解一下就可以了。主要涉及到运行程序涉及到的就是下面这些栈（stack）、堆（heap）、静态域、常量池。2）堆栈 即“栈”，位于通用RAM（随机访问存储器）中，但通过堆栈指针可以从处理器那里获得直接支持。堆栈指针若向下移动，则分配新的内存；若向上移动，则释放那些内存。这是一种快速有效的分配存储方法，仅次于寄存器。创建程序时，Java系统必须知道存储在堆栈内所有项的确切生命周期，以便上下移动堆栈指针。这一约束限制了程序的灵活性，所以虽然某些Java数据存储于堆栈中—–特别是对象引用，但是Java对象并不存储于其中。栈中主要存放一些基本类型的变量（ int, short, long, byte,float, double, boolean, char ）和对象引用。 对象是不会放置在里面的。3）堆 一种通用的内存池（位于RAM区），用于存放所有的Java对象，以及动态生成的对象（包括数组）和程序运行时生成的一些数据（包括对象的定义和变量的定义）。堆不同于堆栈的好处是：编译器不需要知道存储的数据在堆里存活多长时间。因此，在堆里分配存储有很大的灵活性。new一个对象，会自动在堆里进行存储分配。 当然，为这种灵活性必须付出相应的代价：用堆进行存储分配和清理可能比堆栈进行存储分配需要更多的时间。4）常量存储。常量值通常直接存放在程序代码内部，这样做是安全的，因为它们永远不会被改变。5）非RAM存储 比如流对象和持久化对象。在流对象中，对象转化成字节流，通常被发送给另一台机器。在“持久化对象”中，对象被存放磁盘中。这种存储方式的技巧在于：把对象转化成可以存放在其他媒介上的事物，在需要时，可以恢复成常规的、基于RAM的对象。 其他数据共享 这个数据共享主要也是由于引用的是地址来决定的，举个例子：char str1=”str1”;char str2=”str1”;这时候再次声明Str2，同时指定两个不同的引用而相同的变量;这时候并不需要重新开辟另外一份内存，只需要两者都指向相同的地址就可以了。这样数据共享带来的就是内存上的节省。定义和声明 这里需要对这两个动词进行一些说明。因为在平时过程中，我是对这两个概念比较模糊。一说就是定义声明了一个变量。但是事实上确实不一样的。声明就只是定义这个变量的名字，告诉编译器会有这么一个变量。而定义就不同了，定义就是在声明之后对变量进行初始化、设置一个初始值的过程。如：int i；int i=1；就是这个区别。而在java变量的声明过程中，是不允许没有初始化变量的。 Data segment 这个包括静态域和常量池。 静态域 这个就是咱们存放在对象中的静态变量。 常量池 这个主要是在编译完成后，存放在.class文件中（code segment）。包括一些基本的数据类型和相应的类的接口和声明。换言之就是在编译后，程序中经常使用的不会改变的。例如：基本数据类型（这个是规定，肯定没法改）。接口的命名：这个你肯定不会闲到改改这个来解闷的。 内存分析，java程序执行的过程，一般变量的内存粗存放过程。 实例：下面通过分析一个例子来说明java变量是怎么存放在内存中的Code segment：arraylistlist[]=new arraylist[2]；Arraylist[0]=2;arraylist[1]=3;arraylist[2]=4;因为list[]是一个变量，这是一个声明我们放到栈中。而后面每个数组实例化出来的变量，所以放到堆中。而实实在在存在的变量的值都是常量，所以放在常量池中，也就是上图中的datasegment。 运行时数据区域Java虚拟机在执行Java程序的过程中，会把所管理的内存划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有些区域则依赖用户线程的启动和结束而建立和销毁。下图是Java虚拟机运行时数据区： 程序计数器程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。在Java虚拟机中，多线程是通过线程 轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对多核处理器来说是一个内核）都只会执行一条线程中的命令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类区域为“线程私有”的内存。如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Native方法，这个计数器值则为空（Undefined）。此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 Java虚拟机栈（栈）与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。两种异常，如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），如果扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 本地方法栈与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务。本地方法栈也会抛出StackOverflowError和OutOfMemoryError异常。 Java堆对大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 方法区Method Area,Non-Heap(非堆)与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 运行时常量池运行时常量池（RuntiMe ConStant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用的，这部分内容将在类加载后进入方法区的运行时常量池中存放。 直接内存（Direct Memory）NIO类引入了一种基于通道（Channel）与缓冲区（Buffer）的I/O方式，它可以 使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。这样能在一些场景中显著提供性能，因为避免了在Java堆和Native堆中来回复制数据。服务器管理员在配置虚拟机参数时，会根据实际内存设置-Xmx等参数信息，但经常忽略直接内存，使得各个内存区域总和大于物理内存限制（包括物理的和操作系统的限制），从而导致动态扩展时出现OutOfMemoryError异常。 在虚拟机中，对象创建对象的过程？？？？虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程，在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头）。接下来，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息存放在对象的对象头（Object Header）之中。从虚拟机的视角来看，一个新的对象已经差生了，但从Java程序的视角来看，对象创建才刚刚开始—方法还没有执行，所有的字段都还为零。 所以，一般来说，执行new指令之后会接着执行方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试题：百度前200页都在这里了]]></title>
    <url>%2F%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E7%99%BE%E5%BA%A6%E5%89%8D200%E9%A1%B5%E9%83%BD%E5%9C%A8%E8%BF%99%E9%87%8C%E4%BA%86.html</url>
    <content type="text"><![CDATA[基本概念操作系统中 heap 和 stack 的区别什么是基于注解的切面实现什么是 对象/关系 映射集成模块什么是 Java 的反射机制什么是 ACIDBS与CS的联系与区别Cookie 和 Session的区别fail-fast 与 fail-safe 机制有什么区别get 和 post请求的区别Interface 与 abstract 类的区别IOC的优点是什么IO 和 NIO的区别，NIO优点Java 8 / Java 7 为我们提供了什么新功能什么是竞态条件？ 举个例子说明。JRE、JDK、JVM 及 JIT 之间有什么不同MVC的各个部分都有那些技术来实现?如何实现?RPC 通信和 RMI 区别什么是 Web Service（Web服务）JSWDL开发包的介绍。JAXP、JAXM的解释。SOAP、UDDI,WSDL解释。WEB容器主要有哪些功能? 并请列出一些常见的WEB容器名字。一个”.java”源文件中是否可以包含多个类（不是内部类）？有什么限制简单说说你了解的类加载器。是否实现过类加载器解释一下什么叫AOP（面向切面编程）请简述 Servlet 的生命周期及其相关的方法请简述一下 Ajax 的原理及实现步骤简单描述Struts的主要功能什么是 N 层架构什么是CORBA？用途是什么什么是Java虚拟机？为什么Java被称作是“平台无关的编程语言”什么是正则表达式？用途是什么？哪个包使用正则表达式来实现模式匹配什么是懒加载（Lazy Loading）什么是尾递归，为什么需要尾递归什么是控制反转（Inversion of Control）与依赖注入（Dependency Injection）关键字 finalize 什么是finalize()方法finalize()方法什么时候被调用析构函数(finalization)的目的是什么final 和 finalize 的区别finalfinal关键字有哪些用法final 与 static 关键字可以用于哪里？它们的作用是什么final, finally, finalize的区别final、finalize 和 finally 的不同之处？能否在运行时向 static final 类型的赋值使用final关键字修饰一个变量时，是引用不能变，还是引用的对象不能变一个类被声明为final类型，表示了什么意思throws, throw, try, catch, finally分别代表什么意义Java 有几种修饰符？分别用来修饰什么volatile volatile 修饰符的有过什么实践volatile 变量是什么？volatile 变量和 atomic 变量有什么不同volatile 类型变量提供什么保证？能使得一个非原子操作变成原子操作吗能创建 volatile 数组吗？transient变量有什么特点super什么时候使用public static void 写成 static public void会怎样说明一下public static void main(String args[])这段声明里每个关键字的作用请说出作用域public, private, protected, 以及不写时的区别sizeof 是Java 的关键字吗static static class 与 non static class的区别static 关键字是什么意思？Java中是否可以覆盖(override)一个private或者是static的方法静态类型有什么特点main() 方法为什么必须是静态的？能不能声明 main() 方法为非静态是否可以从一个静态（static）方法内部发出对非静态（non-static）方法的调用静态变量在什么时候加载？编译期还是运行期？静态代码块加载的时机呢成员方法是否可以访问静态变量？为什么静态方法不能访问成员变量switch switch 语句中的表达式可以是什么类型数据switch 是否能作用在byte 上，是否能作用在long 上，是否能作用在String上while 循环和 do 循环有什么不同操作符 &amp;操作符和&amp;&amp;操作符有什么区别?a = a + b 与 a += b 的区别？逻辑操作符 (&amp;,|,^)与条件操作符(&amp;&amp;,||)的区别3*0.1 == 0.3 将会返回什么？true 还是 false？float f=3.4; 是否正确？short s1 = 1; s1 = s1 + 1;有什么错?数据结构 基础类型(Primitives) 基础类型(Primitives)与封装类型(Wrappers)的区别在哪里简述九种基本数据类型的大小，以及他们的封装类int 和 Integer 哪个会占用更多的内存？ int 和 Integer 有什么区别？parseInt()函数在什么时候使用到float和double的默认值是多少如何去小数四舍五入保留小数点后两位char 型变量中能不能存贮一个中文汉字，为什么类型转换 怎样将 bytes 转换为 long 类型怎么将 byte 转换为 String如何将数值型字符转换为数字我们能将 int 强制转换为 byte 类型的变量吗？如果该值大于 byte 类型的范围，将会出现什么现象能在不进行强制转换的情况下将一个 double 值赋值给 long 类型的变量吗类型向下转换是什么数组 如何权衡是使用无序的数组还是有序的数组怎么判断数组是 null 还是为空怎么打印数组？ 怎样打印数组中的重复元素Array 和 ArrayList有什么区别？什么时候应该使用Array而不是ArrayList数组和链表数据结构描述，各自的时间复杂度数组有没有length()这个方法? String有没有length()这个方法队列 队列和栈是什么，列出它们的区别BlockingQueue是什么简述 ConcurrentLinkedQueue LinkedBlockingQueue 的用处和不同之处。ArrayList、Vector、LinkedList的存储性能和特性StringStringBuffer ByteBuffer 与 StringBuffer有什么区别HashMap HashMap的工作原理是什么内部的数据结构是什么HashMap 的 table的容量如何确定？loadFactor 是什么？ 该容量如何变化？这种变化会带来什么问题？HashMap 实现的数据结构是什么？如何实现HashMap 和 HashTable、ConcurrentHashMap 的区别HashMap的遍历方式及效率HashMap、LinkedMap、TreeMap的区别如何决定选用HashMap还是TreeMap如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办HashMap 是线程安全的吗？并发下使用的 Map 是什么，它们内部原理分别是什么，比如存储方式、 hashcode、扩容、 默认容量等HashSet HashSet和TreeSet有什么区别HashSet 内部是如何工作的WeakHashMap 是怎么工作的？Set Set 里的元素是不能重复的，那么用什么方法来区分重复与否呢？是用 == 还是 equals()？ 它们有何区别?TreeMap：TreeMap 是采用什么树实现的？TreeMap、HashMap、LindedHashMap的区别。TreeMap和TreeSet在排序时如何比较元素？Collections工具类中的sort()方法如何比较元素？TreeSet：一个已经构建好的 TreeSet，怎么完成倒排序。EnumSet 是什么Hash算法 Hashcode 的作用简述一致性 Hash 算法有没有可能 两个不相等的对象有相同的 hashcode？当两个对象 hashcode 相同怎么办？如何获取值对象为什么在重写 equals 方法的时候需要重写 hashCode 方法？equals与 hashCode 的异同点在哪里a.hashCode() 有什么用？与 a.equals(b) 有什么关系hashCode() 和 equals() 方法的重要性体现在什么地方Object：Object有哪些公用方法？Object类hashcode,equals 设计原则？ sun为什么这么设计？Object类的概述如何在父类中为子类自动完成所有的 hashcode 和 equals 实现？这么做有何优劣。可以在 hashcode() 中使用随机数字吗？LinkedHashMap LinkedHashMap 和 PriorityQueue 的区别是什么List List, Set, Map三个接口，存取元素时各有什么特点List, Set, Map 是否继承自 Collection 接口遍历一个 List 有哪些不同的方式LinkedListLinkedList 是单向链表还是双向链表LinkedList 与 ArrayList 有什么区别描述下 Java 中集合（Collections），接口（Interfaces），实现（Implementations）的概念。LinkedList 与 ArrayList 的区别是什么？插入数据时，ArrayList, LinkedList, Vector谁速度较快？ArrayListArrayList 和 HashMap 的默认大小是多数ArrayList 和 LinkedList 的区别，什么时候用 ArrayList？ArrayList 和 Set 的区别？ArrayList, LinkedList, Vector的区别ArrayList是如何实现的，ArrayList 和 LinkedList 的区别ArrayList如何实现扩容Array 和 ArrayList 有何区别？什么时候更适合用Array说出ArraList,Vector, LinkedList的存储性能和特性Map Map, Set, List, Queue, StackMap 接口提供了哪些不同的集合视图为什么 Map 接口不继承 Collection 接口Collections 介绍Java中的Collection FrameWork。集合类框架的基本接口有哪些Collections类是什么？Collection 和 Collections的区别？Collection、Map的实现集合类框架的最佳实践有哪些为什么 Collection 不从 Cloneable 和 Serializable 接口继承说出几点 Java 中使用 Collections 的最佳实践？Collections 中 遗留类 (HashTable、Vector) 和 现有类的区别什么是 B+树，B-树，列出实际的使用场景。 接口 Comparator 与 Comparable 接口是干什么的？列出它们的区别对象 拷贝(clone) 如何实现对象克隆深拷贝和浅拷贝区别深拷贝和浅拷贝如何实现激活机制写clone()方法时，通常都有一行代码，是什么比较 在比较对象时，”==” 运算符和 equals 运算有何区别如果要重写一个对象的equals方法，还要考虑什么两个对象值相同(x.equals(y) == true)，但却可有不同的hash code，这句话对不对构造器 构造器链是什么创建对象时构造器的调用顺序不可变对象 什么是不可变象（immutable object）为什么 Java 中的 String 是不可变的（Immutable）如何构建不可变的类结构？关键点在哪里能创建一个包含可变对象的不可变对象吗如何对一组对象进行排序 方法 构造器（constructor）是否可被重写（override）方法可以同时即是 static 又是 synchronized 的吗abstract 的 method是否可同时是 static，是否可同时是 native，是否可同时是synchronizedJava支持哪种参数传递类型一个对象被当作参数传递到一个方法，是值传递还是引用传递当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递我们能否重载main()方法如果main方法被声明为private会怎样GC 概念 GC是什么？为什么要有GC什么时候会导致垃圾回收GC是怎么样运行的新老以及永久区是什么GC 有几种方式？怎么配置什么时候一个对象会被GC？ 如何判断一个对象是否存活System.gc() Runtime.gc()会做什么事情？ 能保证 GC 执行吗垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？Minor GC 、Major GC、Young GC 与 Full GC分别在什么时候发生垃圾回收算法的实现原理如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？垃圾回收的最佳做法是什么GC收集器有哪些 垃圾回收器的基本原理是什么？串行(serial)收集器和吞吐量(throughput)收集器的区别是什么Serial 与 Parallel GC之间的不同之处CMS 收集器 与 G1 收集器的特点与区别CMS垃圾回收器的工作过程JVM 中一次完整的 GC 流程是怎样的？ 对象如何晋升到老年代吞吐量优先和响应优先的垃圾收集器选择GC策略 举个实际的场景，选择一个GC策略JVM的永久代中会发生垃圾回收吗收集方法 标记清除、标记整理、复制算法的原理与特点？分别用在什么地方如果让你优化收集方法，有什么思路JVM 参数 说说你知道的几种主要的jvm 参数-XX:+UseCompressedOops 有什么作用类加载器(ClassLoader) Java 类加载器都有哪些JVM如何加载字节码文件内存管理 JVM内存分哪几个区，每个区的作用是什么一个对象从创建到销毁都是怎么在这些部分里存活和转移的解释内存中的栈(stack)、堆(heap)和方法区(method area)的用法JVM中哪个参数是用来控制线程的栈堆栈小简述内存分配与回收策略简述重排序，内存屏障，happen-before，主内存，工作内存Java中存在内存泄漏问题吗？请举例说明简述 Java 中软引用（SoftReferenc）、弱引用（WeakReference）和虚引用内存映射缓存区是什么jstack，jstat，jmap，jconsole怎么用32 位 JVM 和 64 位 JVM 的最大堆内存分别是多数？32 位和 64 位的 JVM，int 类型变量的长度是多数？怎样通过 Java 程序来判断 JVM 是 32 位 还是 64 位JVM自身会维护缓存吗？是不是在堆中进行对象分配，操作系统的堆还是JVM自己管理堆什么情况下会发生栈内存溢出双亲委派模型是什么 多线程 基本概念 什么是线程多线程的优点多线程的几种实现方式用 Runnable 还是 Thread什么是线程安全Vector, SimpleDateFormat 是线程安全类吗什么 Java 原型不是线程安全的哪些集合类是线程安全的多线程中的忙循环是什么如何创建一个线程编写多线程程序有几种实现方式什么是线程局部变量线程和进程有什么区别？进程间如何通讯，线程间如何通讯什么是多线程环境下的伪共享（false sharing）同步和异步有何异同，在什么情况下分别使用他们？举例说明Current ConcurrentHashMap 和 Hashtable的区别ArrayBlockingQueue, CountDownLatch的用法ConcurrentHashMap的并发度是什么CyclicBarrier 和 CountDownLatch有什么不同？各自的内部原理和用法是什么Semaphore的用法Thread 启动一个线程是调用 run() 还是 start() 方法？start() 和 run() 方法有什么区别调用start()方法时会执行run()方法，为什么不能直接调用run()方法sleep() 方法和对象的 wait() 方法都可以让线程暂停执行，它们有什么区别yield方法有什么作用？sleep() 方法和 yield() 方法有什么区别Java 中如何停止一个线程stop() 和 suspend() 方法为何不推荐使用如何在两个线程间共享数据如何强制启动一个线程如何让正在运行的线程暂停一段时间什么是线程组，为什么在Java中不推荐使用你是如何调用 wait（方法的）？使用 if 块还是循环？为什么生命周期 有哪些不同的线程生命周期线程状态，BLOCKED 和 WAITING 有什么区别画一个线程的生命周期状态图ThreadLocal 用途是什么，原理是什么，用的时候要注意什么ThreadPool 线程池是什么？为什么要使用它如何创建一个Java线程池ThreadPool用法与优势提交任务时，线程池队列已满时会发会生什么newCache 和 newFixed 有什么区别？简述原理。构造函数的各个参数的含义是什么，比如 coreSize, maxsize 等线程池的实现策略线程池的关闭方式有几种，各自的区别是什么线程池中submit() 和 execute()方法有什么区别？线程调度 Java中用到的线程调度算法是什么什么是多线程中的上下文切换你对线程优先级的理解是什么什么是线程调度器 (Thread Scheduler) 和时间分片 (Time Slicing)线程同步 请说出你所知的线程同步的方法synchronized 的原理是什么synchronized 和 ReentrantLock 有什么不同什么场景下可以使用 volatile 替换 synchronized有T1，T2，T3三个线程，怎么确保它们按顺序执行？怎样保证T2在T1执行完后执行，T3在T2执行完后执行同步块内的线程抛出异常会发生什么当一个线程进入一个对象的 synchronized 方法A 之后，其它线程是否可进入此对象的 synchronized 方法B使用 synchronized 修饰静态方法和非静态方法有什么区别如何从给定集合那里创建一个 synchronized 的集合锁 Java Concurrency API 中 的 Lock 接口是什么？对比同步它有什么优势Lock 与 Synchronized 的区别？Lock 接口比 synchronized 块的优势是什么ReadWriteLock是什么？锁机制有什么用什么是乐观锁（Optimistic Locking）？如何实现乐观锁？如何避免ABA问题解释以下名词：重排序，自旋锁，偏向锁，轻量级锁，可重入锁，公平锁，非公平锁，乐观锁，悲观锁什么时候应该使用可重入锁简述锁的等级方法锁、对象锁、类锁Java中活锁和死锁有什么区别？什么是死锁(Deadlock)？导致线程死锁的原因？如何确保 N 个线程可以访问 N 个资源同时又不导致死锁死锁与活锁的区别，死锁与饥饿的区别怎么检测一个线程是否拥有锁如何实现分布式锁有哪些无锁数据结构，他们实现的原理是什么读写锁可以用于什么应用场景Executors类是什么？ Executor和Executors的区别什么是Java线程转储(Thread Dump)，如何得到它如何在Java中获取线程堆栈说出 3 条在 Java 中使用线程的最佳实践在线程中你怎么处理不可捕捉异常实际项目中使用多线程举例。你在多线程环境中遇到的常见的问题是什么？你是怎么解决它的请说出与线程同步以及线程调度相关的方法程序中有3个 socket，需要多少个线程来处理假如有一个第三方接口，有很多个线程去调用获取数据，现在规定每秒钟最多有 10 个线程同时调用它，如何做到如何在 Windows 和 Linux 上查找哪个线程使用的 CPU 时间最长如何确保 main() 方法所在的线程是 Java 程序最后结束的线程非常多个线程（可能是不同机器），相互之间需要等待协调才能完成某种工作，问怎么设计这种协调方案你需要实现一个高效的缓存，它允许多个用户读，但只允许一个用户写，以此来保持它的完整性，你会怎样去实现它 异常 基本概念 Error 和 Exception有什么区别UnsupportedOperationException是什么NullPointerException 和 ArrayIndexOutOfBoundException 之间有什么相同之处什么是受检查的异常，什么是运行时异常运行时异常与一般异常有何异同简述一个你最常见到的runtime exception(运行时异常)finally finally关键词在异常处理中如何使用如果执行finally代码块之前方法返回了结果，或者JVM退出了，finally块中的代码还会执行吗try里有return，finally还执行么？那么紧跟在这个try后的finally {}里的code会不会被执行，什么时候被执行，在return前还是后在什么情况下，finally语句不会执行throw 和 throws 有什么区别？OOM你遇到过哪些情况？你是怎么搞定的？SOF你遇到过哪些情况？既然我们可以用RuntimeException来处理错误，那么你认为为什么Java中还存在检查型异常当自己创建异常类的时候应该注意什么导致空指针异常的原因异常处理 handle or declare 原则应该如何理解怎么利用 JUnit 来测试一个方法的异常catch块里别不写代码有什么问题你曾经自定义实现过异常吗？怎么写的什么是 异常链在try块中可以抛出异常吗 JDBC 通过 JDBC 连接数据库有哪几种方式阐述 JDBC 操作数据库的基本步骤JDBC 中如何进行事务处理什么是 JdbcTemplate什么是 DAO 模块使用 JDBC 操作数据库时，如何提升读取数据的性能？如何提升更新数据的性能列出 5 个应该遵循的 JDBC 最佳实践IO FileFile类型中定义了什么方法来创建一级目录File类型中定义了什么方法来判断一个文件是否存在 流为了提高读写性能，可以采用什么流Java中有几种类型的流JDK 为每种类型的流提供了一些抽象类以供继承，分别是哪些类对文本文件操作用什么I/O流对各种基本数据类型和String类型的读写，采用什么流能指定字符编码的 I/O 流类型是什么序列化什么是序列化？如何实现 Java 序列化及注意事项Serializable 与 Externalizable 的区别Socketsocket 选项 TCP NO DELAY 是指什么Socket 工作在 TCP/IP 协议栈是哪一层TCP、UDP 区别及 Java 实现方式说几点 IO 的最佳实践直接缓冲区与非直接缓冲器有什么区别？怎么读写 ByteBuffer？ByteBuffer 中的字节序是什么当用System.in.read(buffer)从键盘输入一行n个字符后，存储在缓冲区buffer中的字节数是多少如何使用扫描器类（Scanner Class）令牌化面向对象编程（OOP） 解释下多态性（polymorphism），封装性（encapsulation），内聚（cohesion）以及耦合（coupling）多态的实现原理封装、继承和多态是什么对象封装的原则是什么?类获得一个类的类对象有哪些方式重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？说出几条 Java 中方法重载的最佳实践抽象类抽象类和接口的区别抽象类中是否可以有静态的main方法抽象类是否可实现(implements)接口抽象类是否可继承具体类(concrete class)匿名类（Anonymous Inner Class）匿名内部类是否可以继承其它类？是否可以实现接口 内部类内部类分为几种内部类可以引用它的包含类（外部类）的成员吗请说一下 Java 中为什么要引入内部类？还有匿名内部类继承继承（Inheritance）与聚合（Aggregation）的区别在哪里继承和组合之间有什么不同为什么类只能单继承，接口可以多继承存在两个类，B 继承 A，C 继承 B，能将 B 转换为 C 么？如 C = (C) B如果类 a 继承类 b，实现接口c，而类 b 和接口 c 中定义了同名变量，请问会出现什么问题接口接口是什么接口是否可继承接口为什么要使用接口而不是直接使用具体类？接口有什么优点泛型 泛型的存在是用来解决什么问题泛型的常用特点List能否转为List工具类 日历Calendar Class的用途如何在Java中获取日历类的实例解释一些日历类中的重要方法GregorianCalendar 类是什么SimpleTimeZone 类是什么Locale类是什么如何格式化日期对象如何添加小时(hour)到一个日期对象(Date Objects)如何将字符串 YYYYMMDD 转换为日期MathMath.round()什么作用？Math.round(11.5) 等于多少？Math.round(-11.5)等于多少？ XMLXML文档定义有几种形式？它们之间有何本质区别？解析XML文档有哪几种方式？DOM 和 SAX 解析器有什么不同？Java解析XML的方式用 jdom 解析 xml 文件时如何解决中文问题？如何解析你在项目中用到了 XML 技术的哪些方面？如何实现动态代理 描述动态代理的几种实现方式，分别说出相应的优缺点设计模式 什么是设计模式（Design Patterns）？你用过哪种设计模式？用在什么场合你知道哪些商业级设计模式？哪些设计模式可以增加系统的可扩展性单例模式除了单例模式，你在生产环境中还用过什么设计模式？写 Singleton 单例模式单例模式的双检锁是什么如何创建线程安全的 Singleton什么是类的单例模式写出三种单例模式实现适配器模式适配器模式是什么？什么时候使用适配器模式和代理模式之前有什么不同适配器模式和装饰器模式有什么区别什么时候使用享元模式什么时候使用组合模式什么时候使用访问者模式什么是模板方法模式请给出1个符合开闭原则的设计模式的例子开放问题 用一句话概括 Web 编程的特点Google是如何在一秒内把搜索结果返回给用户哪种依赖注入方式你建议使用，构造器注入，还是 Setter方法注入树（二叉或其他）形成许多普通数据结构的基础。请描述一些这样的数据结构以及何时可以使用它们某一项功能如何设计线上系统突然变得异常缓慢，你如何查找问题什么样的项目不适合用框架新浪微博是如何实现把微博推给订阅者简要介绍下从浏览器输入 URL 开始到获取到请求界面之后 Java Web 应用中发生了什么请你谈谈SSH整合高并发下，如何做到安全的修改同一行数据12306网站的订票系统如何实现，如何保证不会票不被超卖网站性能优化如何优化的聊了下曾经参与设计的服务器架构请思考一个方案，实现分布式环境下的 countDownLatch请思考一个方案，设计一个可以控制缓存总体大小的自动适应的本地缓存在你的职业生涯中，算得上最困难的技术挑战是什么如何写一篇设计文档，目录是什么大写的O是什么？举几个例子编程中自己都怎么考虑一些设计原则的，比如开闭原则，以及在工作中的应用解释一下网络应用的模式及其特点设计一个在线文档系统，文档可以被编辑，如何防止多人同时对同一份文档进行编辑更新说出数据连接池的工作机制是什么怎么获取一个文件中单词出现的最高频率描述一下你最常用的编程风格如果有机会重新设计你们的产品，你会怎么做如何搭建一个高可用系统如何启动时不需输入用户名与密码如何在基于Java的Web项目中实现文件上传和下载如何实现一个秒杀系统，保证只有几位用户能买到某件商品。如何实现负载均衡，有哪些算法可以实现如何设计一个购物车？想想淘宝的购物车如何实现的如何设计一套高并发支付方案，架构如何设计如何设计建立和保持 100w 的长连接如何避免浏览器缓存。如何防止缓存雪崩如果AB两个系统互相依赖，如何解除依如果有人恶意创建非法连接，怎么解决如果有几十亿的白名单，每天白天需要高并发查询，晚上需要更新一次，如何设计这个功能如果系统要使用超大整数（超过long长度范围），请你设计一个数据结构来存储这种超大型数字以及设计一种算法来实现超大整数加法运算）如果要设计一个图形系统，请你设计基本的图形元件(Point,Line,Rectangle,Triangle)的简单实现如果让你实现一个并发安全的链表，你会怎么做应用服务器与WEB 服务器的区别？应用服务器怎么监控性能，各种方式的区别？你使用过的应用服务器优化技术有哪些大型网站在架构上应当考虑哪些问题有没有处理过线上问题？出现内存泄露，CPU利用率标高，应用无响应时如何处理的最近看什么书，印象最深刻的是什么描述下常用的重构技巧你使用什么版本管理工具？分支（Branch）与标签（Tag）之间的区别在哪里你有了解过存在哪些反模式（Anti-Patterns）吗你用过的网站前端优化的技术有哪些如何分析Thread dump你如何理解AOP中的连接点（Joinpoint）、切点（Pointcut）、增强（Advice）、引介（Introduction）、织入（Weaving）、切面（Aspect）这些概念你是如何处理内存泄露或者栈溢出问题的你们线上应用的 JVM 参数有哪些怎么提升系统的QPS和吞吐量知识面 解释什么是 MESI 协议(缓存一致性)谈谈 reactor 模型Java 9 带来了怎样的新功能Java 与 C++ 对比，C++ 或 Java 中的异常处理机制的简单原理和应用简单讲讲 Tomcat 结构，以及其类加载器流程虚拟内存是什么阐述下 SOLID 原则请简要讲一下你对测试驱动开发（TDD）的认识CDN实现原理Maven 和 ANT 有什么区别UML中有哪些常用的图LinuxLinux 下 IO 模型有几种，各自的含义是什么。Linux 系统下你关注过哪些内核参数，说说你知道的Linux 下用一行命令查看文件的最后五行平时用到哪些 Linux 命令用一行命令输出正在运行的 Java 进程使用什么命令来确定是否有 Tomcat 实例运行在机器上什么是 N+1 难题什么是 paxos 算法什么是 restful，讲讲你理解的 restful什么是 zab 协议什么是领域模型(domain model)？贫血模型(anaemic domain model) 和充血模型(rich domain model)有什么区别什么是领域驱动开发（Domain Driven Development）介绍一下了解的 Java 领域的 Web Service 框架Web Server、Web Container 与 Application Server 的区别是什么微服务（MicroServices）与巨石型应用（Monolithic Applications）之间的区别在哪里描述 Cookie 和 Session 的作用，区别和各自的应用范围，Session工作原理你常用的持续集成（Continuous Integration）、静态代码分析（Static Code Analysis）工具有哪些简述下数据库正则化（Normalizations）KISS,DRY,YAGNI 等原则是什么含义分布式事务的原理，优缺点，如何使用分布式事务？布式集群下如何做到唯一序列号网络HTTPS 的加密方式是什么，讲讲整个加密解密流程HTTPS和HTTP的区别HTTP连接池实现原理HTTP集群方案Nginx、lighttpd、Apache三大主流 Web服务器的区别是否看过框架的一些代码持久层设计要考虑的问题有哪些？你用过的持久层框架有哪些数值提升是什么你能解释一下里氏替换原则吗你是如何测试一个应用的？知道哪些测试框架传输层常见编程协议有哪些？并说出各自的特点编程题 计算加班费 加班10小时以下加班费是时薪的1.5倍。加班10小时或以上，按4元/时算。提示：（一个月工作26天，一天正常工作8小时） 计算1000月薪，加班9小时的加班费计算2500月薪，加班11小时的加班费计算1000月薪，加班15小时的加班费卖东西 一家商场有红苹果和青苹果出售。（红苹果5元/个，青苹果4元/个）。 模拟一个进货。红苹果跟青苹果各进200个。模拟一个出售。红苹果跟青苹果各买出10个。每卖出一个苹果需要进行统计。提示：一个苹果是一个单独的实体。 日期提取 有这样一个时间字符串：2008-8-8 20:08:08 ， 请编写能够匹配它的正则表达式，并编写Java代码将日期后面的时分秒提取出来，即：20:08:08 线程 8设计4个线程，其中两个线程每次对j增加1，另外两个线程对j每次减少1。写出程序。用Java写一个多线程程序，如写四个线程，二个加1，二个对一个变量减一，输出wait-notify 写一段代码来解决生产者-消费者问题数字 判断101-200之间有多少个素数，并输出所有素数用最有效率的方法算出2乘以17等于多少有 1 亿个数字，其中有 2 个是重复的，快速找到它，时间和空间要最优2 亿个随机生成的无序整数,找出中间大小的值10 亿个数字里里面找最小的 10 个1到1亿的自然数，求所有数的拆分后的数字之和，如286 拆分成2、8、6，如1到11拆分后的数字之和 =&gt; 1 + … + 9 + 1 + 0 + 1 + 1一个数如果恰好等于它的因子之和，这个数就称为 “完数 “。例如6=1＋2＋3.编程 找出1000以内的所有完数一个数组中所有的元素都出现了三次，只有一个元素出现了一次找到这个元素一球从100米高度自由落下，每次落地后反跳回原高度的一半；再落下，求它在 第10次落地时，共经过多少米？第10次反弹多高？求100－1000内质数的和求1到100的和的平均数求s=a+a+aaa+aaaa+aa…a的值，其中a是一个数字。例如2+22+222+2222+22222(此时共有5个数相加)，几个数相加有键盘控制。 求出1到100的和算出1到40的质数，放进数组里显示放组里的数找出第[5]个数删除第[9]个数，再显示删除后的第[9]个有 3n+1 个数字，其中 3n 个中是重复的，只有 1 个是不重复的，怎么找出来。有一组数1.1.2.3.5.8.13.21.34。写出程序随便输入一个数就能给出和前一组数字同规律的头5个数计算指定数字的阶乘开发 Fizz Buzz给定一个包含 N 个整数的数组，找出丢失的整数一个排好序的数组，找出两数之和为m的所有组合将一个正整数分解质因数。例如：输入90,打印出90=233*5。打印出所有的 “水仙花数 “，所谓 “水仙花数 “是指一个三位数，其各位数字立方和等于该数本身。例如：153是一个 “水仙花数 “，因为153=1的三次方＋5的三次方＋3的三次方原地交换两个变量的值找出4字节整数的中位数找到整数的平方根实现斐波那契网络 用Java Socket编程，读服务器几个字符，再写入本地显示反射 反射机制提供了什么功能？反射是如何实现的哪里用到反射机制反射中 Class.forName 和 ClassLoader 区别反射创建类实例的三种方式是什么如何通过反射调用对象的方法如何通过反射获取和设置对象私有字段的值反射机制的优缺点数据库 写一段 JDBC 连Oracle的程序,并实现数据查询算法 50个人围坐一圈，当数到三或者三的倍数出圈，问剩下的人是谁，原来的位置是多少实现一个电梯模拟器用写一个冒泡排序写一个折半查找随机产生20个不能重复的字符并排序写一个函数，传入 2 个有序的整数数组，返回一个有序的整数数组写一段代码在遍历 ArrayList 时移除一个元素古典问题：有一对兔子，从出生后第3个月起每个月都生一对兔子，小兔子长到第四个月后每个月又生一对兔子，假如兔子都不死，问每个月的兔子总数为多少约瑟芬环游戏正则 请编写一段匹配IP地址的正则表达式写出一个正则表达式来判断一个字符串是否是一个数字字符串 写一个方法，入一个文件名和一个字符串，统计这个字符串在这个文件中出现的次数。写一个程序找出所有字符串的组合，并检查它们是否是回文串写一个字符串反转函数，输入abcde转换成edcba代码小游戏，倒转句子中的单词将GB2312编码的字符串转换为ISO-8859-1编码的字符串请写一段代码来计算给定文本内字符“A”的个数。分别用迭代和递归两种方式编写一个截取字符串的函数，输入为一个字符串和字节数，输出为按字节截取的字符串。 但是要保证汉字不被截半个，如“我ABC”4，应该截为“我AB”，输入“我ABC汉DEF”，6，应该输出为“我ABC”而不是“我ABC+汉的半个”给定 2 个包含单词列表（每行一个）的文件，编程列出交集打印出一个字符串的所有排列将一个键盘输入的数字转化成中文输出(例如：输入1234567，输出:一百二拾三万四千五百六拾七)在Web应用开发过程中经常遇到输出某种编码的字符，如从 GBK 到 ISO8859-1等，如何输出一个某种编码的字符串日期 计算两个日期之间的差距]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java逃逸分析]]></title>
    <url>%2FJava%E9%80%83%E9%80%B8%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[概念引入 我们都知道，Java 创建的对象都是被分配到堆内存上，但是事实并不是这么绝对，通过对Java对象分配的过程分析，可以知道有两个地方会导致Java中创建出来的对象并一定分别在所认为的堆上。这两个点分别是Java中的逃逸分析和TLAB（Thread Local Allocation Buffer）线程私有的缓存区。 基本概念介绍 逃逸分析，是一种可以有效减少Java程序中同步负载和内存堆分配压力的跨函数全局数据流分析算法。通过逃逸分析，Java Hotspot编译器能够分析出一个新的对象的引用的使用范围从而决定是否要将这个对象分配到堆上。在计算机语言编译器优化原理中，逃逸分析是指分析指针动态范围的方法，它同编译器优化原理的指针分析和外形分析相关联。当变量（或者对象）在方法中分配后，其指针有可能被返回或者被全局引用，这样就会被其他过程或者线程所引用，这种现象称作指针（或者引用）的逃逸(Escape)。通俗点讲，如果一个对象的指针被多个方法或者线程引用时，那么我们就称这个对象的指针发生了逃逸。 Java在Java SE 6u23以及以后的版本中支持并默认开启了逃逸分析的选项。Java的 HotSpot JIT编译器，能够在方法重载或者动态加载代码的时候对代码进行逃逸分析，同时Java对象在堆上分配和内置线程的特点使得逃逸分析成Java的重要功能。 代码示例1234567891011121314151617181920212223242526272829303132333435package me.stormma.gc;/** * &lt;p&gt;Created on 2017/4/21.&lt;/p&gt; * * @author stormma * * @title &lt;p&gt;逃逸分析&lt;/p&gt; */public class EscapeAnalysis &#123; public static B b; /** * &lt;p&gt;全局变量赋值发生指针逃逸&lt;/p&gt; */ public void globalVariablePointerEscape() &#123; b = new B(); &#125; /** * &lt;p&gt;方法返回引用，发生指针逃逸&lt;/p&gt; * @return */ public B methodPointerEscape() &#123; return new B(); &#125; /** * &lt;p&gt;实例引用发生指针逃逸&lt;/p&gt; */ public void instancePassPointerEscape() &#123; methodPointerEscape().printClassName(this); &#125; class B &#123; public void printClassName(EscapeAnalysis clazz) &#123; System.out.println(clazz.getClass().getName()); &#125; &#125;&#125; 逃逸分析研究对于 java 编译器有什么好处呢？我们知道 java 对象总是在堆中被分配的，因此 java 对象的创建和回收对系统的开销是很大的。java 语言被批评的一个地方，也是认为 java 性能慢的一个原因就是 java不支持栈上分配对象。JDK6里的 Swing内存和性能消耗的瓶颈就是由于 GC 来遍历引用树并回收内存的，如果对象的数目比较多，将给 GC 带来较大的压力，也间接得影响了性能。减少临时对象在堆内分配的数量，无疑是最有效的优化方法。java 中应用里普遍存在一种场景，一般是在方法体内，声明了一个局部变量，并且该变量在方法执行生命周期内未发生逃逸，按照 JVM内存分配机制，首先会在堆内存上创建类的实例（对象），然后将此对象的引用压入调用栈，继续执行，这是 JVM优化前的方式。当然，我们可以采用逃逸分析对 JVM 进行优化。即针对栈的重新分配方式，首先我们需要分析并且找到未逃逸的变量，将该变量类的实例化内存直接在栈里分配，无需进入堆，分配完成之后，继续调用栈内执行，最后线程执行结束，栈空间被回收，局部变量对象也被回收，通过这种方式的优化，与优化前的方案主要区别在于对象的存储介质，优化前是在堆中，而优化后的是在栈中，从而减少了堆中临时对象的分配（较耗时），从而优化性能。 使用逃逸分析进行性能优化(-XX:+DoEscapeAnalysis开启逃逸分析)123456public void method() &#123; Test test = new Test(); //处理逻辑 ...... test = null;&#125; 这段代码，之所以可以在栈上进行内存分配，是因为没有发生指针逃逸，即是引用没有暴露出这个方法体。 栈和堆内存分配比较123456789101112131415161718192021package me.stormma.gc;/** * &lt;p&gt;Created on 2017/4/21.&lt;/p&gt; * * @author stormma * @description: &lt;p&gt;内存分配比较&lt;/p&gt; */public class EscapeAnalysisTest &#123; public static void alloc() &#123; byte[] b = new byte[2]; b[0] = 1; &#125; public static void main(String[] args) &#123; long b = System.currentTimeMillis(); for (int i = 0; i &lt; 100000000; i++) &#123; alloc(); &#125; long e = System.currentTimeMillis(); System.out.println(e - b); &#125;&#125; JVM 参数为-server -Xmx10m -Xms10m -XX:-DoEscapeAnalysis -XX:+PrintGC, 运行结果JVM 参数为-server -Xmx10m -Xms10m -XX:+DoEscapeAnalysis -XX:+PrintGC, 运行结果 性能测试12345678910111213141516171819202122232425package me.stormma.gc;/** * &lt;p&gt;Created on 2017/4/21.&lt;/p&gt; * * @author stormma * * @description: &lt;p&gt;利用逃逸分析进行性能优化&lt;/p&gt; */public class EscapeAnalysisTest &#123; private static class Foo &#123; private int x; private static int counter; public Foo() &#123; x = (++counter); &#125; &#125; public static void main(String[] args) &#123; long start = System.nanoTime(); for (int i = 0; i &lt; 1000 * 1000 * 10; ++i) &#123; Foo foo = new Foo(); &#125; long end = System.nanoTime(); System.out.println("Time cost is " + (end - start)); &#125;&#125; 使用逃逸分析优化 JVM输出结果( -server -XX:+DoEscapeAnalysis -XX:+PrintGC)1Time cost is 11012345 未使用逃逸分析优化 JVM 输出结果( -server -Xmx10m -Xms10m -XX:-DoEscapeAnalysis -XX:+PrintGC)12345[GC (Allocation Failure) 33280K-&gt;408K(125952K), 0.0010344 secs][GC (Allocation Failure) 33688K-&gt;424K(125952K), 0.0009799 secs][GC (Allocation Failure) 33704K-&gt;376K(125952K), 0.0007297 secs][GC (Allocation Failure) 33656K-&gt;456K(159232K), 0.0014817 secs]Time cost is 68562263 分析结果，性能优化1/6。原文出处]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据区、代码区、栈区、堆区]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%81%E4%BB%A3%E7%A0%81%E5%8C%BA%E3%80%81%E6%A0%88%E5%8C%BA%E3%80%81%E5%A0%86%E5%8C%BA.html</url>
    <content type="text"><![CDATA[概念 栈区(stack)：由系统的编译器自动的释放，主要用来存放 方法中的参数，一些临时的局部变量 等，并且方法中的参数一般在操作完后，会由编译器自动的释放掉。 堆区(heap)：由程序员决定，在Java中，如果程序员不释放的话，一般会由垃圾回收机制自动的清理掉。此区域主要存放： 创建的对象、动态申请的临时空间等 。 数据区(data seg)：也称 全局区或者静态区 ，存放 静态变量、全局变量等 都会存放到数据区，此区域上的东西都被全局所共享。比如我们通常采用 类名. 的方式就可以访问到方法，这就是所谓的静态方法，存放到数据区的。 代码区：存放程序编译后可以执行代码的地方。比如执行代码时写的While语句、if语句等，都会存放到此。 内存分析123456789// Person类class Person &#123; int id ; int age = 20 ; Person(int _id, int _age)&#123; id = _id ; age = _age ; &#125;&#125; 接下来对new一个对象进行分析执行语句： Person tom = new Person(1,25) ; 第一步：我们知道每一个类都有一个默认的构造函数，即Person(),因此上述会先调用默认的构造函数 第二步：执行构造函数New Person(1,25)时，我们知道调用的是Person(int _id, int _age)，所以此时栈空间会分配方法的参数的临时变量如下 第三步:执行构造函数 Person(int _id, int _age),id = _id; age = _age;此时栈中的临时变量会改变默认构造函数创建的对象,赋值完后，栈中的临时变量会自动的销毁，然后创建的临时变量Tom会自动的指向创建的对象。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java常用类和方法重点总结]]></title>
    <url>%2FJava%E5%B8%B8%E7%94%A8%E7%B1%BB%E5%92%8C%E6%96%B9%E6%B3%95%E9%87%8D%E7%82%B9%E6%80%BB%E7%BB%93.html</url>
    <content type="text"><![CDATA[简述Java中内存分配的问题 凡是new() 出来的东西，都是堆中进行分配的 局部变量【数据类型+变量名】都是在栈中进行分配的 静态变量【static】和字符串常量【”String”】都是在数据区进行分配的 方法【代码】都是在代码区进行存放的 简述Java中Object类的地位 Java中所有的类【自己定义的类以及Sun公司提供的类】都默认自动继承了Obeject类 Java中所有的类都从Object类中继承了toString()方法、hashCode()方法和equals等方法 简述Object类中toString()方法的注意事项 toString()方法的返回值是一个字符串 toString()方法返回的是类的名字和该对象的哈希码组成的一个字符串，即toString()方法返回的是 该对象的字符串表现形式 在Java中，System.out.println(类对象名)实际输出的是该对象的toString()方法返回的字符串，即括号中的内容等价于类对象名.toString()，toString()方法的好处 在碰到println方法的时候，会被自动调用，不用显示的写出来 ， eg: 12String str = new String();System.out.println(str) &lt;==&gt; System.out.println(str.toString()) ----&gt;toString()自动隐藏 建议所有的子类都重写从Object类中继承过来toString方法，否则toString方法的返回值没有什么实际含义（为什么要重写的原因） 简述Object类中equals()类方法的注意事项 equals方法的返回值为true或false Object类中equals方法只有在 两个对象是同一块内存区域时，即不但内容相同、地址还必须相同时，才返回true，否则即便内容相同、如果地址不同只会返回false 重写Object类中的equals方法目的在于：保证只要两个对象的内容相同，equals方法就返回true（为什么要重写的原因） 简述Object类中hashCode()方法的注意事项 哈希码原本指的是 内存空间地址的十六进制表示形式 hashCode()方法返回的是 **该对象的哈希码，即该对象的真实内存地址的十六进制表示形式，但是重写完hashCode()方法之后，返回的不再是该对象真实内存地址的十六进制表示形式 学习Java中toString方法、equals方法、hashCode方法共同的一个注意事项在Java中，凡是动态分配的内存都是没有名字的，而是将其地址赋给一个引用变量【引用】，用引用变量去代表这个事物，所以引用和动态分配的内存有本质上的区别，但是学习Java中的toString方法、equals方法和hashCode方法时默认引用和其指向的动态分配的内存是一个事物，不区分彼此 从逻辑上阐述为什么要重写equals方法和hashCode方法 （重点） 对于用户来说，逻辑上只要两个对象的内容相同，其地址以及这两个对象就应该相等，而要保证地址相同就应该重写hashCode方法，而要保证对象相同就应该重写equals方法 凡是Java中自带的类都已经重写了equals方法和hashCode方法，重写之后只要两个对象的内容相同，hashCode方法的返回值就相同，保证地址相同，equals方法就返回true，保证两个对象是同一个对象，而Java中凡是用户自己定义的类只能自己区重写这两个方法（ 为什么我们自己定义的类，要自己重写这两个方法，toString()也要重写 ）；【new Integer(1)与new Integer(2)】 简述String类中的equals方法与Object类中的equals方法的不同点 String类中的equals方法是用来判断两个对象的内容是否相同、而Object类中的equals方法是用来判断两个对象是否是同一个对象，所谓同一个对象指的是内存中的同一块存储空间 对于Java中StringBuffer类的由来 String类对象表示不可修改的UniCode编码字符串、即String类对象一旦创建就不可在更改，即只要创建一个字符串，就会重新分配一块内存空间，因此如果经常对字符串的内容进行修改而使用String类的话，就会造成空间以及时间的浪费，因此如果经常对字符串的内容进行修改的话，可以使用StringBuffer类，StringBuffer类可以一直对同一块内存空间进行操作，对一个字符串不断的进行修改，正因为StringBuffer类的这个优点，所以StringBuffer类中存在着大量修改字符串的方法，但是String类中却没有 简述String、StringBuffer类中常用的一个方法 valueOf():将基本类型数据转化为字符串 简述String类与StringBuffer类的关联 先使用StringBuffer类将字符串的内容不断的进行修改、最后将成品放到String类里面去 StringBuffer类中的toString方法可以将String类对象转化为StringBuffer类对象String str1 = str2.toString（）; 在Java中双引号括起来的字符串也可以被当做String类对象 如：“zhang”.length();]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给出一组年龄，用冒泡排序求最大年龄，最小年龄]]></title>
    <url>%2F%E7%BB%99%E5%87%BA%E4%B8%80%E7%BB%84%E5%B9%B4%E9%BE%84%EF%BC%8C%E6%B1%82%E6%9C%80%E5%A4%A7%E5%B9%B4%E9%BE%84%EF%BC%8C%E6%9C%80%E5%B0%8F%E5%B9%B4%E9%BE%84.html</url>
    <content type="text"><![CDATA[法一123456789101112131415161718//求出最大年龄,求出最小年龄,求出平均年龄 int[] ages = &#123;34,11,45,23,16&#125;; boolean flag = true ; for (int i = 4; flag &amp;&amp; i &gt;0 ; i--) &#123; flag = false ; for (int j = 0; j &lt; i; j++) &#123; if(ages[j]&gt;ages[j+1])&#123; int temp = ages[j]; ages[j] = ages[j+1]; ages[j+1] = temp ; flag = true ; &#125; &#125; System.out.println("-------"); &#125; for (int i = 0; i &lt; 5; i++) &#123; System.out.print(ages[i]+ "\t"); &#125; 法二12345678910111213141516171819int[] ages = &#123;1,2,3,4,5&#125;; boolean flag = true ; for (int i = 0; flag &amp;&amp; i &lt; ages.length-1; i++) &#123; flag = false ; for (int j = 0; j &lt; ages.length - i - 1; j++) &#123; if(ages[j] &gt; ages[j+1])&#123; int temp = ages[j]; ages[j] = ages[j+1]; ages[j+1] = temp ; flag = true ; &#125; &#125; System.out.println("-----"); &#125; for (int i = 0; i &lt; ages.length; i++) &#123; System.out.print(ages[i]+"\t"); &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不使用第三个变量交换两变量的值]]></title>
    <url>%2F%E4%B8%8D%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%89%E4%B8%AA%E5%8F%98%E9%87%8F%E4%BA%A4%E6%8D%A2%E4%B8%A4%E5%8F%98%E9%87%8F%E7%9A%84%E5%80%BC.html</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435/** 有两个整数 分别是 a=10 b=8 在不使用第三个变量的情况下 对其值进行交换*/class Work1&#123; public static void main(String[] args)&#123; int a = 10; int b = 8; /* a=a+b; b=a-b; a=a-b; a=a^b; b=a^b; a=a^b; 1010 1000 ---------- 0010 2 0010 1000 ----- 1010 10 0010 1010 ----- 1000 8 */ a=b+(b=a)*0; System.out.println("a="+a+",b="+b); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Java倒序输出12345]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8Java%E5%80%92%E5%BA%8F%E8%BE%93%E5%87%BA12345.html</url>
    <content type="text"><![CDATA[原理：%取最后一位，/取整除掉最后一位逐行输出123456int t = 12345 ;while(t&gt;0)&#123; int num = t%10; t /= 10 ; System.out.print(num); &#125; 一次性输出1234567891011121314int num = 12345; /* num%10 5 num/10%10 4 num/100%10 num/10/10%10 3 num/1000%10 num/10/10/10%10 2 num/10000%10 num/10/10/10/10%10 1 */ int result=0; while(num&gt;0)&#123; result = result*10+num%10; num/=10; &#125; System.out.print(result);]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发和并行的区别]]></title>
    <url>%2F%E5%B9%B6%E5%8F%91%E5%92%8C%E5%B9%B6%E8%A1%8C%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content type="text"><![CDATA[观点一并行（parallelise）同时刻（某点），并发（concurrency）同时间（某段） 观点二深入理解计算机系统CSAPP的回答。并发（Concurrency）是说进程B的开始时间是在进程A的开始时间与结束时间之间，我们就说A和B是并发的。并行（Parallel Execution）是并发的真子集，指同一时间两个进程运行在不同的机器上或者同一个机器不同的核心上。作者：starrynight链接：https://www.zhihu.com/question/33515481/answer/67962756来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 观点三如果某个系统支持两个或者多个动作（Action）同时存在，那么这个系统就是一个并发系统。如果某个系统支持两个或者多个动作同时执行，那么这个系统就是一个并行系统。并发系统与并行系统这两个定义之间的关键差异在于“存在”这个词。在并发程序中可以同时拥有两个或者多个线程。这意味着，如果程序在单核处理器上运行，那么这两个线程将交替地换入或者换出内存。这些线程是同时“存在”的——每个线程都处于执行过程中的某个状态。如果程序能够并行执行，那么就一定是运行在多核处理器上。此时，程序中的每个线程都将分配到一个独立的处理器核上，因此可以同时运行。我相信你已经能够得出结论——“并行”概念是“并发”概念的一个子集。也就是说，你可以编写一个拥有多个线程或者进程的并发程序，但如果没有多核处理器来执行这个程序，那么就不能以并行方式来运行代码。因此，凡是在求解单个问题时涉及多个执行流程的编程模式或者执行行为，都属于并发编程的范畴。摘自：《并发的艺术》 — 〔美〕布雷谢斯在豆瓣阅读书店查看：https://read.douban.com/ebook/10034459/ 作者：BeginMan链接：https://www.zhihu.com/question/33515481/answer/105348019来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 观点四并发：交替或者同时做不同事的能力并行：同时做不同事的能力 行话解释：并发：不同代码块交替或者同时执行的性能并行：不同代码块同时执行的性能 观点五作者：李运华链接：https://www.zhihu.com/question/33515481/answer/121050539来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 详细请参考： Concurrency vs. ParallelismConcurrency并发的反义词是顺序，concurrency vs sequential，例如： 顺序处理：你陪女朋友先看电影（Task1），看完后陪女朋友到花店买了一束花（Task2），然后陪女朋友去西餐厅吃烛光晚餐（Task3），这就是“顺序处理”，因为整个过程中只有你这一个处理器，事情只能一件一件的做（要么是你亲自做，要么你要等别人做）。Task1你要花2小时，Task2等花做好你要花30分钟，Task3等菜做好要30分钟，从你开始看电影到开始吃饭，全程需要3小时（假设走路不算时间）。 并发处理：你陪女朋友先看电影（Task1），同时打电话给花店预定一束花，花店安排人员在20：00送到西餐厅（Task2）；同时你打电话给西餐定预定20：00的浪漫烛光晚餐，西餐厅开始给你准备晚餐（Task3）；等到你电影看完跑到西餐厅，花也送到了，晚餐也准备好了，你跑过去直接献花吃饭然后开房即可，这就是并发处理。Task1还是2小时，但Task2和Task3也在这2小时完成了，从你开始看电影到开始吃饭，全程只需要2小时，3个任务是并发完成的。秘诀就是有3个处理器了：你、花店、餐厅在同一个时间段内都在做各自的任务。Parallelism并行的反义词是串行，Parallelism vs Serial，比如说给你一个100万的整形数组，挑出其中最小的值。 串行处理从数组的第一个开始扫描到最后一个，类似冒泡排序一样 并行处理将数组分为10组，每组10万个整形，同时扫描10组得到10个数值，然后再将这10个数值排列一下。上面这个简单的例子也可以看出，串行改为并行其实并不那么简单，涉及到任务分解（有先后依赖的任务就不能做到并行）、任务运行（可能要考虑互斥、锁、共享等）、结果合并。以Java的并行垃圾回收器Parallel为例，标记阶段、回收阶段各自可以多线程并行，但不能将回收阶段和标记阶段一起并行，因为回收阶段的处理依赖标记阶段的结果。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java实现九九乘法表]]></title>
    <url>%2FJava%E5%AE%9E%E7%8E%B0%E4%B9%9D%E4%B9%9D%E4%B9%98%E6%B3%95%E8%A1%A8.html</url>
    <content type="text"><![CDATA[关键难点：内循环的条件判断123456789public static void main(String[] args) &#123; //九九乘法表 for(int i = 1 ;i&lt;=9;i++)&#123; for(int j = 1;j&lt;=i;j++)&#123; System.out.print(j+"*"+i+"="+j*i+" "); &#125; System.out.println("\n"); &#125; &#125; 输出结果：]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中内存分配策略及堆和栈的比较]]></title>
    <url>%2Fjava%E4%B8%AD%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5%E5%8F%8A%E5%A0%86%E5%92%8C%E6%A0%88%E7%9A%84%E6%AF%94%E8%BE%83.html</url>
    <content type="text"><![CDATA[内存分配策略按照编译原理的观点,程序运行时的内存分配有三种策略,分别是静态的,栈式的,和堆式的.静态存储分配是指在编译时就能确定每个数据目标在运行时刻的存储空间需求,因而在编译时就可以给他们分配固定的内存空间.这种分配策略要求程序代码中不允许有可变数据结构(比如可变数组)的存在,也不允许有嵌套或者递归的结构出现,因为它们都会导致编译程序无法计算准确的存储空间需求.栈式存储分配也可称为动态存储分配,是由一个类似于堆栈的运行栈来实现的.和静态存储分配相反,在栈式存储方案中,程序对数据区的需求在编译时是完全未知的,只有到运行的时候才能够知道,但是规定在运行中进入一个程序模块时,必须知道该程序模块所需的数据区大小才能够为其分配内存.和我们在数据结构所熟知的栈一样,栈式存储分配按照先进后出的原则进行分配。静态存储分配要求在编译时能知道所有变量的存储要求,栈式存储分配要求在过程的入口处必须知道所有的存储要求,而堆式存储分配则专门负责在编译时或运行时模块入口处都无法确定存储要求的数据结构的内存分配,比如可变长度串和对象实例.堆由大片的可利用块或空闲块组成,堆中的内存可以按照任意顺序分配和释放. 堆和栈的比较上面的定义从编译原理的教材中总结而来,除静态存储分配之外,都显得很呆板和难以理解,下面撇开静态存储分配,集中比较堆和栈:从堆和栈的功能和作用来通俗的比较,堆主要用来存放对象的，栈主要是用来执行程序的.而这种不同又主要是由于堆和栈的特点决定的:在编程中，例如C/C++中，所有的方法调用都是通过栈来进行的,所有的局部变量,形式参数都是从栈中分配内存空间的。实际上也不是什么分配,只是从栈顶向上用就行,就好像工厂中的传送带(conveyor belt)一样,Stack Pointer会自动指引你到放东西的位置,你所要做的只是把东西放下来就行.退出函数的时候，修改栈指针就可以把栈中的内容销毁.这样的模式速度最快, 当然要用来运行程序了.需要注意的是,在分配的时候,比如为一个即将要调用的程序模块分配数据区时,应事先知道这个数据区的大小,也就说是虽然分配是在程序运行时进行的,但是分配的大小多少是确定的,不变的,而这个”大小多少”是在编译时确定的,不是在运行时.堆是应用程序在运行的时候请求操作系统分配给自己内存，由于从操作系统管理的内存分配,所以在分配和销毁时都要占用时间，因此用堆的效率非常低.但是堆的优点在于,编译器不必知道要从堆里分配多少存储空间，也不必知道存储的数据要在堆里停留多长的时间,因此,用堆保存数据时会得到更大的灵活性。事实上,面向对象的多态性,堆内存分配是必不可少的,因为多态变量所需的存储空间只有在运行时创建了对象之后才能确定.在C++中，要求创建一个对象时，只需用 new命令编制相关的代码即可。执行这些代码时，会在堆里自动进行数据的保存.当然，为达到这种灵活性，必然会付出一定的代价:在堆里分配存储空间时会花掉更长的时间！这也正是导致我们刚才所说的效率低的原因,看来列宁同志说的好,人的优点往往也是人的缺点,人的缺点往往也是人的优点(晕~). JVM中的堆和栈JVM是基于堆栈的虚拟机.JVM为每个新创建的线程都分配一个堆栈.也就是说,对于一个Java程序来说，它的运行就是通过对堆栈的操作来完成的。堆栈以帧为单位保存线程的状态。JVM对堆栈只进行两种操作:以帧为单位的压栈和出栈操作。我们知道,某个线程正在执行的方法称为此线程的当前方法.我们可能不知道,当前方法使用的帧称为当前帧。当线程激活一个Java方法,JVM就会在线程的 Java堆栈里新压入一个帧。这个帧自然成为了当前帧.在此方法执行期间,这个帧将用来保存参数,局部变量,中间计算过程和其他数据.这个帧在这里和编译原理中的活动纪录的概念是差不多的.从Java的这种分配机制来看,堆栈又可以这样理解:堆栈(Stack)是操作系统在建立某个进程时或者线程(在支持多线程的操作系统中是线程)为这个线程建立的存储区域，该区域具有先进后出的特性。个Java应用都唯一对应一个JVM实例，每一个实例唯一对应一个堆。应用程序在运行中所创建的所有类实例或数组都放在这个堆中,并由应用所有的线程共享.跟C/C++不同，Java中分配堆内存是自动初始化的。Java中所有对象的存储空间都是在堆中分配的，但是这个对象的引用却是在堆栈中分配,也就是说在建立一个对象时从两个地方都分配内存，在堆中分配的内存实际建立这个对象，而在堆栈中分配的内存只是一个指向这个堆对象的指针(引用)而已。JVM运行时，将内存分为堆和栈，堆中存放的是创建的对象，JAVA字符串对象内存实现时，在堆中开辟了一快很小的内存，叫字符串常量池，用来存放特定的字符串对象。关于String对象的创建，两种方式是不同的，第一种不用new的简单语法，即String s1=”JAVA”;创建步骤是先看常量池中有没有与”JAVA”相同的的字符串对象，如果有，将s1指向该对象，若没有，则创建一个新对象，并让s1指向它。第二种是new语法String s2=”JAVA”;这种语法是在堆而不是在常量池中创建对象，并将s2指向它，然后去字符串常量池中看看，是否有与之相同的内容的对象，如果有，则将new出来的字符串对象与字符串常量池中的对象联系起来，如果没有，则在字符串常量池中再创建一个包含该内容的字符串对象，并将堆内存中的对象与字符串常量池中新建出来的对象联系起来。这就是字符串的一次投入，终生回报的内存机制，对字符串的比较带来好处。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java内存机制和内存地址]]></title>
    <url>%2FJava%E5%86%85%E5%AD%98%E6%9C%BA%E5%88%B6%E5%92%8C%E5%86%85%E5%AD%98%E5%9C%B0%E5%9D%80.html</url>
    <content type="text"><![CDATA[问题一：String str1 = “abc”;String str2 = “abc”;System.out.println(str1==str2); //true 问题二：String str1 =new String (“abc”);String str2 =new String (“abc”);System.out.println(str1==str2); // false 问题三：String s1 = “ja”;String s2 = “va”;String s3 = “java”;String s4 = s1 + s2;System.out.println(s3 == s4);//falseSystem.out.println(s3.equals(s4));//true由于以上问题让我含糊不清，于是特地搜集了一些有关java内存分配的资料,以下是网摘： Java 中的堆和栈Java把内存划分成两种：一种是栈内存，一种是堆内存。在函数中定义的一些基本类型的变量和对象的引用变量都在函数的栈内存中分配。当在一段代码块定义一个变量时，Java就在栈中为这个变量分配内存空间，当超过变量的作用域后，Java会自动释放掉为该变量所分配的内存空间，该内存空间可以立即被另作他用。堆内存用来存放由new创建的对象和数组。在堆中分配的内存，由Java虚拟机的自动垃圾回收器来管理。在堆中产生了一个数组或对象后，还可以在栈中定义一个特殊的变量，让栈中这个变量的取值等于数组或对象在堆内存中的首地址，栈中的这个变量就成了数组或对象的引用变量。引用变量就相当于是为数组或对象起的一个名称，以后就可以在程序中使用栈中的引用变量来访问堆中的数组或对象。 具体的说：栈与堆都是Java用来在Ram中存放数据的地方。与C++不同，Java自动管理栈和堆，程序员不能直接地设置栈或堆。Java的堆是一个运行时数据区,类的(对象从中分配空间。这些对象通过new、newarray、anewarray和multianewarray等指令建立，它们不需要程序代码来显式的释放。堆是由垃圾回收来负责的，堆的优势是可以动态地分配内存大小，生存期也不必事先告诉编译器，因为它是在运行时动态分配内存的，Java的垃圾收集器会自动收走这些不再使用的数据。但缺点是，由于要在运行时动态分配内存，存取速度较慢。栈的优势是，存取速度比堆要快，仅次于寄存器，栈数据可以共享。但缺点是，存在栈中的数据大小与生存期必须是确定的，缺乏灵活性。栈中主要存放一些基本类型的变量（,int, short, long, byte, float, double, boolean, char）和对象句柄。栈有一个很重要的特殊性，就是存在栈中的数据可以共享。假设我们同时定义：int a = 3;int b = 3；编译器先处理int a = 3；首先它会在栈中创建一个变量为a的引用，然后查找栈中是否有3这个值，如果没找到，就将3存放进来，然后将a指向3。接着处理int b = 3；在创建完b的引用变量后，因为在栈中已经有3这个值，便将b直接指向3。这样，就出现了a与b同时均指向3的情况。这时，如果再令a=4；那么编译器会重新搜索栈中是否有4值，如果没有，则将4存放进来，并令a指向4；如果已经有了，则直接将a指向这个地址。因此a值的改变不会影响到b的值。要注意这种数据的共享与两个对象的引用同时指向一个对象的这种共享是不同的，因为这种情况a的修改并不会影响到b, 它是由编译器完成的，它有利于节省空间。而一个对象引用变量修改了这个对象的内部状态，会影响到另一个对象引用变量。String是一个特殊的包装类数据。可以用：String str = new String(“abc”);String str = “abc”;两种的形式来创建，第一种是用new()来新建对象的，它会在存放于堆中。每调用一次就会创建一个新的对象。而第二种是先在栈中创建一个对String类的对象引用变量str，然后查找栈中有没有存放”abc”，如果没有，则将”abc”存放进栈，并令str指向”abc”，如果已经有”abc” 则直接令str指向“abc”。 比较类里面的数值是否相等时，用equals()方法；当测试两个包装类的引用是否指向同一个对象时，用==，下面用例子说明上面的理论。 方式一：String str1 = “abc”;String str2 = “abc”;System.out.println(str1==str2); //true可以看出str1和str2是指向同一个对象的。 方式二：String str1 =new String (“abc”);String str2 =new String (“abc”);System.out.println(str1==str2); // false用new的方式是生成不同的对象。每一次生成一个。因此用第一种方式创建多个”abc”字符串,在内存中其实只存在一个对象而已. 这种写法有利与节省内存空间. 同时它可以在一定程度上提高程序的运行速度，因为JVM会自动根据栈中数据的实际情况来决定是否有必要创建新对象。而对于String str = new String(“abc”)；的代码，则一概在堆中创建新对象，而不管其字符串值是否相等，是否有必要创建新对象，从而加重了程序的负担。另一方面, 要注意: 我们在使用诸如String str = “abc”；的格式定义类时，总是想当然地认为，创建了String类的对象str。担心陷阱！对象可能并没有被创建！而可能只是指向一个先前已经创建的对象。只有通过new()方法才能保证每次都创建一个新的对象。由于String类的immutable性质，当String变量需要经常变换其值时，应该考虑使用StringBuffer类，以提高程序效率。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java语言基础]]></title>
    <url>%2FJava%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80.html</url>
    <content type="text"><![CDATA[1、 java程序的组成：关键字，标识符，注释，变量，语句，表达式，数组，方法 2、 关键字：Java语言内部使用了的一些用于特殊用途的词汇，那么在程序中用户不能使用。语言本身保留了一些词汇用于语言的语法等用途。 3、 已用到的关键字：class 声明一个类。public: 表示该类可以被外界调用 如果一个类被声明为public,那么该类所在的文件名要和类名一致。编译后的文件名和类名相同。static:表示是静态的。void：表示没有返回值 4、 关键字都是小写的。5、 标识符：标识符就是用户在程序自定义使用的一些名词。在程序中标识符可以用来表示类名，变量名，方法名，参数名等。在java中标识符的组成：大小写字母，数字，下划线以及$组成。不能以数字开头。标识符通常要有意义。不能随便乱取。建议使用英文表示。类名：以大写字母开头，所有单词首字母大写。采用的Pascal命名规则变量名：第一个单词首字母以小写开头，其后的所有单词首字母以大写开头。采用的驼峰命名规则。 6、 注释：在程序中，方便人们阅读代码而写的一些说明文字。在java中有3类注释：单行注释：用// 开头多行注释：用/开头 /结尾文档注释：用/* 开头 /结尾，并且在其中会用到一些java定义好的注解来声明注意：单行注释和多行注释不会被编译 也不会被jvm解释执行，文档注释会编译，可以通过jvm生成对应的html文档,方便用户查询使用。 7、 总结：第一条：常用的关键字要熟悉，能够记住。第二条：标识符的命名要符合规范。第三条：程序一定要注释。建议先写注释，再写代码。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变量和常量]]></title>
    <url>%2F%E5%8F%98%E9%87%8F%E5%92%8C%E5%B8%B8%E9%87%8F.html</url>
    <content type="text"><![CDATA[1、为什么需要变量？方便对一个数据的修改和使用。如：一个数据在多处使用时，如果要修改，那么多个地方同时都需要修改，这个需要一个变量用于存储数据，在使用时，直接使用变量即可，这样当修改数据时，只需要修改变量本身的值就可以了。 2、什么是变量？在java中，变量是一个存储空间的表示。 3、使用变量的语法：数据类型 变量名 = 值 4、数据类型基本可以分为两类：基本数据类型和引用数据类型基本数据类型：整数、小数、布尔、字符 整数：byte:表示是字节 占用的内存空间是1字节short:表示是短整形 占用的内存空间是2字节int:表示整形 占用的内存空间是4字节long:表示长整形 占用的内存空间是8字节 小数：float：单精度浮点数 占用的内存空间是4字节double:双精度浮点数 占用的内存空间是8字节 布尔：boolean:布尔类型的值 只有true和false 1字节 字符：用于表示单个字符char:2个字节 引用数据类型：class ,interface,数组1234567891011121314public static void main(String[] args)&#123; byte age = 125 ; short t = 23 ; char ch = '中'; boolean flag = true ; float score = 98.5f ; double avgScore = 80.4 ; System.out.println(age); System.out.println(t); System.out.println(ch); System.out.println(flag); System.out.println(score); System.out.println(avgScore);&#125; 整数默认是 int,小数默认是 double单精度后面的 f 表示是单精度浮点，需要明确出来，可以 f 或者 F . 5、5. 整数在java中有4种表现形式：二进制：是由数字0和1组成的。八进制：是由数字0到7组成的。十进制：是由数字0到9组成的。十六进制：是由数字0到9和ABCDEF组成。进制之间转换： 6、 变量名：命名规则采用驼峰命名法。7、 数据类型的转换：自动类型转换：强制类型转换：类型提升：当是byte，short，char 进行运算时会自动提升为int。当有float类型，提升为float类型当有double类型，提升为double类型当有long类型，提升为long类型 8、 字符类型：在java中，char用来表示一个字符，一个字符也就是一个字母,用单引号括起来。’A’；也可以用char来表示一个中文。表示中文时特别需要注意编码问题。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java入门]]></title>
    <url>%2FJava%E5%85%A5%E9%97%A8.html</url>
    <content type="text"><![CDATA[1. Java:是SUN(Starfard University Network)公司在1995年开发了一门完全面向对象的，开源的高级编程语言。2. Java发展历史：1995 java诞生1996 jdk1.0发布1998 jdk1.2 发布 J2SE(1.2,1.3,1.4)2004 jdk1.5 发布 JAVASE52014 jdk1.8 发布 JAVASE8 3. Java的优势：跨平台一次编写 多次运行Java是运行在JVM（Java virtual machine）之上的。为不同平台下开发不同的JVM。所有JVM对java语言本身的规范是一样的。 4. Java的版本：JavaSE：java standard edition java标准版 是Java的基础。JavaME：java micro edition :移动端，小型设备，PDA等JavaEE：java enterprise edition：java的企业级版本，java web等 5. JVM,JRE,JDKJvm java virtual machine java虚拟机 运行java程序Jre java runtime environment java运行时环境Jdk java development kit java 开发工具包JDK—&gt;JRE—-&gt;JVM 6. 下载jdk来安装：官方下载地址指定安装位置后 傻瓜式安装在命令行运行时： 需要配置环境变量。在dos中执行命令，操作系统会根据系统的环境变量Path去寻找对应的可执行程序（.exe,.bat）。 7. Path变量的配置计算机 右键 属性 高级系统设置 环境变量添加一个JAVA_HOME变量 值为jdk的安装目录在Path变量中的前面添加%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin;注意：使用分号对每个路径进行分割，都是英文下的分号 8. 编写第一个HelloWorld应用程序：所有java程序都是以.java结尾的。class HelloWorld{ public static void main(String[] args){ System.out.println(“hello world”); }}保存为HelloWorld.java文件 9. 编写好的应用程序,也就是以.java结尾的文件，称为源文件。编写好的源文件需要通过javac命令进行编译，编译的目的是为了让jvm可以认识并且执行。编译后会生成一个.class文件，该文件称为字节码文件，能够被jvm认识并且执行。运行程序：使用java命令进行运行。运行的是.class文件 10. Java程序的编写流程：编写源文件——–&gt;通过javac命令编译源文件——–&gt;通过java命令执行字节码文件。Javac编译时 需要跟上后缀.java,而java命令后直接跟文件即可 不需要后缀。 11. HelloWorld程序详解：12. classpath环境变量：配置path环境变量的目的是在任意路径下都可以执行java,javac命令配置classpath的目的是为了在任意路径下都可以执行.class文件。所谓的classpath指定的是.class文件所在的位置。classpath不设置的时候：java命令会在当前路径下查找.class，如果找不到 如果设置了classpath，那么会从classpath指定的路径去查找.class如果classpath的值后面不加分号：查找的classpath指定的路径下是否有.class文件，如果加了分号：先查询classpath指定的路径，再查找当前目录下是否有.class文件。注意：通常配置classpath会以.;开头，表示先查询的是当前路径CLASSPATH=.;%JAVA_HOME%\lib\tools.jar;%JAVA_HOME%\lib\dt.jar;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机的基础]]></title>
    <url>%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%9F%BA%E7%A1%80.html</url>
    <content type="text"><![CDATA[1、计算机的组成：硬件和软件2、硬件：物理设备（主板、CPU、显示器、存储设备、外设、内存条）3、没有软件的计算机：裸机4、软件：按照一定顺序和逻辑组成的计算机指令——-程序、软件。软件 = 程序 + 数据结构。 5、软件开发：制作软件的过程。6、人机交互：命令（dos）、图形界面。7、计算机语言：人和计算机交流的一种方式。8、计算机语言的分类：a) 机器语言b) 汇编语言c) 高级语言：c , c++ , java , scala , python越高级的语言越接近人们的习惯。越高级的语言执行效率是越低的。 9、操作系统10、了解一些常见的dos命令：win+r —-&gt;运行a) dir 显示当前目录信息b) cd.. 回到上一级目录c) cd. 到当前目录d) cd/ 到根目录e) D: 到D盘f) cd path — &gt; 到指定的目录，不能跳盘符g) md 目录名 — &gt; 新建一个目录 make directoryh) ren oldname newname —–&gt; 重命名 renamei) rd 目录名 —-&gt;删除目录 remove directoryj) move 文件 目标目录 —–&gt; 移动文件k) notepad 打开记事本l) copy 源文件 目标文件 —–&gt; 复制文件m) del 文件名 —-&gt; 删除文件n) cls 清屏]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL备份]]></title>
    <url>%2FMySQL%E5%A4%87%E4%BB%BD.html</url>
    <content type="text"><![CDATA[定时完成数据库的备份① 手动备份数据库(表的)方法cmd控制台:mysqldump –u root –proot 数据库 [表名1 表名2..] &gt; 文件路径比如: 把temp数据库备份到 d:\temp.bakmysqldump –u root –proot temp &gt; d:\temp.bak如果你希望备份是，数据库的某几张表mysqldump –u root –prot temp dept &gt; d:\temp.dept.bak 如何使用备份文件恢复我们的数据.mysql控制台source d:\temp.dept.bak ② 使用定时器来自定完成把备份数据库的指令，写入到 bat文件, 然后通过任务管理器去定时调用 bat文件.mytask.bat 内容是:C:\myenv\mysql5.5.27\bin\mysqldump -u root -proot temp dept &gt; d:\temp.dept.bak☞ 如果你的mysqldump.exe文件路径有空格，则一定要使用 “” 包括.把mytask.bat 做成一个任务，并定时调用在 2:00 调用一次步骤 任务计划-&gt;增加一个任务，选中你的mytask.bat文件 ，最后配置:测试ok 如何在linux下完成定时任务:linux如何备份. 直接执行PHP脚本, 需要在同一个服务器上执行.crontab -e00 /usr/local/bin/php /home/htdocs/phptimer.php2.通过HTTP请求来触发脚本, PHP文件允许不在同一服务器上crontab -e00 /usr/bin/wget -q -O temp.txt http://www.phptimer.com/phptimer.php上面是通过wget来请求PHP文件, PHP输出会保存在临时文件temp.txt中crontab -e00 /usr/bin/curl -o temp.txt http://www.phptimer.com/phptimer.php上面是通过curl -o来请求PHP文件, PHP输出会保存在临时文件temp.txt中crontab -e00 lynx -dump http://www.phptimer.com/phptimer.php上面是通过Lynx文本浏览器来请求PHP文件 分表技术分表技术有(水平分割和垂直分割)当一张越来越大时候，即使添加索引还慢的话，我们可以使用分表以qq用户表来具体的说明一下分表的操作.思路如图 ：首先我创建三张表 user0 / user1 /user2 , 然后我再创建 uuid表，该表的作用就是提供自增的id,走代码:123456789101112131415161718192021create table user0(id int unsigned primary key ,name varchar(32) not null default '',pwd varchar(32) not null default '')engine=myisam charset utf8;create table user1(id int unsigned primary key ,name varchar(32) not null default '',pwd varchar(32) not null default '')engine=myisam charset utf8;create table user2(id int unsigned primary key ,name varchar(32) not null default '',pwd varchar(32) not null default '')engine=myisam charset utf8;create table uuid(id int unsigned primary key auto_increment)engine=myisam charset utf8; 思考: 如果我们做的是一个平安保险公司的一个订单(8999999999000000条)查询功能更.,如何处理海量表?-&gt;按时间. 分表的标准是依赖业务逻辑(时间/地区/….) 安装字符不同. a-z 我们给用户提供的查询界面一定是有条件，不能让用户进行大范围.(世界)，如果需要的可以根据不同的规则，对应多套分表. 检索时候，带分页条件，减少返回的数据. 项目中，灵活的根据需求来考虑. 垂直分割如果一张表某个字段，信息量大，但是我们很少查询，则可以考虑把这些字段，单独的放入到一张表中，这种方式称为垂直分割.]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intellij IDEA 激活方法]]></title>
    <url>%2FIntellij%20IDEA%20%E6%BF%80%E6%B4%BB%E6%96%B9%E6%B3%95.html</url>
    <content type="text"><![CDATA[填入下面的license server: http://intellij.mandroid.cn/ http://idea.imsxm.com/ http://idea.iteblog.com/key.php]]></content>
      <categories>
        <category>有料</category>
      </categories>
      <tags>
        <tag>编程工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL优化（二）]]></title>
    <url>%2FMySQL%E4%BC%98%E5%8C%96%EF%BC%88%E4%BA%8C%EF%BC%89.html</url>
    <content type="text"><![CDATA[四种索引(主键索引/唯一索引/全文索引/普通索引)1. 添加1.1主键索引添加当一张表，把某个列设为主键的时候，则该列就是主键索引create table aaa (id int unsigned primary key auto_increment , name varchar(32) not null defaul ‘’);这是id 列就是主键索引.如果你创建表时，没有指定主键索引，也可以在创建表后，在添加, 指令:alter table 表名 add primary key (列名);举例:create table bbb (id int , name varchar(32) not null default ‘’);alter table bbb add primary key (id); 1.2普通索引一般来说，普通索引的创建，是先创建表，然后在创建普通索引比如:create table ccc(id int unsigned,name varchar(32))create index 索引名 on 表 (列1,列名2); 1.3创建全文索引全文索引，主要是针对对文件，文本的检索, 比如文章, 全文索引针对MyISAM有用.创建 ：CREATE TABLE articles ( id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY, title VARCHAR(200), body TEXT, FULLTEXT (title,body) )engine=myisam charset utf8; INSERT INTO articles (title,body) VALUES (‘MySQL Tutorial’,’DBMS stands for DataBase …’), (‘How To Use MySQL Well’,’After you went through a …’), (‘Optimizing MySQL’,’In this tutorial we will show …’), (‘1001 MySQL Tricks’,’1. Never run mysqld as root. 2. …’), (‘MySQL vs. YourSQL’,’In the following database comparison …’), (‘MySQL Security’,’When configured properly, MySQL …’); 如何使用全文索引:错误用法:select from articles where body like ‘%mysql%’; 【不会使用到全文索引】证明:explain select from articles where body like ‘%mysql%’ 正确的用法是:select * from articles where match(title,body) against(‘database’); 【可以】 ☞ 说明: 在mysql中fulltext 索引只针对 myisam生效 mysql自己提供的fulltext针对英文生效-&gt;sphinx (coreseek) 技术处理中文 使用方法是 match(字段名..) against(‘关键字’) 全文索引一个 叫 停止词, 因为在一个文本中，创建索引是一个无穷大的数，因此，对一些常用词和字符，就不会创建，这些词，称为停止词. 1.4唯一索引①当表的某列被指定为unique约束时，这列就是一个唯一索引create table ddd(id int primary key auto_increment , name varchar(32) unique);这时, name 列就是一个唯一索引.unique字段可以为NULL,并可以有多NULL, 但是如果是具体内容，则不能重复.主键字段，不能为NULL,也不能重复.②在创建表后，再去创建唯一索引create table eee(id int primary key auto_increment, name varchar(32));create unique index 索引名 on 表名 (列表..); 2. 查询索引desc 表名 【该方法的缺点是： 不能够显示索引名.】show index(es) from 表名show keys from 表名 ### 3. 删除 alter table 表名 drop index 索引名;如果删除主键索引。alter table 表名 drop primary key [这里有一个小问题] 4. 修改先删除，再重新创建.为什么创建索引后，速度就会变快? 索引使用的注意事项索引的代价: 占用磁盘空间 对dml操作有影响，变慢 在哪些列上适合添加索引?总结: 满足以下条件的字段，才应该创建索引.a: 肯定在where条经常使用 b: 该字段的内容不是唯一的几个值(sex) c: 字段内容不是频繁变化. 使用索引的注意事项把dept表中，我增加几个部门:alter table dept add index myind (dname,loc); // dname 左边的列,loc就是右边的列说明，如果我们的表中有复合索引(索引作用在多列上)， 此时我们注意:1， 对于创建的多列索引，只要查询条件使用了最左边的列，索引一般就会被使用。explain select from dept where loc=’aaa’\G就不会使用到索引2，对于使用like的查询，查询如果是 ‘%aaa’ 不会使用到索引 ‘aaa%’ 会使用到索引。比如: explain select from dept where dname like ‘%aaa’\G不能使用索引，即，在like查询时，关键的 ‘关键字’ , 最前面，不能使用 % 或者 这样的字符.， 如果一定要前面有变化的值，则考虑使用 全文索引-&gt;sphinx. 如果条件中有or，即使其中有条件带索引也不会使用。换言之，就是要求使用的所有字段，都必须建立索引, 我们建议大家尽量避免使用or 关键字select * from dept where dname=’xxx’ or loc=’xx’ or deptno=45 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来。否则不使用索引。(添加时,字符串必须’’), 也就是，如果列是字符串类型，就一定要用 ‘’ 把他包括起来. 如果mysql估计使用全表扫描要比使用索引快，则不使用索引。explain 可以帮助我们在不真正执行某个sql语句时，就执行mysql怎样执行，这样利用我们去分析sql指令. 如何查看索引使用的情况:show status like ‘Handler_read%’;大家可以注意：handler_read_key:这个值越高越好，越高表示使用索引查询到的次数。 handler_read_rnd_next:这个值越高，说明查询低效。 sql语句的小技巧 在使用group by 分组查询是，默认分组后，还会排序，可能会降低速度.比如:在group by 后面增加 order by null 就可以防止排序. 有些情况下，可以使用连接来替代子查询。因为使用join，MySQL不需要在内存中创建临时表。select from dept, emp where dept.deptno=emp.deptno; [简单处理方式]select from dept left join emp on dept.deptno=emp.deptno; [左外连接，更ok!] 如何选择mysql的存储引擎在开发中，我们经常使用的存储引擎 myisam / innodb/ memorymyisam 存储: 如果表对事务要求不高，同时是以查询和添加为主的，我们考虑使用myisam存储引擎. ,比如 bbs 中的 发帖表，回复表.INNODB 存储: 对事务要求高，保存的数据都是重要数据，我们建议使用INNODB,比如订单表，账号表. 问 MyISAM 和 INNODB的区别 事务安全 查询和添加速度 支持全文索引 锁机制 外键 MyISAM 不支持外键， INNODB支持外键. (在PHP开发中，通常不设置外键，通常是在程序中保证数据的一致)Memory 存储，比如我们数据变化频繁，不需要入库，同时又频繁的查询和修改，我们考虑使用memory, 速度极快. 如果你的数据库的存储引擎是myisam,请一定记住要定时进行碎片整理举例说明:create table test100(id int unsigned ,name varchar(32))engine=myisam;insert into test100 values(1,’aaaaa’);insert into test100 values(2,’bbbb’);insert into test100 values(3,’ccccc’);我们应该定义对myisam进行整理optimize table test100;mysql_query(“optimize tables $表名”);]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL优化（一）]]></title>
    <url>%2FMySQL%E4%BC%98%E5%8C%96%EF%BC%88%E4%B8%80%EF%BC%89.html</url>
    <content type="text"><![CDATA[MySQL数据库的优化技术对MySQL优化时一个综合性的技术，主要包括1、表的设计合理化（符合 3NF）2、添加适当索引(index)：普通索引、主键索引、唯一索引(unique)、全文索引3、分表技术（水平分割、垂直分割）4、读写分离5、存储过程（模块化编程，可以提供读写速度）6、对MySQL配置优化（配置最大并发数my.ini，调整缓存大小）7、升级MySQL服务器硬件8、定时清楚不需要的数据，定时进行碎片处理（MyISAM） 什么样的表才是符合3NF表的范式，是首先符合1NF，才能满足2NF，进一步满足3NF1NF：即表的列具有原子性，不可再分解，即列的信息，不能分解，只要数据库是关系型数据库（mysql/oracle/db2/infomix/sysbase/sql server），就自动满足1NF 数据库的分类关系型数据库：MySQL/oracle/db2/infomix/sysbase/sql server非关系型数据库：（特点：面向对象或者集合）NoSQL数据库：MongoDB(特点是面向文档)2NF：表中的记录是唯一的，就满足2NF，通常我们设计一个主键来实现3NF：即表中不要冗余数据，就是说，表的信息，如果能够被推导出来，就不应该单独的设计一个字段来存放。 SQL语句本身的优化问题是： 如何从一个大项目中，迅速的定位执行速度慢的语句. (定位慢查询)① 首先我们了解mysql数据库的一些运行状态如何查询(比如想知道当前mysql运行的时间/一共执行了多少次select/update/delete.. / 当前连接)show status常用的:show status like ‘uptime’ ;show stauts like ‘com_select’ show stauts like ‘com_insert’ …类推 update delete☞ show [session|global] status like …. 如果你不写 [session|global] 默认是session 会话，指取出当前窗口的执行，如果你想看所有(从mysql 启动到现在，则应该 global)show status like ‘connections’;//显示慢查询次数show status like ‘slow_queries’; ② 如何去定位慢查询构建一个大表(400 万)-&gt; 存储过程构建默认情况下，mysql认为10秒才是一个慢查询. 修改mysql的慢查询.show variables like ‘long_query_time’ ; //可以显示当前慢查询时间set long_query_time=1 ;//可以修改慢查询时间 构建大表-&gt;大表中记录有要求, 记录是不同才有用，否则测试效果和真实的相差大.创建:1234567891011121314151617181920212223CREATE TABLE dept( /*部门表*/deptno MEDIUMINT UNSIGNED NOT NULL DEFAULT 0, /*编号*/dname VARCHAR(20) NOT NULL DEFAULT "", /*名称*/loc VARCHAR(13) NOT NULL DEFAULT "" /*地点*/) ENGINE=MyISAM DEFAULT CHARSET=utf8 ;CREATE TABLE emp(empno MEDIUMINT UNSIGNED NOT NULL DEFAULT 0, /*编号*/ename VARCHAR(20) NOT NULL DEFAULT "", /*名字*/job VARCHAR(9) NOT NULL DEFAULT "",/*工作*/mgr MEDIUMINT UNSIGNED NOT NULL DEFAULT 0,/*上级编号*/hiredate DATE NOT NULL,/*入职时间*/sal DECIMAL(7,2) NOT NULL,/*薪水*/comm DECIMAL(7,2) NOT NULL,/*红利*/deptno MEDIUMINT UNSIGNED NOT NULL DEFAULT 0 /*部门编号*/)ENGINE=MyISAM DEFAULT CHARSET=utf8 ;CREATE TABLE salgrade(grade MEDIUMINT UNSIGNED NOT NULL DEFAULT 0,losal DECIMAL(17,2) NOT NULL,hisal DECIMAL(17,2) NOT NULL)ENGINE=MyISAM DEFAULT CHARSET=utf8; 测试数据：12345INSERT INTO salgrade VALUES (1,700,1200);INSERT INTO salgrade VALUES (2,1201,1400);INSERT INTO salgrade VALUES (3,1401,2000);INSERT INTO salgrade VALUES (4,2001,3000);INSERT INTO salgrade VALUES (5,3001,9999); 为了存储过程能够正常执行，我们需要把命令执行结束符修改123456789101112131415delimiter $$create function rand_string(n INT) returns varchar(255) #该函数会返回一个字符串begin #chars_str定义一个变量 chars_str,类型是 varchar(100),默认值'abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ'; declare chars_str varchar(100) default 'abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ'; declare return_str varchar(255) default ''; declare i int default 0; while i &lt; n do set return_str =concat(return_str,substring(chars_str,floor(1+rand()*52),1)); set i = i + 1; end while; return return_str; end $$ 创建一个存储过程:123456789101112create procedure insert_emp(in start int(10),in max_num int(10))begindeclare i int default 0; #set autocommit =0 把autocommit设置成0 set autocommit = 0; repeat set i = i + 1; insert into emp values ((start+i) ,rand_string(6),'SALESMAN',0001,curdate(),2000,400,rand_num()); until i = max_num end repeat; commit; end $$ #调用刚刚写好的函数, 1800000条记录,从100001号开始call insert_emp(100001,4000000);③ 这时我们如果出现一条语句执行时间超过1秒中，就会统计到.④ 如果把慢查询的sql记录到我们的一个日志中在默认情况下，我们的mysql不会记录慢查询，需要在启动mysql时候，指定记录慢查询才可以bin\mysqld.exe - -safe-mode - -slow-query-log [mysql5.5 可以在my.ini指定]bin\mysqld.exe –log-slow-queries=d:/abc.log [低版本mysql5.0可以在my.ini指定] 先关闭mysql,再启动, 如果启用了慢查询日志，默认把这个文件放在my.ini 文件中记录的位置 #Path to the database rootdatadir=”C:/Documents and Settings/All Users/Application Data/MySQL/MySQL Server 5.5/Data/“ ⑤ 测试,可以看到在日志中就记录下我们的mysql慢sql语句.优化问题.通过 explain 语句可以分析，mysql如何执行你的sql语句]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何阅读源代码]]></title>
    <url>%2F%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E6%BA%90%E4%BB%A3%E7%A0%81.html</url>
    <content type="text"><![CDATA[阅读别人的代码作为开发人员是一件经常要做的事情。一个是学习新的编程语言的时候通过阅读别人的代码是一个最好的学习方法，另外是积累编程经验。如果你有机会阅读一些操作系统的代码会帮助你理解一些基本的原理。还有就是在你作为一个质量保证人员或一个小领导的时候如果你要做白盒测试的时候没有阅读代码的能力是不能完成相应的任务。最后一个就是如果你中途接手一个项目的时候或给一个项目做售后服务的时候是要有阅读代码的能力的。 收集所有可能收集的材料 阅读代码要做的第一件事情是收集所有和项目相关的资料。比如你要做一个项目的售后服务，那么你首先要搞明白项目做什么用的，那么调研文档、概要设计文档、详细设计文档、测试文档、使用手册都是你要最先搞到手的。如果你是为了学习那么尽量收集和你的学习有关的资料，比如你想学习Linux的文件系统的代码，那最好要找到linux的使用手册、以及文件系统设计的方法、数据结构的说明。(这些资料在书店里都可以找到)。 材料的种类分为几种类型基础资料。 比如你阅读turbo c2的源代码你要有turbo c2的函数手册，使用手册等专业书籍，msc 6.0或者Java 的话不但要有函数手册，还要有类库函数手册。这些资料都是你的基础资料。另外你要有一些关于uml的资料可以作为查询手册也是一个不错的选择 和程序相关的专业资料。 每一个程序都是和相关行业相关的。比如我阅读过一个关于气象分析方面的代码，因为里边用到了一个复杂的数据转换公式，所以不得不把自己的大学时候课本 找出来来复习一下高等数学的内容。如果你想阅读linux的文件管理的代码，那么找一本讲解linux文件系统的书对你的帮助会很大。 相关项目的文档资料 这一部分的资料分为两种，一个相关行业的资料，比如你要阅读一个税务系统的代码那么有一些财务/税务系统的专业资料和国家的相关的法律、法规的资料是 必不可少的。此外就是关于这个项目的需求分析报告、概要设计报告、详细设计报告，使用手册、测试报告等，尽量多收集对你以后的代码阅读是很重要的 知识准备 了解基础知识，不要上来就阅读代码，打好基础可以做到事半功倍的效果 留备份,构造可运行的环境 代码拿到手之后的第一件事情是先做备份，最好是刻在一个光盘上，在代码阅读的时候一点不动代码是很困难的一件事情，特别是你要做一些修改性或增强性维护的时候。而一旦做修改就可能发生问题，到时候要恢复是经常发生的事情，如果你不能很好的使用版本控制软件那么先留一个备份是一个最起码的要求了。 在做完备份之后最好给自己构造一个可运行的环境，当然可能会很麻烦，但可运行代码和不可运行的代码阅读起来难度会差很多的。所以多用一点时间搭建一个环境是很值得的，而且我们阅读代码主要是为了修改其中的问题或做移植操作。不能运行的代码除了可以学到一些技术以外，用处有限。 找开始的地方 做什么事情都要知道从那里开始，读程序也不例外。在C语言里,首先要找到main()函数，然后逐层去阅读，其他的程序无论是vb、delphi都要首先找到程序头，否则你是很难分析清楚程序的层次关系。 分层次阅读 在阅读代码的时候不要一头就扎下去，这样往往容易只见树木不见森林，阅读代码比较好的方法有一点象二叉树的广度优先的遍历。在程序主体一般会比较简 单，调用的函数会比较少，根据函数的名字以及层次关系一般可以确定每一个函数的大致用途，将你的理解作为注解写在这些函数的边上。当然很难一次就将全部注 解都写正确，有时候甚至可能是你猜测的结果，不过没有关系这些注解在阅读过程是不断修正的，直到你全部理解了代码为止。一般来说采用逐层阅读的方法可以是 你系统的理解保持在一个正确的方向上。避免一下子扎入到细节的问题上。在分层次阅读的时候要注意一个问题，就是将系统的函数和开发人员编写代码区分开。在 c, c++，java ,delphi中都有自己的系统函数，不要去阅读这些系统函数，除非你要学习他们的编程方法，否则只会浪费你的时间。将系统函数表示出来，注明它们的作用 即可，区分系统函数和自编函数有几个方法，一个是系统函数的编程风格一般会比较好，而自编的函数的编程风格一般比较会比较差。从变量名、行之间的缩进、注 解等方面一般可以分辨出来，另外一个是象ms c6++会在你编程的时候给你生成一大堆文件出来，其中有很多文件是你用不到了，可以根据文件名来区分一下时候是系统函数，最后如果你实在确定不了，那就 用开发系统的帮助系统去查一下函数名，对一下参数等来确定即可。 写注解 写注解是在阅读代码中最重要的一个步骤，在我们阅读的源代码一般来说是我们不熟悉的系统,阅读别人的代码一般会有几个问题，1搞明白别人的编程思想不 是一件很容易的事情，即使你知道这段程序的思路的时候也是一样。2阅读代码的时候代码量一般会比较大，如果不及时写注解往往会造成读明白了后边忘了前边的 现象。3阅读代码的时候难免会出现理解错误，如果没有及时的写注解很难及时的发现这些错误。4不写注解有时候你发生你很难确定一个函数你时候阅读过，它的功能是什么，经常会发生重复阅读、理解的现象。 好了，说一些写注解的基本方法：1猜测的去写，刚开始阅读一个代码的时候，你很难一下子就确定所有的函数的功能，不妨采用采用猜测的方法去写注解，根 据函数的名字、位置写一个大致的注解，当然一般会有错误，但你的注解实际是不但调整的，直到最后你理解了全部代码。2按功能去写，别把注解写成语法说明 书，千万别看到fopen就写打开文件，看到fread就写读数据，这样的注解一点用处都没有，而应该写在此处开发参数配置文件(**。dat)读出 系统初始化参数。。。。。，这样才是有用的注解。3在写注解的使用另外要注意的一个问题是分清楚系统自动生成的代码和用户自 己开发的代码，一般来说没有必要写系统自动生成的代码。象delphi的代码，我们往往要自己编写一些自己的代码段，还要对一些系统自动生成的代码段进行 修改，这些代码在阅读过程是要写注解的，但有一些没有修改过的自动生成的代码就没有必要写注解了。4在主要代码段要写较为详细的注解。有一些函数或类在程 序中起关键的作用，那么要写比较详细的注解。这样对你理解代码有很大的帮助。5对你理解起来比较困难的地方要写详细的注解，在这些地方往往会有一些编程的技巧。不理解这些编程技巧对你以后的理解或移植会有问题。6写中文注解。如果你的英文足够的好，不用看这条了，但很多的人英文实在不怎么样，那就写中文注解吧，我们写注解是为了加快自己的理解速度。中文在大多数的时候比英文更适应中国人。与其写一些谁也看不懂的英文注解还不如不写。 重复阅读 一次就可以将所有的代码都阅读明白的人是没有的。至少我还没有遇到过。反复的去阅读同一段代码有助于得代码的理解。一般来说，在第一次阅读代码的时候 你可以跳过很多一时不明白的代码段，只写一些简单的注解，在以后的重复阅读过程用，你对代码的理解会比上一次理解的更深刻，这样你可以修改那些注解错误的 地方和上一次没有理解的对方。一般来说，对代码阅读3，4次基本可以理解代码的含义和作用。 运行并修改代码 如果你的代码是可运行的，那么先让它运行起来，用单步跟踪的方法来阅读代码，会提高你的代码速度。代码通过看中间变量了解代码的含义,而且对 以后的修改会提供很大的帮助 用自己的代码代替原有代码，看效果，但在之前要保留源代码 600行的一个函数，阅读起来很困难，编程的人不是一个好的习惯。在阅读这个代码的时候将代码进行修改，变成了14个函数。每一个大约是40-50 行左右.]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简析String、StringBuffer与StringBuilder]]></title>
    <url>%2F%E7%AE%80%E6%9E%90String%E3%80%81StringBuffer%E4%B8%8EStringBuilder.html</url>
    <content type="text"><![CDATA[字符串类StringString是final类。Java程序中的所有字符串字面值(如”abc”)都作为此类的实例实现。字符串是常量，它们的值在创建之后不能更改，如果对已经存在的String对象进行修改，都是重新new一个对象，然后把修改后的值保存进去。字符串缓存区支持可变的字符串。因为String对象是不可变的，所以可以共享。字符串的定义很简单，直接给一个字符串类型的变量赋值即可，例如:1String Str = "abc"; 等价于:12char data[] = &#123;'a','b','c'&#125;String str = new String(data); 线程安全的可变字符串类StringBufferStringBuffer 是一个线程安全的可变字符串类，通过构造方法创建对象。类似于String的字符串，不同的是它通过某些方法调用改变该序列的长度和内容。它可将字符串缓存区安全地用于多个线程，可以在必要时对这些方法进行同步。12StringBuffer(); //构造一个空字符串的字符串缓存区StringBuffer(String str); //构造一个字符串缓存区，并将其内容初始化为指定的字符串内容 StringBuffer上的主要操作是append()和insert()方法。通过查看StringBuffer类的三段源码，我们会发现最后调用了System.arraycopy来进行来修改字符串。1234public synchronized StringBuffer append(String str) &#123; super.append(str); return this; &#125; 12345678public AbstractStringBuilder append(String str) &#123; if (str == null) str = "null"; int len = str.length(); ensureCapacityInternal(count + len); str.getChars(0, len, value, count); count += len; return this; &#125; 123456789101112public void getChars(int srcBegin, int srcEnd, char dst[], int dstBegin) &#123; if (srcBegin &lt; 0) &#123; throw new StringIndexOutOfBoundsException(srcBegin); &#125; if (srcEnd &gt; value.length) &#123; throw new StringIndexOutOfBoundsException(srcEnd); &#125; if (srcBegin &gt; srcEnd) &#123; throw new StringIndexOutOfBoundsException(srcEnd - srcBegin); &#125; System.arraycopy(value, srcBegin, dst, dstBegin, srcEnd - srcBegin); &#125; 可变字符串类StringBuilder此类提供一个与StringBuffer兼容的API，但不保证同步。该类被设计用作StringBuffer的一个简易替换，用在字符串缓冲区被单个线程使用的时候。如果可能，建议优先采用该类，因为在大多数实现中，它比StringBuffer要快。 如何选择 String是字符串常量 StringBuffer是字符串变量(线程安全) StringBuilder是字符串变量(非线程安全) 简单地说，String类型和StringBuffer类型的主要性能区别在于: String是不可改变的对象，每次对String类型进行改变的时候，其实都等同于生成了一个新的String对象，然后将引用指向该对象；而对于StringBuffer类，每次操作都是对StringBuffer对象本身进行更改。所以，如果经常改变内容的字符串最好不要用String，因为每次生成对象都会对系统性能产生影响，特别当内存中无引用对象多了以后，JVM的GC就会开始工作，那速度是一定会相当慢的。这种情况推荐使用StringBuffer，特别是字符串对象经常改变的情况下。但是某些特别情况下，String对象的字符串拼接其实是被JVM解释成了StringBuffer对象的拼接，所以这个时候String对象的速度并不会比StringBuffer对象慢，而特别是以下的字符串对象生成中，String效率是远要比StringBuffer快的:12String str = "This is only a " + "simple" + "test";StringBuffer builder = new StringBuilder("This is only a ").append("simple").append("test"); 你会发现，生成str对象的速度明显快多了。其实这是JVM的一个隐藏的实现机制，实际上:1String str = "This is only a " + "simple" + "test"; 其实就是:1String str = "This is only a simple test"; 但是要注意，如果你的字符串是来自另外的String对象，速度就没那么快了,譬如:1234String str1 = "This is only a ";String str2 = "simple";String str3 = "test";String str4 = str1+str2+str3; 为了测试这3种类型当累加不同次数字符串时的效率，我们编写一个测试类，分别按次数累加字符串:1234567891011121314151617181920212223242526272829303132333435363738394041424344public class TestString &#123; static int count = 100; //循环次数 //测试String public static void testString() &#123; long start = System.nanoTime(); String str = ""; for (int i = 0; i &lt; count; i++) &#123; str += "," + i; &#125; long end = System.nanoTime(); System.out.println("String: " + (end - start)); &#125; //测试StringBuffer public static void testStringBuffer() &#123; long start = System.nanoTime(); StringBuffer str = new StringBuffer(); for (int i = 0; i &lt; count; i++) &#123; str.append(",").append(i); &#125; long end = System.nanoTime(); System.out.println("StringBuffer: " + (end - start)); &#125; //测试StringBuilder public static void testStringBuilder() &#123; long start = System.nanoTime(); StringBuilder str = new StringBuilder(); for (int i = 0; i &lt; count; i++) &#123; str = str.append(",").append(i); &#125; long end = System.nanoTime(); System.out.println("StringBuilder: " + (end - start)); &#125; public static void main(String[] args) &#123; TestString.testString(); TestString.testStringBuffer(); TestString.testStringBuilder(); &#125;&#125; 运行该程序执行的测试时间如表所示: 毫微秒 String StringBuffer StringBuilder 1次 38292 32765 1579 10次 52108 45792 6711 100次 230143 70662 64345 1000次 11072907 308305 181193 1万次 400656874 892543 814777 10万次 溢出 4308762 4372318 100万次 溢出 100687270 49812689 String在10万次循环时就溢出了，而StringBuffer在100万次循环时间为100ms，StringBuilder的时间为49ms。显然选择优先级为: StringBuilder &gt; StringBuffer &gt; String 。因此，对于这3个类的使用，我们需要按照以下情况去选择。 如果偶尔对简单的字符串常量进行拼接，那么可以使用String，它足够简单而且轻量级。 如果需要经常进行字符串的拼接、累加操作，请使用StringBuffer或者StringBuilder。 如果是在单线程的环境中，建议使用StringBuilder,它要比StringBuffer快；如果是在多线程的环境中，建议使用StringBuffer，它是线程安全的。因此，StringBuilder实际上是我们的首选，只有在多线程时才可以考虑使用StringBuffer，只有在字符串的拼接足够简单时才使用String。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java基础</tag>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaWeb开发]]></title>
    <url>%2FJavaWeb%E5%AD%A6%E4%B9%A0.html</url>
    <content type="text"><![CDATA[1 JSP3个编译指令 : page,include,taglib2 JSP动作指令7个：jsp:forward ： 执行页面转向，将请求的处理转发到下一个页面jsp:aram : 用于传递参数，必须与其他支持参数的标签一起使用jsp:include ： 用于动态引入一个JSP页面jsp:plugin : 用于下载JavaBean 或 Applet到客户端执行jsp:useBean : 创建一个JavaBean的实例jsp:setProperty : 设置JavaBean 实例的属性值jsp:getProperty : 输出JavaBean 实例的属性值3 include指令include指令是一个动态include指令，也用于包含某个页面，它不会导入被include页面的编译指令，仅仅将被导入页面的body内容插入本页面forward动作指令和include指令的区别：执行forward时，被forward的页面将完全代替原有页面；而执行include时，被include的页面只是插入原有页面。简而言之：forward拿目标页面代替原有页面，而include则拿目标页面插入原有页面。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaWeb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaEE软件开发体系架构]]></title>
    <url>%2FJavaEE%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84.html</url>
    <content type="text"><![CDATA[两层架构传统的客户服务器系统仅只简单地基于两层体系来构建，即客户端（前台）和企业信息系统（后台），没有任何中间件，业务逻辑层与表示层或数据层混在一起。这种两层架构无论从开发、部署、扩展、维护来说，综其只有一个特点——成本高。 三层架构三层架构自上而下将系统分为表示层、逻辑层、持久层。表示层由处理用户交互的客户端组件及其容器所组成；业务逻辑层由解决业务问题的组件组成；数据层由一个或多个数据库组成，并可包含存储过程。这种三层架构，在处理客户端的请求时，使客户端不用进行复杂的数据库处理；透明地为客户端执行许多工作，如查询数据库、执行业务规则和连接现有的应用程序；并且能够帮助开发人员创建适用于企业的大型分布式应用程序。 MVC在MVC模式中，应用程序被划分为模型层（Model）、视图层（View）、控制层（Controller）三部分。MVC模型就是把一个应用程序的开发按照业务逻辑、数据、视图进行分离分层并组织代码。MVC要求把应用的模型按一定的层次规则抽取出来，将业务逻辑聚集到一个部件里面，在改进和个性化定制界面及用户交互的同时，不需要重新编写业务逻辑。模型层负责封装应用的状态，并实现功能，视图层负责将内容呈现给用户，控制层负责控制视图层发送的请求以及程序的流程。Servlet+JSP+JavaBean（MVC）这种模式比较适合开发复杂的web应用，在这种模式下，Servlet负责处理用户请求，JSP负责数据显示，JavaBean负责封装数据。 基于JavaEE架构模式下的MVC在这种架构模式下，模型层（Model）定义了数据模型和业务逻辑。为了将数据访问与业务逻辑分离，降低代码之间的耦合，提高业务精度，模型层又具体划分为了DAO层和业务层，DAO即Data Access Object，其主要职能是将访问数据库的代码封装起来，让这些代码不会在其它层出现或者暴露出来给其它层；业务层是整个系统最核心也是最具有价值的一层，该层封装应用程序的业务逻辑，处理数据，关注客户需求，在业务处理过程中会访问原始数据或产生新数据，DAO层提供的DAO类能很好地帮助业务层完成数据处理，业务层本身侧重于对客户需求的理解和业务规则的适应，总体说来，DAO层不处理业务逻辑，只为业务层提供辅助，完成获取原始数据或持久层数据等操作。JSP：JSP被用来产生Web的动态内容。这层把应用数据以网页的形式呈现给浏览器，然后数据按照在JSP中开发的预定的方式表示出来，这层也可以称之为布局层。Servlet：JSP建立在Servlet之上，Servlet是J2EE的重要组成部分。Servlet负责处理用户请求，Java Web项目的所有配置都写在了web.xml配置文件里，当项目运行的时候，web.xml会将http请求映射给对应的Servlet类。JavaBean：由一些具有私有属性的Java类组成，对外提供get和set方法。JavaBean负责数据，负责处理视图层和业务逻辑之间的通信。Service：业务处理类，对数据进行一些预处理。DAO：数据访问层，JDBC调用存储过程，从数据库（DataBase）那里获取到数据，再封装到Model实体类中去。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaEE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Annotations]]></title>
    <url>%2FAnnotations.html</url>
    <content type="text"><![CDATA[原文：You\’ve probably encountered the need to annotate elements of your Java applications by associating metadata (data that describes other data) with them. java has always provided an ad hoc annotation mechanism via the transient reserved word, which lets you annotate fields that are to be excluded during serialization. But it didn\’t offer a standard way to annotate program elements until Java 5.Java 5\’s general annotation mechanism consists of four components:1.An @interface mechanism for declaring annotation types.2.Meta-annotation types, which you can use to identify the application elements to which an annotation type applies; to identify the lifetime of an annotation (an instance of an annotation type); and more.3.Support for annotation processing via an extension to the Java Reflection API, which you can use to discover a program\’s runtime annotations, and the introduction of a generalized tool for processing annotations.4.Standard annotation types.I\’ll explain how to use these components and point out some of the challenges of annotations in the examples that follow.译文：你可能曾遇到过这种需求：通过关联元数据（描述其他数据的数据）来注解你的java应用程序元素。一直以来，java通过transient关键字来提供一种即时注解机制，它可以让你标注一个成员变量在序列化过程中被排除。但是，直到java 5.0版本才提供一种标准的方式来注解程序。Java 5.0常规注解机制包括四部分：1． 声明注解类型@interface机制2． 元注解类型，你可以用来识别注解类型的程序元素应用以及注解的声明周期（注解类型的一个实例）等；3． 通过java反射API的扩展来支持注解处理，你可以用来发现程序运行时的注解，并引入一个泛型注解处理工具。4． 标准的注解类型。接下来，我将通过下面的例子解释如何运用这些机制，并对一些比较有挑战性的注解部分进行标明。]]></content>
      <categories>
        <category>译文</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IE8开发工具“无法附加进程”]]></title>
    <url>%2FIE8%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E2%80%9C%E6%97%A0%E6%B3%95%E9%99%84%E5%8A%A0%E8%BF%9B%E7%A8%8B%E2%80%9D.html</url>
    <content type="text"><![CDATA[问题描述使用IE8开发工具调试时，遇到“无法附加进程，进程可能附加了另一个调试程序”的解决方案本人在遇到此问题时，百度网上的解决方案一般都是重置IE8设置（工具-&gt;Internet选项-&gt;高级-&gt;重置），卸载重装IE8，或者选择别的浏览器进行调试，但是本人的情况是，项目在其他浏览器均可以正常显示，只有IE8以下版本不可以，后面发现是js兼容性问题。我的想法是，通过IE8自带调试工具调试，看看哪里不兼容。后面折腾了好久发现，360浏览器由于用的是IE内核，其开发工具调用的也是IE的开发工具。如果你的系统安装IE9以上版本，可以选择调用IE9以上版本进行开发调试。]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>IE8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis上手入门实例]]></title>
    <url>%2FMyBatis%E4%B8%8A%E6%89%8B%E5%85%A5%E9%97%A8%E5%AE%9E%E4%BE%8B.html</url>
    <content type="text"><![CDATA[MyBatis下载、配置及测试点击下载MyBatis在线中文文档下载之后打开，如图：第一个使我们需要用到的包，pdf文档是MyBatis英文手册，后面两个分别是javadoc文档和源码。在这里我们还需要导入MySQL数据库驱动jar包。官方驱动jar包下载地址这些准备以后，我们开始配置。 打开MyEclipse，导入jar包。在src下新建一个包，并新建配置文件conf.xml。如图： 新建测试数据库mybatis，并新建两条数据。 在Mybatis中定义Mapper信息有两种方式，一种是利用xml写一个对应的包含Mapper信息的配置文件；另一种就是定义一个Mapper接口，然后定义一些相应的操作方法，再辅以相应的操作注解。conf.xml代码，具体参数的解释请看文档。 1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN""http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;transactionManager type="JDBC" /&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="com.mysql.jdbc.Driver" /&gt; &lt;property name="url" value="jdbc:mysql://localhost:3306/mybatis" /&gt; &lt;property name="username" value="root" /&gt; &lt;property name="password" value="root" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource="com/MyBatis/test1/userMapper.xml"/&gt; &lt;mapper class="com.MyBatis.test1.UserMapper1"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 4.新建一个实体类User。 123456789101112131415161718192021222324252627282930313233343536373839package com.MyBatis.test1;public class User &#123; private int id; private String name; private int age; public User() &#123; super(); &#125; public User(int id, String name, int age) &#123; super(); this.id = id; this.name = name; this.age = age; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return "User [id=" + id + ", name=" + name + ", age=" + age + "]"; &#125;&#125; 5.新建userMapper.xml文件，文件参数解释请看文档，代码如下：1234567891011121314151617181920212223&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="com.MyBatis.test1.userMapper"&gt; &lt;select id="getUser" parameterType="int" resultType="com.MyBatis.test1.User"&gt; select * from users where id=#&#123;id&#125; &lt;/select&gt; &lt;select id = "insertUser" parameterType="com.MyBatis.test1.User"&gt; insert into users(name,age) values(#&#123;name&#125;,#&#123;age&#125;); &lt;/select&gt; &lt;delete id="deleteUser" parameterType="int"&gt; delete from users where id=#&#123;id&#125;; &lt;/delete&gt; &lt;update id="updateUser" parameterType="com.MyBatis.test1.User"&gt; update users set name =#&#123;name&#125;,age=#&#123;age&#125; where id = #&#123;id&#125;; &lt;/update&gt; &lt;select id="selectAllUsers" resultType="com.MyBatis.test1.User"&gt; select * from users; &lt;/select&gt;&lt;/mapper&gt; 6.新建 测试类test，此处采用JUnit4测试，然后逐个方法进行测试。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.MyBatis.test1;import java.io.InputStream;import java.util.List;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Test;public class Test1 &#123; String resource = "conf.xml"; InputStream is = Test1.class.getClassLoader().getResourceAsStream(resource); SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(is); SqlSession session = factory.openSession(true); UserMapper1 mapper = session.getMapper(UserMapper1.class); @Test public void insert() &#123; User user = new User(); user.setName("dudefu"); user.setAge(20); String statementInsert = "com.MyBatis.test1.userMapper.insertUser"; session.insert(statementInsert, user); &#125; public void update() &#123; User user = new User(); user.setName("ddf"); user.setAge(20); user.setId(6); String statement = "com.MyBatis.test1.userMapper.updateUser"; session.update(statement, user); &#125; public void selectAllUsers() &#123; String statementSelectAll = "com.MyBatis.test1.userMapper.selectAllUsers"; List&lt;User&gt; selectAllUser = session.selectList(statementSelectAll); System.out.println(selectAllUser); &#125; public void delete() &#123; String statement = "com.MyBatis.test1.userMapper.deleteUser"; int a = session.delete(statement, 10); System.out.println(a); &#125;&#125; 7.采用注解的方式。首先新建接口类userMapper1，此处注意，如果接口类和userMpper.xml在同一个包内，则接口类的名字不能和userMapper.xml的名字相同，否则会报错java.lang.NoClassDefFoundError: com/MyBatis/test1/userMapper (wrong name: com/MyBatis/test1/UserMapper具体原因，启动程序后，程序首先会加载所有文件，这时userMpper.class和userMpper.xml会发生冲突。所以如果在同一个包内需改名，此处改名为userMapper1.class。userMapper1.class代码如下：12345678910111213141516171819202122232425package com.MyBatis.test1;import java.util.List;import org.apache.ibatis.annotations.Delete;import org.apache.ibatis.annotations.Insert;import org.apache.ibatis.annotations.Select;import org.apache.ibatis.annotations.Update;public interface UserMapper1 &#123; @Insert("insert into users(name,age) values(#&#123;name&#125;,#&#123;age&#125;)") public int insertUser(User user); @Delete("delete from users where id=#&#123;id&#125;") public int deleteUserById(int id); @Update("update users set name =#&#123;name&#125;,age=#&#123;age&#125; where id = #&#123;id&#125;") public int updateUser(User user); @Select("select * from users where id=#&#123;id&#125;") public User getUserById(int id); @Select("select * from users") public List&lt;User&gt; getAllUser();&#125; 8.新建测试类test2。代码如下：1234567891011121314151617181920212223242526272829303132333435363738package com.MyBatis.test2;import java.io.InputStream;import java.util.List;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Test;public class Test2 &#123; String resource = "conf.xml"; InputStream is = Test1.class.getClassLoader().getResourceAsStream(resource); SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(is); SqlSession session = factory.openSession(true); UserMapper1 mapper = session.getMapper(UserMapper1.class); public void testInsert()&#123; int insert = mapper.insertUser(new User(-1,"dudefu",23)); System.out.println(insert); &#125; public void testUpdate()&#123; int update = mapper.updateUser(new User(11,"ddf",34)); System.out.println(update); &#125; public void testdelete()&#123; int delete = mapper.deleteUserById(11); System.out.println(delete); &#125; @Test public void selectAllUser()&#123; List&lt;User&gt; selectAllUsers = mapper.getAllUser(); System.out.println(selectAllUsers); &#125;&#125; 按照以上步骤进行操作，测试应该是不报错的。如果出错了，一般出错在 conf.xml中的路径配置，如下段代码； 1234&lt;mappers&gt; &lt;mapper resource=&quot;com/MyBatis/test1/userMapper.xml&quot;/&gt; &lt;mapper class=&quot;com.MyBatis.test1.UserMapper1&quot;/&gt; &lt;/mappers&gt; 和userMapper.xml中。]]></content>
      <categories>
        <category>框架</category>
      </categories>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云Maven仓库地址]]></title>
    <url>%2F%E9%98%BF%E9%87%8C%E4%BA%91Maven%E4%BB%93%E5%BA%93%E5%9C%B0%E5%9D%80.html</url>
    <content type="text"><![CDATA[阿里云Maven仓库地址http://maven.aliyun.com/nexus/#view-repositories;public~browsestorage在maven的settings.xml 文件里配置mirrors的子节点，添加如下mirror123456&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt;]]></content>
      <categories>
        <category>有料</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deepin下搭建基于github和hexo的个人博客]]></title>
    <url>%2Fdeepin%E4%B8%8B%E6%90%AD%E5%BB%BA%E5%9F%BA%E4%BA%8Egithub%E5%92%8Chexo%E7%9A%84%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2.html</url>
    <content type="text"><![CDATA[系统：Linux Deepin 15.4 x64搭建步骤：1、 安装git1$ sudo apt-get install git 查看git版本1$ git version 2、 安装Node.js及npma. 可以直接命令安装,但是命令安装的不是最新版本。12$ sudo apt-get install nodejs$ sudo apt-get install npm b. 本博客采用第二种方法，首先官网下载最新版，然后解压。将node,npm命令设置全局命令：12$ sudo ln -s /home/dudefu/Documents/node-v8.6.0-linux-x64/bin/node /usr/local/bin/node$ sudo ln -s /home/dudefu/Documents/node-v8.6.0-linux-x64/bin/npm /usr/local/bin/npm 查看版本：12$ node -v$ npm -v 3、 安装hexo1$ npm install -g hexo-cli hexo-cli安装路径/home/dudefu/Documents/node-v8.6.0-linux-x64/lib/node_modules/hexo-cli，此时输入命令hexo会提示“未找到命令”，此时要将hexo-cli/bin/文件夹下的hexo命令设置为全局：12$ sudo ln -s /home/dudefu/Documents/node-v8.6.0-linux-x64/lib/node_modules/hexo-cli/bin/$ hexo /usr/local/bin/hexo 再输入hexo命令可以正常显示。创建一个空文件夹，此处名为hexo：12345$ mkdir hexo$ cd hexo$ hexo init .$ npm install $ hexo server -p 5000 到此，hexo安装完毕。浏览器输入本地，前面配置均正确的情况下，正常显示博客首页。4、 hexo配置主要分为两块站点配置和主题配置。此处先下载NexT主题：1$ git clone https://github.com/iissnan/hexo-theme-next themes/next 接下来的详细配置就不细说了，直接看hexo文档，NexT使用文档，主题配置遇到难点的可以访问github：，所有的问题基本可以解决。 我的博客]]></content>
      <categories>
        <category>有料</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>