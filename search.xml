<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[一次Java线程池误用引发的血案和总结]]></title>
    <url>%2F%E4%B8%80%E6%AC%A1Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%AF%AF%E7%94%A8%E5%BC%95%E5%8F%91%E7%9A%84%E8%A1%80%E6%A1%88%E5%92%8C%E6%80%BB%E7%BB%93.html</url>
    <content type="text"><![CDATA[这是一个十分严重的问题自从最近的某年某月某天起，线上服务开始变得不那么稳定。在高峰期，时常有几台机器的内存持续飙升，并且无法回收，导致服务不可用。 例如GC时间采样曲线： 和内存使用曲线： 图中所示，18:50-19:00的阶段，已经处于服务不可用的状态了。上游服务的超时异常会增加，该台机器会触发熔断。熔断触发后，改台机器的流量会打到其他机器，其他机器发生类似的情况的可能性会提高，极端情况会引起所有服务宕机，曲线掉底。 因为线上内存过大，如果采用 jmap dump的方式，这个任务可能需要很久才可以执行完，同时把这么大的文件存放起来导入工具也是一件很难的事情。再看JVM启动参数，也很久没有变更过 Xms, Xmx, -XX:NewRatio, -XX:SurvivorRatio, 虽然没有仔细分析程序使用内存情况，但看起来也无大碍。 于是开始找代码，某年某天某月～ 嗯，注意到一段这样的代码提交： 12345678910111213private static ExecutorService executor = Executors.newFixedThreadPool(15);public static void push2Kafka(Object msg) &#123; executor.execute(new WriteTask(msg, false)); &#125; 相关代码的完整功能是，每次线上调用，都会把计算结果的日志打到 Kafka，Kafka消费方再继续后续的逻辑。内存被耗尽可能有一个原因是，因为使用了 newFixedThreadPool 线程池，而它的工作机制是，固定了N个线程，而提交给线程池的任务队列是不限制大小的，如果Kafka发消息被阻塞或者变慢，那么显然队列里面的内容会越来越多，也就会导致这样的问题。 为了验证这个想法，做了个小实验，把 newFixedThreadPool 线程池的线程个数调小一点，例如 1。果然压测了一下，很快就复现了内存耗尽，服务不可用的悲剧。 最后的修复策略是使用了自定义的线程池参数，而非 Executors 默认实现解决了问题。下面就把线程池相关的原理和参数总结一下，避免未来踩坑。 1. Java线程池虽然Java线程池理论，以及构造线程池的各种参数，以及 Executors 提供的默认实现之前研读过，不过线上还没有发生过线程池误用引发的事故，所以有必要把这些参数再仔细琢磨一遍。 优先补充一些线程池的工作理论，有助于展开下面的内容。线程池顾名思义，就是由很多线程构成的池子，来一个任务，就从池子中取一个线程，处理这个任务。这个理解是我在第一次接触到这个概念时候的理解，虽然整体基本切入到核心，但是实际上会比这个复杂。例如线程池肯定不会无限扩大的，否则资源会耗尽；当线程数到达一个阶段，提交的任务会被暂时存储在一个队列中，如果队列内容可以不断扩大，极端下也会耗尽资源，那选择什么类型的队列，当队列满如何处理任务，都有涉及很多内容。线程池总体的工作过程如下图： 线程池内的线程数的大小相关的概念有两个，一个是核心池大小，还有最大池大小。如果当前的线程个数比核心池个数小，当任务到来，会优先创建一个新的线程并执行任务。当已经到达核心池大小，则把任务放入队列，为了资源不被耗尽，队列的最大容量可能也是有上限的，如果达到队列上限则考虑继续创建新线程执行任务，如果此刻线程的个数已经到达最大池上限，则考虑把任务丢弃。 在 java.util.concurrent 包中，提供了 ThreadPoolExecutor 的实现。 1234567891011121314151617181920212223242526272829public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123;&#125; 既然有了刚刚对线程池工作原理对概述，这些参数就很容易理解了： corePoolSize- 核心池大小，既然如前原理部分所述。需要注意的是在初创建线程池时线程不会立即启动，直到有任务提交才开始启动线程并逐渐时线程数目达到corePoolSize。若想一开始就创建所有核心线程需调用prestartAllCoreThreads方法。 maximumPoolSize-池中允许的最大线程数。需要注意的是当核心线程满且阻塞队列也满时才会判断当前线程数是否小于最大线程数，并决定是否创建新线程。 keepAliveTime - 当线程数大于核心时，多于的空闲线程最多存活时间 unit - keepAliveTime 参数的时间单位。 workQueue - 当线程数目超过核心线程数时用于保存任务的队列。主要有3种类型的BlockingQueue可供选择：无界队列，有界队列和同步移交。将在下文中详细阐述。从参数中可以看到，此队列仅保存实现Runnable接口的任务。 别看这个参数位置很靠后，但是真的很重要，因为楼主的坑就因这个参数而起，这些细节有必要仔细了解清楚。 threadFactory - 执行程序创建新线程时使用的工厂。 handler - 阻塞队列已满且线程数达到最大值时所采取的饱和策略。java默认提供了4种饱和策略的实现方式：中止、抛弃、抛弃最旧的、调用者运行。将在下文中详细阐述。 2. 可选择的阻塞队列BlockingQueue详解在重复一下新任务进入时线程池的执行策略：如果运行的线程少于corePoolSize，则 Executor始终首选添加新的线程，而不进行排队。（如果当前运行的线程小于corePoolSize，则任务根本不会存入queue中，而是直接运行）如果运行的线程大于等于 corePoolSize，则 Executor始终首选将请求加入队列，而不添加新的线程。如果无法将请求加入队列，则创建新的线程，除非创建此线程超出 maximumPoolSize，在这种情况下，任务将被拒绝。主要有3种类型的BlockingQueue： 无界队列 队列大小无限制，常用的为无界的LinkedBlockingQueue，使用该队列做为阻塞队列时要尤其当心，当任务耗时较长时可能会导致大量新任务在队列中堆积最终导致OOM。阅读代码发现，Executors.newFixedThreadPool 采用就是 LinkedBlockingQueue，而楼主踩到的就是这个坑，当QPS很高，发送数据很大，大量的任务被添加到这个无界LinkedBlockingQueue 中，导致cpu和内存飙升服务器挂掉。 有界队列 常用的有两类，一类是遵循FIFO原则的队列如ArrayBlockingQueue与有界的LinkedBlockingQueue，另一类是优先级队列如PriorityBlockingQueue。PriorityBlockingQueue中的优先级由任务的Comparator决定。使用有界队列时队列大小需和线程池大小互相配合，线程池较小有界队列较大时可减少内存消耗，降低cpu使用率和上下文切换，但是可能会限制系统吞吐量。 在我们的修复方案中，选择的就是这个类型的队列，虽然会有部分任务被丢失，但是我们线上是排序日志搜集任务，所以对部分对丢失是可以容忍的。 同步移交队列 如果不希望任务在队列中等待而是希望将任务直接移交给工作线程，可使用SynchronousQueue作为等待队列。SynchronousQueue不是一个真正的队列，而是一种线程之间移交的机制。要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接收这个元素。只有在使用无界线程池或者有饱和策略时才建议使用该队列。 3. 可选择的饱和策略RejectedExecutionHandler详解JDK主要提供了4种饱和策略供选择。4种策略都做为静态内部类在ThreadPoolExcutor中进行实现。 3.1 AbortPolicy中止策略该策略是默认饱和策略。 1234567891011121314151617public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; throw new RejectedExecutionException(&quot;Task &quot; + r.toString() + &quot; rejected from &quot; + e.toString()); &#125; 使用该策略时在饱和时会抛出RejectedExecutionException（继承自RuntimeException），调用者可捕获该异常自行处理。 3.2 DiscardPolicy抛弃策略12345public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123;&#125; 如代码所示，不做任何处理直接抛弃任务 3.3 DiscardOldestPolicy抛弃旧任务策略123456789101112131415161718192021public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; e.getQueue().poll(); e.execute(r); &#125;&#125; 如代码，先将阻塞队列中的头元素出队抛弃，再尝试提交任务。如果此时阻塞队列使用PriorityBlockingQueue优先级队列，将会导致优先级最高的任务被抛弃，因此不建议将该种策略配合优先级队列使用。 3.4 CallerRunsPolicy调用者运行1234567891011121314151617public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; r.run(); &#125;&#125; 既不抛弃任务也不抛出异常，直接运行任务的run方法，换言之将任务回退给调用者来直接运行。使用该策略时线程池饱和后将由调用线程池的主线程自己来执行任务，因此在执行任务的这段时间里主线程无法再提交新任务，从而使线程池中工作线程有时间将正在处理的任务处理完成。 4. Java提供的四种常用线程池解析既然楼主踩坑就是使用了 JDK 的默认实现，那么再来看看这些默认实现到底干了什么，封装了哪些参数。简而言之 Executors 工厂方法Executors.newCachedThreadPool() 提供了无界线程池，可以进行自动线程回收；Executors.newFixedThreadPool(int) 提供了固定大小线程池，内部使用无界队列；Executors.newSingleThreadExecutor() 提供了单个后台线程。 详细介绍一下上述四种线程池。 4.1 newCachedThreadPool1234567891011121314151617public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 在newCachedThreadPool中如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。初看该构造函数时我有这样的疑惑：核心线程池为0，那按照前面所讲的线程池策略新任务来临时无法进入核心线程池，只能进入 SynchronousQueue中进行等待，而SynchronousQueue的大小为1，那岂不是第一个任务到达时只能等待在队列中，直到第二个任务到达发现无法进入队列才能创建第一个线程？这个问题的答案在上面讲SynchronousQueue时其实已经给出了，要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接收这个元素。因此即便SynchronousQueue一开始为空且大小为1，第一个任务也无法放入其中，因为没有线程在等待从SynchronousQueue中取走元素。因此第一个任务到达时便会创建一个新线程执行该任务。 4.2 newFixedThreadPool1234567891011121314151617public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 看代码一目了然了，线程数量固定，使用无限大的队列。再次强调，楼主就是踩的这个无限大队列的坑。 4.3 newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 123456789public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125; 在来看看ScheduledThreadPoolExecutor（）的构造函数 12345678910111213public ScheduledThreadPoolExecutor(int corePoolSize) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); &#125; ScheduledThreadPoolExecutor的父类即ThreadPoolExecutor，因此这里各参数含义和上面一样。值得关心的是DelayedWorkQueue这个阻塞对列，在上面没有介绍，它作为静态内部类就在ScheduledThreadPoolExecutor中进行了实现。简单的说，DelayedWorkQueue是一个无界队列，它能按一定的顺序对工作队列中的元素进行排列。 4.4 newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 12345678910111213public static ScheduledExecutorService newSingleThreadScheduledExecutor() &#123; return new DelegatedScheduledExecutorService (new ScheduledThreadPoolExecutor(1)); &#125; 首先new了一个线程数目为 1 的ScheduledThreadPoolExecutor，再把该对象传入DelegatedScheduledExecutorService中，看看DelegatedScheduledExecutorService的实现代码： 12345678910111213DelegatedScheduledExecutorService(ScheduledExecutorService executor) &#123; super(executor); e = executor;&#125; 在看看它的父类 123456789DelegatedExecutorService(ExecutorService executor) &#123; e = executor; &#125; 其实就是使用装饰模式增强了ScheduledExecutorService（1）的功能，不仅确保只有一个线程顺序执行任务，也保证线程意外终止后会重新创建一个线程继续执行任务。 结束语虽然之前学习了不少相关知识，但是只有在实践中踩坑才能印象深刻吧 原文地址：https://zhuanlan.zhihu.com/p/32867181?utm_source=wechat_session&amp;utm_medium=social]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程和线程池]]></title>
    <url>%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%B1%A0.html</url>
    <content type="text"><![CDATA[并发编程和线程池 练气期（并发编程基础）练气期一层（this）synchronized(this)和synchronized方法都是锁当前对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Test_01 &#123; private int count = 0; // 存在堆中 private Object o = new Object(); // 存在堆中 // 多个线程都能找到的，都能访问的对象叫临界资源对象 public void testSync1() &#123; synchronized (o) &#123; System.out.println(Thread.currentThread().getName() + " count = " + count++); &#125; &#125; public void testSync2() &#123; synchronized (this) &#123; System.out.println(Thread.currentThread().getName() + " count = " + count++); &#125; &#125; public synchronized void testSync3() &#123; System.out.println(Thread.currentThread().getName() + " count = " + count++); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; final Test_01 t = new Test_01(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.testSync3(); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.testSync3(); &#125; &#125;).start(); &#125;&#125; 练气期二层（static）静态同步方法，锁的是当前类型的类对象。 12345678910111213141516171819202122public class Test_02 &#123; private static int staticCount = 0; public static synchronized void testSync4()&#123; System.out.println(Thread.currentThread().getName() + " staticCount = " + staticCount++); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public static void testSync5()&#123; synchronized(Test_02.class)&#123; System.out.println(Thread.currentThread().getName() + " staticCount = " + staticCount++); &#125; &#125;&#125; 练气期三层（原子性）加锁的目的就是为了保证操作的原子性 123456789101112131415161718public class Test_03 implements Runnable &#123; private int count = 0; @Override public /*synchronized*/ void run() &#123; System.out.println(Thread.currentThread().getName() + " count = " + count++); &#125; public static void main(String[] args) &#123; Test_03 t = new Test_03(); for (int i = 0; i &lt; 5; i++) &#123; new Thread(t, "Thread - " + i).start(); &#125; &#125;&#125; 练气期四层（同步与非同步方法间调用）同步方法只影响锁定同一个锁对象的同步方法。不影响其他线程调用非同步方法，或调用其他锁资源的同步方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class Test_04 &#123; Object o = new Object(); public synchronized void m1() &#123; // 重量级的访问操作。 System.out.println("public synchronized void m1() start"); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("public synchronized void m1() end"); &#125; public void m3() &#123; synchronized (o) &#123; System.out.println("public void m3() start"); try &#123; Thread.sleep(1500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("public void m3() end"); &#125; &#125; public void m2() &#123; System.out.println("public void m2() start"); try &#123; Thread.sleep(1500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("public void m2() end"); &#125; public static class MyThread01 implements Runnable &#123; public MyThread01(int i, Test_04 t) &#123; this.i = i; this.t = t; &#125; int i; Test_04 t; public void run() &#123; if (i == 0) &#123; t.m1(); &#125; else if (i &gt; 0) &#123; t.m2(); &#125; else &#123; t.m3(); &#125; &#125; &#125; public static void main(String[] args) &#123; Test_04 t = new Test_04(); new Thread(new Test_04.MyThread01(0, t)).start(); new Thread(new Test_04.MyThread01(1, t)).start(); new Thread(new Test_04.MyThread01(-1, t)).start(); &#125;&#125; 练气期五层（存在原子性问题）同步方法只能保证当前方法的原子性，不能保证多个业务方法之间的互相访问的原子性。一般来说，商业项目中，不考虑业务逻辑上的脏读问题。如你买东西下订单后，提示订单已下，查询时候，可能看不到。一般我们只关注数据脏读。但是在金融领域，保险领域严格要求。 1234567891011121314151617181920212223242526272829303132333435public class Test_05 &#123; private double d = 0.0; public synchronized void m1(double d)&#123; try &#123; // 相当于复杂的业务逻辑代码。 TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; this.d = d; &#125; public double m2()&#123; return this.d; &#125; public static void main(String[] args) &#123; final Test_05 t = new Test_05(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m1(100); &#125; &#125;).start(); System.out.println(t.m2()); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(t.m2()); &#125;&#125; 练气期六层（锁可重入）同一个线程，多次调用同步代码，锁定同一个锁对象，可重入。 1234567891011121314151617181920212223242526272829public class Test_06 &#123; synchronized void m1()&#123; // 锁this System.out.println("m1 start"); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; m2(); System.out.println("m1 end"); &#125; synchronized void m2()&#123; // 锁this System.out.println("m2 start"); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("m2 end"); &#125; public static void main(String[] args) &#123; new Test_06().m1(); &#125;&#125; 练气期七层（调用父类的同步方法）子类同步方法覆盖父类同步方法，可以指定调用父类的同步方法， 相当于锁的重入。父类的方法 &lt;&lt;==&gt;&gt; 本类的方法 12345678910111213141516171819202122232425public class Test_07 &#123; synchronized void m() &#123; System.out.println("Super Class m start"); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("Super Class m end"); &#125; public static void main(String[] args) &#123; new Sub_Test_07().m(); &#125;&#125;class Sub_Test_07 extends Test_07 &#123; synchronized void m() &#123; System.out.println("Sub Class m start"); super.m(); System.out.println("Sub Class m end"); &#125;&#125; 练气期八层（锁与异常）当同步方法中发生异常的时候，自动释放锁资源，不会影响其他线程的执行。我们需要注意的是在同步业务逻辑中，如果发生异常如何处理——— try/catch 。如存钱时，发送网络中断，查询的时候查到多少钱，存的钱要返还 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Test_08 &#123; int i = 0; synchronized void m() &#123; System.out.println(Thread.currentThread().getName() + " - start"); while (true) &#123; i++; System.out.println(Thread.currentThread().getName() + " - " + i); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; /*if(i == 5)&#123; i = 1/0; &#125;*/ //模拟存钱，中断处理 if (i == 5) &#123; try &#123; i = 1 / 0; &#125; catch (Exception e) &#123; i = 0; &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; final Test_08 t = new Test_08(); // 锁的是当前对象 new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;, "t1").start(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;, "t2").start(); &#125;&#125; 练气期九层（volatile）cpu默认查询cpu的高速缓存区域，CPU中每一个核都有自己的缓存，当cpu有中断的时候，他可能清空高速缓存区域数据，重新从内存中读取数据。volatile改变内存中的数据，通知底层OS系统，每次使用b的时候，最好看下内存数据是否发生变动。即volatile做的是一个通知OS系统的作。 123456789101112131415161718192021222324252627282930public class Test_09 &#123; volatile boolean b = true; //线程可见性问题 void m()&#123; System.out.println("start"); while(b)&#123;&#125; System.out.println("end"); &#125; public static void main(String[] args) &#123; final Test_09 t = new Test_09(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; t.b = false; //堆空间的对象，线程共享 &#125;&#125; volatile的非原子性问题，只能保证可见性，不能保证原子性。 那什么时候使用volatile？棋牌室的人数，新增的人有一个线程去+1。这是可以使用volatile join()多个线程在运行结束时，我把多个线程再main线程的位置连在一起，当其他线程都结束，即保证在所有线程循环执行+1后，再执行main线程打印。 1234567891011121314151617181920212223242526272829303132333435public class Test_10 &#123; volatile int count = 0; /*synchronized*/ void m() &#123; //保证原子性的解决方法是使用synchronized或者是Atomic for (int i = 0; i &lt; 10000; i++) &#123; count++; &#125; &#125; public static void main(String[] args) &#123; final Test_10 t = new Test_10(); List&lt;Thread&gt; threads = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; threads.add(new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;)); &#125; for (Thread thread : threads) &#123; thread.start(); &#125; for (Thread thread : threads) &#123; try &#123; thread.join(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; System.out.println(t.count); //理论上是10w。实际少于这个数 &#125;&#125; 练气期十层（AtomicXxx）什么时候有原子性，没有可见性？ 答：所谓原子性是指多个线程访问一个变量时，其结果必须保证正确性。所谓可见性是指多线程间可以看最终结果的变量 1234567891011121314151617181920212223242526272829303132333435public class Test_11 &#123; AtomicInteger count = new AtomicInteger(0); void m() &#123; for (int i = 0; i &lt; 10000; i++) &#123; /*if(count.get() &lt; 1000)*/ count.incrementAndGet(); //相当于++count,count.getAndAccumulate()是count++; &#125; &#125; public static void main(String[] args) &#123; final Test_11 t = new Test_11(); List&lt;Thread&gt; threads = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; threads.add(new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;)); &#125; for (Thread thread : threads) &#123; thread.start(); &#125; for (Thread thread : threads) &#123; try &#123; thread.join(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; System.out.println(t.count.intValue()); &#125;&#125; 练气期十一层（锁对象变更） 同步代码一旦加锁后，那么会有一个临时的锁引用指向锁对象，和真实的引用无直接关联。在锁未释放之前，修改锁引用，不会影响同步代码的执行。 我们打印的是Test_13中的o。不是锁引用的_O;下面synchronized锁的是两个对象。打印的是同一个对象。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Test_13 &#123; Object o = new Object(); //变量引用 int i = 0; int a() &#123; try &#123; /* * return i -&gt; * int _returnValue = i; // 0; * return _returnValue; */ return i; &#125; finally &#123; i = 10; &#125; &#125; void m() &#123; System.out.println(Thread.currentThread().getName() + " start"); synchronized (o) &#123; //计算变量引用与变量引用不是一回事 while (true) &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " - " + o); &#125; &#125; &#125; public static void main(String[] args) &#123; final Test_13 t = new Test_13(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;, "thread1").start(); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; Thread thread2 = new Thread(new Runnable() &#123; @Override public void run() &#123; t.m(); &#125; &#125;, "thread2"); t.o = new Object(); thread2.start(); //更改临界资源对象 System.out.println(t.i); System.out.println(t.a()); System.out.println(t.i); &#125;&#125; 练气期十二层（CountDownLatch） 不会进入等待队列，可以和锁混合使用，或替代锁的功能。 一次性在门上挂多个锁。 作用如：init对象的时候有一个前后顺序的问题。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Test_15 &#123; CountDownLatch latch = new CountDownLatch(5); void m1() &#123; try &#123; latch.await();// 等待门闩开放。 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("m1() method"); &#125; void m2() &#123; for (int i = 0; i &lt; 10; i++) &#123; if (latch.getCount() != 0) &#123; System.out.println("latch count : " + latch.getCount()); latch.countDown(); // 减门闩上的锁。 &#125; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; System.out.println("m2() method : " + i); &#125; &#125; public static void main(String[] args) &#123; final Test_15 t = new Test_15(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m1(); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m2(); &#125; &#125;).start(); &#125;&#125; 练气期大圆满123456789101112131415161718192021222324252627282930313233343536373839404142public class Test_14 &#123; String s1 = "hello"; String s2 = new String("hello"); // new关键字，一定是在堆中创建一个新的对象。 Integer i1 = 1; // i1与i2是同一个变量，在常量池中，new是放在堆内存 Integer i2 = 1; void m1() &#123; synchronized (i1) &#123; //s1与s2 System.out.println("m1()"); while (true) &#123; &#125; &#125; &#125; void m2() &#123; synchronized (i2) &#123; System.out.println("m2()"); while (true) &#123; &#125; &#125; &#125; public static void main(String[] args) &#123; final Test_14 t = new Test_14(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m1(); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m2(); &#125; &#125;).start(); &#125;&#125; 自定义容器，提供新增元素（add）和获取元素数量（size）方法。启动两个线程。线程1向容器中新增10个数据。线程2监听容器元素数量，当容器元素数量为5时，线程2输出信息并终止。 使用volatile 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Test_01 &#123; public static void main(String[] args) &#123; final Test_01_Container t = new Test_01_Container(); new Thread(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; System.out.println("add Object to Container " + i); t.add(new Object()); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; while (true) &#123; if (t.size() == 5) &#123; System.out.println("size = 5"); break; &#125; &#125; &#125; &#125;).start(); &#125;&#125;class Test_01_Container &#123; volatile List&lt;Object&gt; container = new ArrayList&lt;&gt;(); public void add(Object o) &#123; this.container.add(o); &#125; public int size() &#123; return this.container.size(); &#125;&#125; 使用synchronized和wait(), 调用wait()将释放锁，并且进入等待队列中，生产者与消费者模型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Test_02 &#123; public static void main(String[] args) &#123; final Test_02_Container t = new Test_02_Container(); final Object lock = new Object(); new Thread(new Runnable()&#123; @Override public void run() &#123; synchronized (lock) &#123; if(t.size() != 5)&#123; try &#123; lock.wait(); // 线程进入等待队列。 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println("size = 5"); lock.notifyAll(); // 唤醒其他等待线程 &#125; &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; synchronized (lock) &#123; for(int i = 0; i &lt; 10; i++)&#123; System.out.println("add Object to Container " + i); t.add(new Object()); if(t.size() == 5)&#123; lock.notifyAll(); try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125;).start(); &#125;&#125;class Test_02_Container&#123; List&lt;Object&gt; container = new ArrayList&lt;&gt;(); public void add(Object o)&#123; this.container.add(o); &#125; public int size()&#123; return this.container.size(); &#125;&#125; 使用门闩避免进入等待队列，效率更高。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class Test_03 &#123; public static void main(String[] args) &#123; final Test_03_Container t = new Test_03_Container(); final CountDownLatch latch = new CountDownLatch(1); new Thread(new Runnable()&#123; @Override public void run() &#123; if(t.size() != 5)&#123; try &#123; latch.await(); // 等待门闩的开放。 不是进入等待队列 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println("size = 5"); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; for(int i = 0; i &lt; 10; i++)&#123; System.out.println("add Object to Container " + i); t.add(new Object()); if(t.size() == 5)&#123; latch.countDown(); // 门闩-1 &#125; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;).start(); &#125;&#125;class Test_03_Container&#123; List&lt;Object&gt; container = new ArrayList&lt;&gt;(); public void add(Object o)&#123; this.container.add(o); &#125; public int size()&#123; return this.container.size(); &#125;&#125; 小编是一枚Java Coder，业余写文章，现主营微信公众号《Java患者》，喜欢的话关注我的公众号或者加我微信我们一起学习Java 筑基期（ReentrantLock）筑基初期（lock等待锁） concurrent是jdk1.5后的包，避免synchronized的出现而设计出来的一种锁机制。 ReentrantLock 重入锁，在一个对象上加一个标记信息，这个标记信息代表锁机制。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Test_01 &#123; Lock lock = new ReentrantLock(); void m1() &#123; try &#123; lock.lock(); // 加锁 for (int i = 0; i &lt; 10; i++) &#123; TimeUnit.SECONDS.sleep(1); System.out.println("m1() method " + i); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); // 解锁 &#125; &#125; void m2() &#123; lock.lock(); System.out.println("m2() method"); lock.unlock(); &#125; public static void main(String[] args) &#123; final Test_01 t = new Test_01(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m1(); &#125; &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(new Runnable() &#123; @Override public void run() &#123; t.m2(); &#125; &#125;).start(); &#125;&#125; 筑基中期（tryLock尝试锁）尝试锁有阻塞和非阻塞两种 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class Test_02 &#123; Lock lock = new ReentrantLock(); void m1()&#123; try&#123; lock.lock(); for(int i = 0; i &lt; 10; i++)&#123; TimeUnit.SECONDS.sleep(1); System.out.println("m1() method " + i); &#125; &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125;finally&#123; lock.unlock(); &#125; &#125; void m2()&#123; boolean isLocked = false; try&#123; // 尝试锁， 如果有锁，无法获取锁标记，返回false。 // 非阻塞，如果获取锁标记，返回true // isLocked = lock.tryLock(); // 阻塞尝试锁，阻塞参数代表的时长，尝试获取锁标记。 // 如果超时，不等待。直接返回。 isLocked = lock.tryLock(5, TimeUnit.SECONDS); if(isLocked)&#123; System.out.println("m2() method synchronized"); &#125;else&#123; System.out.println("m2() method unsynchronized"); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(isLocked)&#123; // 尝试锁在解除锁标记的时候，一定要判断是否获取到锁标记。 // 如果当前线程没有获取到锁标记，会抛出异常。 lock.unlock(); &#125; &#125; &#125; public static void main(String[] args) &#123; final Test_02 t = new Test_02(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m1(); &#125; &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; new Thread(new Runnable() &#123; @Override public void run() &#123; t.m2(); &#125; &#125;).start(); &#125;&#125; 筑基后期（lockInterruptibly可打断锁） 阻塞状态有3种： 包括普通阻塞（不释放锁），等待队列（释放锁），锁池队列。 普通阻塞： sleep(10000)， 可以被打断。调用thread.interrupt()方法，可以打断阻塞状态，抛出异常。 等待队列： wait()方法被调用，也是一种阻塞状态，只能由notify唤醒。无法打断。 锁池队列： 执行过程中，遇到同步代码，无法获取锁标记。不是所有的锁池队列都可被打断。 使用ReentrantLock的lock方法，获取锁标记的时候，如果需要阻塞等待锁标记，无法被打断。 使用ReentrantLock的lockInterruptibly方法，获取锁标记的时候，如果需要阻塞等待，可以被打断。 可打断锁意义：软件锁死了，无响应，去去任务管理器结束任务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class Test_03 &#123; Lock lock = new ReentrantLock(); void m1() &#123; try &#123; lock.lock(); for (int i = 0; i &lt; 5; i++) &#123; TimeUnit.SECONDS.sleep(1); System.out.println("m1() method " + i); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; void m2() &#123; try &#123; // 线程执行到这里，本来是不能获得锁标记的，要进入等待队列的。 // 当通过调用当前线程的interrupt()，通过打断当前线程，抛出异常，使线程被唤醒，阻塞结束 lock.lockInterruptibly(); // 可尝试打断的，阻塞等待锁。可以被其他的线程打断阻塞状态 System.out.println("m2() method"); &#125; catch (InterruptedException e) &#123; // 被打断的异常，被打断与唤醒、阻塞结束都是不一样的 // sleep任何一个线程都可以把他打断，强行唤醒 // 如果是lock不可被打断的 // 如果是lockInterruptibly，阻塞等待这把锁，类似sleep，可以通过interrupt()打断 System.out.println("m2() method interrupted"); &#125; finally &#123; try &#123; lock.unlock(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; final Test_03 t = new Test_03(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.m1(); &#125; &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; t.m2(); &#125; &#125;); t2.start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; // 不调用interrupt()方法，t2最后可以获得锁，继续执行 t2.interrupt();// 打断t2线程，锁的位置会抛出异常。 &#125;&#125; 筑基圆满（公平锁） 在cpu和os中本身线程竞争锁标记是不公平的，不考虑线程的等待时间的。 运用在轮询的场景，如打牌。 需要效果一部分的cpu资源计算等待的时间，性能有所降低。要仅能少用，并发量在10之内。 123456789101112131415161718192021222324252627282930313233343536373839public class Test_04 &#123; public static void main(String[] args) &#123; TestReentrantlock t = new TestReentrantlock(); //TestSync t = new TestSync(); Thread t1 = new Thread(t); Thread t2 = new Thread(t); t1.start(); t2.start(); &#125;&#125;class TestReentrantlock extends Thread &#123; // 定义一个公平锁 private static ReentrantLock lock = new ReentrantLock(true); public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + " get lock"); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;&#125;class TestSync extends Thread &#123; public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; //不公平的 synchronized (this) &#123; System.out.println(Thread.currentThread().getName() + " get lock in TestSync"); &#125; &#125; &#125;&#125; 小编是一枚Java Coder，业余写文章，现主营微信公众号《Java患者》，喜欢的话关注我的公众号或者加我微信我们一起学习Java 金丹期金丹初期（生产者&amp;消费者） ReenTrantLock建议应用在同步方式，相对效率比synchronized高，量级较轻。 synchronized在JDK1.5版本尝试优化，到JDK1.7后，优化效率已经非常好了。在绝对效率上不比ReenTrantLock差多少。 使用ReenTrantLock必须释放锁标记。一般在finally代码块释放锁标记的。 12练习（生产者消费者模式）：自定义同步容器，容器容量上限为10。可以在多线程中应用，并保证数据线程安全。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class TestContainer01&lt;E&gt; &#123; private final LinkedList&lt;E&gt; list = new LinkedList&lt;&gt;(); private final int MAX = 10; private int count = 0; public synchronized int getCount()&#123; return count; &#125; public synchronized void put(E e)&#123; while(list.size() == MAX)&#123; try &#123; this.wait(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; &#125; list.add(e); count++; this.notifyAll(); &#125; public synchronized E get()&#123; E e = null; while(list.size() == 0)&#123; try&#123; this.wait(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; &#125; e = list.removeFirst(); count--; this.notifyAll(); return e; &#125; public static void main(String[] args) &#123; final TestContainer01&lt;String&gt; c = new TestContainer01&lt;&gt;(); for(int i = 0; i &lt; 10; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; for(int j = 0; j &lt; 5; j++)&#123; System.out.println(c.get()); &#125; &#125; &#125;, "consumer"+i).start(); &#125; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; for(int i = 0; i &lt; 2; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; for(int j = 0; j &lt; 25; j++)&#123; c.put("container value " + j); &#125; &#125; &#125;, "producer"+i).start(); &#125; &#125;&#125; 使用ReentrantLock完成生产者-消费者 Condition， 为Lock增加条件。当条件满足时（生成了或者是被消费），做什么事情，如加锁或解锁。如等待或唤醒 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class TestContainer02&lt;E&gt; &#123; private final LinkedList&lt;E&gt; list = new LinkedList&lt;&gt;(); private final int MAX = 10; private int count = 0; private Lock lock = new ReentrantLock(); private Condition producer = lock.newCondition(); private Condition consumer = lock.newCondition(); public int getCount()&#123; return count; &#125; public void put(E e)&#123; lock.lock(); try &#123; while(list.size() == MAX)&#123; System.out.println(Thread.currentThread().getName() + " 等待。。。"); // 进入等待队列。释放锁标记。 // 借助条件，进入的等待队列。 producer.await(); &#125; System.out.println(Thread.currentThread().getName() + " put 。。。"); list.add(e); count++; // 借助条件，唤醒所有的消费者。 consumer.signalAll(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public E get()&#123; E e = null; lock.lock(); try &#123; while(list.size() == 0)&#123; System.out.println(Thread.currentThread().getName() + " 等待。。。"); // 借助条件，消费者进入等待队列 consumer.await(); &#125; System.out.println(Thread.currentThread().getName() + " get 。。。"); e = list.removeFirst(); count--; // 借助条件，唤醒所有的生产者 producer.signalAll(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; return e; &#125; public static void main(String[] args) &#123; final TestContainer02&lt;String&gt; c = new TestContainer02&lt;&gt;(); for(int i = 0; i &lt; 10; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; for(int j = 0; j &lt; 5; j++)&#123; System.out.println(c.get()); &#125; &#125; &#125;, "consumer"+i).start(); &#125; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; for(int i = 0; i &lt; 2; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; for(int j = 0; j &lt; 25; j++)&#123; c.put("container value " + j); &#125; &#125; &#125;, "producer"+i).start(); &#125; &#125;&#125; 金丹中期（锁的底层实现）​ Java 虚拟机中的同步(Synchronization)基于进入和退出管程(Monitor)对象实现。同步方法 并不是由 monitor enter 和 monitor exit 指令来实现同步的，而是由方法调用指令读取运行时常量池中方法的ACC_SYNCHRONIZED 标志来隐式实现的。注：monitor enter 和 monitor exit 指令是C语言的内容。 对象的内存模型（一个对象包含3部分，没有方法，方法是在方法区域中的） 对象头：存储对象的 hashCode、锁信息或分代年龄或 GC 标志，类型指针指向对象的类元数据，JVM 通过这个指针确定该对象是哪个类的实例等信息。（关注锁信息） 实例变量：存放类的属性数据信息，包括父类的属性信息 填充数据：由于虚拟机要求对象起始地址必须是 8 字节的整数倍。填充数据不是必须存在的，仅仅是为了字节对齐 monitor在栈中，但不是在线程栈中。 _Owner指向线程。 ​ 当线程在对象上加锁时，对象头都会指向monitor，记录锁信息。当执行 synchronized 同步方法或同步代码块时，会在对象头中记录锁标记，锁标记指向的是 monitor 对象（也称为管程或监视器锁）的起始地址。每个对象都存在着一个 monitor 与之关联，对象与其 monitor 之间的关系有存在多种实现方式，如 monitor 可以与对象一起创建销毁或当线程试图获取对象锁时自动生成，但当一个 monitor 被某个线程持有后，它便处于锁定状态。 ​ 另外的线程想获取对象头中的锁信息的时候，会发现对象头中已经记录一把锁（monitor），他就获取不到。monitor是互斥的，对象头记录的monitor就不会分配给其他线程了，此时这个线程就会进入阻塞状态。当执行中的线程发生异常，或者是释放锁标记，对象头的锁信息就会释放它记录的monitor。阻塞状态的线程就会弹出来争夺对象中的锁信息，重新在锁信息中记录monitor。 ​ ObjectMonitor 中有两个队列，_WaitSet 和 _EntryList，以及_Owner 标记。其中_WaitSet是用于管理等待队列(wait)线程的，_EntryList 是用于管理锁池阻塞线程的，_Owner 标记用于记录当前执行线程。 线程状态图 ​ 当多线程并发访问同一个同步代码时，首先会进入_EntryList，当线程获取锁标记后，monitor 中的_Owner 记录此线程，并在 monitor 中的计数器执行递增计算（+1），代表锁定，其他线程在_EntryList 中继续阻塞。若执行线程调用 wait 方法，则 monitor 中的计数器执行赋值为 0 计算，并将_Owner 标记赋值为 null，代表放弃锁，执行线程进如_WaitSet 中阻塞。若执行线程调用 notify/notifyAll 方法，_WaitSet 中的线程被唤醒，进入_EntryList 中阻塞，等待获取锁标记。若执行线程的同步代码执行结束，同样会释放锁标记，monitor 中的_Owner标记赋值为 null，且计数器赋值为 0 计算。 ​ interrupt() 方法可以任何打断阻塞状态的线程，以抛异常的代价。 ​ InterruptedException异常是阻塞异常。阻塞中的线程抛出的。 锁的重入 ​ 在 Java 中，同步锁是可以重入的。只有同一线程调用同步方法或执行同步代码块，对同一个对象加锁时才可重入。​ 当线程持有锁时，会在 monitor 的计数器中执行递增计算，若当前线程调用其他同步代码，且同步代码的锁对象相同时，monitor 中的计数器继续递增。每个同步代码执行结束，monitor 中的计数器都会递减，直至所有同步代码执行结束，monitor 中的计数器为 0 时，释放锁标记，_Owner 标记赋值为 null。 金丹后期（锁的种类） Java 中锁的种类包括偏向锁，自旋锁，轻量级锁，重量级锁。 锁的使用方式先提供偏向锁，如果不满足的时候，升级为轻量级锁，再不满足，升级为重量级锁。自旋锁是一个过渡的锁状态，不是一种实际的锁类型。锁只能升级，不能降级。 在金丹初期提到的就是重量级锁。 偏向锁： ​ 是一种编译解释锁。如果代码中不可能出现多线程并发争抢同一个锁的时候，JVM 编译代码，解释执行的时候，会自动的放弃同步信息。消除 synchronized 的同步代码结果。使用锁标记的形式记录锁状态。在 Monitor 中有变量 ACC_SYNCHRONIZED。当变量值使用的时候，代表偏向锁锁定。可以避免锁的争抢和锁池状态的维护。提高JVM解释效率。 123456Object o = new Object();public void m() &#123; o = new Object(); synchronized (o) &#123; &#125;&#125; 轻量级锁： ​ 是一个过渡锁。当偏向锁不满足，也就是有多线程并发访问，锁定同一个对象的时候，先提升为轻量级锁。也是使用标记 ACC_SYNCHRONIZED 标记记录的。ACC_UNSYNCHRONIZED 标记记录未获取到锁信息的线程。就是只有两个线程争抢锁标记的时候，优先使用轻量级锁。A线程和monitor有直接关联的。B线程不记录monitor，是monitor记录B线程，线程A结束后，B两个线程才找到monitor。也可能出现重量级锁。 自旋锁： ​ 是一个过渡锁，是偏向锁和轻量级锁的过渡。当获取锁的过程中，未获取到。为了提高效率，JVM 自动执行若干次空循环，再次申请锁，而不是进入阻塞状态的情况。称为自旋锁。自旋锁提高效率就是避免线程状态的变更。 金丹圆满（ThreadLocal） 就是一个Map。key 是Thread.getCurrentThread()，value 是线程需要保存的变量。 ThreadLocal.set(value)相当map.put(Thread.getCurrentThread(), value)。 ThreadLocal.get() 相当map.get(Thread.getCurrentThread())。 内存问题 ： 在并发量高的时候，可能有内存溢出。 使用ThreadLocal的时候，一定注意回收资源问题，每个线程结束之前，将当前线程保存的线程变量一定要删除 ，调用ThreadLocal.remove()，要不会发生泄露。run方法的finally代码块。 ​ 在一个操作系统中，线程和进程是有数量上限的。在操作系统中，确定线程和进程唯一性的唯一条件就是线程或进程 ID。操作系统在回收线程或进程的时候，不是一定杀死线程或进程，在繁忙的时候，只会做情况线程或进程栈数据的操作，重复使用线程或进程。 12345678910111213141516171819202122232425262728293031323334public class Test_01 &#123; volatile static String name = "zhangsan"; static ThreadLocal&lt;String&gt; tl = new ThreadLocal&lt;&gt;(); public static void main(String[] args) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(name); // lisi System.out.println(tl.get()); // null &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; name = "lisi"; tl.set("wangwu"); &#125; &#125;).start(); &#125;&#125; 小编是一枚Java Coder，业余写文章，现主营微信公众号《Java患者》，喜欢的话关注我的公众号或者加我微信我们一起学习Java 元婴期（并发容器）​ 解决并发情况下的容器线程安全问题的。给多线程环境准备一个线程安全的容器对象。线程安全的容器对象： Vector, Hashtable。线程安全容器对象，都是使用 synchronized方法实现的。​ concurrent 包中的同步容器，大多数是使用系统底层技术实现的线程安全。类似 native。Java8 中使用 CAS。 元婴前期（Map/Set） ConcurrentHashMap/ConcurrentHashSet底层哈希实现的同步 Map(Set)。效率高，线程安全。使用系统底层技术实现线程安全。量级较 synchronized 低。key 和 value 不能为 null。 ConcurrentSkipListMap/ConcurrentSkipListSet底层跳表（SkipList）实现的同步 Map(Set)。有序，效率比 ConcurrentHashMap 稍低。 1234567891011121314151617181920212223242526272829303132333435public class Test_01_ConcurrentMap &#123; public static void main(String[] args) &#123; final Map&lt;String, String&gt; map = new Hashtable&lt;&gt;(); // Collections.syncxxxxxx // final Map&lt;String, String&gt; map = new ConcurrentHashMap&lt;&gt;(); // final Map&lt;String, String&gt; map = new ConcurrentSkipListMap&lt;&gt;(); 数据结构跳表 final Random r = new Random(); Thread[] array = new Thread[100]; final CountDownLatch latch = new CountDownLatch(array.length); long begin = System.currentTimeMillis(); for (int i = 0; i &lt; array.length; i++) &#123; array[i] = new Thread(new Runnable() &#123; @Override public void run() &#123; for (int j = 0; j &lt; 10000; j++) &#123; map.put("key" + r.nextInt(100000), "value" + r.nextInt(100000)); &#125; latch.countDown(); &#125; &#125;); &#125; for (Thread t : array) &#123; t.start(); &#125; try &#123; latch.await(); //等待门闩开放 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; long end = System.currentTimeMillis(); System.out.println("执行时间为 ： " + (end - begin) + "毫秒！"); &#125;&#125; 调表机构：存10、18、15、20、19。 元婴中期（List） CopyOnWriteArrayList：写时复制集合，效率低，读取效率高。每次写入数据，都会创建一个新的底层数组。 浪费空间保证数据的安全。 初始容量1，每次新增的内容，创建容量+1。 取得时候，取最新的数组。remove最后一个数据，直接用上一个数组。 set和remove其他数据，重新创建数组。 存在幻读（写的时候，有读操作，不是最新添加的数据） 存在脏读（写的时候，有读操作，不是最新添加的数据） 123456789101112131415161718192021222324252627282930313233343536public class Test_02_CopyOnWriteList &#123; public static void main(String[] args) &#123; // final List&lt;String&gt; list = new ArrayList&lt;&gt;(); 线程不安全 // final List&lt;String&gt; list = new Vector&lt;&gt;(); 线程安全 更快 final List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); final Random r = new Random(); Thread[] array = new Thread[100]; final CountDownLatch latch = new CountDownLatch(array.length); long begin = System.currentTimeMillis(); for(int i = 0; i &lt; array.length; i++)&#123; array[i] = new Thread(new Runnable() &#123; @Override public void run() &#123; for(int j = 0; j &lt; 1000; j++)&#123; list.add("value" + r.nextInt(100000)); &#125; latch.countDown(); &#125; &#125;); &#125; for(Thread t : array)&#123; t.start(); &#125; try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; long end = System.currentTimeMillis(); System.out.println("执行时间为 ： " + (end-begin) + "毫秒！"); System.out.println("List.size() : " + list.size()); &#125;&#125; 元婴后期（Queue）ConcurrentLinkedQueue：基础链表同步队列。 123456789101112131415161718192021public class Test_03_ConcurrentLinkedQueue &#123; public static void main(String[] args) &#123; Queue&lt;String&gt; queue = new ConcurrentLinkedQueue&lt;&gt;(); for(int i = 0; i &lt; 10; i++)&#123; queue.offer("value" + i); &#125; System.out.println(queue); System.out.println(queue.size()); // peek() -&gt; 查看queue中的首数据 System.out.println(queue.peek()); System.out.println(queue.size()); // poll() -&gt; 获取queue中的首数据 System.out.println(queue.poll()); System.out.println(queue.size()); &#125;&#125; LinkedBlockingQueue：阻塞队列，队列容量不足自动阻塞，队列容量为 0 自动阻塞。 put自动阻塞， 队列容量满后，自动阻塞。 take自动阻塞方法， 队列容量为0后，自动阻塞。 12345678910111213141516171819202122232425262728293031323334353637383940public class Test_04_LinkedBlockingQueue &#123; final BlockingQueue&lt;String&gt; queue = new LinkedBlockingQueue&lt;&gt;(); final Random r = new Random(); public static void main(String[] args) &#123; final Test_04_LinkedBlockingQueue t = new Test_04_LinkedBlockingQueue(); new Thread(new Runnable() &#123; @Override public void run() &#123; while(true)&#123; try &#123; t.queue.put("value"+t.r.nextInt(1000)); TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;, "producer").start(); for(int i = 0; i &lt; 3; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; while(true)&#123; try &#123; System.out.println(Thread.currentThread().getName() + " - " + t.queue.take()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;, "consumer"+i).start(); &#125; &#125;&#125; ArrayBlockingQueue：底层数组实现的有界队列。自动阻塞。根据调用 API（add/put/offer）不同，有不同特性。当容量不足的时候，有阻塞能力。 add 方法在容量不足的时候，抛出异常。 put 方法在容量不足的时候，阻塞等待。 offer 方法， 单参数 offer 方法，不阻塞。容量不足的时候，返回 false。当前新增数据操作放弃。 三参数 offer 方法（offer(value,times,timeunit)），容量不足的时候，阻塞 times 时长（单位为 timeunit），如果在阻塞时长内，有容量空闲，新增数据返回 true。如果阻塞时长范围内，无容量空闲，放弃新增数据，返回 false。 1234567891011121314151617181920212223242526272829303132public class Test_05_ArrayBlockingQueue &#123; final BlockingQueue&lt;String&gt; queue = new ArrayBlockingQueue&lt;&gt;(3); public static void main(String[] args) &#123; final Test_05_ArrayBlockingQueue t = new Test_05_ArrayBlockingQueue(); for(int i = 0; i &lt; 5; i++)&#123; // System.out.println("add method : " + t.queue.add("value"+i)); --------------------------------------------------------------------- /*try &#123; t.queue.put("put"+i); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("put method : " + i);*/ --------------------------------------------------------------------- // System.out.println("offer method : " + t.queue.offer("value"+i)); --------------------------------------------------------------------- try &#123; System.out.println("offer method : " + t.queue.offer("value"+i, 1, TimeUnit.SECONDS)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(t.queue); &#125;&#125; DelayQueue：延时队列。根据比较机制，实现自定义处理顺序的队列。常用于定时任务。 通过比较方法，比较排列，获取。 可以保存的对象一定要实现Delayed接口。Delayed接口继承Comparable接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Test_06_DelayQueue &#123; static BlockingQueue&lt;MyTask_06&gt; queue = new DelayQueue&lt;&gt;(); public static void main(String[] args) throws InterruptedException &#123; long value = System.currentTimeMillis(); MyTask_06 task1 = new MyTask_06(value + 2000); MyTask_06 task2 = new MyTask_06(value + 1000); MyTask_06 task3 = new MyTask_06(value + 3000); MyTask_06 task4 = new MyTask_06(value + 2500); MyTask_06 task5 = new MyTask_06(value + 1500); queue.put(task1); queue.put(task2); queue.put(task3); queue.put(task4); queue.put(task5); System.out.println(queue); System.out.println(value); for(int i = 0; i &lt; 5; i++)&#123; System.out.println(queue.take()); &#125; &#125;&#125;class MyTask_06 implements Delayed &#123; private long compareValue; public MyTask_06(long compareValue)&#123; this.compareValue = compareValue; &#125; /** * 比较大小。自动实现升序 * 建议和getDelay方法配合完成。 * 如果在DelayQueue是需要按时间完成的计划任务，必须配合getDelay方法完成。 */ @Override public int compareTo(Delayed o) &#123; return (int)(this.getDelay(TimeUnit.MILLISECONDS) - o.getDelay(TimeUnit.MILLISECONDS)); &#125; /** * 获取计划时长的方法。 * 根据参数TimeUnit来决定，如何返回结果值。秒，毫秒 */ @Override public long getDelay(TimeUnit unit) &#123; return unit.convert(compareValue - System.currentTimeMillis(), TimeUnit.MILLISECONDS); &#125; @Override public String toString()&#123; return "Task compare value is : " + this.compareValue; &#125;&#125; LinkedTransferQueue：转移队列， 使用 transfer 方法，没有消费者，就阻塞。必须有消费者（take()方法的调用者），实现数据的即时处理（电话）。 无容量的，放数组，容量为零，这时候要阻塞。等另一个线程来拿，不经过容器的存储来转移数组。 使用 add方法，直接存在容器中。队列会保存数据，不做阻塞等待（短信）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class Test_07_TransferQueue &#123; TransferQueue&lt;String&gt; queue = new LinkedTransferQueue&lt;&gt;(); public static void main(String[] args) &#123; final Test_07_TransferQueue t = new Test_07_TransferQueue();//为了匿名内部累方法中获得queue引用。 /*new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(Thread.currentThread().getName() + " thread begin " ); System.out.println(Thread.currentThread().getName() + " - " + t.queue.take()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, "output thread").start(); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; try &#123; t.queue.transfer("test string"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;*/ new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; t.queue.transfer("test string"); // t.queue.add("test string"); System.out.println("add ok"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(Thread.currentThread().getName() + " thread begin " ); System.out.println(Thread.currentThread().getName() + " - " + t.queue.take()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, "output thread").start(); &#125;&#125; SynchronusQueue：同步队列，是一个容量为 0 的队列。是一个特殊的 TransferQueue。 必须现有消费线程等待，才能使用的队列。 add 方法，无阻塞。若没有消费线程阻塞等待数据，则抛出非阻塞异常。 put 方法，有阻塞。若没有消费线程阻塞等待数据，则阻塞。 场景：玩家与玩家之间的匹配。 12345678910111213141516171819202122232425262728293031323334353637383940public class Test_08_SynchronusQueue &#123; BlockingQueue&lt;String&gt; queue = new SynchronousQueue&lt;&gt;(); public static void main(String[] args) &#123; final Test_08_SynchronusQueue t = new Test_08_SynchronusQueue(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(Thread.currentThread().getName() + " thread begin " ); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " - " + t.queue.take()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, "output thread").start(); /*try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;*/ // t.queue.add("test add"); try &#123; t.queue.put("test put"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " queue size : " + t.queue.size()); &#125;&#125; 元婴圆满（线程池）​ Executor：线程池顶级接口。定义方法，void execute(Runnable)。方法是用于处理任务的一个服务方法。调用者提供 Runnable 接口的实现，线程池通过线程执行这个 Runnable。服务方法无返回值的。是 Runnable 接口中的 run 方法无返回值。常用方法 - void execute(Runnable)，作用是启动线程任务的。 ​ 他不是线程池，他是线程池线程池底层处理机制。在使用线程池的时候，底层如何处理本线程的逻辑。 123456789101112131415public class Test_01_MyExecutor implements Executor &#123; public static void main(String[] args) &#123; new Test_01_MyExecutor().execute(new Runnable() &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + " - test executor"); &#125; &#125;); &#125; @Override public void execute(Runnable command) &#123; new Thread(command).start(); &#125;&#125; ExecutorService：Executor 接口的子接口。提供了一个新的服务方法，submit。有返回值（Future 类型）。submit 方法提供了 overload 方法。其中有参数类型为 Runnable 的，不需要提供返回值的；有参数类型为 Callable，可以提供线程执行后的返回值。 他是线程池服务类型。所有的线程池类型都实现这个接口，实现这个接口，代表可以提供线程池能力。 Future是 submit 方法的返回值。代表未来，也就是线程执行结束后的一种结果。如返回值。 常见方法 void execute(Runnable) Future submit(Callable) Future submit(Runnable) shutdown()：优雅关闭。 不是强行关闭线程池，回收线程池中的资源。而是不再处理新的任务，将已接收的任务处理完毕后再关闭。 线程池状态 Running - 线程池正在执行中。活动状态。 ShuttingDown - 线程池正在关闭过程中。优雅关闭。一旦进入这个状态，线程池不再接收新的任务，处理所有已接收的任务，处理完毕后，关闭线程池。不能执行submit方法和execute方法。 Terminated - 线程池已经关闭。不能执行submit方法和execute方法。 Future：未来结果，代表线程任务执行结束后的结果。获取线程执行结果的方式是通过 get 方法获取的。 get 无参，阻塞等待线程执行结束，并得到结果。 get 有参，阻塞固定时长，等待线程执行结束后的结果，如果在阻塞时长范围内，线程未执行结束，抛出异常。 常用方法： T get() T get(long, TimeUnit) 123456789101112131415161718192021222324252627282930313233343536public class Test_03_Future &#123; public static void main(String[] args) throws InterruptedException, ExecutionException &#123; /*FutureTask&lt;String&gt; task = new FutureTask&lt;&gt;(new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; return "first future task"; &#125; &#125;); new Thread(task).start(); System.out.println(task.get());*/ // 上面代码和下面一模一样的 ExecutorService service = Executors.newFixedThreadPool(1); Future&lt;String&gt; future = service.submit(new Callable&lt;String&gt;() &#123; @Override public String call() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("aaa"); return Thread.currentThread().getName() + " - test executor"; &#125; &#125;); System.out.println(future); System.out.println(future.isDone()); // 查看线程是否结束， 任务是否完成。 call方法是否执行结束 System.out.println(future.get()); // 获取call方法的返回值。 System.out.println(future.isDone()); &#125;&#125; Callable：可执行接口， 类似 Runnable 接口，也是可以启动一个线程的接口。其中定义的方法是call，call 方法的作用和 Runnable 中的 run 方法完全一致，call 方法有返回值。 接口方法 ： Object call();相当于 Runnable 接口中的 run 方法。区别为此方法有返回值。不能抛出已检查异常。 和 Runnable 接口的选择 - 需要返回值或需要抛出异常时，使用 Callable，其他情况可任意选择。 Executors：工具类型，为 Executor 线程池提供工具方法。 可以快速的提供若干种线程池。如：固定容量的，无限容量的，容量为 1 等各种线程池。 线程池是一个进程级的重量级资源。默认的生命周期和 JVM 一致。当开启线程池后，直到 JVM 关闭为止，是线程池的默认生命周期。如果手工调用 shutdown 方法，那么线程池执行所有的任务后，自动关闭。不调用shutdown方法，程序一直不关闭的。 开始 - 创建线程池。 结束 - JVM 关闭或调用 shutdown 并处理完所有的任务。 类似 Arrays，Collections 等工具类型的功用。 FixedThreadPool：容量固定的线程池。活动状态和线程池容量是有上限的线程池。 所有的线程池中，都有一个任务队列。使用的是 BlockingQueue作为任务的载体。当任务数量大于线程池容量的时候，没有运行的任务保存在任务队列中，当线程有空闲的，自动从队列中取出任务执行。 使用场景： 大多数情况下，使用的线程池，首选推荐 FixedThreadPool。OS 系统和硬件是有线程支持上限。不能随意的无限制提供线程池。 线程池默认的容量上限是 Integer.MAX_VALUE。 常见的线程池容量： PC - 200。 服务器 - 1000~10000 queued tasks - 任务队列，completed tasks - 结束任务队列 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Test_02_FixedThreadPool &#123; public static void main(String[] args) &#123; ExecutorService service = Executors.newFixedThreadPool(5); for (int i = 0; i &lt; 6; i++) &#123; service.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " - test executor"); &#125; &#125;); &#125; //[Running, pool size = 5, active threads = 5, queued tasks = 1, completed tasks = 0] System.out.println(service); service.shutdown(); // 是否已经结束， 相当于回收了资源。 System.out.println(service.isTerminated()); // false // 是否已经关闭， 是否调用过shutdown方法 System.out.println(service.isShutdown()); //true // [Shutting down, pool size = 5, active threads = 5, queued tasks = 1, completed tasks = 0] System.out.println(service); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // service.shutdown(); System.out.println(service.isTerminated()); //true System.out.println(service.isShutdown()); //true // [Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 6] System.out.println(service); &#125;&#125; ​ CachedThreadPool：缓存的线程池。容量不限（Integer.MAX_VALUE）。自动扩容。容量管理策略：如果线程池中的线程数量不满足任务执行，创建新的线程。每次有新任务无法即时处理的时候，都会创建新的线程。 当线程池中的线程空闲时长达到一定的临界值（默认 60 秒），自动释放线程。 默认线程空闲 60 秒，自动销毁。 应用场景： 内部应用或测试应用。 内部应用，有条件的内部数据瞬间处理时应用，如：电信平台夜间执行数据整理（有把握在短时间内处理完所有工作，且对硬件和软件有足够的信心）。 测试应用，在测试的时候，尝试得到硬件或软件的最高负载量，用于提供FixedThreadPool 容量的指导。 1234567891011121314151617181920212223242526272829303132public class Test_05_CachedThreadPool &#123; public static void main(String[] args) &#123; ExecutorService service = Executors.newCachedThreadPool(); System.out.println(service); // 容量为0 for(int i = 0; i &lt; 5; i++)&#123; service.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " - test executor"); &#125; &#125;); &#125; System.out.println(service); // 容量为5 try &#123; TimeUnit.SECONDS.sleep(65); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(service); &#125;&#125; ScheduledThreadPool：计划任务线程池。可以根据计划自动执行任务的线程池。 scheduleAtFixedRate(Runnable, start_limit, limit, timeunit) runnable - 要执行的任务。 start_limit - 第一次任务执行的间隔。 limit - 多次任务执行的间隔。 timeunit - 多次任务执行间隔的时间单位。 他是阻塞的，效率低下。 他本质就是DelayedQueue 每间隔一定的时间，随机一个线程运行，并且运行完的线程，不会销毁，会继续等待下次选中运行。 使用场景： 计划任务时选用（具体与DelaydQueue比较后选择），如：电信行业中的数据整理，每分钟整理，每消失整理，每天整理等. 1234567891011121314151617181920212223public class Test_07_ScheduledThreadPool &#123; public static void main(String[] args) &#123; ScheduledExecutorService service = Executors.newScheduledThreadPool(3); System.out.println(service); // 定时完成任务。 scheduleAtFixedRate(Runnable, start_limit, limit, timeunit) // runnable - 要执行的任务。 service.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()); &#125; &#125;, 0, 300, TimeUnit.MILLISECONDS); &#125;&#125; SingleThreadExceutor：单一容量的线程池。使用场景： 所有任务交给它处理，保证任务顺序时使用。如： 游戏大厅中的公共频道聊天。秒杀。 1234567891011121314151617181920212223public class Test_06_SingleThreadExecutor &#123; public static void main(String[] args) &#123; ExecutorService service = Executors.newSingleThreadExecutor(); System.out.println(service); for(int i = 0; i &lt; 5; i++)&#123; service.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " - test executor"); &#125; &#125;); &#125; &#125;&#125; ForkJoinPool：分支合并线程池（mapduce 类似的设计思想，递归思想的运用）。适合用于处理复杂任务。 初始化线程容量与 CPU 核心数相关。 线程池中运行的内容必须是 ForkJoinTask 的子类型（RecursiveTask,RecursiveAction）。 ForkJoinPool - 分支合并线程池。 可以递归完成复杂任务。要求可分支合并的任务必须是 ForkJoinTask 类型的子类型。其中提供了分支和合并的能力。ForkJoinTask 类型提供了两个抽象子类型，RecursiveTask 有返回结果的分支合并任务,RecursiveAction 无返回结果的分支合并任务。（Callable/Runnable）compute 方法：就是任务的执行逻辑。 ForkJoinPool 没有所谓的容量。默认都是 1 个线程。根据任务自动的分支新的子线程。当子线程任务结束后，自动合并。所谓自动是根据 fork 和 join 两个方法实现的。 应用： 主要是做科学计算或天文计算的。数据分析的. 拿空间换时间，效率高，但要看CPU能力。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class Test_08_ForkJoinPool &#123; final static int[] numbers = new int[1000000]; final static int MAX_SIZE = 50000; final static Random r = new Random(); static&#123; for(int i = 0; i &lt; numbers.length; i++)&#123; numbers[i] = r.nextInt(1000); &#125; &#125; static class AddTask extends RecursiveTask&lt;Long&gt;&#123; // RecursiveAction int begin, end; public AddTask(int begin, int end)&#123; this.begin = begin; this.end = end; &#125; // protected Long compute()&#123; if((end - begin) &lt; MAX_SIZE)&#123; long sum = 0L; for(int i = begin; i &lt; end; i++)&#123; sum += numbers[i]; &#125; // System.out.println("form " + begin + " to " + end + " sum is : " + sum); return sum; &#125;else&#123; int middle = begin + (end - begin)/2; AddTask task1 = new AddTask(begin, middle); AddTask task2 = new AddTask(middle, end); task1.fork();// 就是用于开启新的任务的。 就是分支工作的。 就是开启一个新的线程任务。 task2.fork(); // join - 合并。将任务的结果获取。 这是一个阻塞方法。一定会得到结果数据。 return task1.join() + task2.join(); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException, ExecutionException, IOException &#123; long result = 0L; for(int i = 0; i &lt; numbers.length; i++)&#123; result += numbers[i]; &#125; System.out.println(result); ForkJoinPool pool = new ForkJoinPool(); AddTask task = new AddTask(0, numbers.length); Future&lt;Long&gt; future = pool.submit(task); System.out.println(future.get()); &#125;&#125; ThreadPoolExecutor：线程池底层实现。除 ForkJoinPool 外，其他常用线程池底层都是使用ThreadPoolExecutor实现的。public ThreadPoolExecutor(int corePoolSize，int maximumPoolSize，long keepAliveTime，TimeUnit unit，BlockingQueue workQueue); corePoolSize： 核心容量，创建线程池的时候，默认有多少线程。也是线程池保持的最少线程数 maximumPoolSize： 最大容量，线程池最多有多少线程 keepAliveTime：生命周期，0 为永久。当线程空闲多久后，自动回收。 unit：生命周期单位，为生命周期提供单位，如：秒，毫秒 workQueue：任务队列，阻塞队列。注意，泛型必须是 使用场景： 默认提供的线程池不满足条件时使用。如：初始线程数据 4，最大线程数200，线程空闲周期 30 秒。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Test_09_ThreadPoolExecutor &#123; public static void main(String[] args) &#123; // 模拟fixedThreadPool， 核心线程5个，最大容量5个，线程的生命周期无限。 ExecutorService service = new ThreadPoolExecutor(5, 5, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); for(int i = 0; i &lt; 6; i++)&#123; service.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " - test executor"); &#125; &#125;); &#125; System.out.println(service); service.shutdown(); System.out.println(service.isTerminated()); System.out.println(service.isShutdown()); System.out.println(service); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; service.shutdown(); System.out.println(service.isTerminated()); System.out.println(service.isShutdown()); System.out.println(service); &#125;&#125; 123练习：启动若干线程，并行访问同一个容器中的数据。保证获取容器中数据时没有数据错误，且线程安全。如：售票，秒杀等业务。 使用synchronized 1234567891011121314151617181920212223242526272829303132333435363738394041public class Test_01 &#123; static List&lt;String&gt; list = new ArrayList&lt;&gt;(); // static List&lt;String&gt; list = new Vector&lt;&gt;(); static&#123; for(int i = 0; i &lt; 10000; i++)&#123; list.add("String " + i); &#125; &#125; public static void main(String[] args) &#123; for(int i = 0; i &lt; 10; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; while(list.size() &gt; 0)&#123; System.out.println(Thread.currentThread().getName() + " - " + list.remove(0)); &#125; &#125; &#125;, "Thread" + i).start(); &#125; /*for(int i = 0; i &lt; 10; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; while(true)&#123; synchronized (list) &#123; if(list.size() &lt;= 0)&#123; break; &#125; System.out.println(Thread.currentThread().getName() + " - " + list.remove(0)); &#125; &#125; &#125; &#125;, "Thread" + i).start(); &#125;*/ &#125;&#125; 使用queue 12345678910111213141516171819202122232425262728public class Test_02 &#123; static Queue&lt;String&gt; list = new ConcurrentLinkedQueue&lt;&gt;(); static&#123; for(int i = 0; i &lt; 10000; i++)&#123; list.add(&quot;String &quot; + i); &#125; &#125; public static void main(String[] args) &#123; for(int i = 0; i &lt; 10; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; while(true)&#123; String str = list.poll(); if(str == null)&#123; break; &#125; System.out.println(Thread.currentThread().getName() + &quot; - &quot; + str); &#125; &#125; &#125;, &quot;Thread&quot; + i).start(); &#125; &#125;&#125; 小编是一枚Java Coder，业余写文章，现主营微信公众号《Java患者》，喜欢的话关注我的公众号或者加我微信我们一起学习Java 化神期（JVM1.7）化神前期（jvm结构）jvm基本结构图： 类加载子系统：类加载子系统负责从文件系统或者网络中加载 Class 信息，如ClassLoad这里面的组件。 方法区：加载的类信息存放于一块称为方法区的内存空间。除了类的信息外，方法区中可能还会存放运行时常量池信息，包括字符串字面量和数字常量（这部分常量信息是 Class 文件中常量池部分的内存映射）。 Java 堆：java 堆在虚拟机启动的时候建立，它是 java 程序最主要的内存工作区域。几乎所有的java 对象实例都存放在 java 堆中。堆空间是所有线程共享的，这是一块与 java 应用密切相关的内存空间。 直接内存：java 的 NIO 库允许 java 程序使用直接内存。直接内存是在 java 堆外的、直接向系统申请的内存空间。通常访问直接内存的速度会优于 java 堆。因此出于性能的考虑，读写频繁的场合可能会考虑使用直接内存。由于直接内存在 java 堆外，因此它的大小不会直接受限于 Xmx 指定的最大堆大小，但是系统内存是有限的，java 堆和直接内存的总和依然受限于操作系统能给出的最大内存。 垃圾回收系统：垃圾回收系统是 java 虚拟机的重要组成部分，垃圾回收器可以对方法区、java 堆和直接内存进行回收。其中，java 堆是垃圾收集器的工作重点。和 C/C++不同，java 中所有的对象空间释放都是隐式的，也就是说，java 中没有类似 free()或者 delete()这样的函数释放指定的内存区域。对于不再使用的垃圾对象，垃圾回收系统会在后台默默工作，默默查找、标识并释放垃圾对象，完成包括 java 堆、方法区和直接内存中的全自动化管理。 Java 栈：每一个 java 虚拟机线程都有一个私有的 java 栈，一个线程的 java 栈在线程创建的时候被创建，java 栈中保存着帧信息，java 栈中保存着局部变量、方法参数，同时和 java 方法的调用、返回密切相关。 本地方法栈：本地方法栈和 java 栈非常类似，最大的不同在于 java 栈用于方法的调用，而本地方法栈则用于本地方法的调用，作为对 java 虚拟机的重要扩展，java 虚拟机允许 java 直接调用本地方法（通常使用 C 编写）。 PC 寄存器：PC（Program Counter）寄存器也是每一个线程私有的空间，java 虚拟机会为每一个 java线程创建 PC 寄存器。在任意时刻，一个 java 线程总是在执行一个方法，这个正在被执行的方法称为当前方法。如果当前方法不是本地方法，PC 寄存器就会指向当前正在被执行的指令。如果当前方法是本地方法，那么 PC 寄存器的值就是 undefined。 Java HotSpot Client 模式和 Server 模式的区别：当虚拟机运行在-client 模式的时候,使用的是一个代号为 C1 的轻量级编译器, 而-server模式启动的虚拟机采用相对重量级,代号为 C2 的编译器. C2 比 C1 编译器编译的相对彻底,服务起来之后,性能更高。 JDK 安装目录/jre/lib/（x86、i386、amd32、amd64）/jvm.cfg文件中的内容，-server 和-client 哪一个配置在上，执行引擎就是哪一个。如果是 JDK1.5版本且是 64 位系统应用时，-client 无效。 –64 位系统内容-server KNOWN-client IGNORE –32 位系统内容-server KNOWN-client KNOWN 注意 ：在部分 JDK1.6 版本和后续的 JDK 版本 (64 位系统 ) 中， -client 参数已经不起作用了， Server 模式成为唯一。 化神中期（堆结构及对象分代） 什么是分代，分代的必要性是什么？ ​ Java 虚拟机根据对象存活的周期不同，把堆内存划分为几块，一般分为新生代、老年代和永久代（对 HotSpot 虚拟机而言），这就是 JVM 的内存分代策略。​ 堆内存是虚拟机管理的内存中最大的一块，也是垃圾回收最频繁的一块区域，我们程序所有的对象实例都存放在堆内存中。给堆内存分代是为了提高对象内存分配和垃圾回收的效率。试想一下，如果堆内存没有区域划分，所有的新创建的对象和生命周期很长的对象放在一起，随着程序的执行，堆内存需要频繁进行垃圾收集，而每次回收都要遍历所有的对象，遍历这些对象所花费的时间代价是巨大的，会严重影响我们的 GC 效率，并且会产生碎片。​ 有了内存分代，情况就不同了，新创建的对象会在新生代中分配内存，经过多次回收仍然存活下来的对象存放在老年代中，静态属性、类信息等存放在永久代中，新生代中的对象存活时间短，只需要在新生代区域中频繁进行 GC，老年代中对象生命周期长，内存回收的频率相对较低，不需要频繁进行回收，永久代中回收效果太差，一般不进行垃圾回收，还可以根据不同年代的特点采用合适的垃圾收集算法。分代收集大大提升了收集效率，这些都是内存分代带来的好处。 分代的划分Java ​ 虚拟机将堆内存划分为 新生代、老年代和永久代 ，永久代是 HotSpot 虚拟机特有的概念（JDK1.8 之后为 metaspace 替代永久代），它采用永久代的方式来实现方法区，其他的虚拟机实现没有这一概念，而且 HotSpot 也有取消永久代的趋势，在 JDK 1.7 中 HotSpot 已经开始了“去永久化”，把原本放在永久代的字符串常量池移出。永久主要存放常量、类信息、静态变量等数据，与垃圾回收关系不大，新生代和老年代是垃圾回收的主要区域。 新生代（Young Generation) ​ 新生成的对象优先存放在新生代中，新生代对象朝生夕死，存活率很低，在新生代Eden中，常规应用进行一次垃圾收集一般可以回收 70% ~ 95% 的空间，回收效率很高。​ HotSpot 将新生代划分为三块，一块较大的 Eden（伊甸）空间和两块较小的 Survivor（幸存者）空间，默认比例为 8：1：1。划分的目的是因为 HotSpot 采用复制算法来回收新生代，设置这个比例是为了充分利用内存空间，减少浪费。新生成的对象在 Eden 区分配（大对象除外，大对象直接进入老年代），当 Eden 区没有足够的空间进行分配时，虚拟机将发起一次Minor GC。​ GC 开始时，对象只会存在于 Eden 区和 From Survivor 区，To Survivor 区是空的（作为保留区域）。GC 进行时，Eden 区中所有存活的对象都会被复制到 To Survivor 区，而在 FromSurvivor 区中，仍存活的对象会根据它们的年龄值决定去向，年龄值达到年龄阀值（默认为15，新生代中的对象每熬过一轮垃圾回收，年龄值就加 1，GC 分代年龄存储在对象的 header中）的对象会被移到老年代中，没有达到阀值的对象会被复制到 To Survivor 区。接着清空Eden 区和 From Survivor 区，新生代中存活的对象都在 To Survivor 区。接着， From Survivor区和 To Survivor 区会交换它们的角色（复制算法减少碎片），也就是新的 To Survivor 区就是上次 GC 清空的 FromSurvivor 区，新的 From Survivor 区就是上次 GC 的 To Survivor 区，总之，不管怎样都会保证To Survivor 区在一轮 GC 后是空的。GC 时当 To Survivor 区没有足够的空间存放上一次新生代收集下来的存活对象时，需要依赖老年代进行分配担保，将这些对象存放在老年代中。 老年代（Old Generationn ） ​ 在新生代中经历了多次（具体看虚拟机配置的阀值）GC 后仍然存活下来的对象会进入老年代中。老年代中的对象生命周期较长，存活率比较高，在老年代中进行 GC 的频率相对而言较低，而且回收的速度也比较慢。 永久代（Permanent Generationn） ​ 永久代存储类信息、常量、静态变量、即时编译器编译后的代码等数据，对这一区域而言，Java 虚拟机规范指出可以不进行垃圾收集，一般而言不会进行垃圾回收。 化神后期（垃圾回收算法及分代垃圾） 常见 垃圾回收算法 引用计数 （Reference Counting ）：比较古老的回收算法。原理是此对象有一个引用，即增加一个计数，删除一个引用则减少一个计数。垃圾回收时，只用收集计数为 0 的对象。此算法最致命的是无法处理循环引用的问题。 复制（Copying）：此算法把内存空间划为两个相等的区域，每次只使用其中一个区域。垃圾回收时，遍历当前使用区域，把正在使用中的对象复制到另外一个区域中。此算法每次只处理正在使用中的对象，因此复制成本比较小，同时复制过去以后还能进行相应的内存整理，不会出现“碎片”问题。当然，此算法的缺点也是很明显的，就是需要两倍内存空间。简图如下： 标记- 清除（Mark-Sweep ）：最古老的算法，此算法执行分两阶段。第一阶段从引用根节点开始标记所有被引用的对象，第二阶段遍历整个堆，把未标记的对象清除。此算法需要暂停整个应用，同时，会产生内存碎片。简图如下： 标记- 整理（Mark-Compact ）：此算法结合了“标记-清除”和“复制”两个算法的优点。也是分两阶段，第一阶段从根节点开始标记所有被引用对象，第二阶段遍历整个堆，把清除未标记对象并且把存活对象“压缩”到堆的其中一块，按顺序排放。此算法避免了“标记-清除”的碎片问题，同时也避免了“复制”算法的空间问题。简图如下： 垃圾收集器的分类 次收集器：Scavenge GC，指发生在新生代的 GC，因为新生代的 Java 对象大多都是朝生夕死，所以Scavenge GC 非常频繁，一般回收速度也比较快。 当 Eden 空间不足以为对象分配内存时，会触发 Scavenge GC。 一般情况下，当新对象生成，并且在 Eden 申请空间失败时，就会触发 Scavenge GC，对Eden 区域进行 GC，清除非存活对象，并且把尚且存活的对象移动到 Survivor 区。然后整理Survivor 的两个区。这种方式的 GC 是对年轻代的 Eden 区进行，不会影响到年老代。因为大部分对象都是从 Eden 区开始的，同时 Eden 区不会分配的很大，所以 Eden 区的 GC 会频繁进行。因而，一般在这里需要使用速度快、效率高的算法，使 Eden 去能尽快空闲出来。 当年轻代堆空间紧张时会被触发 相对于全收集而言，收集间隔较短 全收集器：Full GC，指发生在老年代的 GC，出现了 Full GC 一般会伴随着至少一次的 Minor GC（老年代的对象大部分是 Scavenge GC 过程中从新生代进入老年代），比如：分配担保失败。FullGC 的速度一般会比 Scavenge GC 慢 10 倍以上。 当老年代内存不足或者显式调用 System.gc()方法时，会触发 Full GC。 当老年代或者持久代堆空间满了，会触发全收集操作。 可以使用 System.gc()方法来显式的启动全收集，全收集一般根据堆大小的不同，需要的时间不尽相同，但一般会比较长。 垃圾回收器的常规组合使用： Serial、ParNew、Parallel Scabenage构成新生代回收器。 Serial Old、Parallel Old、CMS是老年代回收器。 G1新老通用 分代垃圾收集器 串行收集器（Serial ）：JDK1.3之前JVM唯一一个次收集器（新生代收集器），1.5版本也是默认次收集器，它是串收集器。 Serial 收集器是 Hotspot 运行在 Client 模式下的默认新生代收集器。 它的特点是：只用一个 CPU（计算核心）/一条线程去完成 GC 工作, 且在进行垃圾收集时必须暂停其他所有的工作线程(“Stop The World” -后面简称 STW)。 可以使用-XX:+UseSerialGC 打开。虽然是单线程收集, 但它却简单而高效, 在 VM 管理内存不大的情况下(收集几十 M~一两百 M 的新生代), 停顿时间完全可以控制在几十毫秒~一百多毫秒内。 大多数收集器都是在串行收集器进行优化，减少他停顿的时间。 并行收集器（ParNew ）：ParNew 收集器其实是前面 Serial 的多线程版本,考虑用户等待的时间， 除使用多条线程进行 GC外, 包括 Serial可用的所有控制参数、收集算法、STW、对象分配规则、回收策略等都与 Serial 完全一样(也是VM启用 CMS 收集器-XX: +UseConcMarkSweepGC 的默认新生代收集器)。 由于存在线程切换的开销, ParNew 在单 CPU 的环境中比不上 Serial, 且在通过超线程技术实现的两个 CPU 的环境中也不能 100%保证能超越 Serial. 但随着可用的 CPU 数量的增加,收集效率肯定也会大大增加(ParNew 收集线程数与 CPU 的数量相同, 因此在 CPU 数量过大的环境中, 可用-XX:ParallelGCThreads=参数控制 GC 线程数，一般与CPU的线程数相同)。 Parallel Scavenge 收集器：与 ParNew 类似, Parallel Scavenge 也是使用复制算法, 也是并行多线程收集器. 但与其他收集器关注尽可能缩短垃圾收集时间不同, Parallel Scavenge 更关注系统吞吐量: 系统吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间) 停顿时间越短就越适用于用户交互的程序-良好的响应速度能提升用户的体验; 而高吞吐量则适用于后台运算而不需要太多交互的任务-可以最高效率地利用CPU时间,尽快地完成程序的运算任务. Parallel Scavenge 提供了如下参数设置系统吞吐量: Serial Old 收集器：Serial Old 是 Serial 收集器的老年代版本, 同样是单线程收集器,使用“标记-整理”算法 Parallel Old 收集器：Parallel Old 是 Parallel Scavenge 收集器的老年代版本, 使用多线程和“标记－整理”算法, 吞吐量优先, 主要与 Parallel Scavenge 配合在注重吞吐量及 CPU 资源敏感系统内使用； CMS 收集器 （Concurrent Mark Sweep ）：CMS(Concurrent Mark Sweep)收集器是一款具有划时代意义的收集器, 一款真正意义上的并发收集器, 虽然现在已经有了理论意义上表现更好的 G1 收集器, 但现在主流互联网企业线上选用的仍是 CMS(如 Taobao、微店)。 并发（concurrent）包含用户线程，并行（Parallel）不包含。 CMS是一种以获取最短回收停顿时间为目标的收集器(CMS又称多并发低暂停的收集器)，基于”标记-清除”算法实现， 整个 GC 过程分为以下 4 个步骤: 初始标记(CMS initial mark) 并发标记(CMS concurrent mark: GC Roots Tracing 过程) 重新标记(CMS remark) 并发清除(CMS concurrent sweep: 已死对象将会就地释放, 注意:此处没有压缩) 其中 1，3 两个步骤(初始标记、重新标记)仍需 STW. 但初始标记仅只标记一下 GC Roots能直接关联到的对象, 速度很快; 而重新标记则是为了修正并发标记期间因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录, 虽然一般比初始标记阶段稍长, 但要远小于并发标记时间。CMS 特点： CMS 默认启动的回收线程数=(CPU 数目+3)4，当 CPU 数&gt;4 时, GC线程一般占用不超过 25%的 CPU 资源, 但是当 CPU 数&lt;=4 时, GC线程可能就会过多的占用用户 CPU 资源, 从而导致应用程序变慢, 总吞吐量降低。 无法处理浮动垃圾, 可能出现 Promotion Failure、Concurrent Mode Failure 而导致另一次 Full GC 的产生: 浮动垃圾是指在 CMS 并发清理阶段用户线程运行而产生的新垃圾. 由于在 GC 阶段用户线程还需运行, 因此还需要预留足够的内存空间给用户线程使用, 导致 CMS不 能 像 其 他收 集 器那 样 等到 老 年 代几 乎 填满 了 再进 行 收 集. 因此 CMS 提 供 了-XX:CMSInitiatingOccupancyFraction 参 数 来 设 置 GC 的 触 发 百 分 比 ( 以 及-XX:+UseCMSInitiatingOccupancyOnly 来启用该触发百分比), 当老年代的使用空间超过该比例后 CMS 就会被触发(JDK 1.6 之后默认 92%). 但当 CMS 运行期间预留的内存无法满足程序需要, 就会出现上述 Promotion Failure 等失败, 这时 VM 将启动后备预案: 临时启用 Serial Old收集器来重新执行Full GC(CMS通常配合大内存使用, 一旦大内存转入串行的Serial GC, 那停顿的时间就是大家都不愿看到的了). 最后, 由于 CMS 采用”标记-清除”算法实现, 可能会产生大量内存碎片. 内存碎片过多 可 能 会 导 致 无 法 分 配 大 对 象 而 提 前 触 发 Full GC. 因 此 CMS 提 供 了-XX:+UseCMSCompactAtFullCollection 开关参数, 用于在 Full GC 后再执行一个碎片整理过程.但内存整理是无法并发的, 内存碎片问题虽然没有了, 但停顿时间也因此变长了, 因此 CMS还提供了另外一个参数-XX:CMSFullGCsBeforeCompaction 用于设置在执行 N 次不进行内存整理的 Full GC 后, 跟着来一次带整理的(默认为 0: 每次进入 Full GC 时都进行碎片整理). 分区收集- G1 收集器：G1(Garbage-First)是一款面向服务端应用的收集器, 主要目标用于配备多颗 CPU 的服务器治理大内存，-XX:+UseG1GC 启用 G1 收集器。 与其他基于分代的收集器不同, G1 将整个 Java 堆划分为多个大小相等的独立区域(Region), 虽然还保留有新生代和老年代的概念, 但新生代和老年代不再是物理隔离的了,它们都是一部分 Region(不需要连续)的集合.如： 每块区域既有可能属于 O 区、也有可能是 Y 区，因此不需要一次就对整个老年代/新生代回收。而是当线程并发寻找可回收的对象时，有些区块包含可回收的对象要比其他区块多很多。 虽然在清理这些区块时 G1 仍然需要暂停应用线程,，但可以用相对较少的时间优先回收垃圾较多的 Region。这种方式保证了 G1 可以在有限的时间内获取尽可能高的收集效率。 G1的新生代收集跟ParNew类似: 存活的对象被转移到一个或多个Survivor Regions.，如果存活时间达到阀值, 这部分对象就会被提升到老年代.如图： 其特定是： 一整块堆内存被分为多个 Regions. 存活对象被拷贝到新的 Survivor 区或老年代. 年轻代内存由一组不连续的 heap 区组成, 这种方法使得可以动态调整各代区域尺寸. Young GC 会有 STW 事件, 进行时所有应用程序线程都会被暂停. 多线程并发 GC. G1 老年代 GC 特点如下 : 并发标记阶段 在与应用程序并发执行的过程中会计算活跃度信息 . 这些活跃度信息标识出那些 regions 最适合在 STW 期间回收 (which regions will be bestto reclaim during an evacuation pause). 不像 CMS 有清理阶段 . 再次标记阶段 使用 Snapshot-at-the-Beginning(SATB) 算法比 CMS 快得多 . 空 region 直接被回收 . 拷贝 / 清理阶段 (Copying/Cleanup Phase) 年轻代与老年代同时回收 . 老年代内存回收会基于他的活跃度信息 化神圆满（JVM优化）JDK 常用 JVM 优化相关命令 jps jps - l：显示线程 id 和执行线程的主类名 jps -v：显示线程 id 和执行线程的主类名和 JVM 配置信息 jstat jstat -参数 线程 id 执行时间（单位毫秒） 执行次数 如：jstat -gc 4488 30 10 SXC - survivor 初始空间大小，单位字节。（X为survivor中X区域） SXU - survivor 使用空间大小， 单位字节。 EC - eden 初始空间大小 EU - eden 使用空间大小 OC - old 初始空间大小 OU - old 使用空间大小 PC - permanent 初始空间大小 PU - permanent 使用空间大小 YGC - youngGC 收集次数 YGCT - youngGC 收集使用时长， 单位秒 FGC - fullGC 收集次数 FGCT - fullGC 收集使用时长 GCT - 总计收集使用总时长 YGCT+FGCT jvisualvm：一个 JDK 内置的图形化 VM 监视管理工具。一般我们会在里面安装visualgc 插件。（工具、插件、可用插件），设置编辑url连接地址。 JVM 常见参数配置方式：java [options] MainClass [arguments]options - JVM 启动参数。 配置多个参数的时候，参数之间使用空格分隔。参数命名： 常见为 -参数名参数赋值： 常见为 -参数名=参数值 | -参数名:参数值 12345678910111213public class Test &#123; public static void main(String[] args) &#123; List&lt;GarbageCollectorMXBean&gt; l = ManagementFactory.getGarbageCollectorMXBeans(); for(GarbageCollectorMXBean b : l) &#123; System.out.println(b.getName()); // 收集器名称 &#125; try &#123; System.in.read(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 内存设置 -Xms:初始堆大小，JVM 启动的时候，给定堆空间大小。 -Xmx:最大堆大小，JVM 运行过程中，如果初始堆空间不足的时候，最大可以扩展到多少。 -Xmn：设置年轻代大小。整个堆大小=年轻代大小+年老代大小+持久代大小。持久代一般固定大小为 64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun 官方推荐配置为整个堆的 3/8。 -Xss： 设置每个线程的 Java 栈大小。JDK5.0 以后每个线程 Java 栈大小为 1M，以前每个线程堆栈大小为 256K。根据应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在 3000~5000 左右。 -XX:NewSize=n:设置年轻代大小 -XX:NewRatio=n:设置年轻代和年老代的比值。如:为 3，表示年轻代与年老代比值为 1：3，年轻代占整个年轻代+年老代和的 1/4 -XX:SurvivorRatio=n:年轻代中 Eden 区与两个 Survivor 区的比值。注意 Survivor 区有两个。如：3，表示 Eden：Survivor=3：2，一个 Survivor 区占整个年轻代的 1/5 -XX:MaxPermSize=n:设置持久代大小 -XX:MaxTenuringThreshold：设置垃圾最大年龄。如果设置为 0 的话，则年轻代对象不经过 Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在 Survivor 区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概率。 收集器设置 -XX:+UseSerialGC:设置串行收集器，年轻带收集器， 次收集器 -XX:+UseParallelGC:设置并行收集器 -XX:+UseParNewGC:设置年轻代为并行收集。可与 CMS 收集同时使用。JDK5.0 以上，JVM会根据系统配置自行设置，所以无需再设置此值。 -XX:+UseParallelOldGC:设置并行年老代收集器，JDK6.0 支持对年老代并行收集。 -XX:+UseConcMarkSweepGC:设置年老代并发收集器，测试中配置这个以后，-XX:NewRatio的配置失效，原因不明。所以，此时年轻代大小最好用-Xmn 设置。 -XX:+UseG1GC:设置 G1 收集器 垃圾回收统计信息，类似日志的配置信息。会有控制台相关信息输出。 商业项目上线的时候，使用 loggc -XX:+PrintGC -XX:+Printetails -XX:+PrintGCTimeStamps -Xloggc:filename 并行收集器设置 -XX:ParallelGCThreads=n:设置并行收集器收集时最大线程数使用的 CPU 数。并行收集线程数。 -XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间，单位毫秒。可以减少 STW 时间。 -XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为 1/(1+n)并发收集器设置 -XX:+CMSIncrementalMode:设置为增量模式。适用于单 CPU 情况。 -XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动选择年轻代区大小和相应的 Survivor 区比例，以达到目标系统规定的最低相应时间或者收集频率等，此值建议使用并行收集器时，一直打开。 -XX:CMSFullGCsBeforeCompaction=n：由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此值设置运行多少次 GC 以后对内存空间进行压缩、整理。 -XX:+UseCMSCompactAtFullCollection：打开对年老代的压缩。可能会影响性能，但是可以消除碎片 内存设置经验分享 JVM 中最大堆大小有三方面限制： 相关操作系统的数据模型（32-bt 还是 64-bit）限制； 系统的可用虚拟内存限制； 系统的可用物理内存限制。 32 位系统 下，一般限制在 1.5G~2G；64 为操作系统对内存无限制。 Tomcat 配置方式： 编写 catalina.bat|catalina.sh ，增加 JAVA_OPTS 参数设置。 windows和 linux 配置方式不同。 windows - set “JAVA_OPTS=%JAVA_OPTS% 自定义参数 “ ； linux -JAVA_OPTS=”$JAVA_OPTS 自定义参数 “常见设置： -Xmx3550m -Xms3550m -Xmn2g -Xss128k 适合开发过程的测试应用。要求物理内存大于4G。 设置JVM最大可用内存与初始内存相同，可以避免每次垃圾完成后JVM重新分配内存。 -Xmn2g，设置年轻代为2个g，持久代一般固定大小为64m，所以增大年轻代后，将会减小老年代大小。官方推荐设置成整个堆的3/8。 -Xss128k ，设置每个线程的堆栈大小，JDK1.5后每个线程栈大小为1M，以前每个线程栈大小为256k。在相同物理内存下，减少这个值能生成更多的线程，但是操作系统对进程内的线程数量是有限的，不能无线生成，经验值在3000~5000左右。 收集器设置经验分享 关于收集器的选择 JVM 给了三种选择：串行收集器、并行收集器、并发收集器，但是串行收集器只适用于小数据量的情况，所以这里的选择主要针对并行收集器和并发收集器。默认情况下，JDK5.0 以前都是使用串行收集器，如果想使用其他收集器需要在启动时加入相应参数。JDK5.0 以后，JVM 会根据当前系统配置进行判断。 常见配置： 并行收集器主要以到达一定的吞吐量为目标，适用于科学计算和后台处理等。 -Xmx3800m -Xms3800m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20 使用 ParallelGC 作为并行收集器， GC 线程为 20（CPU 核心数&gt;=20 时），内存问题根据硬件配置具体提供。建议使用物理内存的 80%左右作为 JVM 内存容量。 -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20-XX:+UseParallelOldGC 指定老年代收集器，在JDK5.0之后的版本，ParallelGC对应的全收集器就是ParallelOldGC。可以忽略 -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:MaxGCPauseMillis=100 指定 GC 时最大暂停时间。单位是毫秒。每次 GC 最长使用 100 毫秒。可以尽可能提高工作线程的执行资源。 -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:MaxGCPauseMillis=100-XX:+UseAdaptiveSizePolicy UseAdaptiveSizePolicy 是提高年轻代 GC 效率的配置。次收集器执行效率。 并发收集器主要是保证系统的响应时间，减少垃圾收集时的停顿时间。适用于应用服务器、电信领域、互联网领域等。 -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:ParallelGCThreads=20-XX:+UseConcMarkSweepGC -XX:+UseParNewGC 指定年轻代收集器为 ParNew，年老代收集器 ConcurrentMarkSweep，并发 GC 线程数为20（CPU 核心&gt;=20），并发 GC 的线程数建议使用（CPU 核心数+3）/4 或 CPU 核心数【不推荐使用】。 -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseConcMarkSweepGC-XX:CMSFullGCsBeforeCompaction=5 -XX:+UseCMSCompactAtFullCollection CMSFullGCsBeforeCompaction=5 执行 5 次 GC 后，运行一次内存的整理。 UseCMSCompactAtFullCollection 执行老年代内存整理。可以避免内存碎片，提高 GC 过程中的效率，减少停顿时间。 简单总结 年轻代大小选择 响应时间优先的应用：尽可能设大，直到接近系统的最低响应时间限制（根据实际情况选择）。在此种情况下，年轻代收集发生的频率也是最小的。同时，减少到达年老代的对象。 吞吐量优先的应用：尽可能的设置大，可能到达 Gbit 的程度。因为对响应时间没有要求，垃圾收集可以并行进行，一般适合 8CPU 以上的应用。 年老代大小选择 响应时间优先的应用： 年老代使用并发收集器，所以其大小需要小心设置，一般要考虑并发会话率和会话持续时间等一些参数。如果堆设置小了，可以会造成内存碎片、高回收频率以及应用暂停而使用传统的标记清除方式；如果堆大了，则需要较 长的收集时间。最优化的方案，一般需要参考以下数据获得： 并发垃圾收集信息 持久代并发收集次数 传统 GC 信息 花在年轻代和年老代回收上的时间比例 减少年轻代和年老代花费的时间，一般会提高应用的效率 吞吐量优先的应用：一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代。原因是，这样可以尽可能回收掉大部分短期对象，减少中期的对象，而年老代存放长期存活对象。 较小堆引起的碎片问题，因为年老代的并发收集器使用标记、清除算法，所以不会对堆进行压缩。当收集器回收时，他会把相邻的空间进行合并，这样可以分配给较大的对象。但是，当堆空间较小时，运行一段时间以后，就会出现“碎片”，如果并发收集器找不到足够的空间，那么并发收集器将会停止，然后使用传统的标记、整理方式进行回收。如果出现“碎片”，可能需要进行如下配置： -XX:+UseCMSCompactAtFullCollection：使用并发收集器时，开启对年老代的压缩。 -XX:CMSFullGCsBeforeCompaction=0：上面配置开启的情况下，这里设置多少次 Full GC后，对年老代进行压缩 小编是一枚Java Coder，业余写文章，现主营微信公众号《Java患者》，喜欢的话关注我的公众号或者加我微信我们一起学习Java 合体期（网络编程）合体前期（Socket）​ 首先注意，Socket不是Java中独有的概念，而是一个语言无关标准。任何可以实现网络编程的编程语言都有Socket。 什么是 Socket? ​ 网络上的两个程序通过一个双向的通信连接实现数据的交换，这个连接的一端称为一个socket。 ​ 建立网络通信连接至少要一个端口号。socket 本质是编程接口(API)，对 TCP/IP 的封装，TCP/IP 也要提供可供程序员做网络开发所用的接口，这就是 Socket 编程接口；HTTP 是轿车， 提供了封装或者显示数据的具体形式；Socket 是发动机，提供了网络通信的能力。 ​ Socket 的英文原义是“孔”或“插座”。作为 BSD UNIX 的进程通信机制，取后一种意思。通 常也称作“套接字“，用于描述 IP 地址和端口，是一个通信链的句柄，可以用来实现不同虚 拟机或不同计算机之间的通信。在 Internet 上的主机一般运行了多个服务软件，同时提供几 种服务。每种服务都打开一个 Socket，并绑定到一个端口上，不同的端口对应于不同的服务。Socket 正如其英文原义那样，像一个多孔插座。一台主机犹如布满各种插座的房间，每个插 座有一个编号，有的插座提供 220 伏交流电， 有的提供 110 伏交流电，有的则提供有线电 视节目。 客户软件将插头插到不同编号的插座，就可以得到不同的服务。 Socket 连接步骤 ​ 根据连接启动的方式以及本地套接字要连接的目标，套接字之间的连接过程可以分为三个步骤：服务器监听，客户端请求，连接确认。【如果包含数据交互+断开连接，那么一共是 五个步骤】 服务器监听：是服务器端套接字并不定位具体的客户端套接字，而是处于等待连接的状态，实时监控网络状态。 客户端请求：是指由客户端的套接字提出连接请求，要连接的目标是服务器端的套接字。为此，客户端的套接字必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址和端口号，然后就向服务器端套接字提出连接请求。 连接确认(三层握手)：是指当服务器端套接字监听到或者说接收到客户端套接字的连接请求【1】，它就响应客户端套接字的请求，建立一个新的线程，把服务器端套接字的描述发给客户端【2】，一旦客户端确认了此描述，连接就建立好了【3】。而服务器端套接字继续处于监听状态，继续接收其他客户端套接字的连接请求。 断开连接：客户端向服务发起一个请求关闭消息【1】，服务器根据自己状态，等到可以关闭时候，发一个可以关闭的消息给客户端【2】，并且服务器再向浏览器发一个关闭成功的消息【3】，客户端发一个光笔成功的消息，至于服务器可以收到不管【4】。 Java 中的 Socket ​ 在java.net包是网络编程的基础类库。其中ServerSocket和Socket是网络编程的基础类型ServerSocket是服务端应用类型。Socket是建立连接的类型。当连接建立成功后，服务器和客户端都会有一个 Socket对象示例，可以通过这个Socket 对象示例，完成会话的所有操作。 ​ 对于一个完整的网络连接来说，Socket是平等的，没有服务器客户端分级情况 什么是同步和异步 ​ 同步和异步是针对应用程序和内核OS的交互而言的，同步指的是用户进程触发 IO 操作并 等待或者轮询的去查看 IO 操作是否就绪，而异步是指用户进程触发 IO 操作以后便开始做自 己的事情，而当 IO 操作已经完成的时候会得到 IO 完成的通知，异步是OS底层支持的一种操作。以银行取款为例： 同步 ： 自己亲自出马持银行卡到银行取钱（使用同步 IO 时，Java 自己处理 IO 读写）； 异步 ： 委托一小弟拿银行卡到银行取钱，然后给你（使用异步 IO 时，Java 将 IO 读写 委托给 OS 处理，需要将数据缓冲区地址和大小传给 OS(银行卡和密码)，OS 需要支持异步 IO 操作 API） 什么是阻塞和非阻塞 ​ 阻塞和非阻塞是针对于进程在访问数据的时候，根据 IO 操作的就绪状态来采取的不同 方式，说白了是一种读取或者写入操作方法的实现方式，阻塞方式下读取或者写入函数将一直等待，而非阻塞方式下，读取或者写入方法会立即返回一个状态值。 以银行取款为例： 阻塞 ： ATM 排队取款，你只能等待（使用阻塞 IO 时，Java 调用会一直阻塞到读写完 成才返回）； 非阻塞 ： 柜台取款，取个号，然后坐在椅子上做其它事，等号广播会通知你办理，没到号你就不能去，你可以不断问大堂经理排到了没有，大堂经理如果说还没到你就不能去（使用非阻塞 IO 时，如果不能读写 Java 调用会马上返回，当 IO 事件分发器通知可读写时再继 续进行读写，不断循环直到读写完成） ajax是异步阻塞的。 合体中期（BIO、NIO 、AIO） BIO 编程 ：Blocking IO同步阻塞的编程方式。 BIO 编程方式通常是在 JDK1.4 版本之前常用的编程方式。编程实现过程为：首先在服务端启动一个 ServerSocket 来监听网络请求，客户端启动 Socket 发起网络请求，默认情况下 ServerSocket 回建立一个线程来处理此请求，如果服务端没有线程可用，客户端则会阻塞等待或遭到拒绝。 且建立好的连接，在通讯过程中，是同步的。在并发处理效率上比较低。大致结构如下： 每次请求都要创建一个server socket和一个thread 同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就 需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可 以通过线程池机制改善（有人把这种叫做伪异步，实际上不能实现任何的异步的操作，归根还是同步）。 BIO 方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4 以前的唯一选择，但程序直观简单易理解。 使用线程池机制改善后的 BIO 模型图如下: Client： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class Client &#123; public static void main(String[] args) &#123; String host = null; int port = 0; if(args.length &gt; 2)&#123; host = args[0]; port = Integer.parseInt(args[1]); &#125;else&#123; host = "127.0.0.1"; port = 9999; &#125; Socket socket = null; BufferedReader reader = null; PrintWriter writer = null; Scanner s = new Scanner(System.in); try&#123; socket = new Socket(host, port); String message = null; reader = new BufferedReader( new InputStreamReader(socket.getInputStream(), "UTF-8")); writer = new PrintWriter( socket.getOutputStream(), true); while(true)&#123; message = s.nextLine(); if(message.equals("exit"))&#123; break; &#125; writer.println(message); writer.flush(); System.out.println(reader.readLine()); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(socket != null)&#123; try &#123; socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; socket = null; if(reader != null)&#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; reader = null; if(writer != null)&#123; writer.close(); &#125; writer = null; &#125; &#125;&#125; server 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495public class Server &#123; public static void main(String[] args) &#123; int port = genPort(args); ServerSocket server = null; try&#123; server = new ServerSocket(port); System.out.println("server started!"); while(true)&#123; Socket socket = server.accept(); new Thread(new Handler(socket)).start(); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(server != null)&#123; try &#123; server.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; server = null; &#125; &#125; static class Handler implements Runnable&#123; Socket socket = null; public Handler(Socket socket)&#123; this.socket = socket; &#125; @Override public void run() &#123; BufferedReader reader = null; PrintWriter writer = null; try&#123; reader = new BufferedReader( new InputStreamReader(socket.getInputStream(), "UTF-8")); writer = new PrintWriter( new OutputStreamWriter(socket.getOutputStream(), "UTF-8")); String readMessage = null; while(true)&#123; System.out.println("server reading... "); if((readMessage = reader.readLine()) == null)&#123; break; &#125; System.out.println(readMessage); writer.println("server recive : " + readMessage); writer.flush(); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(socket != null)&#123; try &#123; socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; socket = null; if(reader != null)&#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; reader = null; if(writer != null)&#123; writer.close(); &#125; writer = null; &#125; &#125; &#125; private static int genPort(String[] args)&#123; if(args.length &gt; 0)&#123; try&#123; return Integer.parseInt(args[0]); &#125;catch(NumberFormatException e)&#123; return 9999; &#125; &#125;else&#123; return 9999; &#125; &#125;&#125; ThreadPool版的Server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596public class Server &#123; public static void main(String[] args) &#123; int port = genPort(args); ServerSocket server = null; ExecutorService service = Executors.newFixedThreadPool(50); try&#123; server = new ServerSocket(port); System.out.println("server started!"); while(true)&#123; Socket socket = server.accept(); service.execute(new Handler(socket)); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(server != null)&#123; try &#123; server.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; server = null; &#125; &#125; static class Handler implements Runnable&#123; Socket socket = null; public Handler(Socket socket)&#123; this.socket = socket; &#125; @Override public void run() &#123; BufferedReader reader = null; PrintWriter writer = null; try&#123; reader = new BufferedReader( new InputStreamReader(socket.getInputStream(), "UTF-8")); writer = new PrintWriter( new OutputStreamWriter(socket.getOutputStream(), "UTF-8")); String readMessage = null; while(true)&#123; System.out.println("server reading... "); if((readMessage = reader.readLine()) == null)&#123; break; &#125; System.out.println(readMessage); writer.println("server recive : " + readMessage); writer.flush(); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(socket != null)&#123; try &#123; socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; socket = null; if(reader != null)&#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; reader = null; if(writer != null)&#123; writer.close(); &#125; writer = null; &#125; &#125; &#125; private static int genPort(String[] args)&#123; if(args.length &gt; 0)&#123; try&#123; return Integer.parseInt(args[0]); &#125;catch(NumberFormatException e)&#123; return 9999; &#125; &#125;else&#123; return 9999; &#125; &#125;&#125; NIO 编程 Unblocking IO（New IO）： 同步非阻塞的编程方式。 NIO 本身是基于事件驱动思想来完成的，其主要想解决的是 BIO 的大并发问题，NIO 基于Reactor，当 socket 有流可读或可写入 socket 时，操作系统会相应的通知应用程序进行处理，应用再将流读取到缓冲区或写入操作系统。也就是说，这个时候，已经不是一个连接就要对应一个处理线程了，而是有效的请求，对应一个线程，当连接没有数据时，是没有工作线程来处理的（即一个线程对应多个有效请求）。 NIO 的最重要的地方是当一个连接创建后，不需要对应一个线程，这个连接会被注册到多路复用器上面，所以所有的连接只需要一个线程就可以搞定，当这个线程中的多路复用器进行轮询的时候，发现连接上有请求的话，才开启一个线程进行处理，也就是一个请求一个线程模式，并且面向缓存。 在 NIO 的处理方式中，当一个请求来的话，开启线程进行处理，可能会等待后端应用的资源(JDBC 连接等待)，其实这个线程就被阻塞了，当并发上来的话，还是会有 BIO 一样的问题。 同步非阻塞，服务器实现模式为一个请求一个通道，即客户端发送的连接请求都会注册 到多路复用器上，多路复用器轮询到连接有 I/O 请求时才启动一个线程进行处理。 NIO 方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局 限于应用中，编程复杂，JDK1.4 开始支持。 Buffer:ByteBuffer,CharBuffer,ShortBuffer,IntBuffer,LongBuffer,FloatBuffer,DoubleBuffer*。* Channel:SocketChannel,ServerSocketChannel*。* Selector:Selector,AbstractSelector SelectionKey:OP_READ,OP_WRITE,OP_CONNECT,OP_ACCEPT client 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class NIOClient &#123; public static void main(String[] args) &#123; // 远程地址创建 InetSocketAddress remote = new InetSocketAddress("localhost", 9999); SocketChannel channel = null; // 定义缓存。 ByteBuffer buffer = ByteBuffer.allocate(1024); try &#123; // 开启通道 channel = SocketChannel.open(); // 连接远程服务器。 channel.connect(remote); Scanner reader = new Scanner(System.in); while(true)&#123; System.out.print("put message for send to server &gt; "); String line = reader.nextLine(); if(line.equals("exit"))&#123; break; &#125; // 将控制台输入的数据写入到缓存。 buffer.put(line.getBytes("UTF-8")); // 重置缓存游标 buffer.flip(); // 将数据发送给服务器 channel.write(buffer); // 清空缓存数据。 buffer.clear(); // 读取服务器返回的数据 int readLength = channel.read(buffer); if(readLength == -1)&#123; break; &#125; // 重置缓存游标 buffer.flip(); byte[] datas = new byte[buffer.remaining()]; // 读取数据到字节数组。 buffer.get(datas); System.out.println("from server : " + new String(datas, "UTF-8")); // 清空缓存。 buffer.clear(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally&#123; if(null != channel)&#123; try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168public class NIOServer implements Runnable &#123; // 多路复用器， 选择器。 用于注册通道的。 private Selector selector; // 定义了两个缓存。分别用于读和写。 初始化空间大小单位为字节。 // Buffer1是不安全的，要想安全模仿BIO，独立定义为Handler对象 private ByteBuffer readBuffer = ByteBuffer.allocate(1024); private ByteBuffer writeBuffer = ByteBuffer.allocate(1024); public static void main(String[] args) &#123; new Thread(new NIOServer(9999)).start(); &#125; public NIOServer(int port) &#123; init(port); &#125; private void init(int port)&#123; try &#123; System.out.println("server starting at port " + port + " ..."); // 开启多路复用器 this.selector = Selector.open(); // 开启服务通道 ServerSocketChannel serverChannel = ServerSocketChannel.open(); // 非阻塞， 如果传递参数true，为阻塞模式。 serverChannel.configureBlocking(false); // 绑定端口 serverChannel.bind(new InetSocketAddress(port)); // 注册，并标记当前服务通道状态 /* * register(Selector, int) * int - 状态编码 * OP_ACCEPT ： 连接成功的标记位。 * OP_READ ： 可以读取数据的标记 * OP_WRITE ： 可以写入数据的标记 * OP_CONNECT ： 连接建立后的标记 */ serverChannel.register(this.selector, SelectionKey.OP_ACCEPT); System.out.println("server started."); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public void run()&#123; while(true)&#123; try &#123; // 阻塞方法，当至少一个通道被选中，此方法返回。 // 通道是否选择，由注册到多路复用器中的通道标记决定。 this.selector.select(); // 返回以选中的通道标记集合， 集合中保存的是通道的标记。相当于是通道的ID。 Iterator&lt;SelectionKey&gt; keys = this.selector.selectedKeys().iterator(); while(keys.hasNext())&#123; SelectionKey key = keys.next(); // 将本次要处理的通道从集合中删除，下次循环根据新的通道列表再次执行必要的业务逻辑 keys.remove(); // 通道是否有效 if(key.isValid())&#123; // 阻塞状态 try&#123; if(key.isAcceptable())&#123; accept(key); &#125; &#125;catch(CancelledKeyException cke)&#123; // 断开连接。 出现异常。 key.cancel(); &#125; // 可读状态 try&#123; if(key.isReadable())&#123; read(key); &#125; &#125;catch(CancelledKeyException cke)&#123; key.cancel(); &#125; // 可写状态 try&#123; if(key.isWritable())&#123; write(key); &#125; &#125;catch(CancelledKeyException cke)&#123; key.cancel(); &#125; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private void write(SelectionKey key)&#123; this.writeBuffer.clear(); SocketChannel channel = (SocketChannel)key.channel(); Scanner reader = new Scanner(System.in); try &#123; System.out.print("put message for send to client &gt; "); String line = reader.nextLine(); // 将控制台输入的字符串写入Buffer中。 写入的数据是一个字节数组。 writeBuffer.put(line.getBytes("UTF-8")); writeBuffer.flip(); channel.write(writeBuffer); channel.register(this.selector, SelectionKey.OP_READ); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private void read(SelectionKey key)&#123; try &#123; // 清空读缓存。 this.readBuffer.clear(); // 获取通道 SocketChannel channel = (SocketChannel)key.channel(); // 将通道中的数据读取到缓存中。通道中的数据，就是客户端发送给服务器的数据。 int readLength = channel.read(readBuffer); // 检查客户端是否写入数据。 if(readLength == -1)&#123; // 关闭通道 key.channel().close(); // 关闭连接 key.cancel(); return; &#125; /* * flip， NIO中最复杂的操作就是Buffer的控制。 * Buffer中有一个游标。游标信息在操作后不会归零，如果直接访问Buffer的话，数据有不一致的可能。 * flip是重置游标的方法。NIO编程中，flip方法是常用方法。 */ this.readBuffer.flip(); // 字节数组，保存具体数据的。 Buffer.remaining() -&gt; 是获取Buffer中有效数据长度的方法。 byte[] datas = new byte[readBuffer.remaining()]; // 是将Buffer中的有效数据保存到字节数组中。 readBuffer.get(datas); System.out.println("from " + channel.getRemoteAddress() + " client : " + new String(datas, "UTF-8")); // 注册通道， 标记为写操作。 channel.register(this.selector, SelectionKey.OP_WRITE); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); try &#123; key.channel().close(); key.cancel(); &#125; catch (IOException e1) &#123; e1.printStackTrace(); &#125; &#125; &#125; private void accept(SelectionKey key)&#123; try &#123; // 此通道为init方法中注册到Selector上的ServerSocketChannel ServerSocketChannel serverChannel = (ServerSocketChannel)key.channel(); // 阻塞方法，当客户端发起请求后返回。 此通道和客户端一一对应。 SocketChannel channel = serverChannel.accept(); channel.configureBlocking(false); // 设置对应客户端的通道标记状态，此通道为读取数据使用的。 channel.register(this.selector, SelectionKey.OP_READ); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; TestBuffer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * * Buffer的应用固定逻辑 * 写操作顺序 * 1. clear() * 2. put() -&gt; 写操作 * 3. flip() -&gt; 重置游标 * 4. SocketChannel.write(buffer); -&gt; 将缓存数据发送到网络的另一端 * 5. clear() * * 读操作顺序 * 1. clear() * 2. SocketChannel.read(buffer); -&gt; 从网络中读取数据 * 3. buffer.flip() -&gt; 重置游标 * 4. buffer.get() -&gt; 读取数据 * 5. buffer.clear() * */public class TestBuffer &#123; public static void main(String[] args) throws Exception &#123; ByteBuffer buffer = ByteBuffer.allocate(8); byte[] temp = new byte[]&#123;3,2,1&#125;; // 写入数据之前 ： java.nio.HeapByteBuffer[pos=0 lim=8 cap=8] // pos - 游标位置， lim - 限制数量， cap - 最大容量 System.out.println("写入数据之前 ： " + buffer); // 写入字节数组到缓存 buffer.put(temp); // 写入数据之后 ： java.nio.HeapByteBuffer[pos=3 lim=8 cap=8] // 游标为3， 限制为8， 容量为8，默认限制与容量一样大小 System.out.println("写入数据之后 ： " + buffer); // 重置游标 ， lim = pos ; pos = 0; buffer.flip();//把这行注释掉后，下面的循环就是5次了。 //在重置一次，pos为0，lim（可读写操作有效数据位数）为0，get会报错，也不让写。要写要clear // 重置游标之后 ： java.nio.HeapByteBuffer[pos=0 lim=3 cap=8] // 游标为0， 限制为3， System.out.println("重置游标之后 ： " + buffer); // 清空Buffer， pos = 0; lim = cap; // buffer.clear(); // get() -&gt; 获取当前游标指向的位置的数据。 // System.out.println(buffer.get()); // remaining是lim-pos /*for(int i = 0; i &lt; buffer.remaining(); i++)&#123; // get(int index) -&gt; 获取指定位置的数据。 int data = buffer.get(i); System.out.println(i + " - " + data); &#125;*/ &#125;&#125; AIO 编程 AsynchronousIO： 异步非阻塞的编程方式 与 NIO 不同，当进行读写操作时，只须直接调用 API 的 read 或 write 方法即可。这两种 方法均为异步的，对于读操作而言，当有流可读取时，操作系统会将可读的流传入 read 方 法的缓冲区，并通知应用程序；对于写操作而言，当操作系统将 write 方法传递的流写入完 毕时，操作系统主动通知应用程序。即可以理解为，read/write 方法都是异步的，完成后会 主动调用回调函数。 客户端向服务端发数据，首先是OS接收到，他会将数据写到buffer里，然后通知应用程序代码，数据已经准备好了，可以read拿走了。应用代码write时，也会不数据同步进入OS的Buffer里，通过反向通知告诉应用程序已经写完了。buffer数据会自动地反回给client。client与server交互借助OS实现异步操作。 在 JDK1.7 中，这部分内容被称作 NIO.2，主要在 java.nio.channels 包下增加了下面四个异步通道： AsynchronousSocketChannel AsynchronousServerSocketChannel AsynchronousFileChannel AsynchronousDatagramChannel 异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的 I/O 请求都是由 OS 先完成了再通知服务器应用去启动线程进行处理。 AIO 方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调 用 OS 参与并发操作，编程比较复杂，JDK7 开始支持。 server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class AIOServer &#123; // 线程池， 提高服务端效率。 private ExecutorService service; // 线程组 // private AsynchronousChannelGroup group; // 服务端通道， 针对服务器端定义的通道。 private AsynchronousServerSocketChannel serverChannel; public AIOServer(int port)&#123; init(9999); &#125; private void init(int port)&#123; try &#123; System.out.println("server starting at port : " + port + " ..."); // 定长线程池 service = Executors.newFixedThreadPool(4); /* 使用线程组 group = AsynchronousChannelGroup.withThreadPool(service); serverChannel = AsynchronousServerSocketChannel.open(group); */ // 开启服务端通道， 通过静态方法创建的。 serverChannel = AsynchronousServerSocketChannel.open(); // 绑定监听端口， 服务器启动成功，但是未监听请求。 serverChannel.bind(new InetSocketAddress(port)); System.out.println("server started."); // 开始监听 // accept(T attachment, CompletionHandler&lt;AsynchronousSocketChannel, ? super T&gt;) // AIO开发中，监听是一个类似递归的监听操作。每次监听到客户端请求后，都需要处理逻辑开启下一次的监听。 // 下一次的监听，需要服务器的资源继续支持。this传递到AIOServerHandler的completed方法中 serverChannel.accept(this, new AIOServerHandler()); try &#123; TimeUnit.SECONDS.sleep(Integer.MAX_VALUE); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; new AIOServer(9999); &#125; public ExecutorService getService() &#123; return service; &#125; public void setService(ExecutorService service) &#123; this.service = service; &#125; public AsynchronousServerSocketChannel getServerChannel() &#123; return serverChannel; &#125; public void setServerChannel(AsynchronousServerSocketChannel serverChannel) &#123; this.serverChannel = serverChannel; &#125;&#125; AIOServerHandler 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public class AIOServerHandler implements CompletionHandler&lt;AsynchronousSocketChannel, AIOServer&gt; &#123; /** * 业务处理逻辑， 当请求到来后，监听成功，应该做什么。 * 一定要实现的逻辑： 为下一次客户端请求开启监听。accept方法调用。 * result参数 ： 就是和客户端直接建立关联的通道。 * 无论BIO、NIO、AIO中，一旦连接建立，两端是平等的。 * result中有通道中的所有相关数据。如：OS操作系统准备好的读取数据缓存，或等待返回数据的缓存。 */ @Override public void completed(AsynchronousSocketChannel result, AIOServer attachment) &#123; // 处理下一次的客户端请求。类似递归逻辑。 attachment.getServerChannel().accept(attachment, this); doRead(result); &#125; /** * 异常处理逻辑， 当服务端代码出现异常的时候，做什么事情。 */ @Override public void failed(Throwable exc, AIOServer attachment) &#123; exc.printStackTrace(); &#125; /** * 真实项目中，服务器返回的结果应该是根据客户端的请求数据计算得到的。不是等待控制台输入的。 * @param result */ private void doWrite(AsynchronousSocketChannel result)&#123; try &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); System.out.print("enter message send to client &gt; "); Scanner s = new Scanner(System.in); String line = s.nextLine(); buffer.put(line.getBytes("UTF-8")); // 重点：必须复位，必须复位，必须复位 buffer.flip(); // write方法是一个异步操作。具体实现由OS实现。 可以增加get方法，实现阻塞，等待OS的写操作结束。 result.write(buffer); // result.write(buffer).get(); // 调用get代表服务端线程阻塞，等待写操作完成 &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125;/* catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125;*/ &#125; private void doRead(final AsynchronousSocketChannel channel)&#123; ByteBuffer buffer = ByteBuffer.allocate(1024); /* * 异步读操作， read(Buffer destination, A attachment, * CompletionHandler&lt;Integer, ? super A&gt; handler) * destination - 目的地， 是处理客户端传递数据的中转缓存。 可以不使用。 * attachment - 处理客户端传递数据的对象。 通常使用Buffer处理。 * handler - 处理逻辑 */ channel.read(buffer, buffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123; /** * 业务逻辑，读取客户端传输数据 * attachment - 在completed方法执行的时候，OS已经将客户端请求的数据写入到Buffer中了。 * 但是未复位（flip）。 使用前一定要复位。 */ @Override public void completed(Integer result, ByteBuffer attachment) &#123; try &#123; System.out.println(attachment.capacity()); // 复位 attachment.flip(); System.out.println("from client : " + new String(attachment.array(), "UTF-8")); doWrite(channel); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; exc.printStackTrace(); &#125; &#125;); &#125;&#125; client 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class AIOClient &#123; private AsynchronousSocketChannel channel; public AIOClient(String host, int port)&#123; init(host, port); &#125; private void init(String host, int port)&#123; try &#123; // 开启通道 channel = AsynchronousSocketChannel.open(); // 发起请求，建立连接。 channel.connect(new InetSocketAddress(host, port)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public void write(String line)&#123; try &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); buffer.put(line.getBytes("UTF-8")); buffer.flip(); channel.write(buffer); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; &#125; public void read()&#123; ByteBuffer buffer = ByteBuffer.allocate(1024); try &#123; // read方法是异步方法，OS实现的。get方法是一个阻塞方法，会等待OS处理结束后再返回，要不代码不等待，下面没数据打印出来。真实开发可以不加，其结果依靠OS自己去等待，拿到数据再返回通知， channel.read(buffer).get(); // channel.read(dst, attachment, handler); buffer.flip(); System.out.println("from server : " + new String(buffer.array(), "UTF-8")); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; &#125; public void doDestory()&#123; if(null != channel)&#123; try &#123; channel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; AIOClient client = new AIOClient("localhost", 9999); try&#123; System.out.print("enter message send to server &gt; "); Scanner s = new Scanner(System.in); String line = s.nextLine(); client.write(line); client.read(); &#125;finally&#123; client.doDestory(); &#125; &#125;&#125; 合体后期（Netty） 简介 ​ Netty 是由 JBOSS 提供的一个 java 开源框架。Netty 提供异步的、事件驱动的网络应用 程序框架和工具，用以快速开发高性能、高可靠性的网络服务器和客户端程序。 它是建立再NIO和AIO基础之上的。 ​ 也就是说，Netty 是一个基于 NIO 的客户、服务器端编程框架，使用 Netty 可以确保你 快速和简单的开发出一个网络应用，例如实现了某种协议的客户，服务端应用。Netty 相当 简化和流线化了网络应用的编程开发过程，例如，TCP 和 UDP 的 socket 服务开发。 ​ “快速”和“简单”并不用产生维护性或性能上的问题。Netty 是一个吸收了多种协议的实 现经验，这些协议包括 FTP,SMTP,HTTP，各种二进制，文本协议，并经过相当精心设计的项 目，最终，Netty* 成功的找到了一种方式，在保证易于开发的同时还保证了其应用的性能， 稳定性和伸缩性。 ​ Netty 从 4.x 版本开始，需要使用 JDK1.6 及以上版本提供基础支撑。 ​ 在设计上：针对多种传输类型的统一接口 - 阻塞和非阻塞；简单但更强大的线程模型； 真正的无连接的数据报套接字支持；链接逻辑支持复用； ​ 在性能上：比核心 Java API 更好的吞吐量，较低的延时；资源消耗更少，这个得益于 共享池和重用；减少内存拷贝 ​ 在健壮性上：消除由于慢，快，或重载连接产生的 OutOfMemoryError；消除经常发现 在 NIO 在高速网络中的应用中的不公平的读/写比 ​ 在安全上：完整的 SSL / TLS 和 StartTLS 的支持 ​ 且已得到大量商业应用的真实验证,如：Hadoop 项目的 Avro（RPC 框架）、Dubbo、Dubbox等 RPC 框架。 ​ Netty 的官网是：http://netty.io ​ 有 三 方 提 供 的 中 文 翻 译 Netty 用 户 手 册 （ 官 网 提 供 源 信 息 ）： http://ifeve.com/netty5-user-guide/ Netty 架构 线程模型（acceptor是一个监听线程） .jpg) 单线程模型 ​ 在 ServerBootstrap 调用方法 group 的时候，传递的参数是同一个线程组，且在构造线程 组的时候，构造参数为 1，这种开发方式，就是一个单线程模型。 个人机开发测试使用。不推荐。 ​ 在处理acceptor和runnable task的线程组合并成一个，并且只有一个线程。 123456// 初始化线程组,构建线程组的时候，如果不传递参数，则默认构建的线程组线程数是CPU核心数量。acceptorGroup = new NioEventLoopGroup(1);// 初始化服务的配置bootstrap = new ServerBootstrap();// 绑定线程组bootstrap.group(acceptorGroup, acceptorGroup); 多线程模型 ​ 在 ServerBootstrap 调用方法 group 的时候，传递的参数是两个不同的线程组。负责监听 的 acceptor 线程组，线程数为 1，也就是构造参数为 1。负责处理客户端任务的线程组，线 程数大于 1，也就是构造参数大于 1。这种开发方式，就是多线程模型。 ​ 长连接，且客户端数量较少，连接持续时间较长情况下使用。如：企业内部交流应用。 1234567// 初始化线程组,构建线程组的时候，如果不传递参数，则默认构建的线程组线程数是CPU核心数量。acceptorGroup = new NioEventLoopGroup(1);clientGroup = new NioEventLoopGroup(&gt;1);// 初始化服务的配置bootstrap = new ServerBootstrap();// 绑定线程组bootstrap.group(acceptorGroup, clientGroup); 主从多线程模型 ​ 在 ServerBootstrap 调用方法 group 的时候，传递的参数是两个不同的线程组。负责监听 的 acceptor 线程组，线程数大于 1，也就是构造参数大于 1。负责处理客户端任务的线程组， 线程数大于 1，也就是构造参数大于 1。这种开发方式，就是主从多线程模型。 ​ 长连接，客户端数量相对较多，连接持续时间比较长的情况下使用。如：对外提供服务 的相册服务器。 案例： 12345678910111213141516171819202122232425262728293031323334353637383940&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;5.0.0.Alpha2&lt;/version&gt; &lt;!-- &lt;version&gt;4.1.24.Final&lt;/version&gt; --&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-codec-http&lt;/artifactId&gt; &lt;version&gt;5.0.0.Alpha2&lt;/version&gt; &lt;!-- &lt;version&gt;4.1.24.Final&lt;/version&gt; --&gt; &lt;/dependency&gt; &lt;!-- 接收处理工具 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.marshalling&lt;/groupId&gt; &lt;artifactId&gt;jboss-marshalling-river&lt;/artifactId&gt; &lt;version&gt;1.4.11.Final&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 序列化处理工具 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.marshalling&lt;/groupId&gt; &lt;artifactId&gt;jboss-marshalling-serial&lt;/artifactId&gt; &lt;version&gt;1.4.11.Final&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 系统信息收集 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.hyperic.sigar&lt;/groupId&gt; &lt;artifactId&gt;com.springsource.org.hyperic.sigar&lt;/artifactId&gt; &lt;version&gt;1.6.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kaazing&lt;/groupId&gt; &lt;artifactId&gt;sigar.dist&lt;/artifactId&gt; &lt;version&gt;1.0.0.0&lt;/version&gt; &lt;classifier&gt;distribution&lt;/classifier&gt; &lt;type&gt;zip&lt;/type&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 工具类 GzipUtils 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class GzipUtils &#123; public static void main(String[] args) throws Exception &#123; FileInputStream fis = new FileInputStream("D:\\3\\1.jpg"); byte[] temp = new byte[fis.available()]; int length = fis.read(temp); System.out.println("长度 : " + length); byte[] zipArray = GzipUtils.zip(temp); System.out.println("压缩后的长度 : " + zipArray.length); byte[] unzipArray = GzipUtils.unzip(zipArray); System.out.println("解压缩后的长度 : " + unzipArray.length); FileOutputStream fos = new FileOutputStream("D:\\3\\101.jpg"); fos.write(unzipArray); fos.flush(); fos.close(); fis.close(); &#125; /** * 解压缩 * @param source 源数据。需要解压的数据。 * @return 解压后的数据。 恢复的数据。 * @throws Exception */ public static byte[] unzip(byte[] source) throws Exception&#123; ByteArrayOutputStream out = new ByteArrayOutputStream(); ByteArrayInputStream in = new ByteArrayInputStream(source); // JDK提供的。 专门用于压缩使用的流对象。可以处理字节数组数据。 GZIPInputStream zipIn = new GZIPInputStream(in); byte[] temp = new byte[256]; int length = 0; while((length = zipIn.read(temp, 0, temp.length)) != -1)&#123; out.write(temp, 0, length); &#125; // 将字节数组输出流中的数据，转换为一个字节数组。 byte[] target = out.toByteArray(); zipIn.close(); out.close(); return target; &#125; /** * 压缩 * @param source 源数据，需要压缩的数据 * @return 压缩后的数据。 * @throws Exception */ public static byte[] zip(byte[] source) throws Exception&#123; ByteArrayOutputStream out = new ByteArrayOutputStream(); // 输出流，JDK提供的，提供解压缩功能。 GZIPOutputStream zipOut = new GZIPOutputStream(out); // 将压缩信息写入到内存。 写入的过程会实现解压。 zipOut.write(source); // 结束。 zipOut.finish(); byte[] target = out.toByteArray(); zipOut.close(); return target; &#125;&#125; OSUtils 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265public class OSUtils &#123; public static void main(String[] args) &#123; try &#123; // System信息，从jvm获取 property(); System.out.println("----------------------------------"); // cpu信息 cpu(); System.out.println("----------------------------------"); // 内存信息 memory(); System.out.println("----------------------------------"); // 操作系统信息 os(); System.out.println("----------------------------------"); // 用户信息 who(); System.out.println("----------------------------------"); // 文件系统信息 file(); System.out.println("----------------------------------"); // 网络信息 net(); System.out.println("----------------------------------"); // 以太网信息 ethernet(); System.out.println("----------------------------------"); &#125; catch (Exception e1) &#123; e1.printStackTrace(); &#125; &#125; private static void property() throws UnknownHostException &#123; Runtime r = Runtime.getRuntime(); Properties props = System.getProperties(); InetAddress addr; addr = InetAddress.getLocalHost(); String ip = addr.getHostAddress(); Map&lt;String, String&gt; map = System.getenv(); String userName = map.get("USERNAME");// 获取用户名 String computerName = map.get("COMPUTERNAME");// 获取计算机名 String userDomain = map.get("USERDOMAIN");// 获取计算机域名 System.out.println("用户名: " + userName); System.out.println("计算机名: " + computerName); System.out.println("计算机域名: " + userDomain); System.out.println("本地ip地址: " + ip); System.out.println("本地主机名: " + addr.getHostName()); System.out.println("JVM可以使用的总内存: " + r.totalMemory()); System.out.println("JVM可以使用的剩余内存: " + r.freeMemory()); System.out.println("JVM可以使用的处理器个数: " + r.availableProcessors()); System.out.println("Java的运行环境版本： " + props.getProperty("java.version")); System.out.println("Java的运行环境供应商： " + props.getProperty("java.vendor")); System.out.println("Java供应商的URL： " + props.getProperty("java.vendor.url")); System.out.println("Java的安装路径： " + props.getProperty("java.home")); System.out.println("Java的虚拟机规范版本： " + props.getProperty("java.vm.specification.version")); System.out.println("Java的虚拟机规范供应商： " + props.getProperty("java.vm.specification.vendor")); System.out.println("Java的虚拟机规范名称： " + props.getProperty("java.vm.specification.name")); System.out.println("Java的虚拟机实现版本： " + props.getProperty("java.vm.version")); System.out.println("Java的虚拟机实现供应商： " + props.getProperty("java.vm.vendor")); System.out.println("Java的虚拟机实现名称： " + props.getProperty("java.vm.name")); System.out.println("Java运行时环境规范版本： " + props.getProperty("java.specification.version")); System.out.println("Java运行时环境规范供应商： " + props.getProperty("java.specification.vender")); System.out.println("Java运行时环境规范名称： " + props.getProperty("java.specification.name")); System.out.println("Java的类格式版本号： " + props.getProperty("java.class.version")); System.out.println("Java的类路径： " + props.getProperty("java.class.path")); System.out.println("加载库时搜索的路径列表： " + props.getProperty("java.library.path")); System.out.println("默认的临时文件路径： " + props.getProperty("java.io.tmpdir")); System.out.println("一个或多个扩展目录的路径： " + props.getProperty("java.ext.dirs")); System.out.println("操作系统的名称： " + props.getProperty("os.name")); System.out.println("操作系统的构架： " + props.getProperty("os.arch")); System.out.println("操作系统的版本： " + props.getProperty("os.version")); System.out.println("文件分隔符： " + props.getProperty("file.separator")); System.out.println("路径分隔符： " + props.getProperty("path.separator")); System.out.println("行分隔符： " + props.getProperty("line.separator")); System.out.println("用户的账户名称： " + props.getProperty("user.name")); System.out.println("用户的主目录： " + props.getProperty("user.home")); System.out.println("用户的当前工作目录： " + props.getProperty("user.dir")); &#125; private static void memory() throws SigarException &#123; Sigar sigar = new Sigar(); Mem mem = sigar.getMem(); // 内存总量 System.out.println("内存总量: " + mem.getTotal() / 1024L + "K av"); // 当前内存使用量 System.out.println("当前内存使用量: " + mem.getUsed() / 1024L + "K used"); // 当前内存剩余量 System.out.println("当前内存剩余量: " + mem.getFree() / 1024L + "K free"); Swap swap = sigar.getSwap(); // 交换区总量 System.out.println("交换区总量: " + swap.getTotal() / 1024L + "K av"); // 当前交换区使用量 System.out.println("当前交换区使用量: " + swap.getUsed() / 1024L + "K used"); // 当前交换区剩余量 System.out.println("当前交换区剩余量: " + swap.getFree() / 1024L + "K free"); &#125; private static void cpu() throws SigarException &#123; Sigar sigar = new Sigar(); CpuInfo infos[] = sigar.getCpuInfoList(); CpuPerc cpuList[] = null; cpuList = sigar.getCpuPercList(); for (int i = 0; i &lt; infos.length; i++) &#123;// 不管是单块CPU还是多CPU都适用 CpuInfo info = infos[i]; System.out.println("第" + (i + 1) + "块CPU信息"); System.out.println("CPU的总量MHz: " + info.getMhz());// CPU的总量MHz System.out.println("CPU生产商: " + info.getVendor());// 获得CPU的卖主，如：Intel System.out.println("CPU类别: " + info.getModel());// 获得CPU的类别，如：Celeron System.out.println("CPU缓存数量: " + info.getCacheSize());// 缓冲存储器数量 printCpuPerc(cpuList[i]); &#125; &#125; private static void printCpuPerc(CpuPerc cpu) &#123; System.out.println("CPU用户使用率: " + CpuPerc.format(cpu.getUser()));// 用户使用率 System.out.println("CPU系统使用率: " + CpuPerc.format(cpu.getSys()));// 系统使用率 System.out.println("CPU当前等待率: " + CpuPerc.format(cpu.getWait()));// 当前等待率 System.out.println("CPU当前错误率: " + CpuPerc.format(cpu.getNice()));// System.out.println("CPU当前空闲率: " + CpuPerc.format(cpu.getIdle()));// 当前空闲率 System.out.println("CPU总的使用率: " + CpuPerc.format(cpu.getCombined()));// 总的使用率 &#125; private static void os() &#123; OperatingSystem OS = OperatingSystem.getInstance(); // 操作系统内核类型如： 386、486、586等x86 System.out.println("操作系统: " + OS.getArch()); System.out.println("操作系统CpuEndian(): " + OS.getCpuEndian());// System.out.println("操作系统DataModel(): " + OS.getDataModel());// // 系统描述 System.out.println("操作系统的描述: " + OS.getDescription()); // 操作系统类型 // System.out.println("OS.getName(): " + OS.getName()); // System.out.println("OS.getPatchLevel(): " + OS.getPatchLevel());// // 操作系统的卖主 System.out.println("操作系统的卖主: " + OS.getVendor()); // 卖主名称 System.out.println("操作系统的卖主名: " + OS.getVendorCodeName()); // 操作系统名称 System.out.println("操作系统名称: " + OS.getVendorName()); // 操作系统卖主类型 System.out.println("操作系统卖主类型: " + OS.getVendorVersion()); // 操作系统的版本号 System.out.println("操作系统的版本号: " + OS.getVersion()); &#125; private static void who() throws SigarException &#123; Sigar sigar = new Sigar(); Who who[] = sigar.getWhoList(); if (who != null &amp;&amp; who.length &gt; 0) &#123; for (int i = 0; i &lt; who.length; i++) &#123; // System.out.println("当前系统进程表中的用户名" + String.valueOf(i)); Who _who = who[i]; System.out.println("用户控制台: " + _who.getDevice()); System.out.println("用户host: " + _who.getHost()); // System.out.println("getTime(): " + _who.getTime()); // 当前系统进程表中的用户名 System.out.println("当前系统进程表中的用户名: " + _who.getUser()); &#125; &#125; &#125; private static void file() throws Exception &#123; Sigar sigar = new Sigar(); FileSystem fslist[] = sigar.getFileSystemList(); try &#123; for (int i = 0; i &lt; fslist.length; i++) &#123; System.out.println("分区的盘符名称" + i); FileSystem fs = fslist[i]; // 分区的盘符名称 System.out.println("盘符名称: " + fs.getDevName()); // 分区的盘符名称 System.out.println("盘符路径: " + fs.getDirName()); System.out.println("盘符标志: " + fs.getFlags());// // 文件系统类型，比如 FAT32、NTFS System.out.println("盘符类型: " + fs.getSysTypeName()); // 文件系统类型名，比如本地硬盘、光驱、网络文件系统等 System.out.println("盘符类型名: " + fs.getTypeName()); // 文件系统类型 System.out.println("盘符文件系统类型: " + fs.getType()); FileSystemUsage usage = null; usage = sigar.getFileSystemUsage(fs.getDirName()); switch (fs.getType()) &#123; case 0: // TYPE_UNKNOWN ：未知 break; case 1: // TYPE_NONE break; case 2: // TYPE_LOCAL_DISK : 本地硬盘 // 文件系统总大小 System.out.println(fs.getDevName() + "总大小: " + usage.getTotal() + "KB"); // 文件系统剩余大小 System.out.println(fs.getDevName() + "剩余大小: " + usage.getFree() + "KB"); // 文件系统可用大小 System.out.println(fs.getDevName() + "可用大小: " + usage.getAvail() + "KB"); // 文件系统已经使用量 System.out.println(fs.getDevName() + "已经使用量: " + usage.getUsed() + "KB"); double usePercent = usage.getUsePercent() * 100D; // 文件系统资源的利用率 System.out.println(fs.getDevName() + "资源的利用率: " + usePercent + "%"); break; case 3:// TYPE_NETWORK ：网络 break; case 4:// TYPE_RAM_DISK ：闪存 break; case 5:// TYPE_CDROM ：光驱 break; case 6:// TYPE_SWAP ：页面交换 break; &#125; System.out.println(fs.getDevName() + "读出： " + usage.getDiskReads()); System.out.println(fs.getDevName() + "写入： " + usage.getDiskWrites()); &#125; &#125; catch (Exception e) &#123; // TODO: handle exception e.printStackTrace(); &#125; return; &#125; private static void net() throws Exception &#123; Sigar sigar = new Sigar(); String ifNames[] = sigar.getNetInterfaceList(); for (int i = 0; i &lt; ifNames.length; i++) &#123; String name = ifNames[i]; NetInterfaceConfig ifconfig = sigar.getNetInterfaceConfig(name); System.out.println("网络设备名: " + name);// 网络设备名 System.out.println("IP地址: " + ifconfig.getAddress());// IP地址 System.out.println("子网掩码: " + ifconfig.getNetmask());// 子网掩码 if ((ifconfig.getFlags() &amp; 1L) &lt;= 0L) &#123; System.out.println("!IFF_UP...skipping getNetInterfaceStat"); continue; &#125; NetInterfaceStat ifstat = sigar.getNetInterfaceStat(name); System.out.println(name + "接收的总包裹数:" + ifstat.getRxPackets());// 接收的总包裹数 System.out.println(name + "发送的总包裹数:" + ifstat.getTxPackets());// 发送的总包裹数 System.out.println(name + "接收到的总字节数:" + ifstat.getRxBytes());// 接收到的总字节数 System.out.println(name + "发送的总字节数:" + ifstat.getTxBytes());// 发送的总字节数 System.out.println(name + "接收到的错误包数:" + ifstat.getRxErrors());// 接收到的错误包数 System.out.println(name + "发送数据包时的错误数:" + ifstat.getTxErrors());// 发送数据包时的错误数 System.out.println(name + "接收时丢弃的包数:" + ifstat.getRxDropped());// 接收时丢弃的包数 System.out.println(name + "发送时丢弃的包数:" + ifstat.getTxDropped());// 发送时丢弃的包数 &#125; &#125; private static void ethernet() throws SigarException &#123; Sigar sigar = null; sigar = new Sigar(); String[] ifaces = sigar.getNetInterfaceList(); for (int i = 0; i &lt; ifaces.length; i++) &#123; NetInterfaceConfig cfg = sigar.getNetInterfaceConfig(ifaces[i]); if (NetFlags.LOOPBACK_ADDRESS.equals(cfg.getAddress()) || (cfg.getFlags() &amp; NetFlags.IFF_LOOPBACK) != 0 || NetFlags.NULL_HWADDR.equals(cfg.getHwaddr())) &#123; continue; &#125; System.out.println(cfg.getName() + "IP地址:" + cfg.getAddress());// IP地址 System.out.println(cfg.getName() + "网关广播地址:" + cfg.getBroadcast());// 网关广播地址 System.out.println(cfg.getName() + "网卡MAC地址:" + cfg.getHwaddr());// 网卡MAC地址 System.out.println(cfg.getName() + "子网掩码:" + cfg.getNetmask());// 子网掩码 System.out.println(cfg.getName() + "网卡描述信息:" + cfg.getDescription());// 网卡描述信息 System.out.println(cfg.getName() + "网卡类型" + cfg.getType());// &#125; &#125;&#125; SerializableFactory4Marshalling 123456789101112131415161718192021222324252627282930313233343536public class SerializableFactory4Marshalling &#123; /** * 创建Jboss Marshalling解码器MarshallingDecoder * @return MarshallingDecoder */ public static MarshallingDecoder buildMarshallingDecoder() &#123; //首先通过Marshalling工具类的精通方法获取Marshalling实例对象 参数serial标识创建的是java序列化工厂对象。 //jboss-marshalling-serial 包提供 final MarshallerFactory marshallerFactory = Marshalling.getProvidedMarshallerFactory("serial"); //创建了MarshallingConfiguration对象，配置了版本号为5 final MarshallingConfiguration configuration = new MarshallingConfiguration(); // 序列化版本。只要使用JDK5以上版本，version只能定义为5。 configuration.setVersion(5); //根据marshallerFactory和configuration创建provider UnmarshallerProvider provider = new DefaultUnmarshallerProvider(marshallerFactory, configuration); //构建Netty的MarshallingDecoder对象，俩个参数分别为provider和单个消息序列化后的最大长度 MarshallingDecoder decoder = new MarshallingDecoder(provider, 1024 * 1024 * 1); return decoder; &#125; /** * 创建Jboss Marshalling编码器MarshallingEncoder * @return MarshallingEncoder */ public static MarshallingEncoder buildMarshallingEncoder() &#123; final MarshallerFactory marshallerFactory = Marshalling.getProvidedMarshallerFactory("serial"); final MarshallingConfiguration configuration = new MarshallingConfiguration(); configuration.setVersion(5); MarshallerProvider provider = new DefaultMarshallerProvider(marshallerFactory, configuration); //构建Netty的MarshallingEncoder对象，MarshallingEncoder用于实现序列化接口的POJO对象序列化为二进制数组 MarshallingEncoder encoder = new MarshallingEncoder(provider); return encoder; &#125;&#125; HeatbeatMessage 1234567891011121314151617181920212223242526272829303132333435363738394041public class HeatbeatMessage implements Serializable &#123; private static final long serialVersionUID = 2827219147304706826L; private String ip; private Map&lt;String, Object&gt; cpuMsgMap; private Map&lt;String, Object&gt; memMsgMap; private Map&lt;String, Object&gt; fileSysMsgMap; @Override public String toString() &#123; return "HeatbeatMessage [\nip=" + ip + ", \ncpuMsgMap=" + cpuMsgMap + ", \nmemMsgMap=" + memMsgMap + ", \nfileSysMsgMap=" + fileSysMsgMap + "]"; &#125; public String getIp() &#123; return ip; &#125; public void setIp(String ip) &#123; this.ip = ip; &#125; public Map&lt;String, Object&gt; getCpuMsgMap() &#123; return cpuMsgMap; &#125; public void setCpuMsgMap(Map&lt;String, Object&gt; cpuMsgMap) &#123; this.cpuMsgMap = cpuMsgMap; &#125; public Map&lt;String, Object&gt; getMemMsgMap() &#123; return memMsgMap; &#125; public void setMemMsgMap(Map&lt;String, Object&gt; memMsgMap) &#123; this.memMsgMap = memMsgMap; &#125; public Map&lt;String, Object&gt; getFileSysMsgMap() &#123; return fileSysMsgMap; &#125; public void setFileSysMsgMap(Map&lt;String, Object&gt; fileSysMsgMap) &#123; this.fileSysMsgMap = fileSysMsgMap; &#125;&#125; RequestMessage 12345678910111213141516171819202122232425262728293031323334353637public class RequestMessage implements Serializable &#123; private static final long serialVersionUID = 7084843947860990140L; private Long id; private String message; private byte[] attachment; @Override public String toString() &#123; return "RequestMessage [id=" + id + ", message=" + message + "]"; &#125; public RequestMessage() &#123; super(); &#125; public RequestMessage(Long id, String message, byte[] attachment) &#123; super(); this.id = id; this.message = message; this.attachment = attachment; &#125; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125; public byte[] getAttachment() &#123; return attachment; &#125; public void setAttachment(byte[] attachment) &#123; this.attachment = attachment; &#125;&#125; ResponseMessage 1234567891011121314151617181920212223242526272829public class ResponseMessage implements Serializable &#123; private static final long serialVersionUID = -8134313953478922076L; private Long id; private String message; @Override public String toString() &#123; return "ResponseMessage [id=" + id + ", message=" + message + "]"; &#125; public ResponseMessage() &#123; super(); &#125; public ResponseMessage(Long id, String message) &#123; super(); this.id = id; this.message = message; &#125; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125;&#125; server端开发： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * 1. 双线程组 * 2. Bootstrap配置启动信息 * 3. 注册业务处理Handler * 4. 绑定服务监听端口并启动服务 */public class Server4HelloWorld &#123; // 监听线程组，监听客户端请求 private EventLoopGroup acceptorGroup = null; // 处理客户端相关操作线程组，负责处理与客户端的数据通讯 private EventLoopGroup clientGroup = null; // 服务端启动相关配置信息 private ServerBootstrap bootstrap = null; public Server4HelloWorld()&#123; init(); &#125; private void init()&#123; // 初始化线程组,构建线程组的时候，如果不传递参数，则默认构建的线程组线程数是CPU核心数量。 acceptorGroup = new NioEventLoopGroup(); //监听线程组 clientGroup = new NioEventLoopGroup(); //处理客户端线程组 // 初始化服务的配置 bootstrap = new ServerBootstrap(); // 绑定线程组 bootstrap.group(acceptorGroup, clientGroup); // 设定通讯模式为NIO， 同步非阻塞 bootstrap.channel(NioServerSocketChannel.class); // 设定缓冲区大小， 缓存区的单位是字节。 bootstrap.option(ChannelOption.SO_BACKLOG, 1024); // SO_SNDBUF发送缓冲区，SO_RCVBUF接收缓冲区，SO_KEEPALIVE开启心跳监测（保证连接有效） bootstrap.option(ChannelOption.SO_SNDBUF, 16*1024) .option(ChannelOption.SO_RCVBUF, 16*1024) .option(ChannelOption.SO_KEEPALIVE, true); &#125; /** * 监听处理逻辑。 * @param port 监听端口。 * @param acceptorHandlers 处理器， 如何处理客户端请求。 * @return * @throws InterruptedException */ public ChannelFuture doAccept(int port, final ChannelHandler... acceptorHandlers) throws InterruptedException&#123; /* * childHandler是服务的Bootstrap独有的方法。是用于提供处理对象的。 * 可以一次性增加若干个处理逻辑。是类似责任链模式的处理方式。 * 增加A，B两个处理逻辑，在处理客户端请求数据的时候，根据A-》B顺序依次处理。 * * ChannelInitializer - 用于提供处理器的一个模型对象。 * 其中定义了一个方法，initChannel方法。 * 方法是用于初始化处理逻辑责任链条的。 * 可以保证服务端的Bootstrap只初始化一次处理器，尽量提供处理逻辑的重用。 * 避免反复的创建处理器对象。节约资源开销。 */ bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(acceptorHandlers); &#125; &#125;); // bind方法 - 绑定监听端口的。ServerBootstrap可以绑定多个监听端口。 多次调用bind方法即可 // sync - 开始监听逻辑。 返回一个ChannelFuture。 返回结果代表的是监听成功后的一个对应的未来结果 // 可以使用ChannelFuture实现后续的服务器和客户端的交互。 ChannelFuture future = bootstrap.bind(port).sync(); return future; &#125; /** * shutdownGracefully - 方法是一个安全关闭的方法。可以保证不放弃任何一个已接收的客户端请求。 */ public void release()&#123; this.acceptorGroup.shutdownGracefully(); this.clientGroup.shutdownGracefully(); &#125; public static void main(String[] args)&#123; ChannelFuture future = null; Server4HelloWorld server = null; try&#123; server = new Server4HelloWorld(); future = server.doAccept(9999,new Server4HelloWorldHandler()); System.out.println("server started."); // 关闭连接的。 future.channel().closeFuture().sync(); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //回收线程组资源 if(null != server)&#123; server.release(); &#125; &#125; &#125;&#125; serverHandler 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * @Sharable注解 - * 代表当前Handler是一个可以分享的处理器。也就意味着，服务器注册此Handler后，可以分享给多个客户端同时使用。 * 如果不使用注解描述类型，则每次客户端请求时，必须为客户端重新创建一个新的Handler对象。 * 如果handler是一个Sharable的，一定避免定义可写的实例变量。修改会发生混乱 * bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new XxxHandler()); &#125; &#125;); */package com.sxt.netty.first;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelFutureListener;import io.netty.channel.ChannelHandler.Sharable;import io.netty.channel.socket.SocketChannel;import io.netty.channel.ChannelHandlerAdapter;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInitializer;@Sharablepublic class Server4HelloWorldHandler extends ChannelHandlerAdapter &#123; /** * 业务处理逻辑 * 用于处理读取数据请求的逻辑。 * ctx - 上下文对象。其中包含于客户端建立连接的所有资源。 如： 对应的Channel * msg - 读取到的数据。 默认类型是ByteBuf，是Netty自定义的。是对ByteBuffer的封装。 不需要考虑复位问题。 */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; // 获取读取的数据， 是一个缓冲。 ByteBuf readBuffer = (ByteBuf) msg; // 创建一个字节数组，用于保存缓存中的数据。 byte[] tempDatas = new byte[readBuffer.readableBytes()]; // 将缓存中的数据读取到字节数组中。 readBuffer.readBytes(tempDatas); String message = new String(tempDatas, "UTF-8"); System.out.println("from client : " + message); if("exit".equals(message))&#123; ctx.close(); return; &#125; String line = "server message to client!"; // 写操作自动释放缓存，避免内存溢出问题。 ctx.writeAndFlush(Unpooled.copiedBuffer(line.getBytes("UTF-8"))); // 注意，如果调用的是write方法。不会刷新缓存，缓存中的数据不会发送到客户端，必须再次调用flush方法才行。 // ctx.write(Unpooled.copiedBuffer(line.getBytes("UTF-8"))); // ctx.flush() &#125; /** * 异常处理逻辑， 当客户端异常退出的时候，也会运行。 * ChannelHandlerContext关闭，也代表当前与客户端连接的资源关闭。 */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("server exceptionCaught method run..."); // cause.printStackTrace(); ctx.close(); &#125;&#125; client 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * 因为客户端是请求的发起者，不需要监听。 * 只需要定义唯一的一个线程组即可。 */public class Client4HelloWorld &#123; // 处理请求和处理服务端响应的线程组 private EventLoopGroup group = null; // 客户端启动相关配置信息 private Bootstrap bootstrap = null; public Client4HelloWorld()&#123; init(); &#125; private void init()&#123; group = new NioEventLoopGroup(); bootstrap = new Bootstrap(); // 绑定线程组 bootstrap.group(group); // 设定通讯模式为NIO bootstrap.channel(NioSocketChannel.class); &#125; public ChannelFuture doRequest(String host, int port, final ChannelHandler... handlers) throws InterruptedException&#123; /* * 客户端的Bootstrap没有childHandler方法。只有handler方法。 * 方法含义等同ServerBootstrap中的childHandler * 在客户端必须绑定处理器，也就是必须调用handler方法。 * 服务器必须绑定处理器，必须调用childHandler方法。 */ this.bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(handlers); &#125; &#125;); // 建立连接。 ChannelFuture future = this.bootstrap.connect(host, port).sync(); return future; &#125; public void release()&#123; this.group.shutdownGracefully(); &#125; public static void main(String[] args) &#123; Client4HelloWorld client = null; ChannelFuture future = null; try&#123; client = new Client4HelloWorld(); future = client.doRequest("localhost", 9999, new Client4HelloWorldHandler()); Scanner s = null; while(true)&#123; s = new Scanner(System.in); System.out.print("enter message send to server (enter 'exit' for close client) &gt; "); String line = s.nextLine(); if("exit".equals(line))&#123; // addListener - 增加监听，当某条件满足的时候，触发监听器。 // ChannelFutureListener.CLOSE - 关闭监听器，代表ChannelFuture执行返回后，关闭连接。 future.channel().writeAndFlush(Unpooled.copiedBuffer(line.getBytes("UTF-8"))) .addListener(ChannelFutureListener.CLOSE); break; &#125; future.channel().writeAndFlush(Unpooled.copiedBuffer(line.getBytes("UTF-8"))); TimeUnit.SECONDS.sleep(1); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; if(null != client)&#123; client.release(); &#125; &#125; &#125;&#125; clientHandler 1234567891011121314151617181920212223242526272829303132333435363738public class Client4HelloWorldHandler extends ChannelHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; try&#123; ByteBuf readBuffer = (ByteBuf) msg; byte[] tempDatas = new byte[readBuffer.readableBytes()]; readBuffer.readBytes(tempDatas); System.out.println("from server : " + new String(tempDatas, "UTF-8")); &#125;finally&#123; // 用于释放缓存。避免内存溢出 ReferenceCountUtil.release(msg); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("client exceptionCaught method run..."); // cause.printStackTrace(); ctx.close(); &#125; /*@Override // 断开连接时执行 public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channelInactive method run..."); &#125; @Override // 连接通道建立成功时执行 public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channelActive method run..."); &#125; @Override // 每次读取完成时执行 public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channelReadComplete method run..."); &#125;*/&#125; 合体圆满（基础代码演示）拆包粘包问题解决 ： ​ netty 使用 tcp/ip 协议传输数据。而 tcp/ip 协议是类似水流一样的数据传输方式。多次 访问的时候有可能出现数据粘包的问题。（Netty是NIO的模型，是同步非阻塞的，一定执行read/write时候就继续向下执行了，让底层的代码给我们处理执行数据的准备，怎么去读写操作，如果我们客户端发起多个数据，read方法到底是读几条数据？不知道客户端发送过来到底是几套数据，每一条间到底有什么间隔）解决这种问题的方式如下： 定长数据流 ：客户端和服务器，提前协调好，每个消息长度固定。（如：长度 10）。如果客户端或服 务器写出的数据不足 10，则使用空白字符补足（如：使用空格）。 server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * 1. 双线程组 * 2. Bootstrap配置启动信息 * 3. 注册业务处理Handler * 4. 绑定服务监听端口并启动服务 */public class Server4FixedLength &#123; // 监听线程组，监听客户端请求 private EventLoopGroup acceptorGroup = null; // 处理客户端相关操作线程组，负责处理与客户端的数据通讯 private EventLoopGroup clientGroup = null; // 服务启动相关配置信息 private ServerBootstrap bootstrap = null; public Server4FixedLength()&#123; init(); &#125; private void init()&#123; acceptorGroup = new NioEventLoopGroup(); clientGroup = new NioEventLoopGroup(); bootstrap = new ServerBootstrap(); // 绑定线程组 bootstrap.group(acceptorGroup, clientGroup); // 设定通讯模式为NIO bootstrap.channel(NioServerSocketChannel.class); // 设定缓冲区大小 bootstrap.option(ChannelOption.SO_BACKLOG, 1024); // SO_SNDBUF发送缓冲区，SO_RCVBUF接收缓冲区，SO_KEEPALIVE开启心跳监测（保证连接有效） bootstrap.option(ChannelOption.SO_SNDBUF, 16*1024) .option(ChannelOption.SO_RCVBUF, 16*1024) .option(ChannelOption.SO_KEEPALIVE, true); &#125; public ChannelFuture doAccept(int port) throws InterruptedException&#123; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelHandler[] acceptorHandlers = new ChannelHandler[3]; // 定长Handler。通过构造参数设置消息长度（单位是字节）。发送的消息长度不足可以使用空格补全。 acceptorHandlers[0] = new FixedLengthFrameDecoder(3); // 字符串解码器Handler，会自动处理channelRead方法的msg参数，将ByteBuf类型的数据转换为字符串对象 acceptorHandlers[1] = new StringDecoder(Charset.forName("UTF-8")); acceptorHandlers[2] = new Server4FixedLengthHandler(); ch.pipeline().addLast(acceptorHandlers); &#125; &#125;); ChannelFuture future = bootstrap.bind(port).sync(); return future; &#125; public void release()&#123; this.acceptorGroup.shutdownGracefully(); this.clientGroup.shutdownGracefully(); &#125; public static void main(String[] args)&#123; ChannelFuture future = null; Server4FixedLength server = null; try&#123; server = new Server4FixedLength(); future = server.doAccept(9999); System.out.println("server started."); future.channel().closeFuture().sync(); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; if(null != server)&#123; server.release(); &#125; &#125; &#125;&#125; serverHandler 123456789101112131415161718192021public class Server4FixedLengthHandler extends ChannelHandlerAdapter &#123; // 业务处理逻辑 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String message = msg.toString(); System.out.println("from client : " + message.trim()); String line = "ok "; ctx.writeAndFlush(Unpooled.copiedBuffer(line.getBytes("UTF-8"))); &#125; // 异常处理逻辑 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("server exceptionCaught method run..."); // cause.printStackTrace(); ctx.close(); &#125;&#125; client 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788/** * 1. 单线程组 * 2. Bootstrap配置启动信息 * 3. 注册业务处理Handler * 4. connect连接服务，并发起请求 */public class Client4FixedLength &#123; // 处理请求和处理服务端响应的线程组 private EventLoopGroup group = null; // 服务启动相关配置信息 private Bootstrap bootstrap = null; public Client4FixedLength()&#123; init(); &#125; private void init()&#123; group = new NioEventLoopGroup(); bootstrap = new Bootstrap(); // 绑定线程组 bootstrap.group(group); // 设定通讯模式为NIO bootstrap.channel(NioSocketChannel.class); &#125; public ChannelFuture doRequest(String host, int port) throws InterruptedException&#123; this.bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelHandler[] handlers = new ChannelHandler[3]; handlers[0] = new FixedLengthFrameDecoder(3); // 字符串解码器Handler，会自动处理channelRead方法的msg参数，将ByteBuf类型的数据转换为字符串对象 handlers[1] = new StringDecoder(Charset.forName("UTF-8")); handlers[2] = new Client4FixedLengthHandler(); ch.pipeline().addLast(handlers); &#125; &#125;); ChannelFuture future = this.bootstrap.connect(host, port).sync(); return future; &#125; public void release()&#123; this.group.shutdownGracefully(); &#125; public static void main(String[] args) &#123; Client4FixedLength client = null; ChannelFuture future = null; try&#123; client = new Client4FixedLength(); future = client.doRequest("localhost", 9999); Scanner s = null; while(true)&#123; s = new Scanner(System.in); System.out.print("enter message send to server &gt; "); String line = s.nextLine(); byte[] bs = new byte[5]; byte[] temp = line.getBytes("UTF-8"); if(temp.length &lt;= 5)&#123; for(int i = 0; i &lt; temp.length; i++)&#123; bs[i] = temp[i]; &#125; &#125; future.channel().writeAndFlush(Unpooled.copiedBuffer(bs)); TimeUnit.SECONDS.sleep(1); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; if(null != client)&#123; client.release(); &#125; &#125; &#125;&#125; clientHandler 12345678910111213141516171819public class Client4FixedLengthHandler extends ChannelHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; try&#123; String message = msg.toString(); System.out.println("from server : " + message); &#125;finally&#123; // 用于释放缓存。避免内存溢出 ReferenceCountUtil.release(msg); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("client exceptionCaught method run..."); // cause.printStackTrace(); ctx.close(); &#125; 注意：此时客户端只要不能满足的数据是3的长度，要可以进行多次发送。服务端才能收到数据，并且是连起来的。当客户端发的是大3的长度是，如发送abc123def4，服务器会把它当成三条数据，4不会出来，客户端在发送2个长度数据，服务端才可以接收到。 ​ 中文问题，unicode中，一个汉字可能长度是2也可能是3。如果在上面代码中的客户端发送”中国”二个字就有意思了。服务端会接收到2次，一个字一次。 特殊结束符 ：客户端和服务器，协商定义一个特殊的分隔符号，分隔符号长度自定义。如：‘#’、‘$_$’、 ‘AA@’。在通讯的时候，只要没有发送分隔符号，则代表一条数据没有结束。 server 1234567891011121314151617181920public ChannelFuture doAccept(int port) throws InterruptedException&#123; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; // 数据分隔符, 定义的数据分隔符一定是一个ByteBuf类型的数据对象。 ByteBuf delimiter = Unpooled.copiedBuffer("$E$".getBytes()); ChannelHandler[] acceptorHandlers = new ChannelHandler[3]; // 处理固定结束标记符号的Handler。这个Handler没有@Sharable注解修饰， // 必须每次初始化通道时创建一个新对象 // 使用特殊符号分隔处理数据粘包问题，也要定义每个数据包最大长度。netty建议数据有最大长度。 acceptorHandlers[0] = new DelimiterBasedFrameDecoder(1024, delimiter); // 字符串解码器Handler，会自动处理channelRead方法的msg参数，将ByteBuf类型的数据转换为字符串对象 acceptorHandlers[1] = new StringDecoder(Charset.forName("UTF-8")); acceptorHandlers[2] = new Server4DelimiterHandler(); ch.pipeline().addLast(acceptorHandlers); &#125; &#125;); ChannelFuture future = bootstrap.bind(port).sync(); serverHandler 1234567// 业务处理逻辑@Overridepublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String message = msg.toString(); System.out.println("from client : " + message); //会放回三条字符串 String line = "server message $E$ test delimiter handler!! $E$ second message $E$"; ctx.writeAndFlush(Unpooled.copiedBuffer(line.getBytes("UTF-8")));&#125; client和server改动基本一样。此时如果客户端不输入分隔符，可以进行不断输入。 协议 ：相对最成熟的数据传递方式。有服务器的开发者提供一个固定格式的协议标准。客户端 和服务器发送数据和接受数据的时候，都依据协议制定和解析消息。 1协议格式：HEADcontent-length:xxxxHEADBODYxxxxxxBODY 12345678910111213public ChannelFuture doAccept(int port, final ChannelHandler... acceptorHandlers) throws InterruptedException&#123; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new StringDecoder(Charset.forName("UTF-8"))); ch.pipeline().addLast(acceptorHandlers); &#125; &#125;); ChannelFuture future = bootstrap.bind(port).sync(); return future;&#125; serverHandler 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Sharablepublic class Server4ProtocolHandler extends ChannelHandlerAdapter &#123; // 业务处理逻辑 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String message = msg.toString(); System.out.println("server receive protocol content : " + message); message = ProtocolParser.parse(message); if(null == message)&#123; System.out.println("error request from client"); return ; &#125; System.out.println("from client : " + message); String line = "server message"; line = ProtocolParser.transferTo(line); System.out.println("server send protocol content : " + line); ctx.writeAndFlush(Unpooled.copiedBuffer(line.getBytes("UTF-8"))); &#125; // 异常处理逻辑 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("server exceptionCaught method run..."); cause.printStackTrace(); ctx.close(); &#125; static class ProtocolParser&#123; public static String parse(String message)&#123; String[] temp = message.split("HEADBODY"); temp[0] = temp[0].substring(4); temp[1] = temp[1].substring(0, (temp[1].length()-4)); int length = Integer.parseInt(temp[0].substring(temp[0].indexOf(":")+1)); if(length != temp[1].length())&#123; return null; &#125; return temp[1]; &#125; public static String transferTo(String message)&#123; message = "HEADcontent-length:" + message.length() + "HEADBODY" + message + "BODY"; return message; &#125; &#125;&#125; 协议限定拆分的格式。解决粘包。 序列化对象 ：JBoss Marshalling 序列化 ，Java 是面向对象的开发语言。传递的数据如果是 Java 对象，应该是最方便且可靠。 1234567891011121314public ChannelFuture doAccept(int port, final ChannelHandler... acceptorHandlers) throws InterruptedException&#123; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingDecoder()); ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingEncoder()); ch.pipeline().addLast(acceptorHandlers); &#125; &#125;); ChannelFuture future = bootstrap.bind(port).sync(); return future;&#125; serverHandler 123456789101112131415161718192021222324252627@Sharablepublic class Server4SerializableHandler extends ChannelHandlerAdapter &#123; // 业务处理逻辑 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("from client : ClassName - " + msg.getClass().getName() + " ; message : " + msg.toString()); if(msg instanceof RequestMessage)&#123; RequestMessage request = (RequestMessage)msg; // 解压缩 // byte[] attachment = GzipUtils.unzip(request.getAttachment()); // System.out.println(new String(attachment)); &#125; ResponseMessage response = new ResponseMessage(0L, "test response"); ctx.writeAndFlush(response); &#125; // 异常处理逻辑 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("server exceptionCaught method run..."); cause.printStackTrace(); ctx.close(); &#125;&#125; client 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public ChannelFuture doRequest(String host, int port, final ChannelHandler... handlers) throws InterruptedException&#123; this.bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingDecoder()); ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingEncoder()); ch.pipeline().addLast(handlers); &#125; &#125;); ChannelFuture future = this.bootstrap.connect(host, port).sync(); return future;&#125;public void release()&#123; this.group.shutdownGracefully();&#125;public static void main(String[] args) &#123; Client4Serializable client = null; ChannelFuture future = null; try&#123; client = new Client4Serializable(); future = client.doRequest("localhost", 9999, new Client4SerializableHandler()); String attachment = "test attachment"; byte[] attBuf = attachment.getBytes(); // attBuf = GzipUtils.zip(attBuf); // RequestMessage msg = new RequestMessage(new Random().nextLong(), // "test", new byte[0]); // 压缩，有效减少网络中传递的数据 attBuf = GzipUtils.zip(attBuf); RequestMessage msg = new RequestMessage(new Random().nextLong(), "test", attBuf); future.channel().writeAndFlush(msg); TimeUnit.SECONDS.sleep(1); future.addListener(ChannelFutureListener.CLOSE); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; if(null != client)&#123; client.release(); &#125; &#125;&#125; clientHandler 12345678910111213141516public class Client4SerializableHandler extends ChannelHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("from server : ClassName - " + msg.getClass().getName() + " ; message : " + msg.toString()); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("client exceptionCaught method run..."); cause.printStackTrace(); ctx.close(); &#125;&#125; 定时断线重连 ：客户端断线重连机制。 客户端数量多，且需要传递的数据量级较大。可以周期性的发送数据的时候，使用。要 求对数据的即时性不高的时候，才可使用。 优点： 可以使用数据缓存。不是每条数据进行一次数据交互。可以定时回收资源，对 资源利用率高。相对来说，即时性可以通过其他方式保证。如： 120 秒自动断线。数据变 化 1000 次请求服务器一次。300 秒中自动发送不足 1000 次的变化数据（Timer）。 server 12345678910111213141516171819public ChannelFuture doAccept(int port) throws InterruptedException&#123; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingDecoder()); ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingEncoder()); // 定义一个定时断线处理器，当多长时间内，没有任何的可读取数据，自动断开连接。 // 没有@Sharable的 // 构造参数，就是间隔时长。 默认的单位是秒。 // 自定义间隔时长单位。 new ReadTimeoutHandler(long times, TimeUnit unit); ch.pipeline().addLast(new ReadTimeoutHandler(3)); ch.pipeline().addLast(new Server4TimerHandler()); &#125; &#125;); ChannelFuture future = bootstrap.bind(port).sync(); return future;&#125; client 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public void setHandlers() throws InterruptedException&#123; this.bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingDecoder()); ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingEncoder()); // 写操作自定断线。 在指定时间内，没有写操作，自动断线。 ch.pipeline().addLast(new WriteTimeoutHandler(3)); ch.pipeline().addLast(new Client4TimerHandler()); &#125; &#125;);&#125;public ChannelFuture getChannelFuture(String host, int port) throws InterruptedException&#123; // future是null，我们就要建立连接 if(future == null)&#123; future = this.bootstrap.connect(host, port).sync(); &#125; // 如果非空，看些连接未来状态中的通道是否有效。 // 如果future非空，但是有个channel，证明已经和服务器断开了，但是future没有被回收 // 重连操作 if(!future.channel().isActive())&#123; future = this.bootstrap.connect(host, port).sync(); &#125; return future;&#125;public void release()&#123; this.group.shutdownGracefully();&#125;public static void main(String[] args) &#123; Client4Timer client = null; ChannelFuture future = null; try&#123; client = new Client4Timer(); client.setHandlers(); future = client.getChannelFuture("localhost", 9999); // 循环3从。睡眠2秒，没有超出时长 for(int i = 0; i &lt; 3; i++)&#123; RequestMessage msg = new RequestMessage(new Random().nextLong(), "test"+i, new byte[0]); future.channel().writeAndFlush(msg); TimeUnit.SECONDS.sleep(2); &#125; // 有超出时长 TimeUnit.SECONDS.sleep(5); future = client.getChannelFuture("localhost", 9999); RequestMessage msg = new RequestMessage(new Random().nextLong(), "test", new byte[0]); future.channel().writeAndFlush(msg); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; if(null != client)&#123; client.release(); &#125; &#125;&#125; clientHandler 123456789101112131415161718192021222324public class Client4TimerHandler extends ChannelHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("from server : ClassName - " + msg.getClass().getName() + " ; message : " + msg.toString()); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("client exceptionCaught method run..."); cause.printStackTrace(); ctx.close(); &#125; /** * 当连接建立成功后，出发的代码逻辑。 * 在一次连接中只运行唯一一次。 * 通常用于实现连接确认和资源初始化的。 */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("client channel active"); &#125; server端运行图。有断线有异常， .jpg) .jpg) 心跳监测 ：使用定时发送消息的方式，实现硬件检测，达到心态检测的目的。 心跳监测是用于检测电脑硬件和软件信息的一种技术。如：CPU 使用率，磁盘使用率， 内存使用率，进程情况，线程情况等。 sigar ：需要下载一个 zip 压缩包。内部包含若干 sigar 需要的操作系统文件。sigar 插件是通过 JVM 访问操作系统，读取计算机硬件的一个插件库。读取计算机硬件过程中，必须由操作系统提供硬件信息。硬件信息是通过操作系统提供的。zip 压缩包中是 sigar 编写的操作系统文 件，如：windows 中的动态链接库文件。 解压需要的操作系统文件，将操作系统文件赋值到${Java_home}/bin 目录中。 server 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class Server4Heatbeat &#123; // 监听线程组，监听客户端请求 private EventLoopGroup acceptorGroup = null; // 处理客户端相关操作线程组，负责处理与客户端的数据通讯 private EventLoopGroup clientGroup = null; // 服务启动相关配置信息 private ServerBootstrap bootstrap = null; public Server4Heatbeat()&#123; init(); &#125; private void init()&#123; acceptorGroup = new NioEventLoopGroup(); clientGroup = new NioEventLoopGroup(); bootstrap = new ServerBootstrap(); // 绑定线程组 bootstrap.group(acceptorGroup, clientGroup); // 设定通讯模式为NIO bootstrap.channel(NioServerSocketChannel.class); // 设定缓冲区大小 bootstrap.option(ChannelOption.SO_BACKLOG, 1024); // SO_SNDBUF发送缓冲区，SO_RCVBUF接收缓冲区，SO_KEEPALIVE开启心跳监测（保证连接有效） bootstrap.option(ChannelOption.SO_SNDBUF, 16*1024) .option(ChannelOption.SO_RCVBUF, 16*1024) .option(ChannelOption.SO_KEEPALIVE, true); &#125; public ChannelFuture doAccept(int port) throws InterruptedException&#123; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingDecoder()); ch.pipeline().addLast(SerializableFactory4Marshalling.buildMarshallingEncoder()); ch.pipeline().addLast(new Server4HeatbeatHandler()); &#125; &#125;); ChannelFuture future = bootstrap.bind(port).sync(); return future; &#125; public void release()&#123; this.acceptorGroup.shutdownGracefully(); this.clientGroup.shutdownGracefully(); &#125; public static void main(String[] args)&#123; ChannelFuture future = null; Server4Heatbeat server = null; try&#123; server = new Server4Heatbeat(); future = server.doAccept(9999); System.out.println("server started."); future.channel().closeFuture().sync(); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125;finally&#123; if(null != future)&#123; try &#123; future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; if(null != server)&#123; server.release(); &#125; &#125; &#125;&#125; serverHandler 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Sharablepublic class Server4HeatbeatHandler extends ChannelHandlerAdapter &#123; private static List&lt;String&gt; credentials = new ArrayList&lt;&gt;(); private static final String HEATBEAT_SUCCESS = "SERVER_RETURN_HEATBEAT_SUCCESS"; public Server4HeatbeatHandler()&#123; // 初始化客户端列表信息。一般通过配置文件读取或数据库读取。 credentials.add("192.168.199.222_WIN-QIUB2JF5TDP"); &#125; // 业务处理逻辑 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if(msg instanceof String)&#123; this.checkCredential(ctx, msg.toString()); &#125; else if (msg instanceof HeatbeatMessage)&#123; this.readHeatbeatMessage(ctx, msg); &#125; else &#123; // 服务器发送的信息是错误的，或者对外暴露的ip和端口号被与系统无关的人访问到了，自动断开连接 ctx.writeAndFlush("wrong message").addListener(ChannelFutureListener.CLOSE); &#125; &#125; private void readHeatbeatMessage(ChannelHandlerContext ctx, Object msg)&#123; HeatbeatMessage message = (HeatbeatMessage) msg; System.out.println(message); System.out.println("======================================="); ctx.writeAndFlush("receive heatbeat message"); &#125; /** * 身份检查。检查客户端身份是否有效。 * 客户端身份信息应该是通过数据库或数据文件定制的。 * 身份通过 - 返回确认消息。 * 身份无效 - 断开连接 * @param ctx * @param credential */ private void checkCredential(ChannelHandlerContext ctx, String credential)&#123; System.out.println(credential); System.out.println(credentials); if(credentials.contains(credential))&#123; ctx.writeAndFlush(HEATBEAT_SUCCESS); &#125;else&#123; ctx.writeAndFlush("no credential contains").addListener(ChannelFutureListener.CLOSE); &#125; &#125; // 异常处理逻辑 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("server exceptionCaught method run..."); // cause.printStackTrace(); ctx.close(); &#125;&#125; clientHandler 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public class Client4HeatbeatHandler extends ChannelHandlerAdapter &#123; private ScheduledExecutorService executorService = Executors.newScheduledThreadPool(1); private ScheduledFuture heatbeat; private InetAddress remoteAddr; private static final String HEATBEAT_SUCCESS = "SERVER_RETURN_HEATBEAT_SUCCESS"; //只运行一次 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; // 获取本地INET信息 this.remoteAddr = InetAddress.getLocalHost(); // 获取本地计算机名 String computerName = System.getenv().get("COMPUTERNAME"); String credentials = this.remoteAddr.getHostAddress() + "_" + computerName; System.out.println(credentials); // 发送到服务器，作为信息比对证书 ctx.writeAndFlush(credentials); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; try&#123; if(msg instanceof String)&#123; if(HEATBEAT_SUCCESS.equals(msg))&#123; this.heatbeat = this.executorService.scheduleWithFixedDelay(new HeatbeatTask(ctx), 0L, 2L, TimeUnit.SECONDS); System.out.println("client receive - " + msg); &#125;else&#123; System.out.println("client receive - " + msg); &#125; &#125; &#125;finally&#123; ReferenceCountUtil.release(msg); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("client exceptionCaught method run..."); // cause.printStackTrace(); // 回收资源 if(this.heatbeat != null)&#123; this.heatbeat.cancel(true); this.heatbeat = null; &#125; ctx.close(); &#125; class HeatbeatTask implements Runnable&#123; private ChannelHandlerContext ctx; public HeatbeatTask()&#123; &#125; public HeatbeatTask(ChannelHandlerContext ctx)&#123; this.ctx = ctx; &#125; public void run()&#123; try &#123; HeatbeatMessage msg = new HeatbeatMessage(); msg.setIp(remoteAddr.getHostAddress()); Sigar sigar = new Sigar(); // CPU信息 CpuPerc cpuPerc = sigar.getCpuPerc(); Map&lt;String, Object&gt; cpuMsgMap = new HashMap&lt;&gt;(); cpuMsgMap.put("Combined", cpuPerc.getCombined()); cpuMsgMap.put("User", cpuPerc.getUser()); cpuMsgMap.put("Sys", cpuPerc.getSys()); cpuMsgMap.put("Wait", cpuPerc.getWait()); cpuMsgMap.put("Idle", cpuPerc.getIdle()); // 内存信息 Map&lt;String, Object&gt; memMsgMap = new HashMap&lt;&gt;(); Mem mem = sigar.getMem(); memMsgMap.put("Total", mem.getTotal()); memMsgMap.put("Used", mem.getUsed()); memMsgMap.put("Free", mem.getFree()); // 文件系统 Map&lt;String, Object&gt; fileSysMsgMap = new HashMap&lt;&gt;(); FileSystem[] list = sigar.getFileSystemList(); fileSysMsgMap.put("FileSysCount", list.length); List&lt;String&gt; msgList = null; for(FileSystem fs : list)&#123; msgList = new ArrayList&lt;&gt;(); msgList.add(fs.getDevName() + "总大小: " + sigar.getFileSystemUsage(fs.getDirName()).getTotal() + "KB"); msgList.add(fs.getDevName() + "剩余大小: " + sigar.getFileSystemUsage(fs.getDirName()).getFree() + "KB"); fileSysMsgMap.put(fs.getDevName(), msgList); &#125; msg.setCpuMsgMap(cpuMsgMap); msg.setMemMsgMap(memMsgMap); msg.setFileSysMsgMap(fileSysMsgMap); ctx.writeAndFlush(msg); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; HTTP 协议处理 ：使用 Netty 服务开发。实现 HTTP 协议处理逻辑。 没有客户端，做一个http协议文件传输到netty服务器 server 12345678910111213141516171819202122232425262728293031323334353637383940/** * http协议文件传输 * @author Qixuan.Chen * 创建时间：2015年5月4日 */ public class HttpStaticFileServer &#123; private final int port;//端口 public HttpStaticFileServer(int port) &#123; this.port = port; &#125; public void run() throws Exception &#123; EventLoopGroup bossGroup = new NioEventLoopGroup();//线程一 //这个是用于serversocketchannel的event EventLoopGroup workerGroup = new NioEventLoopGroup();//线程二//这个是用于处理accept到的channel try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new HttpStaticFileServerInitializer()); b.bind(port).sync().channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) throws Exception &#123; int port = 8089; if (args.length &gt; 0) &#123; port = Integer.parseInt(args[0]); &#125; else &#123; port = 8089; &#125; new HttpStaticFileServer(port).run();//启动服务 &#125; &#125; 12345678910111213141516171819202122232425262728293031323334public class HttpStaticFileServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; // Create a default pipeline implementation. // 通道的连接点 ChannelPipeline pipeline = ch.pipeline(); // Uncomment the following line if you want HTTPS //SSLEngine engine = SecureChatSslContextFactory.getServerContext().createSSLEngine(); //engine.setUseClientMode(false); //pipeline.addLast("ssl", new SslHandler(engine)); /** * （1）ReadTimeoutHandler，用于控制读取数据的时候的超时，10表示如果10秒钟都没有数据读取了，那么就引发超时，然后关闭当前的channel （2）WriteTimeoutHandler，用于控制数据输出的时候的超时，构造参数1表示如果持续1秒钟都没有数据写了，那么就超时。 （3）HttpRequestrianDecoder，这个handler用于从读取的数据中将http报文信息解析出来，无非就是什么requestline，header，body什么的。。。 （4）然后HttpObjectAggregator则是用于将上卖解析出来的http报文的数据组装成为封装好的httprequest对象。。 （5）HttpresponseEncoder，用于将用户返回的httpresponse编码成为http报文格式的数据 （6）HttpHandler，自定义的handler，用于处理接收到的http请求。 */ pipeline.addLast("decoder", new HttpRequestDecoder());// http-request解码器,http服务器端对request解码 pipeline.addLast("aggregator", new HttpObjectAggregator(65536));//对传输文件大少进行限制 pipeline.addLast("encoder", new HttpResponseEncoder());//http-response解码器,http服务器端对response编码 // 向客户端发送数据的一个Handler pipeline.addLast("chunkedWriter", new ChunkedWriteHandler()); pipeline.addLast("handler", new HttpStaticFileServerHandler(true)); // Specify false if SSL.(如果是ssl,就指定为false) &#125; &#125; serverHandler 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310public class HttpStaticFileServerHandler extends SimpleChannelInboundHandler&lt;FullHttpRequest&gt; &#123; public static final String HTTP_DATE_FORMAT = "EEE, dd MMM yyyy HH:mm:ss zzz"; public static final String HTTP_DATE_GMT_TIMEZONE = "GMT"; public static final int HTTP_CACHE_SECONDS = 60; private final boolean useSendFile; public HttpStaticFileServerHandler(boolean useSendFile) &#123; this.useSendFile = useSendFile; &#125; /** * 类似channelRead方法。 */ @Override public void messageReceived( ChannelHandlerContext ctx, FullHttpRequest request) throws Exception &#123; // 请求头信息是否正确的 if (!request.decoderResult().isSuccess()) &#123; sendError(ctx, BAD_REQUEST); return; &#125; if (request.method() != GET) &#123; sendError(ctx, METHOD_NOT_ALLOWED); return; &#125; final String uri = request.uri(); System.out.println("-----uri----"+uri); final String path = sanitizeUri(uri); System.out.println("-----path----"+path); if (path == null) &#123; sendError(ctx, FORBIDDEN); return; &#125; File file = new File(path); if (file.isHidden() || !file.exists()) &#123; sendError(ctx, NOT_FOUND); return; &#125; if (file.isDirectory()) &#123; if (uri.endsWith("/")) &#123; sendListing(ctx, file); &#125; else &#123; sendRedirect(ctx, uri + '/'); &#125; return; &#125; if (!file.isFile()) &#123; sendError(ctx, FORBIDDEN); return; &#125; // Cache Validation String ifModifiedSince = (String) request.headers().get(IF_MODIFIED_SINCE); if (ifModifiedSince != null &amp;&amp; !ifModifiedSince.isEmpty()) &#123; SimpleDateFormat dateFormatter = new SimpleDateFormat(HTTP_DATE_FORMAT, Locale.US); Date ifModifiedSinceDate = dateFormatter.parse(ifModifiedSince); // Only compare up to the second because the datetime format we send to the client // does not have milliseconds long ifModifiedSinceDateSeconds = ifModifiedSinceDate.getTime() / 1000; long fileLastModifiedSeconds = file.lastModified() / 1000; if (ifModifiedSinceDateSeconds == fileLastModifiedSeconds) &#123; sendNotModified(ctx); return; &#125; &#125; // 文件下载 RandomAccessFile raf; try &#123; raf = new RandomAccessFile(file, "r"); &#125; catch (FileNotFoundException fnfe) &#123; sendError(ctx, NOT_FOUND); return; &#125; long fileLength = raf.length(); HttpResponse response = new DefaultHttpResponse(HTTP_1_1, OK); //setContentLength(response, fileLength); HttpHeaderUtil.setContentLength(response, fileLength); setContentTypeHeader(response, file); setDateAndCacheHeaders(response, file); if (HttpHeaderUtil.isKeepAlive(request)) &#123; response.headers().set(CONNECTION, HttpHeaderValues.KEEP_ALIVE); &#125; // Write the initial line and the header. ctx.write(response); // Write the content. ChannelFuture sendFileFuture; if (useSendFile) &#123; sendFileFuture = ctx.write(new DefaultFileRegion(raf.getChannel(), 0, fileLength), ctx.newProgressivePromise()); &#125; else &#123; sendFileFuture = ctx.write(new ChunkedFile(raf, 0, fileLength, 8192), ctx.newProgressivePromise()); &#125; sendFileFuture.addListener(new ChannelProgressiveFutureListener() &#123; @Override public void operationProgressed(ChannelProgressiveFuture future, long progress, long total) &#123; if (total &lt; 0) &#123; // total unknown System.err.println("Transfer progress: " + progress); &#125; else &#123; System.err.println("Transfer progress: " + progress + " / " + total); &#125; &#125; @Override public void operationComplete(ChannelProgressiveFuture future) throws Exception &#123; System.err.println("Transfer complete."); &#125; &#125;); // Write the end marker ChannelFuture lastContentFuture = ctx.writeAndFlush(LastHttpContent.EMPTY_LAST_CONTENT); // Decide whether to close the connection or not. if (!HttpHeaderUtil.isKeepAlive(request)) &#123; // Close the connection when the whole content is written out. lastContentFuture.addListener(ChannelFutureListener.CLOSE); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); if (ctx.channel().isActive()) &#123; sendError(ctx, INTERNAL_SERVER_ERROR); &#125; &#125; private static final Pattern INSECURE_URI = Pattern.compile(".*[&lt;&gt;&amp;\"].*"); /** * 路径解码 * @param uri * @return */ private static String sanitizeUri(String uri) &#123; // Decode the path. try &#123; uri = URLDecoder.decode(uri, "UTF-8"); &#125; catch (UnsupportedEncodingException e) &#123; try &#123; uri = URLDecoder.decode(uri, "ISO-8859-1"); &#125; catch (UnsupportedEncodingException e1) &#123; throw new Error(); &#125; &#125; if (!uri.startsWith("/")) &#123; return null; &#125; // Convert file separators. uri = uri.replace('/', File.separatorChar); // Simplistic dumb security check. // You will have to do something serious in the production environment. if (uri.contains(File.separator + '.') || uri.contains('.' + File.separator) || uri.startsWith(".") || uri.endsWith(".") || INSECURE_URI.matcher(uri).matches()) &#123; return null; &#125; // Convert to absolute path. return System.getProperty("user.dir") + File.separator + uri; &#125; private static final Pattern ALLOWED_FILE_NAME = Pattern.compile("[A-Za-z0-9][-_A-Za-z0-9\\.]*"); private static void sendListing(ChannelHandlerContext ctx, File dir) &#123; FullHttpResponse response = new DefaultFullHttpResponse(HTTP_1_1, OK); response.headers().set(CONTENT_TYPE, "text/html; charset=UTF-8"); StringBuilder buf = new StringBuilder(); String dirPath = dir.getPath(); buf.append("&lt;!DOCTYPE html&gt;\r\n"); buf.append("&lt;html&gt;&lt;head&gt;&lt;title&gt;"); buf.append("Listing of: "); buf.append(dirPath); buf.append("&lt;/title&gt;&lt;/head&gt;&lt;body&gt;\r\n"); buf.append("&lt;h3&gt;Listing of: "); buf.append(dirPath); buf.append("&lt;/h3&gt;\r\n"); buf.append("&lt;ul&gt;"); buf.append("&lt;li&gt;&lt;a href=\"../\"&gt;..&lt;/a&gt;&lt;/li&gt;\r\n"); for (File f: dir.listFiles()) &#123; if (f.isHidden() || !f.canRead()) &#123; continue; &#125; String name = f.getName(); if (!ALLOWED_FILE_NAME.matcher(name).matches()) &#123; continue; &#125; buf.append("&lt;li&gt;&lt;a href=\""); buf.append(name); buf.append("\"&gt;"); buf.append(name); buf.append("&lt;/a&gt;&lt;/li&gt;\r\n"); &#125; buf.append("&lt;/ul&gt;&lt;/body&gt;&lt;/html&gt;\r\n"); ByteBuf buffer = Unpooled.copiedBuffer(buf, CharsetUtil.UTF_8); response.content().writeBytes(buffer); buffer.release(); // Close the connection as soon as the error message is sent. ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); &#125; private static void sendRedirect(ChannelHandlerContext ctx, String newUri) &#123; FullHttpResponse response = new DefaultFullHttpResponse(HTTP_1_1, FOUND); response.headers().set(LOCATION, newUri); // Close the connection as soon as the error message is sent. ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); &#125; private static void sendError(ChannelHandlerContext ctx, HttpResponseStatus status) &#123; FullHttpResponse response = new DefaultFullHttpResponse( HTTP_1_1, status, Unpooled.copiedBuffer("Failure: " + status.toString() + "\r\n", CharsetUtil.UTF_8)); response.headers().set(CONTENT_TYPE, "text/plain; charset=UTF-8"); // Close the connection as soon as the error message is sent. ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); &#125; /** * When file timestamp is the same as what the browser is sending up, send a "304 Not Modified" * * @param ctx * Context */ private static void sendNotModified(ChannelHandlerContext ctx) &#123; FullHttpResponse response = new DefaultFullHttpResponse(HTTP_1_1, NOT_MODIFIED); setDateHeader(response); // Close the connection as soon as the error message is sent. ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE); &#125; /** * Sets the Date header for the HTTP response * * @param response * HTTP response */ private static void setDateHeader(FullHttpResponse response) &#123; SimpleDateFormat dateFormatter = new SimpleDateFormat(HTTP_DATE_FORMAT, Locale.US); dateFormatter.setTimeZone(TimeZone.getTimeZone(HTTP_DATE_GMT_TIMEZONE)); Calendar time = new GregorianCalendar(); response.headers().set(DATE, dateFormatter.format(time.getTime())); &#125; /** * Sets the Date and Cache headers for the HTTP Response * * @param response * HTTP response * @param fileToCache * file to extract content type */ private static void setDateAndCacheHeaders(HttpResponse response, File fileToCache) &#123; SimpleDateFormat dateFormatter = new SimpleDateFormat(HTTP_DATE_FORMAT, Locale.US); dateFormatter.setTimeZone(TimeZone.getTimeZone(HTTP_DATE_GMT_TIMEZONE)); // Date header Calendar time = new GregorianCalendar(); response.headers().set(DATE, dateFormatter.format(time.getTime())); // Add cache headers time.add(Calendar.SECOND, HTTP_CACHE_SECONDS); response.headers().set(EXPIRES, dateFormatter.format(time.getTime())); response.headers().set(CACHE_CONTROL, "private, max-age=" + HTTP_CACHE_SECONDS); response.headers().set( LAST_MODIFIED, dateFormatter.format(new Date(fileToCache.lastModified()))); &#125; /** * Sets the content type header for the HTTP Response * * @param response * HTTP response * @param file * file to extract content type */ private static void setContentTypeHeader(HttpResponse response, File file) &#123; MimetypesFileTypeMap mimeTypesMap = new MimetypesFileTypeMap(); response.headers().set(CONTENT_TYPE, mimeTypesMap.getContentType(file.getPath())); &#125; &#125; 流数据的传输处理 ​ 在基于流的传输里比如 TCP/IP，接收到的数据会先被存储到一个 socket 接收缓冲里。不 幸的是，基于流的传输并不是一个数据包队列，而是一个字节队列。即使你发送了 2 个独立 的数据包，操作系统也不会作为 2 个消息处理而仅仅是作为一连串的字节而言。因此这是不 能保证你远程写入的数据就会准确地读取。所以一个接收方不管他是客户端还是服务端，都 应该把接收到的数据整理成一个或者多个更有意思并且能够让程序的业务逻辑更好理解的 数据。 在处理流数据粘包拆包时，可以使用下述处理方式： 使用定长数据处理，如：每个完整请求数据长度为 8 字节等。（FixedLengthFrameDecoder） 使用特殊分隔符的方式处理，如：每个完整请求数据末尾使用’\0’作为数据结束标记。（DelimiterBasedFrameDecoder） 使用自定义协议方式处理，如：http 协议格式等。 使用 POJO 来替代传递的流数据，如：每个完整的请求数据都是一个 RequestMessage 对象，在 Java 语言中，使用 POJO 更符合语种特性，推荐使用。 转载至微信公众号《Java患者》。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>并发和多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于fastjson解析后的属性排序问题解决方案]]></title>
    <url>%2F%E5%85%B3%E4%BA%8Efastjson%E8%A7%A3%E6%9E%90%E5%90%8E%E7%9A%84%E5%B1%9E%E6%80%A7%E6%8E%92%E5%BA%8F%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.html</url>
    <content type="text"><![CDATA[问题起因：123String testJson = "&#123;\"lh_u\":32,\"ps\":7,\"qt\":24,\"ns\":87,\"gm\":11,\"dp\":2,\"lh_ua\":34,\"ft\":82,\"jzsj\":\"24时\",\"lg\":71,\"yt\":6,\"jzrq\":\"3月4日\",\"ba\":62&#125;";JSONObject jsonObject = JSON.parseObject(jsonString); ----- ①System.out.println(jsonObject.toJSONString()); 上述方法得到的jsonObject属性排序和字符串的字段排序是一样的，如果想对jsonObject的属性进行排序，可以做此处理：12JSONObject jsonObjectNew1 = JSON.parseObject(JSON.toJSONString(jsonObject,SerializerFeature.MapSortField));JSONObject jsonObjectNew2 = JSON.parseObject(JSON.toJSONString(jsonObject,SerializerFeature.MapSortField),Feature.OrderedField); 将jsonObject转成按字母排序的字符串，然后再转成jsonObject对象，再次转成jsonObject时如果不加Feature.OrderedField，则jsonObject和①中的一样，加上则保持属性顺序不变。参考文章]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>fastjson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hashcode详解]]></title>
    <url>%2Fhashcode%E8%AF%A6%E8%A7%A3.html</url>
    <content type="text"><![CDATA[序言写这篇文章是因为在看hashMap源码时遇到有什么hashcode值，然后就去查，脑袋里面是有映像的，不就是在Object中有equals和hashcode方法嘛，这在学java基础的时候就遇到过，不过那时候无所谓，不懂就不懂，就一笔带过去了，然后到现在，又回过头来补本应该以前就搞清楚的问题，所以知道了一个道理，学习不是一个追求速度的事情，不懂就要去查清楚，弄明白，一步一个脚印，虽然刚开始可能会很慢，不过慢慢的，学习的多了，理解的多了，会越来越快的。越来越轻松，不至于现在还在补原来的知识。后悔也无事于补了，起码现在知道了这个道理，学习永远都不会迟，只要突然一天的醒悟，一切都会慢慢好起来的。 弄懂这知识点，查了很多博文，发现很多类似，我不知道哪个才是原著，所以复制一个我觉得比较好的，不过看了也有些迷惑，我会写下来。引用博文 一、hashcode是什么？1、hash和hash表是什么？ 想要知道这个hashcode，首先得知道hash，通过百度百科看一下 hash是一个函数，该函数中的实现就是一种算法，就是通过一系列的算法来得到一个hash值，这个时候，我们就需要知道另一个东西，hash表，通过hash算法得到的hash值就在这张hash表中，也就是说，hash表就是所有的hash值组成的，有很多种hash函数，也就代表着有很多种算法得到hash值，如上面截图的三种，等会我们就拿第一种来说。 2、hashcode 有了前面的基础，这里讲解就简单了，hashcode就是通过hash函数得来的，通俗的说，就是通过某一种算法得到的，hashcode就是在hash表中有对应的位置。每个对象都有hashcode，对象的hashcode怎么得来的呢？ 首先一个对象肯定有物理地址，在别的博文中会hashcode说成是代表对象的地址，这里肯定会让读者形成误区，对象的物理地址跟这个hashcode地址不一样，hashcode代表对象的地址说的是对象在hash表中的位置，物理地址说的对象存放在内存中的地址，那么对象如何得到hashcode呢？通过对象的内部地址(也就是物理地址)转换成一个整数，然后该整数通过hash函数的算法就得到了hashcode，所以，hashcode是什么呢？就是在hash表中对应的位置。这里如果还不是很清楚的话，举个例子，hash表中有 hashcode为1、hashcode为2、(…)3、4、5、6、7、8这样八个位置，有一个对象A，A的物理地址转换为一个整数17(这是假如)，就通过直接取余算法，17%8=1，那么A的hashcode就为1，且A就在hash表中1的位置。肯定会有其他疑问，接着看下面，这里只是举个例子来让你们知道什么是hashcode的意义。 二、hashcode有什么作用呢？前面说了这么多关于hash函数，和hashcode是怎么得来的，还有hashcode对应的是hash表中的位置，可能大家就有疑问，为什么hashcode不直接写物理地址呢，还要另外用一张hash表来代表对象的地址？接下来就告诉你hashcode的作用， 1、HashCode的存在主要是为了查找的快捷性，HashCode是用来在散列存储结构中确定对象的存储地址的(后半句说的用hashcode来代表对象就是在hash表中的位置) 为什么hashcode就查找的更快，比如：我们有一个能存放1000个数这样大的内存中，在其中要存放1000个不一样的数字，用最笨的方法，就是存一个数字，就遍历一遍，看有没有相同得数，当存了900个数字，开始存901个数字的时候，就需要跟900个数字进行对比，这样就很麻烦，很是消耗时间，用hashcode来记录对象的位置，来看一下。hash表中有1、2、3、4、5、6、7、8个位置，存第一个数，hashcode为1，该数就放在hash表中1的位置，存到100个数字，hash表中8个位置会有很多数字了，1中可能有20个数字，存101个数字时，他先查hashcode值对应的位置，假设为1，那么就有20个数字和他的hashcode相同，他只需要跟这20个数字相比较(equals)，如果每一个相同，那么就放在1这个位置，这样比较的次数就少了很多，实际上hash表中有很多位置，这里只是举例只有8个，所以比较的次数会让你觉得也挺多的，实际上，如果hash表很大，那么比较的次数就很少很少了。 通过对原始方法和使用hashcode方法进行对比，我们就知道了hashcode的作用，并且为什么要使用hashcode了 三、equals方法和hashcode的关系？通过前面这个例子，大概可以知道，先通过hashcode来比较，如果hashcode相等，那么就用equals方法来比较两个对象是否相等，用个例子说明：上面说的hash表中的8个位置，就好比8个桶，每个桶里能装很多的对象，对象A通过hash函数算法得到将它放到1号桶中，当然肯定有别的对象也会放到1号桶中，如果对象B也通过算法分到了1号桶，那么它如何识别桶中其他对象是否和它一样呢，这时候就需要equals方法来进行筛选了。1、如果两个对象equals相等，那么这两个对象的HashCode一定也相同 2、如果两个对象的HashCode相同，不代表两个对象就相同，只能说明这两个对象在散列存储结构中，存放于同一个位置这两条你们就能够理解了。 四、为什么equals方法重写的话，建议也一起重写hashcode方法？（如果对象的equals方法被重写，那么对象的HashCode方法也尽量重写）举个例子，其实就明白了这个道理，比如：有个A类重写了equals方法，但是没有重写hashCode方法，看输出结果，对象a1和对象a2使用equals方法相等，按照上面的hashcode的用法，那么他们两个的hashcode肯定相等，但是这里由于没重写hashcode方法，他们两个hashcode并不一样，所以，我们在重写了equals方法后，尽量也重写了hashcode方法，通过一定的算法，使他们在equals相等时，也会有相同的hashcode值。 实例：现在来看一下String的源码中的equals方法和hashcode方法。这个类就重写了这两个方法，现在为什么需要重写这两个方法了吧？equals方法：其实跟我上面写的那个例子是一样的原理，所以通过源码又知道了String的equals方法验证的是两个字符串的值是否一样。还有Double类也重写了这些方法。很多类有比较这类的，都重写了这两个方法，因为在所有类的父类Object中。equals的功能就是 “==”号的功能。你们还可以比较String对象的equals和==的区别啦。这里不再说明。 转载至]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[greenplum安装与部署（详细）]]></title>
    <url>%2Fgreenplum%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2%EF%BC%88%E8%AF%A6%E7%BB%86%EF%BC%89.html</url>
    <content type="text"><![CDATA[修改系统配置1.1 gp服务器列表192.168.7.11 hadoop01 主节点192.168.7.12 hadoop02 数据节点1192.168.7.13 hadoop03 数据节点2192.168.7.14 hadoop04 主节点切换备份节点 1.2 修改系统配置项关闭SELINUXvi /etc/selinux/configSELINUX=disabled 1.3 关闭防火墙systemctl status firewalldsystemctl stop firewalld.servicesystemctl disable firewalld.service 1.4 修改内核配置参数,并执行 sysctl -p使之生效123456789101112131415161718192021vi /etc/sysctl.confkernel.shmmax = 500000000kernel.shmmni = 4096kernel.shmall = 4000000000kernel.sem = 2000 4096000 2000 2048kernel.sysrq = 1kernel.core_uses_pid = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.msgmni = 2048net.ipv4.tcp_syncookies = 1net.ipv4.ip_forward = 0net.ipv4.conf.default.accept_source_route = 0net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_max_syn_backlog = 4096net.ipv4.conf.all.arp_filter = 1net.ipv4.ip_local_port_range = 1025 65535net.core.netdev_max_backlog = 10000net.core.rmem_max = 2097152net.core.wmem_max = 2097152vm.overcommit_memory = 2 (服务器核数，根据实际情况修改参数) 12345vi /etc/security/limits.conf* soft nofile 65536* hard nofile 65536* soft nproc 131072* hard nproc 131072 1.5 配置集群中各节点hosts信息12345vi /etc/hosts192.168.7.11 hadoop01 192.168.7.12 hadoop02 192.168.7.13 hadoop03 192.168.7.14 hadoop04 安装greenplum-db2.1 下载greenplum下载地址执行安装包：1rpm -ivh greenplum-db-6.1.0-rhel7-x86_64.rpm --nodeps --force 2.2 创建gpadmin用户解压完成后以root身份创建gpadmin用户和组，用来管理greenplum-db123456groupadd gpadmin # 创建分组 useradd gpadmin -g gpadmin # 创建用户并分配组 passwd gpadmin # 为gpadmin分配密码 cd /usr/local/greenplum-db chown -R gpadmin:gpadmin greenplum-db/ chown -R gpadmin:gpadmin greenplum-cc-web/ 2.3 创建配置文件切换到gpadmin用户下123su - gpadmin mkdir -p /usr/local/greenplum-db/gpconfigs cd /usr/local/greenplum-db/gpconfigs 创建配置文件12345678910vim hostfile_exkeys输入hadoop01hadoop02hadoop03hadoop04vi hostfilehadoop02hadoop03hadoop04 2.4 在gpadmin和root用户下添加环境变量12345678vi ~/.bashrc##添加以下内容 export LD_LIBRARY_PATH=$GPHOME/lib export MASTER_DATA_DIRECTORY=/home/gpadmin/gpdata/master/gpseg-1 source /usr/local/greenplum-db/greenplum_path.sh source /usr/local/greenplum-cc-web/gpcc_path.sh保存退出，执行下面语句使其生效source ~/.bashrc 2.5 切换到root用户123gpssh-exkeys -f /usr/local/greenplum-db/gpconfigs/hostfile_exkeys #拷贝mster节点公钥至各segment节点 gpseginstall -f /usr/local/greenplum-db/gpconfigs/hostfile_exkeys -p gpadmin 说明gpssh-exkeys -f hostfile_exkeys 将会在master节点生成公私钥，并拷贝至hostfile_exkeys各segment节点，实现后续无密钥登陆gpseginstall -f /usr/local/greenplum-db/gpconfigs/hostfile_exkeys -p gpadmin 使用默认用户名(gpadmin) 密码：gpadmin 在各segment节点安装Greenplum-db 2.6 在master及各segment节点创建数据存储目录1234567mkdir /home/gpadmin/gpdata/mastercd /home/gpadmin/chown -R gpadmin:gpadmin gpdata/mastergpssh -f /usr/local/greenplum-db/gpconfigs/hostfile -e "mkdir -p /home/gpadmin/gpdata/data1/primary;mkdir -p /home/gpadmin/gpdata/data2/primary”gpssh -f /usr/local/greenplum-db/gpconfigs/hostfile -e "mkdir -p /home/gpadmin/gpdata/data1/mirror;mkdir -p /home/gpadmin/gpdata/data2/mirror"gpssh -f /usr/local/greenplum-db/gpconfigs/hostfile -e "chown -R gpadmin:gpadmin /home/gpadmin/gpdata"gpssh -f /usr/local/greenplum-db/gpconfigs/hostfile_exkeys -v -e 'ntpd' 切换到gpadmin用户下,初始化数据库集群12345678910111213141516su - gpadmin cd /usr/local/greenplum-db/ cp /usr/local/greenplum-db/docs/cli_help/gpconfigs/gpinitsystem_config /usr/local/greenplum-db/gpconfigs/gpinitsystem_config vi /usr/local/greenplum-db/gpconfigs/gpinitsystem_config###修改以下内容MASTER_HOSTNAME=mdwPORT_BASE=40000declare -a DATA_DIRECTORY=(/home/gpadmin/gpdata/data1/primary /home/gpadmin/gpdata/data1/primary /home/gpadmin/gpdata/data2/primary /home/gpadmin/gpdata/data2/primary)MASTER_DIRECTORY=/home/gpadmin/gpdata/masterMASTER_PORT=5432MIRROR_PORT_BASE=50000REPLICATION_PORT_BASE=41000MIRROR_REPLICATION_PORT_BASE=51000declare -a MIRROR_DATA_DIRECTORY=(/home/gpadmin/gpdata/data1/mirror /home/gpadmin/gpdata/data1/mirror /home/gpadmin/gpdata/data2/mirror /home/gpadmin/gpdata/data2/mirror)DATABASE_NAME=gpdbMACHINE_LIST_FILE=/usr/local/greenplum-db/gpconfigs/hostfile 2.7 配置修改完成之后，执行以下命令初始化数据库1gpinitsystem -c /usr/local/greenplum-db/gpconfigs/gpinitsystem_config -h /usr/local/greenplum-db/gpconfigs/hostfile 2.8 psql修改数据库密码123456psql -d gpdb gpdb=# alter user gpadmin with password 'gpadmin' gpdb=# \q 然后，重启greenplumdb集群gpstop -agpstart -a 2.9 使用pgadmin，navicat 等工具连接连接时，如果登录不成功，一般报错如下样子： psql: FATAL: no pg_hba.conf entry for host “192.168.xxx.xxx”,表示访问权限不够修改文件/home/gpadmin/gpdata/master/gpseg-1/pg_hba.conf]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>greenplum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git配置多个SSH-Key]]></title>
    <url>%2Fgit%E9%85%8D%E7%BD%AE%E5%A4%9A%E4%B8%AASSH-Key.html</url>
    <content type="text"><![CDATA[1、生成一个公司用的SSH-Key1$ ssh-keygen -t rsa -C "youremail@yourcompany.com” -f ~/.ssh/id-rsa 在~/.ssh/目录会生成id-rsa和id-rsa.pub私钥和公钥。 我们将id-rsa.pub中的内容粘帖到公司gitlab服务器的SSH-key的配置中。 2、 生成一个github用的SSH-Key1$ ssh-keygen -t rsa -C "youremail@your.com” -f ~/.ssh/github-rsa 在~/.ssh/目录会生成github-rsa和github-rsa.pub私钥和公钥。 我们将github-rsa.pub中的内容粘帖到github服务器的SSH-key的配置中。 3、添加私钥1$ ssh-add ~/.ssh/id_rsa $ ssh-add ~/.ssh/github_rsa 如果执行ssh-add时提示”Could not open a connection to your authentication agent”，可以现执行命令：1$ ssh-agent bash 然后再运行ssh-add命令。可以通过 ssh-add -l 来确私钥列表1$ ssh-add -l 可以通过 ssh-add -D 来清空私钥列表1$ ssh-add -D 4、修改配置文件在 ~/.ssh 目录下新建一个config文件1touch config 添加内容：12345678910# gitlabHost gitlab.com HostName gitlab.com PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa# githubHost github.com HostName github.com PreferredAuthentications publickey IdentityFile ~/.ssh/github_rsa 5、测试1$ ssh -T git@github.com 输出Hi stefzhlg! You’ve successfully authenticated, but GitHub does not provide shell access.]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows添加公钥后，无法克隆的问题解决]]></title>
    <url>%2FPermission.html</url>
    <content type="text"><![CDATA[背景：windows下添加公钥后，无法克隆github 上的文件，报错Permission denied (publickey). fatal: Could not read from remote repository. 博主在github上下载tiny face的的源代码的时候，遇到git clone命令为：git clone –recursive git@github.com:peiyunh/tiny.git 而当我在ternimal下执行这条语句的时候，出现错误： Permissiondenied (publickey). fatal:Could not read from remote repository. Pleasemake sure you have the correct access rights and the repository exists. 但是其实执行命令：git clone git@github.com:peiyunh/tiny.git 是没有问题的（不加–recursive参数），于是百度了一番，我理解的是原因是由于你在本地（或者服务器上）没有生成ssh key，你可以在ternimal下执行： cd ~/.ssh ls来查看是否有文件id_rsa以及文件id_rsa.pub，如下图所示：（我的已经生成了，所以我ls后会显示。） 下面记录下解决办法： 1.首先，如果你没有ssh key的话，在ternimal下输入命令：ssh-keygen -t rsa -C “youremail@example.com”， youremail@example.com改为自己的邮箱即可，途中会让你输入密码啥的，不需要管，一路回车即可，会生成你的ssh key。（如果重新生成的话会覆盖之前的ssh key。） 2.然后再ternimal下执行命令： ssh -v git@github.com 最后两句会出现： No more authentication methods to try. Permission denied (publickey). 3.这时候再在ternimal下输入： ssh-agent -s 然后会提示类似的信息： SSH_AUTH_SOCK=/tmp/ssh-GTpABX1a05qH/agent.404; export SSH_AUTH_SOCK; SSH_AGENT_PID=13144; export SSH_AGENT_PID; echo Agent pid 13144; 4.接着再输入： ssh-add ~/.ssh/id_rsa 这时候应该会提示： Identity added: …（这里是一些ssh key文件路径的信息） （注意）如果出现错误提示： Could not open a connection to your authentication agent. 请执行命令：eval ssh-agent -s后继续执行命令 ssh-add ~/.ssh/id_rsa，这时候一般没问题啦。 5.打开你刚刚生成的id_rsa.pub，将里面的内容复制，进入你的github账号，在settings下，SSH and GPG keys下new SSH key，title随便取一个名字，然后将id_rsa.pub里的内容复制到Key中，完成后Add SSH Key。 6.最后一步，验证Key 在ternimal下输入命令： ssh -T git@github.com 提示：Hi xxx! You’ve successfully authenticated, but GitHub does not provide shell access. 这时候你的问题就解决啦，可以使用命令 git clone –recursive git@github.com:peiyunh/tiny.git 去下载你的代码啦。]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka基本操作]]></title>
    <url>%2Fkafka%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C.html</url>
    <content type="text"><![CDATA[kafka基本操作：查看topic主题：kafka-topics.sh –list –zookeeper node1:2181,node2:2181,node3:2181 查看分区：kafka-topics.sh –zookeeper node2:2181,node3:2181,node4:2181 –describe –topic MotorVehiclekafka-topics.sh –zookeeper node2:2181,node3:2181,node4:2181 –describe –topic WifiRecordkafka-topics.sh –zookeeper node2:2181,node3:2181,node4:2181 –describe –topic ImsiRecord 创建topic主题：kafka-topics.sh –create –zookeeper node1:2181,node2:2181,node3:2181 –replication-factor 1 –partitions 1 –topic MotorImsi 删除主题：kafka-topics.sh –delete –zookeeper node1:2181,node2:2181,node3:2181 –topic Test004 开启生产者：（注意kafka所在节点）目前都是单节点kafka-console-producer.sh –broker-list node3:6667 –topic MotorWifi 开启消费者：kafka-console-consumer.sh –bootstrap-server node1:6667,node2:6667,node3:6667 –topic Test003 –from-beginning kafka中默认消息的保留时间是7天，若想更改，需在配置文件server.properties里更改选项：log.retention.hours=168但是有的时候我们需要对某一个主题的消息存留的时间进行变更，而不影响其他主题。 可以使用命令：kafka-configs.sh –zookeeper localhost:2181 –entity-type topics –entity-name topicName –alter –add-config log.retention.hours=120使得主题的留存时间保存为5天 如果报错的话，可以将时间单位更改成毫秒：kafka-configs.sh –zookeeper localhost:2181 –entity-type topics –entity-name test –alter –add-config retention.ms=43200000 将jar包放入后台运行：nohup java -jar xinyi_kafka_consumer-0.0.1-SNAPSHOT-imsi.jar &gt;&gt; xinyi_kafka_consumer-0.0.1-SNAPSHOT-imsi.out 2&gt;&amp;1 &amp; nohup java -jar xinfo-spark-scheduler.jar &gt;&gt; xinfo-spark-scheduler.out 2&gt;&amp;1 &amp; nohup java -jar xinyi_kafka_consumer_video-0.0.1-SNAPSHOT.jar &gt;&gt; xinyi_kafka_consumer_video-0.0.1-SNAPSHOT.out 2&gt;&amp;1 &amp;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>KafKa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka如何彻底删除topic及数据]]></title>
    <url>%2Fkafka%E5%A6%82%E4%BD%95%E5%BD%BB%E5%BA%95%E5%88%A0%E9%99%A4topic%E5%8F%8A%E6%95%B0%E6%8D%AE.html</url>
    <content type="text"><![CDATA[前言：删除kafka topic及其数据，严格来说并不是很难的操作。但是，往往给kafka 使用者带来诸多问题。项目组之前接触过多个开发者，发现都会偶然出现无法彻底删除kafka的情况。本文总结多个删除kafka topic的应用场景，总结一套删除kafka topic的标准操作方法。 step1：如果需要被删除topic 此时正在被程序 produce和consume，则这些生产和消费程序需要停止。因为如果有程序正在生产或者消费该topic，则该topic的offset信息一致会在broker更新。调用kafka delete命令则无法删除该topic。同时，需要设置 auto.create.topics.enable = false，默认设置为true。如果设置为true，则produce或者fetch 不存在的topic也会自动创建这个topic。这样会给删除topic带来很多意向不到的问题。所以，这一步很重要，必须设置auto.create.topics.enable = false，并认真把生产和消费程序彻底全部停止。 step2：server.properties 设置 delete.topic.enable=true如果没有设置 delete.topic.enable=true，则调用kafka 的delete命令无法真正将topic删除，而是显示（marked for deletion） step3：调用命令删除topic：./bin/kafka-topics –delete –zookeeper 【zookeeper server:port】 –topic 【topic name】 step4：删除kafka存储目录（server.properties文件log.dirs配置，默认为”/data/kafka-logs”）相关topic的数据目录。注意：如果kafka 有多个 broker，且每个broker 配置了多个数据盘（比如 /data/kafka-logs,/data1/kafka-logs …），且topic也有多个分区和replica，则需要对所有broker的所有数据盘进行扫描，删除该topic的所有分区数据。 一般而言，经过上面4步就可以正常删除掉topic和topic的数据。但是，如果经过上面四步，还是无法正常删除topic，则需要对kafka在zookeeer的存储信息进行删除。具体操作如下：（注意：以下步骤里面，kafka在zk里面的节点信息是采用默认值，如果你的系统修改过kafka在zk里面的节点信息，则需要根据系统的实际情况找到准确位置进行操作） step5：找一台部署了zk的服务器，使用命令：bin/zkCli.sh -server 【zookeeper server:port】登录到zk shell，然后找到topic所在的目录：ls /brokers/topics，找到要删除的topic，然后执行命令：rmr /brokers/topics/【topic name】即可，此时topic被彻底删除。如果topic 是被标记为 marked for deletion，则通过命令 ls /admin/delete_topics，找到要删除的topic，然后执行命令：rmr /admin/delete_topics/【topic name】备注：网络上很多其它文章还说明，需要删除topic在zk上面的消费节点记录、配置节点记录，比如：rmr /consumers/【consumer-group】rmr /config/topics/【topic name】其实正常情况是不需要进行这两个操作的，如果需要，那都是由于操作不当导致的。比如step1停止生产和消费程序没有做，step2没有正确配置。也就是说，正常情况下严格按照step1 – step5 的步骤，是一定能够正常删除topic的。step6：完成之后，调用命令：./bin/kafka-topics.sh –list –zookeeper 【zookeeper server:port】查看现在kafka的topic信息。正常情况下删除的topic就不会再显示。但是，如果还能够查询到删除的topic，则重启zk和kafka即可。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改kafka保留天数对数据入库的影响]]></title>
    <url>%2F%E4%BF%AE%E6%94%B9kafka%E4%BF%9D%E7%95%99%E5%A4%A9%E6%95%B0%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%85%A5%E5%BA%93%E7%9A%84%E5%BD%B1%E5%93%8D.html</url>
    <content type="text"><![CDATA[场景介绍：Kafka集群三个broker，同时有一个生产者和一个消费者，生产者producer 已生产2个多小时约10万条数据，同时消费者将数据插入hbase中。 测试：1、打开ambari操作界面，在kafka配置页下修改保留天数的参数log.retention.hours 为1小时，同时修改文件大小segment为10000。（此处数值的设置，为了方便测试）https://hexoblog-1254111960.cos.ap-guangzhou.myqcloud.com/%E4%BF%AE%E6%94%B9kafka%E4%BF%9D%E7%95%99%E5%A4%A9%E6%95%B0%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%85%A5%E5%BA%93%E7%9A%84%E5%BD%B1%E5%93%8D.png2、修改完参数之后，逐一重启broker，在重启的过程中，在消费者控制台可以看到如下错误：重启完之后，错误消失。 3、测试结果：当把保留天数修改为1小时，则kafka中的数据只保留最近1小时的数据，早于1小时之前的数据将被删除。注意：如果被删除的数据没有被消费，则该被删除的数据不会被插入库中。所以，修改参数之前，确保数据已经消费过，并入库。]]></content>
  </entry>
  <entry>
    <title><![CDATA[创建分布式图数据库JanusGraph对象的两种方法]]></title>
    <url>%2F%E5%88%9B%E5%BB%BA%E5%88%86%E5%B8%83%E5%BC%8F%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93JanusGraph%E5%AF%B9%E8%B1%A1%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95.html</url>
    <content type="text"><![CDATA[JanusGraph 是一个分布式图数据库，相对于neo4j可进行横向扩展，且存储和图引擎分离，架构优美，本文将介绍JanusGraph的两种创建方式。1、添加Maven依赖1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.janusgraph&lt;/groupId&gt; &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.janusgraph&lt;/groupId&gt; &lt;artifactId&gt;janusgraph-cassandra&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.janusgraph&lt;/groupId&gt; &lt;artifactId&gt;janusgraph-es&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt;&lt;/dependency&gt; 有以下两种方式构建JanusGraph对象1、通过配置文件构建图对象12JanusGraph graph = JanusGraphFactory.open("janusgraph/conf/janusgraph-cassandra-es.properties");graph.close(); 2、通过Configuration构建图对象123456789101112131415161718192021222324252627282930313233import org.apache.commons.configuration.BaseConfiguration;import org.apache.tinkerpop.gremlin.process.traversal.dsl.graph.GraphTraversalSource;import org.janusgraph.core.JanusGraph;import org.janusgraph.core.JanusGraphFactory; public class Test &#123; public static void main(String[] args) &#123; BaseConfiguration config = new BaseConfiguration(); ////////////使用内存作为存储端 //config.setProperty("storage.backend", "inmemory"); //////////使用cassandra+es作为存储端 config.setProperty("storage.backend", "cassandrathrift"); config.setProperty("storage.cassandra.keyspace", "janus"); config.setProperty("storage.hostname", "127.0.0.1"); config.setProperty("index.search.backend", "elasticsearch"); config.setProperty("index.search.hostname", "127.0.0.1"); config.setProperty("cache.db-cache", "true"); config.setProperty("cache.db-cache-time", "300000"); config.setProperty("cache.db-cache-size", "0.5"); ; JanusGraph graph = JanusGraphFactory.open(config); GraphTraversalSource g = graph.traversal(); //其它逻辑代码 g.tx().rollback(); graph.close(); &#125;&#125;]]></content>
      <categories>
        <category>图数据库</category>
      </categories>
      <tags>
        <tag>JanusGraph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JanusGraph系统架构]]></title>
    <url>%2FJanusGraph%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84.html</url>
    <content type="text"><![CDATA[JanusGraph是一个图形数据库引擎。 JanusGraph本身专注于压缩图序列化、丰富图数据建模、高效的查询执行。 此外，JanusGraph利用Hadoop进行图分析和批处理。JanusGraph为数据持久化，数据索引和客户端访问实现了强大的模块化接口。 JanusGraph的模块化架构使其能够与各种存储，索引和客户端技术进行互操作; 这也使得JanusGraph升级对应的组件过程变得更加简单。在JanusGraph和磁盘之间有一个或多个存储和索引适配器。 JanusGraph标配以下适配器，但JanusGraph的模块化架构支持第三方适配器。 数据存储:Apache CassandraApache HBaseOracle Berkeley DB Java企业版 索引，用于加快访问速度并支持更复杂的查询语句:ElasticsearchApache SolrApache Lucene总体来讲，应用程序可以通过两种方式与JanusGraph进行交互：嵌在应用程序中的JanusGraph在同一个JVM中执行Gremlin语句。 查询任务、JanusGraph缓存和事务处理都在同一个JVM中，而后端数据检索可能是在本地或远程。通过向服务器提交Gremlin查询语句来与本地或远程JanusGraph实例交互。 JanusGraph本身支持Apache TinkerPop栈的Gremlin Server组件。 图 2.1. 高层JanusGraph架构和上下文]]></content>
      <categories>
        <category>图数据库</category>
      </categories>
      <tags>
        <tag>JanusGraph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多节点时间同步]]></title>
    <url>%2F%E5%A4%9A%E8%8A%82%E7%82%B9%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5.html</url>
    <content type="text"><![CDATA[linux 系统有两个时钟：一个是硬件时钟，即BIOS时间；另一个是系统时钟，是linux系统Kernel（内核）时间。系统开启时，系统会读取硬件时间，设置系统时间。因此，设置了系统时间，重启时会失效。要想永久更改时间，可以先同步系统时间（基于网络时间，准确性较高），再同步系统时间。 第一种情况 ：有网以网络时间为准校验 （1）查看时区date – 查看系统时间[root@develop Asia]# date -RFri, 22 Mar 2019 14:11:10 +0800 – +0800 代表是东八区，如果不是，自行更改到东八区 （2）安装ntpdate工具yum install ntpdate （3）同步时间，用的是阿里云的服务器systemctl stop ntpd – 停掉ntpd 服务，使 ntpdate 可以运行ntpdate ntp1.aliyun.com或者 ntpdate time.windows.comntpdate asia.pool.ntp.orgntpdate time.nuri.net （4）同步硬件时间hwclock 查看硬件时间hwclock –systohc –localtime – 同步硬件时间 （5）永久生效hwclock -wsystemctl start ntpd – 结束完之后 ，开启ntpd 第二种情况 ：没有网络这种同步时间的方法，很适合在无网的情况下，同步机器集群时间下面一起操作一遍。如果 有两台机器，选择其中一台机器A，作为服务端，机器B，作为客户端 （1）修改服务端A① 修改配置文件[root@hanadevelop Asia]# vi /etc/ntp.conf ②重启ntpdsystemctl restart ntpd – 要保证ntpd 服务开启，不然其它机器不能同步该机器的时间 （2）修改客户端机器B①修改配置[root@develop Asia]# vi /etc/ntp.conf ②[root@demo sysconfig]# systemctl restart ntpd[root@demo sysconfig]# systemctl enable ntpd[root@demo sysconfig]# systemctl stop ntpd （3）测试①修改机器A的时间[root@hanadevelop Asia]# date -s ‘2019-3-22 17:00:11’2019年 03月 22日 星期五 17:00:11 CST ②同步机器B的时间查看同步完成 ③ 同步硬件时间hwclock –systohc –localtime – 同步硬件时间hwclock -w – 永久生效]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FastJson反序列化为什么用TypeReference]]></title>
    <url>%2FFastJson%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8TypeReference.html</url>
    <content type="text"><![CDATA[泛型序列化非TypeReference code：1234567891011public static void main(String[] args) &#123; Map&lt;String, Person&gt; map = new HashMap&lt;&gt;(16); map.put("one", new Person("zhangsan")); map.put("two", new Person("lisi")); String jsonStr = JSON.toJSONString(map); byte[] bytes = jsonStr.getBytes(); String json = new String(bytes); Map&lt;String, Person&gt; res = JSON.parseObject(json, Map.class); System.out.println(res.get("one")); System.out.println(res.get("one").getName());&#125; 输出：12345678910Exception in thread "main" java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.CommandLineWrapper.main(CommandLineWrapper.java:67)Caused by: java.lang.ClassCastException: com.alibaba.fastjson.JSONObject cannot be cast to subtitle.io.Person at subtitle.io.MyMain.main(MyMain.java:48) ... 5 more&#123;"name":"zhangsan"&#125; 泛型序列化TypeReference code：1234567891011public static void main(String[] args) &#123; Map&lt;String, Person&gt; map = new HashMap&lt;&gt;(16); map.put("one", new Person("zhangsan")); map.put("two", new Person("lisi")); String jsonStr = JSON.toJSONString(map); byte[] bytes = jsonStr.getBytes(); String json = new String(bytes); Map&lt;String, Person&gt; res = JSON.parseObject(json, new TypeReference&lt;Map&lt;String, Person&gt;&gt;()&#123;&#125;); System.out.println(res.get("one")); System.out.println(res.get("one").getName());&#125; 输出：12Person&#123;name='zhangsan'&#125;zhangsan 报错原因： 反序列化时候，虽然添加Map.class，但是没有办法指定Person类型，导致反序列化后的对象为Map]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Fastjson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark项目在IDEA运行正常，spark-submit提示没有合适驱动]]></title>
    <url>%2Fdriver.html</url>
    <content type="text"><![CDATA[java.sql.SQLException: No suitable driver 解决方案报错代码：1Exception in thread "main" java.sql.SQLException: No suitable driver at java.sql.DriverManager.getDriver(DriverManager.java:315) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:85) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:85) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:84) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:35) at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:60) at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80) at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656) at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77) at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656) at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273) at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267) at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:501) at com.xinyi.multiCollision.service.JobService$.dealWithMoreData(JobService.scala:406) at com.xinyi.multiCollision.sparkJob.MultiCollisionApp$.main(MultiCollisionApp.scala:70) at com.xinyi.multiCollision.sparkJob.MultiCollisionApp.main(MultiCollisionApp.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 一般情况下，此报错原因是因为缺少连接数据库的驱动。正常情况下，连接参数，驱动都是能考虑到的。但是在连接参数，驱动已添加的情况下也会报错。本人spark项目在IDEA正常运行，但是spark-submit的时候出现此错误（之前是可以正常运行的，后面报错，怀疑跟集群环境有关）。提交脚本已经配置了参数–conf spark.yarn.jars （虽然已经配置，驱动，jar包有可能不加载进去）。此时的解决方案：将mysql-connector-java-5.1.24-bin.jar的jar包加入集群JAVA_HOME\jre\lib\ext文件夹下，再次运行，问题得以解决。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark2.4.2编译（mac系统下）]]></title>
    <url>%2Fspark2-4-2%E7%BC%96%E8%AF%91.html</url>
    <content type="text"><![CDATA[编译前所注意事项：首先，尽可能阅读官网编译文档 Building Apache Spark源码下载推荐git clone 或者 wget 。编译前确保网络良好。 下载所需要的软件（注意版本）· Spark-2.4.2.tgz· Hadoop-2.7.6· Scala-2.11.12· jdk1.8.0_191· apache-maven-3.6.x· git注意：其中spark是源码，其他是可运行包 解压安装并配置环境变量（过程略）配置完，注意测试。其中，maven配置本地库，镜像地址设置为阿里云地址。1234# 创建本地仓库文件夹mkdir ~/maven_repo# 修改settings.xml文件vim $MAVEN_HOME/conf/settings.xml 部分代码：1234567891011121314151617&lt;!-- localRepository | The path to the local repository maven will use to store artifacts. | | Default: $&#123;user.home&#125;/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt;&lt;localRepository&gt;/home/max/maven_repo&lt;/localRepository&gt;&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*,!cloudera&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt; http://maven.aliyun.com/nexus/content/groups/public &lt;/url&gt;&lt;/mirror&gt; 修改脚本make-distribution.sh编译不使用mvn这个命令,直接用make-distribution.sh脚本，但是需要修改该脚本1234567891011121314151617181920212223242526272829#spark-2.4.2文件夹下vim ./dev/make-distribution.sh#将这些行注释掉 此处为最佳实践，为的是通过指定版本号减少编译时间#VERSION=$("$MVN" help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null\# | grep -v "INFO"\# | grep -v "WARNING"\# | tail -n 1)#SCALA_VERSION=$("$MVN" help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\# | grep -v "INFO"\# | grep -v "WARNING"\# | tail -n 1)#SPARK_HADOOP_VERSION=$("$MVN" help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\# | grep -v "INFO"\# | grep -v "WARNING"\# | tail -n 1)#SPARK_HIVE=$("$MVN" help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\# | grep -v "INFO"\# | grep -v "WARNING"\# | fgrep --count "&lt;id&gt;hive&lt;/id&gt;";\# # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\# # because we use "set -o pipefail"# echo -n)##添加一下参数，注意，版本号要对应自己想要的生产环境VERSION=2.4.2SCALA_VERSION=2.11SPARK_HADOOP_VERSION=hadoop-2.6.0-cdh5.14.0SPARK_HIVE=1 修改源码包spark-2.4.2下的pom.xml123456789101112131415161718192021222324252627282930313233&lt;repositories&gt; &lt;!--&lt;repositories&gt; This should be at top, it makes maven try the central repo first and then othersand hence faster dep resolution &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Maven Repository&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;--&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public//&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 开始编译12345678./dev/make-distribution.sh \--name hadoop-2.6.0-cdh5.14.0 \--tgz \-Phadoop-2.6 \-Dhadoop.version=2.6.0-cdh5.14.0 \-Phive -Phive-thriftserver \-Pyarn \-Pkubernetes 编译大概需要半小时以上，耐心等待就行。编译过程中如果报错，一般有error字样。出现以下字样，代表编译完成：编译后包所在位置，源码包spark-2.4.2根目录下：至此，编译完！]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop分布式文件系统：架构和设计]]></title>
    <url>%2FHadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%EF%BC%9A%E6%9E%B6%E6%9E%84%E5%92%8C%E8%AE%BE%E8%AE%A1.html</url>
    <content type="text"><![CDATA[引言Hadoop分布式文件系统(HDFS)被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统。它和现有的分布式文件系统有很多共同点。但同时，它和其他的分布式文件系统的区别也是很明显的。HDFS是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS放宽了一部分POSIX约束，来实现流式读取文件系统数据的目的。HDFS在最开始是作为Apache Nutch搜索引擎项目的基础架构而开发的。HDFS是Apache Hadoop Core项目的一部分。这个项目的地址是http://hadoop.apache.org/core/。 前提和设计目标硬件错误硬件错误是常态而不是异常。HDFS可能由成百上千的服务器所构成，每个服务器上存储着文件系统的部分数据。我们面对的现实是构成系统的组件数目是巨大的，而且任一组件都有可能失效，这意味着总是有一部分HDFS的组件是不工作的。因此错误检测和快速、自动的恢复是HDFS最核心的架构目标。 流式数据访问运行在HDFS上的应用和普通的应用不同，需要流式访问它们的数据集。HDFS的设计中更多的考虑到了数据批处理，而不是用户交互处理。比之数据访问的低延迟问题，更关键的在于数据访问的高吞吐量。POSIX标准设置的很多硬性约束对HDFS应用系统不是必需的。为了提高数据的吞吐量，在一些关键方面对POSIX的语义做了一些修改。 大规模数据集运行在HDFS上的应用具有很大的数据集。HDFS上的一个典型文件大小一般都在G字节至T字节。因此，HDFS被调节以支持大文件存储。它应该能提供整体上高的数据传输带宽，能在一个集群里扩展到数百个节点。一个单一的HDFS实例应该能支撑数以千万计的文件。 简单的一致性模型HDFS应用需要一个“一次写入多次读取”的文件访问模型。一个文件经过创建、写入和关闭之后就不需要改变。这一假设简化了数据一致性问题，并且使高吞吐量的数据访问成为可能。Map/Reduce应用或者网络爬虫应用都非常适合这个模型。目前还有计划在将来扩充这个模型，使之支持文件的附加写操作。 “移动计算比移动数据更划算”一个应用请求的计算，离它操作的数据越近就越高效，在数据达到海量级别的时候更是如此。因为这样就能降低网络阻塞的影响，提高系统数据的吞吐量。将计算移动到数据附近，比之将数据移动到应用所在显然更好。HDFS为应用提供了将它们自己移动到数据附近的接口。 异构软硬件平台间的可移植性HDFS在设计的时候就考虑到平台的可移植性。这种特性方便了HDFS作为大规模数据应用平台的推广。 Namenode 和 DatanodeHDFS采用master/slave架构。一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。集群中的Datanode一般是一个节点一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组Datanode上。Namenode执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体Datanode节点的映射。Datanode负责处理文件系统客户端的读写请求。在Namenode的统一调度下进行数据块的创建、删除和复制。 HDFS 架构Namenode和Datanode被设计成可以在普通的商用机器上运行。这些机器一般运行着GNU/Linux操作系统(OS)。HDFS采用Java语言开发，因此任何支持Java的机器都可以部署Namenode或Datanode。由于采用了可移植性极强的Java语言，使得HDFS可以部署到多种类型的机器上。一个典型的部署场景是一台机器上只运行一个Namenode实例，而集群中的其它机器分别运行一个Datanode实例。这种架构并不排斥在一台机器上运行多个Datanode，只不过这样的情况比较少见。 集群中单一Namenode的结构大大简化了系统的架构。Namenode是所有HDFS元数据的仲裁者和管理者，这样，用户数据永远不会流过Namenode。 文件系统的名字空间 (namespace)HDFS支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。当前，HDFS不支持用户磁盘配额和访问权限控制，也不支持硬链接和软链接。但是HDFS架构并不妨碍实现这些特性。 Namenode负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被Namenode记录下来。应用程序可以设置HDFS保存的文件的副本数目。文件副本的数目称为文件的副本系数，这个信息也是由Namenode保存的。 数据复制HDFS被设计成能够在一个大集群中跨机器可靠地存储超大文件。它将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。为了容错，文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。HDFS中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。 Namenode全权管理数据块的复制，它周期性地从集群中的每个Datanode接收心跳信号和块状态报告(Blockreport)。接收到心跳信号意味着该Datanode节点工作正常。块状态报告包含了一个该Datanode上所有数据块的列表。 HDFS Datanodes副本存放: 最最开始的一步副本的存放是HDFS可靠性和性能的关键。优化的副本存放策略是HDFS区分于其他大部分分布式文件系统的重要特性。这种特性需要做大量的调优，并需要经验的积累。HDFS采用一种称为机架感知(rack-aware)的策略来改进数据的可靠性、可用性和网络带宽的利用率。目前实现的副本存放策略只是在这个方向上的第一步。实现这个策略的短期目标是验证它在生产环境下的有效性，观察它的行为，为实现更先进的策略打下测试和研究的基础。 大型HDFS实例一般运行在跨越多个机架的计算机组成的集群上，不同机架上的两台机器之间的通讯需要经过交换机。在大多数情况下，同一个机架内的两台机器间的带宽会比不同机架的两台机器间的带宽大。 通过一个机架感知的过程，Namenode可以确定每个Datanode所属的机架id。一个简单但没有优化的策略就是将副本存放在不同的机架上。这样可以有效防止当整个机架失效时数据的丢失，并且允许读数据的时候充分利用多个机架的带宽。这种策略设置可以将副本均匀分布在集群中，有利于当组件失效情况下的负载均衡。但是，因为这种策略的一个写操作需要传输数据块到多个机架，这增加了写的代价。 在大多数情况下，副本系数是3，HDFS的存放策略是将一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。这种策略减少了机架间的数据传输，这就提高了写操作的效率。机架的错误远远比节点的错误少，所以这个策略不会影响到数据的可靠性和可用性。于此同时，因为数据块只放在两个（不是三个）不同的机架上，所以此策略减少了读取数据时需要的网络传输总带宽。在这种策略下，副本并不是均匀分布在不同的机架上。三分之一的副本在一个节点上，三分之二的副本在一个机架上，其他副本均匀分布在剩下的机架中，这一策略在不损害数据可靠性和读取性能的情况下改进了写的性能。 当前，这里介绍的默认副本存放策略正在开发的过程中。 副本选择为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。如果在读取程序的同一个机架上有一个副本，那么就读取该副本。如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。 安全模式Namenode启动后会进入一个称为安全模式的特殊状态。处于安全模式的Namenode是不会进行数据块的复制的。Namenode从所有的 Datanode接收心跳信号和块状态报告。块状态报告包括了某个Datanode所有的数据块列表。每个数据块都有一个指定的最小副本数。当Namenode检测确认某个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全(safely replicated)的；在一定百分比（这个参数可配置）的数据块被Namenode检测确认是安全之后（加上一个额外的30秒等待时间），Namenode将退出安全模式状态。接下来它会确定还有哪些数据块的副本没有达到指定数目，并将这些数据块复制到其他Datanode上。 文件系统元数据的持久化Namenode上保存着HDFS的名字空间。对于任何对文件系统元数据产生修改的操作，Namenode都会使用一种称为EditLog的事务日志记录下来。例如，在HDFS中创建一个文件，Namenode就会在Editlog中插入一条记录来表示；同样地，修改文件的副本系数也将往Editlog插入一条记录。Namenode在本地操作系统的文件系统中存储这个Editlog。整个文件系统的名字空间，包括数据块到文件的映射、文件的属性等，都存储在一个称为FsImage的文件中，这个文件也是放在Namenode所在的本地文件系统上。 Namenode在内存中保存着整个文件系统的名字空间和文件数据块映射(Blockmap)的映像。这个关键的元数据结构设计得很紧凑，因而一个有4G内存的Namenode足够支撑大量的文件和目录。当Namenode启动时，它从硬盘中读取Editlog和FsImage，将所有Editlog中的事务作用在内存中的FsImage上，并将这个新版本的FsImage从内存中保存到本地磁盘上，然后删除旧的Editlog，因为这个旧的Editlog的事务都已经作用在FsImage上了。这个过程称为一个检查点(checkpoint)。在当前实现中，检查点只发生在Namenode启动时，在不久的将来将实现支持周期性的检查点。 Datanode将HDFS数据以文件的形式存储在本地的文件系统中，它并不知道有关HDFS文件的信息。它把每个HDFS数据块存储在本地文件系统的一个单独的文件中。Datanode并不在同一个目录创建所有的文件，实际上，它用试探的方法来确定每个目录的最佳文件数目，并且在适当的时候创建子目录。在同一个目录中创建所有的本地文件并不是最优的选择，这是因为本地文件系统可能无法高效地在单个目录中支持大量的文件。当一个Datanode启动时，它会扫描本地文件系统，产生一个这些本地文件对应的所有HDFS数据块的列表，然后作为报告发送到Namenode，这个报告就是块状态报告。 通讯协议所有的HDFS通讯协议都是建立在TCP/IP协议之上。客户端通过一个可配置的TCP端口连接到Namenode，通过ClientProtocol协议与Namenode交互。而Datanode使用DatanodeProtocol协议与Namenode交互。一个远程过程调用(RPC)模型被抽象出来封装ClientProtocol和Datanodeprotocol协议。在设计上，Namenode不会主动发起RPC，而是响应来自客户端或 Datanode 的RPC请求。 健壮性HDFS的主要目标就是即使在出错的情况下也要保证数据存储的可靠性。常见的三种出错情况是：Namenode出错, Datanode出错和网络割裂(network partitions)。 磁盘数据错误，心跳检测和重新复制每个Datanode节点周期性地向Namenode发送心跳信号。网络割裂可能导致一部分Datanode跟Namenode失去联系。Namenode通过心跳信号的缺失来检测这一情况，并将这些近期不再发送心跳信号Datanode标记为宕机，不会再将新的IO请求发给它们。任何存储在宕机Datanode上的数据将不再有效。Datanode的宕机可能会引起一些数据块的副本系数低于指定值，Namenode不断地检测这些需要复制的数据块，一旦发现就启动复制操作。在下列情况下，可能需要重新复制：某个Datanode节点失效，某个副本遭到损坏，Datanode上的硬盘错误，或者文件的副本系数增大。 集群均衡HDFS的架构支持数据均衡策略。如果某个Datanode节点上的空闲空间低于特定的临界点，按照均衡策略系统就会自动地将数据从这个Datanode移动到其他空闲的Datanode。当对某个文件的请求突然增加，那么也可能启动一个计划创建该文件新的副本，并且同时重新平衡集群中的其他数据。这些均衡策略目前还没有实现。 数据完整性从某个Datanode获取的数据块有可能是损坏的，损坏可能是由Datanode的存储设备错误、网络错误或者软件bug造成的。HDFS客户端软件实现了对HDFS文件内容的校验和(checksum)检查。当客户端创建一个新的HDFS文件，会计算这个文件每个数据块的校验和，并将校验和作为一个单独的隐藏文件保存在同一个HDFS名字空间下。当客户端获取文件内容后，它会检验从Datanode获取的数据跟相应的校验和文件中的校验和是否匹配，如果不匹配，客户端可以选择从其他Datanode获取该数据块的副本。 元数据磁盘错误FsImage和Editlog是HDFS的核心数据结构。如果这些文件损坏了，整个HDFS实例都将失效。因而，Namenode可以配置成支持维护多个FsImage和Editlog的副本。任何对FsImage或者Editlog的修改，都将同步到它们的副本上。这种多副本的同步操作可能会降低Namenode每秒处理的名字空间事务数量。然而这个代价是可以接受的，因为即使HDFS的应用是数据密集的，它们也非元数据密集的。当Namenode重启的时候，它会选取最近的完整的FsImage和Editlog来使用。 Namenode是HDFS集群中的单点故障(single point of failure)所在。如果Namenode机器故障，是需要手工干预的。目前，自动重启或在另一台机器上做Namenode故障转移的功能还没实现。 快照快照支持某一特定时刻的数据的复制备份。利用快照，可以让HDFS在数据损坏时恢复到过去一个已知正确的时间点。HDFS目前还不支持快照功能，但计划在将来的版本进行支持。 数据组织数据块HDFS被设计成支持大文件，适用HDFS的是那些需要处理大规模的数据集的应用。这些应用都是只写入数据一次，但却读取一次或多次，并且读取速度应能满足流式读取的需要。HDFS支持文件的“一次写入多次读取”语义。一个典型的数据块大小是64MB。因而，HDFS中的文件总是按照64M被切分成不同的块，每个块尽可能地存储于不同的Datanode中。 Staging客户端创建文件的请求其实并没有立即发送给Namenode，事实上，在刚开始阶段HDFS客户端会先将文件数据缓存到本地的一个临时文件。应用程序的写操作被透明地重定向到这个临时文件。当这个临时文件累积的数据量超过一个数据块的大小，客户端才会联系Namenode。Namenode将文件名插入文件系统的层次结构中，并且分配一个数据块给它。然后返回Datanode的标识符和目标数据块给客户端。接着客户端将这块数据从本地临时文件上传到指定的Datanode上。当文件关闭时，在临时文件中剩余的没有上传的数据也会传输到指定的Datanode上。然后客户端告诉Namenode文件已经关闭。此时Namenode才将文件创建操作提交到日志里进行存储。如果Namenode在文件关闭前宕机了，则该文件将丢失。 上述方法是对在HDFS上运行的目标应用进行认真考虑后得到的结果。这些应用需要进行文件的流式写入。如果不采用客户端缓存，由于网络速度和网络堵塞会对吞估量造成比较大的影响。这种方法并不是没有先例的，早期的文件系统，比如AFS，就用客户端缓存来提高性能。为了达到更高的数据上传效率，已经放松了POSIX标准的要求。 流水线复制当客户端向HDFS文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副本系数设置为3，当本地临时文件累积到一个数据块的大小时，客户端会从Namenode获取一个Datanode列表用于存放副本。然后客户端开始向第一个Datanode传输数据，第一个Datanode一小部分一小部分(4 KB)地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中第二个Datanode节点。第二个Datanode也是这样，一小部分一小部分地接收数据，写入本地仓库，并同时传给第三个Datanode。最后，第三个Datanode接收数据并存储在本地。因此，Datanode能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的方式从前一个Datanode复制到下一个。 可访问性HDFS给应用提供了多种访问方式。用户可以通过Java API接口访问，也可以通过C语言的封装API访问，还可以通过浏览器的方式访问HDFS中的文件。通过WebDAV协议访问的方式正在开发中。 DFSShellHDFS以文件和目录的形式组织用户数据。它提供了一个命令行的接口(DFSShell)让用户与HDFS中的数据进行交互。命令的语法和用户熟悉的其他shell(例如 bash, csh)工具类似。下面是一些动作/命令的示例： 动作命令创建一个名为 /foodir 的目录 bin/hadoop dfs -mkdir /foodir创建一个名为 /foodir 的目录 bin/hadoop dfs -mkdir /foodir查看名为 /foodir/myfile.txt 的文件内容 bin/hadoop dfs -cat /foodir/myfile.txtDFSShell 可以用在那些通过脚本语言和文件系统进行交互的应用程序上。 DFSAdminDFSAdmin 命令用来管理HDFS集群。这些命令只有HDSF的管理员才能使用。下面是一些动作/命令的示例： 动作命令将集群置于安全模式 bin/hadoop dfsadmin -safemode enter显示Datanode列表 bin/hadoop dfsadmin -report使Datanode节点 datanodename退役 bin/hadoop dfsadmin -decommission datanodename 浏览器接口一个典型的HDFS安装会在一个可配置的TCP端口开启一个Web服务器用于暴露HDFS的名字空间。用户可以用浏览器来浏览HDFS的名字空间和查看文件的内容。 存储空间回收文件的删除和恢复当用户或应用程序删除某个文件时，这个文件并没有立刻从HDFS中删除。实际上，HDFS会将这个文件重命名转移到/trash目录。只要文件还在/trash目录中，该文件就可以被迅速地恢复。文件在/trash中保存的时间是可配置的，当超过这个时间时，Namenode就会将该文件从名字空间中删除。删除文件会使得该文件相关的数据块被释放。注意，从用户删除文件到HDFS空闲空间的增加之间会有一定时间的延迟。 只要被删除的文件还在/trash目录中，用户就可以恢复这个文件。如果用户想恢复被删除的文件，他/她可以浏览/trash目录找回该文件。/trash目录仅仅保存被删除文件的最后副本。/trash目录与其他的目录没有什么区别，除了一点：在该目录上HDFS会应用一个特殊策略来自动删除文件。目前的默认策略是删除/trash中保留时间超过6小时的文件。将来，这个策略可以通过一个被良好定义的接口配置。 减少副本系数当一个文件的副本系数被减小后，Namenode会选择过剩的副本删除。下次心跳检测时会将该信息传递给Datanode。Datanode遂即移除相应的数据块，集群中的空闲空间加大。同样，在调用setReplication API结束和集群中空闲空间增加间会有一定的延迟。 参考资料HDFS Java APIHDFS 源代码]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop2.7.5HA集群搭建]]></title>
    <url>%2FHadoop2-7-5HA%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA.html</url>
    <content type="text"><![CDATA[hadoop HA原理概述为什么会有 hadoop HA 机制呢？ HA：High Available，高可用在Hadoop 2.0之前,在HDFS 集群中NameNode 存在单点故障 (SPOF：A Single Point of Failure)。对于只有一个 NameNode 的集群，如果 NameNode 机器出现故障(比如宕机或是软件、硬件升级)，那么整个集群将无法使用，直到 NameNode 重新启动。 那如何解决呢？HDFS 的 HA 功能通过配置 Active/Standby 两个 NameNodes 实现在集群中对 NameNode 的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将 NameNode 很快的切换到另外一台机器。 在一个典型的 HDFS(HA) 集群中，使用两台单独的机器配置为 NameNodes 。在任何时间点，确保 NameNodes 中只有一个处于 Active 状态，其他的处在 Standby 状态。其中ActiveNameNode 负责集群中的所有客户端操作，StandbyNameNode 仅仅充当备机，保证一旦 ActiveNameNode 出现问题能够快速切换。 为了能够实时同步 Active 和 Standby 两个 NameNode 的元数据信息（实际上 editlog），需提供一个共享存储系统，可以是 NFS、QJM（Quorum Journal Manager）或者 Zookeeper，ActiveNamenode 将数据写入共享存储系统，而 Standby 监听该系统，一旦发现有新数据写入，则读取这些数据，并加载到自己内存中，以保证自己内存状态与 Active NameNode 保持基本一致，如此这般，在紧急情况下 standby 便可快速切为 active namenode。为了实现快速切换，Standby 节点获取集群的最新文件块信息也是很有必要的。为了实现这一目标，DataNode 需要配置 NameNodes 的位置，并同时给他们发送文件块信息以及心跳检测。 SecondaryNameNode 和 Standby Namenode 的区别？在1.x版本中，SecondaryNameNode将fsimage跟edits进行合并，生成新的fsimage文件用http post传回NameNode节点。SecondaryNameNode不能做NameNode的备份。在hadoop 2.x版本中才引入StandbyNameNode，从journalNode上拷贝的。StandbyNameNode是可以做namenode的备份。 Hadoop2的高可用并取代SecondaryNamenode在hadoop2实际生产环境中，为什么还需要SecondeNamenodesecondary namenode和namenode的区别 集群规划描述：hadoop HA 集群的搭建依赖于 zookeeper，所以选取三台当做 zookeeper 集群我总共准备了四台主机，分别是 hadoop02，hadoop03，hadoop04，hadoop05其中 hadoop02 和 hadoop03 做 namenode 的主备切换，hadoop04 和 hadoop05 做resourcemanager 的主备切换 集群服务器准备1、 修改主机名2、 修改 IP 地址3、 添加主机名和 IP 映射4、 添加普通用户 hadoop 用户并配置 sudoer 权限5、 设置系统启动级别6、 关闭防火墙/关闭 Selinux7、 安装 JDK两种准备方式：1、 每个节点都单独设置，这样比较麻烦。线上环境可以编写脚本实现2、 虚拟机环境可是在做完以上 7 步之后，就进行克隆3、 然后接着再给你的集群配置 SSH 免密登陆和搭建时间同步服务8、 配置 SSH 免密登录9、 同步服务器时间 集群安装1、安装 Zookeeper 集群（略）2、 安装 hadoop 集群 修改配置文件：core-site.xml :1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;!-- 指定hdfs的nameservice为myha01 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://myha01/&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop临时目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data1/hadoopdata/&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop01:2181,hadoop02:2181,hadoop03:2181,hadoop04:2181,hadoop05:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- hadoop链接zookeeper的超时时长设置 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;1000&lt;/value&gt; &lt;description&gt;ms&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118&lt;configuration&gt; &lt;!-- 指定副本数--&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置namenode和datanode的工作目录-数据存储目录--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data1/hadoopdata/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data1/hadoopdata/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;!-- 启用webhdfs--&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--指定hdfs的nameservice为myha01，需要和core-site.xml中的保持一致 dfs.ha.namenodes.[nameservice id]为在nameservice中的每一个NameNode设置唯一标示符。 配置一个逗号分隔的NameNode ID列表。这将是被DataNode识别为所有的NameNode。 例如，如果使用&quot;myha01&quot;作为nameservice ID，并且使用&quot;nn1&quot;和&quot;nn2&quot;作为NameNodes标示符 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;myha01&lt;/value&gt; &lt;/property&gt; &lt;!-- myha01下面有两个NameNode,分别是nn1,nn2--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.myha01&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.myha01.nn1&lt;/name&gt; &lt;value&gt;hadoop01:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.myha01.nn1&lt;/name&gt; &lt;value&gt;hadoop01:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.myha01.nn2&lt;/name&gt; &lt;value&gt;hadoop02:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.myha01.nn2&lt;/name&gt; &lt;value&gt;hadoop02:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode的edits元数据的共享存储位置。也就是JournalNode列表 该url的配置格式：qjournal://host1:port1;host2:port2;host3:port3/journalId journalId推荐使用nameservice，默认端口号是：8485 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop02:8485;hadoop03:8485;hadoop04:8485/myha01&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定JournalNode在本地磁盘存放数据的位置--&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data1/journaldata&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启NameNode失败自动切换--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置失败自动切换实现方式--&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.myha01&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;!-- 使用sshfence隔离机制时需要ssh免密登陆--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置sshfence隔离机制赶超时间--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.failover-controller.cli-check.rpc-timeout.ms&lt;/name&gt; &lt;value&gt;60000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml: 12345678910111213141516171819&lt;configuration&gt; &lt;!--指定mr框架为yarn方式--&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!--指定mapreduce jobhistory--&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop01:10020&lt;/value&gt; &lt;/property&gt; &lt;!--任务历史服务器的web地址--&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop01:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;configuration&gt; &lt;!-- 开启RM高可用--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的cluster id--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yrc&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的名字--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!--分别指定RM的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop03&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop04&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk集群地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop01:2181,hadoop02:2181,hadoop03:2181,hadoop04:2181,hadoop05:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;86400&lt;/value&gt; &lt;/property&gt; &lt;!--启用自动恢复--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 制定resourcemanager的状态信息存储在zookeeper集群上 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hadoop-env.sh: 修改JAVA_HOMEslaves文件：修改映射 #注意：配置文件是最容易出错的地方，建议复制，不建议手写，容易出错。格式化namenode之后，如果出错了，建议删除临时文件日志目录，每台机器都要删除，重新格式化，注意顺序步骤。 如果DFSZKFailoverController自动死掉，则有可能是因为以下配置问题：错误配置，会导致DFSZKFailoverController死掉：正确写法，注意sshfence不能换行： 分发安装包到其他机器1234scp -r hadoop-2.7.5 hadoop@hadoop02:$PWDscp -r hadoop-2.7.5 hadoop@hadoop03:$PWDscp -r hadoop-2.7.5 hadoop@hadoop04:$PWDscp -r hadoop-2.7.5 hadoop@hadoop05:$PWD 并分别配置环境变量vi ~/.bashrc添加两行：export HADOOP_HOME=/home/hadoop/apps/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin保存退出 集群初始化操作：（注意：严格按照以下步骤执行）1、 先启动 zookeeper 集群启动：zkServer.sh start检查启动是否正常：zkServer.sh status2、 分别在每个 zookeeper（也就是规划的三个 journalnode 节点，不一定跟 zookeeper节点一样）节点上启动 journalnode 进程 12345[hadoop@hadoop01 ~]$ hadoop-daemon.sh start journalnode[hadoop@hadoop02 ~]$ hadoop-daemon.sh start journalnode[hadoop@hadoop03 ~]$ hadoop-daemon.sh start journalnode[hadoop@hadoop04 ~]$ hadoop-daemon.sh start journalnode[hadoop@hadoop05 ~]$ hadoop-daemon.sh start journalnode 然后用 jps 命令查看是否各个 datanode 节点上都启动了 journalnode 进程如果报错，根据错误提示改进3、在第一个 namenode 上执行格式化操作然后会在 core-site.xml 中配置的临时目录中生成一些集群的信息把他拷贝的第二个 namenode 的相同目录下12&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/data/hadoopdata/&lt;/value&gt; 这个目录下，千万记住：两个 namenode 节点该目录中的数据结构是一致的[hadoop@hadoop02 ~]$ scp -r ~/data/hadoopdata/ hadoop03:~/data或者也可以在另一个 namenode 上执行：hadoop namenode -bootstrapStandby 1[hadoop@hadoop02 ~]$ hadoop namenode -format 4、格式化 ZKFC 1[hadoop@hadoop02 ~]$ hdfs zkfc -formatZK 在第一台机器上即可5、启动 HDFS 1[hadoop@hadoop02 ~]$ start-dfs.sh 查看各节点进程是否启动正常：依次为 2345 四台机器的进程 最终效果：访问 web 页面 http://hadoop01:50070访问 web 页面 http://hadoop02:50070 启动 YARN[hadoop@hadoop04 ~]$ start-yarn.sh在主备 resourcemanager 中随便选择一台进行启动，正常启动之后，检查各节点的进程：若备用节点的 resourcemanager 没有启动起来，则手动启动起来1[hadoop@hadoop04 ~]$ yarn-daemon.sh start resourcemanager 访问页面：http://hadoop03:8088访问页面：http://hadoop04:8088 自动跳转至hadoop03机器 查看各主节点的状态HDFS:hdfs haadmin -getServiceState nn1hdfs haadmin -getServiceState nn2YARN:yarn rmadmin -getServiceState rm1yarn rmadmin -getServiceState rm2 启动 mapreduce 任务历史服务器1[hadoop@hadoop01 ~]$ mr-jobhistory-daemon.sh start historyserver 按照配置文件配置的历史服务器的 web 访问地址去访问：http://hadoop01:19888 集群启动测试1、干掉 active namenode， 看看集群有什么变化 干掉active namenode，standby namenode瞬间转为active状态；重新启动刚才那台干掉的节点后，该节点变为standby 状态。2、在上传文件的时候干掉 active namenode， 看看有什么变化 会报错，但是可以上传成功。3、干掉 active resourcemanager， 看看集群有什么变化 hadoop03上的yarn节点就不能访问，hadoop05上的yarn节点可以正常访问。4、在执行任务的时候干掉 active resourcemanager，看看集群 执行wordcount程序，执行的时候，在hadoop03节点上杀死ResourceManager，最终能够成功执行。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在大数据环境中执行情感分析]]></title>
    <url>%2F%E5%9C%A8%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E4%B8%AD%E6%89%A7%E8%A1%8C%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[情感分析情感分析是利用文本分析来挖掘各种观点的数据来源的过程。通常情况下，情感分析是在从互联网和各种社交媒体平台收集的数据上执行的。政治家和政府经常利用情感分析来了解人们如何看待他们和他们的政策。随着社交媒体的出现，人们可以从各种不同来源（比如移动设备和 Web 浏览器）捕获数据，并用不同的数据格式存储这些数据。由于社交媒体内容对于传统存储系统（比如 RDBMS、关系数据库管理系统）是非结构化的，所以我们需要一些可以处理和分析各种不同数据的工具。不过，大数据技术旨在处理不同来源、不同格式的结构化和非结构化数据。在本文中，我将介绍如何利用大数据工具来捕获数据，以便存储和处理用于情感分析的数据。 处理大数据无论何时从采用多种格式（结构化、半结构化或非结构化的）的多个来源收集数据，都需要考虑建立一个 Hadoop 集群和一个 Hadoop 分布式文件系统（HDFS）来存储数据。HDFS 提供了一种管理大数据的灵活方式： 可以将您的一些分析数据移动到现有的关系数据库管理系统（RDBMS）中，比如 Oracle 或 MySQL，这样您就可以利用现有的 BI 和报告工具。 可以将数据存储在 HDFS 中，供将来分析使用，例如，通过执行像 ANOVA.T 这样的测试来比较旧数据与新数据。 如果只需要分析数据的影响，那么可以删除这些数据。要了解如何设置 Hadoop 集群，请将数据导入 HDFS，然后在您的 Hadoop 环境中分析这些数据，请参阅我的其他 developerWorks 文章， “将 Hadoop 与现有的 RDBMS 相集成“。检索数据并将数据存储在 HDFS 中最好的情感分析包括来自多个来源的数据。在本文中，我将介绍如何从这些来源中检索数据： Twitter 提要 RSS 提要 移动应用程序我还将解释如何将来自不同来源的数据存储在 HDFS 中（存储在您的 Hadoop 集群中）。从 Twitter 提要中检索数据Twitter（一种流行的微博网站）有一组 API，它们使得我们能够检索和操作 tweet。但是首先，我们需要实现 Twitter 的 OAuth 框架。简单地讲，有了这个框架，应用程序就可以代表您登录到 Twitter，无需您登录到 Twitter 网站。查看 Twitter 开发人员站点的设置过程，其中解释了如何指派实现此操作的应用程序。在这个过程中，会为您分配一个密钥和一个密钥令牌，您的应用程序将使用它们来代表您执行身份验证。在您的应用程序完成身份验证后，您就可以使用 Twitter API 来获取 tweet。您可以通过使用 R 或通过使用 Jaql 获取来自 Twitter 提要的数据。因为 Jaql 被设计用于处理 JSON 数据，所以它是适用于 tweet 的默认数据格式，使用 Jaql 可能更简单一些。有人可能会决定使用 R，这样做可能纯粹是因为他们自己的 R 技能。通过使用 Jaql 检索来自 Twitter 的数据在您的应用程序完成身份验证后，我们就可以使用 Twitter API 来获取 tweet。因为我们想要在流化模式下，所以我们的 Twitter URL 是：url = “https://stream.twitter.com/1.1/statuses/filter.json?track=governmentTopic“;使用我们正在挖掘的政府主题的名称来替换 governmentTopic。通过使用与以下代码类似的代码，我们可以用一个变量来获取 tweet：12jsonResultTweets = read(http(url));jsonResultTweets; 在运行 Jaql 脚本时，它会提取与政府主题相关的 tweet。这些 tweet 是以 JSON 格式返回的。如果我们想通过位置知道关于我们的政府主题的讨论范围，可以使用下面的代码片段来获取 tweet：123governmentTopicDiscussionByLocation = jsonResultTweets -&gt; transform&#123;location: $.location,user_id: $.from_user_id_str,date_created:$.created_at,comment:$text&#125; -&gt; group by key = $.location 然后，我们可以使用下面的代码片段将此信息存储到您的 HDFS 中：123governmentTopicDiscussionByLocation Cnt -&gt;write(del(&quot;/user/governmentTopics/governmentTopic_1Tweets.del&quot;, schema =schema &#123; list_of_comma_seperated_json_fields&#125; 其中的 list_of_comma_seperated_json_fields 是一些逗号分隔的字段：location、from_user_id_str 和 created_at。这样就可以通过 Oozie 工作流来运行整个 Jaql 脚本，代码可能类似于以下代码示例：12345678910url = &quot;https://stream.twitter.com/1.1/statuses/filter.json?track=governmentTopic&quot;; jsonResultTweets = read(http(url));jsonResultTweets;governmentTopicDiscussionByLocation = jsonResultTweets -&gt; transform &#123;location: $.location,user_id: $.from_user_id_str,user_name: $.user.name,user_location: $.user.location,date_created: $.created_at,comment: $.text&#125; -&gt; group by key = $.location governmentTopicDiscussionByLocation -&gt; write(del(&quot;/user/governmentTopics/governmentTopic_1Tweets.del&quot;, schema = schema &#123;location,user_id,user_name,user_location,date_created,comment&#125; transform 方法将会清除数据，而 write 方法会将数据保存到 HDFS。要处理流数据或动态数据，需要将此脚本与 Flume 整合，Flume 是 Apache Hadoop 生态系统中的另一个大数据工具。（您可以通过阅读了解有关此 developerWorks 文章中的 Flume 的更多信息，”使用 Flume 部署和管理可扩展的 Web 服务”。）通过使用 R 从 Twitter 中检索数据要使用 R 检索 tweet，需要在您的系统上安装某些软件包。虽然我们可以使用 RStudio，但下面这些步骤显示了如何设置和使用 R 控制台。在 Ubuntu 电脑上，我完成了下面这些步骤来安装必要的 R 软件包：安装这些软件包：12345libcurl4-gnutls-dev libcurl4-nss-dev libcurl4-openssl-dev r-base r-base-devr-cran-rjson 打开 R 控制台，并运行这些命令来安装这些包来访问 Twitter：123install.packages(“twitteR”)install.packages(“ROAuth”)install.packages(“RCurl”) 将这些库加载到您的 R 工作区中：1234rm(list=ls())library(twitteR)library(ROAuth)library(RCurl) 现在，我们可以用下面的 R 脚本对 Twitter 进行身份验证：12345678910111213141516download.file(url=&quot;http://curl.haxx.se/ca/cacert.pem&quot;,destfile=&quot;cacert.pem&quot;)requestURL &lt;- &quot;https://api.twitter.com/oauth/request_token&quot;accessURL &lt;- &quot;https://api.twitter.com/oauth/access_token&quot;authURL &lt;- &quot;https://api.twitter.com/oauth/authorize&quot;consumerKey &lt;- myConsumerKeyFromTwitterconsumerSecret &lt;- myConsumerSeccretFromTwittermyCred &lt;- OAuthFactory$new(consumerKey=consumerKey, consumerSecret=consumerSecret, requestURL=requestURL, accessURL=accessURL, authURL=authURL) accessToken &lt;- myAccessTokenFromTwitteraccessSecret &lt;- myAccessSecretFromTwitter setup_twitter_oauth(consumerKey,consumerSecret,accessToken,accessSecret) 然后，我们可以使用下面的代码片段来获取 tweet：govt_sentiment_data &lt;- searchTwitter(“#keyWord”,since={last_date_pulled}keyWord 是您要分析的政府主题，last_date_pulled 是您最后一次获取 tweet 的日期。如果您想要按固定时间间隔自动流化 Twitter 数据和拉取数据，可以使用以下代码片段替换前面的代码：12govt_sentiment_data &lt;- filterStream( file=&quot;tweets_rstats.json&quot;,track=&quot;#keyWord&quot;, timeout=3600, oauth=myCred) 我们可以用下面的 R 脚本来清理数据：123456789101112131415govt_sentiment_data_txt = govt_sentiment_data$text# remove retweet entitiesgovt_sentiment_data_txt = gsub(“(RT|via)((?:\\b\\W*@\\w+)+)”, “”, tweet_txt)# remove at peoplegovt_sentiment_data_txt = gsub(“@\\w+”, “”, tweet_txt)# remove punctuationgovt_sentiment_data_txt = gsub(“[[:punct:]]”, “”, tweet_txt)# remove numbersgovt_sentiment_data_txt = gsub(“[[:digit:]]”, “”, tweet_txt)# remove html linksgovt_sentiment_data_txt = gsub(“http\\w+”, “”, tweet_txt)# remove unnecessary spacesgovt_sentiment_data_txt = gsub(“[ \t]&#123;2,&#125;”, “”, tweet_txt)govt_sentiment_data_txt = gsub(“^\\s+|\\s+$”, “”, tweet_txt)govt_sentiment_data_txt=gsub(“[^0-9a-zA-Z ,./?&gt;&lt;:;’~`!@#&amp;*’]”,””, tweet_txt) 最后，要将已清理的数据保存到您的 HDFS，可以使用下面的代码片段：1234hdfsFile &lt;- hdfs.file(&quot;/tmp/govt_sentiment_data.txt&quot;, &quot;w&quot;)hdfs.write(govt_sentiment_data_txt, hdfsFile)hdfs.close(hdfsFile)write(govt_sentiment_data, &quot;govt_sentiment_data.txt&quot;) 从 RSS 提要检索数据除了 tweet 之外，我们还想从新闻文章中收集个人意见或观点。对于这种类型的数据，建议您组合使用 Java 和 Rome 工具从 RSS 提要中获取数据。Rome 是一个 Java 库，用于访问和操纵网络上的新闻提要。在本示例中，我们获得了有关新闻文章的以下信息：标题、链接和描述。然后，我们从这些数据点提取我们所需的信息。要确定将要使用的新闻提要，需要使用某种形式的网页排名 技术。该技术被用在搜索算法中，用于确定某一事项在其引用和普及方面的相关性。基本原理是，被外部实体点击或引用的几率越高，优先级就越高，因此就会出现在搜索结果的顶部。下面的 Java 代码标识了一些新闻提要和使用网页排名，以确定它们与我们的数据相关：1234567891011121314151617181920212223242526272829303132333435363738private static void getFeeds(String newsFeedUrlLink)&#123; File f = new File(“newsFeeds.txt”); boolean ok = false; try &#123; URL feedUrl = new URL(newsFeedUrlLink); SyndFeedInput input = new SyndFeedInput(); InputSource source = new InputSource(feedUrl.openStream()); SyndFeed feed = input.build(source); for (Iterator i = feed.getEntries().iterator(); i.hasNext();) &#123; SyndEntry entry = (SyndEntry) i.next(); writeToFile(f,entry); &#125; ok = true; &#125; catch (Exception ex) &#123; ex.printStackTrace(); System.out.println(&quot;ERROR: &quot;+ex.getMessage()); &#125; if (!ok) &#123; System.out.println(); System.out.println(&quot;FeedReader reads and prints any RSS/Atom feed type.&quot;); System.out.println(&quot;The first parameter must be the URL of the feed to read.&quot;); System.out.println(); &#125; &#125; private static void writeToFile(File f, SyndEntry entry) throws IOException &#123; FileWriter fw = new FileWriter(f.getName(),true); BufferedWriter bw = new BufferedWriter(fw); bw.write(entry.getTitle()+”\n”); bw.close(); &#125; 接下来，我们可以使用下面的代码片段将数据存储在我们使用 Twitter 数据创建的 HDFS 文件中。要将此数据添加到我们使用 Twitter 数据创建的 HDFS 文件中，必须修改 hdfs-site.xml 文件中的 dfs.support.append 属性值，因为 HDFS 默认情况下不允许将数据添加到文件。123456789mydata &lt;- readLines(&quot;newsFeeds.txt&quot;)myfile &lt;- hdfs.file(&quot;/tmp/govt_sentiment_data.txt&quot;, &quot;r&quot;)dfserialized &lt;- hdfs.read(myfile)df &lt;- unserialize(dfserialized)hdfs.close(myfile) //write(mydata, file = &quot;/tmp/govt_sentiment_data.txt&quot;,append = TRUE)hdfs.write(mydata, file = &quot;/tmp/govt_sentiment_data.txt&quot;,append = TRUE)government_sentiment_data &lt;- read.hdfs(“/tmp/govt_sentiment_data.txt”) 从移动应用程序中检索数据除了 Twitter 数据和 RSS 提要数据之外，我们还可以从包含个人意见和观点的移动应用程序中收集数据。在本示例中，我假设您创建了一个简单的移动应用程序，该应用程序已安装在允许用户提供关于政府主题或政策的意见的移动设备上。可以将 J2ME 应用程序上传到某个 WAP 服务器，移动设备（甚至是像诺基亚 3310 这样的老款设备）可以从该服务器下载和安装应用程序。用户提供的信息被发送回一个 RDBMS 并进行储存，以供将来分析使用。您可以使用 Sqoop 将数据从 RDBMS 服务器移动到我们的 Hadoop 集群。在 Hadoop 集群上运行 sqoop 脚本的以下行：12sqoop import --options-file dbCredentials.txt --connectjdbc:mysql://217.8.156.117/govt_policy_app --table opinions –-target-dir /tmp \ --append –append 标记告诉 Sqoop 将导入的数据添加到我们已经从以前的数据来源获得的数据集中，该数据集通过 –target-dir 标记来指示。将已收集的数据合并成一个数据源在收集了来自 Twitter 的数据（通过使用 Jaql 或 R）、来自 RSS 提要的数据（通过使用 Java）和来自移动应用程序的数据（通过使用 Sqoop）后，我们会将数据添加到单个 HDFS 文件中。可以通过实现了 Oozie 工作流引擎来自动化这些脚本，并设置命令来按照某个时间间隔运行脚本，或者作为触发事件发生的结果。有关如何设置 Sqoop 和 Oozie 的更多信息，请参阅我的其他 developerWorks 文章，”将 Hadoop 与现有的 RDBMS 相集成”。您可以增强您的 Oozie 工作流程，以便实现减少重复数据的限制，重复数据是整合来自不同来源的数据所导致的。例如，您可能会限制每个话题一个 Twitter 句柄，在您的数据集中，每个观点一个移动号码。在组合数据上执行情感分析在组合数据之后，我们就可以在单个数据源上完成情感分析，这使我们可以获得分析的统一性、一致性和准确性。您可以使用 R、Jaql、Pig 或 Hive 来执行这些分析。Pig 和 Hive 是具有类似 SQL 的语法的语言，运行在 Hadoop 平台上。本例中，我决定用 R 来分析检索数据，因为 R 具有用于图形表示的丰富的内置模型函数和库，比如 ggplot2。要完成情感分析，需要有一个词典或单词列表。字典包括一组描述某一范围内的积极词和消极词的标准单词。词典确定了社交媒体中常常使用的嘲讽词、影射词、俚语、新词汇、字符和表情。这些词汇列表可从互联网上获得，定期更新，并整合到我们的情感分析逻辑中。以下代码利用了检索到的数据，并将它们与我们的单词列表相匹配，以获得积极词和消极词的数量。积极词和消极词的总数差距为我们提供了一个得分，该得分指示了我们的数据对于我们要分析的政府主题是积极的还是消极的。1234sentiment.pos=scan(&apos;/Users/charles/Downloads/r/positive-words.txt&apos;,what=&apos;character&apos;,comment.char=&apos;;&apos;)sentiment.neg=scan(&apos;/Users/charles/Downloads/r/negative-words.txt&apos;,what=&apos;character&apos;,comment.char=&apos;;&apos;)pos.words=c(sentiment.pos,&apos;good&apos;,&apos;reelect&apos;,&apos;accountable&apos;,&apos;stable&apos;)neg.words=c(sentiment.neg,&apos;bad&apos;,&apos;corrupt&apos;,&apos;greedy&apos;,&apos;unstable&apos;) 此外，以下代码表示了情感评分算法：1234567891011121314151617require(plyr)require(stringr)score.sentiment = function(sentences, pos.words, neg.words, .progress=&apos;none&apos;)&#123;sentence = tolower(sentence)word.list = str_split(sentence, &apos;\\s+&apos;)words = unlist(word.list)pos.matches = match(words, pos.words)neg.matches = match(words, neg.words)pos.matches = !is.na(pos.matches)neg.matches = !is.na(neg.matches)score = sum(pos.matches) - sum(neg.matches)return(score)&#125;, pos.words, neg.words, .progress=.progress )scores.df = data.frame(score=scores, text=sentences)return(scores.df)&#125; 然后，我们可以通过使用下面的代码片段，调用情感得分算法函数来计算数据的得分：12require(plyr)opinion.score &lt;- score.sentiment(opinion.txt,pos.words,neg.words,progress=&apos;text&apos;) 最后，我们可以通过使用 R 的内置图表和图形功能，对得分数据执行进一步分析，并通过使用下面的代码片段，绘制一幅图表来显示分数条：123library(&quot;ggplot2&quot;)hist(opinion.scores$score)qplot(opinion.scores$score) 您可以通过使用 BigSheets 进一步地分析数据，BigSheets 由 IBM InfoSphere BigInsights 提供。该工具使得非技术用户可以进行各种分析，并用图表查看数据。有关如何使用 BigSheets 工具的更多信息，请阅读 developerWorks 文章 “适用于普通人的 BigSheets”。结束语大数据工具可以根据来自任何来源或空间的数据，提供不带偏见的洞察，从而制定正确的、准确的决策，并实施这些决策。通过采用大数据工具，比如本文中所描述的那些工具，您可以轻松地实现自己的投资回报。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>情感数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[金融行业大数据用户画像实践]]></title>
    <url>%2F%E9%87%91%E8%9E%8D%E8%A1%8C%E4%B8%9A%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E5%AE%9E%E8%B7%B5.html</url>
    <content type="text"><![CDATA[金融消费者逐渐年轻化，80、90后成为客户主力，他们的消费意识和金融意识正在增强。金融服务正在从以产品为中心，转向以消费者为中心。所有金融行业面对的最大挑战是消费者的消费行为和消费需求的转变，金融企业迫切需要为产品寻找目标客户和为客户定制产品。进入移动互联网时代之后，金融业务地域限制被打破。金融企业没有固定业务区域，金融服务面对所有用户是平的。金融消费者逐渐年轻化，80、90后成为客户主力，他们的消费意识和金融意识正在增强。金融服务正在从以产品为中心，转向以消费者为中心。所有金融行业面对的最大挑战是消费者的消费行为和消费需求的转变，金融企业迫切需要为产品寻找目标客户和为客户定制产品。 一、用户画像背后的原因1、金融消费行为的改变，企业无法接触到客户80后、90后总计共有3.4亿人口，并日益成为金融企业主要的消费者，但是他们的金融消费习惯正在改变，他们不愿意到金融网点办理业务，不喜欢被动接受金融产品和服务。年轻人将主要的时间都消费在移动互联网，消费在智能手机上。平均每个人，每天使用智能手机的时间超过了3小时，年轻人可能会超过4个小时。浏览手机已经成为工作和睡觉之后的，人类第三大生活习惯，移动APP也成为所有金融企业的客户入口、服务入口、消费入口、数据入口。金融企业越来越难面对面接触到年轻人，无法像过去一样，从对话中了解年轻人的想法，了解年轻人金融产品的需求。 2、消费者需求出现分化，需要寻找目标客户客户群体正在出现分化，市场上很少有一种产品和一种金融服务可以满足所有用户的需求。金融产品也需要进行细化，为不同客户提供不同产品。金融企业面对的客户群体基数很大，有的客户高风险偏好高，希望高风险高收益;有的客户风险偏好低，希望稳健收益;有的客户金融理财意识低，只需服务较好即可;有的客户完全没有主意，你说是啥就是啥;有的客户注重体验，有的客户注重实惠，有的客户注重品牌，有的客户注重风险等等。不同年龄，不同收入，不同职业，不同资产的客户对金融产品的需求都不尽相同。金融企业需要为不同的客户定制产品，满足不同客户的需要。对于金融企业，理财和消费是主要的业务需求。客户消费习惯的改变，企业无法接触到客户，无法了解客户需求;客户需求的分化，企业需要细分客户，为目标客户开发设计产品。金融企业需要借助于户画像，来了解客户，找到目标客户，触达客户。 二、用户画像的目的用户画像是在解客户需求和消费能力，以及客户信用额度的基础上，寻找潜在产品的目标客户，并利用画像信息为客户开发产品。提到用户画像，很多厂商都会提到360度用户画像，其实经常360度客户画像是一个广告宣传用语，根本不存数据可以全面描述客户，透彻了解客户。人是非常复杂的动物，信息纬度非常复杂，仅仅依靠外部信息来刻画客户内心需要根本不可能。用户画像一词具有很重的场景因素，不同企业对于用户画像有着不同对理解和需求。举个例子，金融行业和汽车行业对于用户画像需求的信息完全不一样，信息纬度也不同，对画像结果要求也不同。每个行业都有一套适合自己行业的用户画像方法，但是其核心都是为客户服务，为业务场景服务。用户画像本质就是从业务角度出发对用户进行分析，了解用户需求，寻找目标客户。另外一个方面就是，金融企业利用统计的信息，开发出适合目标客户的产品。从商业角度出发的用户画像对企业具有很大的价值，用户画像目的有两个。一个是业务场景出发，寻找目标客户。另外一个就是，参考用户画像的信息，为用户设计产品或开展营销活动。 三、用户画像工作坚持的原则市场上用户画像的方法很多，许多企业也提供用户画像服务，将用户画像提升到很有逼格一件事。金融企业是最早开始用户画像的行业，由于拥有丰富的数据，金融企业在进行用户画像时，对众多纬度的数据无从下手，总是认为用户画像数据纬度越多越好，画像数据越丰富越好，某些输入的数据还设定了权重甚至建立了模型，搞的用户画像是一个巨大而负责的工程。但是费力很大力气进行了画像之后，却发现只剩下了用户画像，和业务相聚甚远，没有办法直接支持业务运营，投入精力巨大但是回报微小，可以说是得不偿失，无法向领导交代。事实上，用户画像涉及数据的纬度需要业务场景结合，既要简单干练又要和业务强相关，既要筛选便捷又要方便进一步操作。用户画像需要坚持三个原则，分别是人口属性和信用信息为主，强相关信息为主，定性数据为主。下面就分别展开进行解释和分析。 1、信用信息和人口属性为主描述一个用户的信息很多，信用信息是用户画像中重要的信息，信用信息是描述一个人在社会中的消费能力信息。任何企业进行用户画像的目的是寻找目标客户，其必须是具有潜在消费能力的用户。信用信息可以直接证明客户的消费能力，是用户画像中最重要和基础的信息。一句戏言，所有的信息都是信用信息就是这个道理。其包含消费者工作、收入、学历、财产等信息。定位完目标客户之后，金融企业需要触达客户，人口属性信息就是起到触达客户的作用，人口属性信息包含姓名、性别，电话号码，邮件地址，家庭住址等信息。这些信息可以帮助金融企业联系客户，将产品和服务推销给客户。 2、采用强相关信息，忽略弱相关信息我们需要介绍一下强相关信息和弱相关信息。强相关信息就是同场景需求直接相关的信息，其可以是因果信息，也可以是相关程度很高的信息。如果定义采用0到1作为相关系数取值范围的化，0.6以上的相关系数就应该定义为强相关信息。例如在其他条件相同的前提下，35岁左右人的平均工资高于平均年龄为30岁的人，计算机专业毕业的学生平均工资高于哲学专业学生，从事金融行业工作的平均工资高于从事纺织行业的平均工资，上海的平均工资超过海南省平均工资。从这些信息可以看出来人的年龄、学历、职业、地点对收入的影响较大，同收入高低是强相关关系。简单的将，对信用信息影响较大的信息就是强相关信息，反之则是弱相关信息。用户其他的信息，例如用户的身高、体重、姓名、星座等信息，很难从概率上分析出其对消费能力的影响，这些弱相关信息，这些信息就不应该放到用户画像中进行分析，对用户的信用消费能力影响很小，不具有较大的商业价值。用户画像和用户分析时，需要考虑强相关信息，不要考虑弱相关信息，这是用户画像的一个原则。 3、将定量的信息归类为定性的信息用户画像的目的是为产品筛选出目标客户，定量的信息不利于对客户进行筛选，需要将定量信息转化为定性信息，通过信息类别来筛选人群。例如可以将年龄段对客户进行划分，18岁-25岁定义为年轻人，25岁-35岁定义为中青年，36-45定义为中年人等。可以参考个人收入信息，将人群定义为高收入人群，中等收入人群，低收入人群。参考资产信息也可以将客户定义为高、中、低级别。定性信息的类别和方式方法，金融可以从自身业务出发，没有固定的模式。将金融企业各类定量信息，集中在一起，对定性信息进行分类，并进行定性化，有利与对用户进行筛选，快速定位目标客户，是用户画像的另外一个原则。 四、用户画像的方法介绍，不要太复杂金融企业需要结合业务需求进行用户画像，从实用角度出发，我们可以将用户画像信息分成五类信息。分别是人口属性，信用属性，消费特征，兴趣爱好，社交属性。它们基本覆盖了业务需求所需要的强相关信息，结合外部场景数据将会产生巨大的商业价值。我们先了解下用户画像的五大类信息的作用，以及涉及的强相关信息。特别复杂的用户画像纬度例如八个纬度，十个纬度信息都不利于商业应用，不建议金融企业进行采用，其他具有价值的信息，基本上都可以归纳到这五个纬度。金融企业达到其商业需求，从这五个纬度信息进行应用就可以了，不需要过于复杂用户画像这个工作，同时商业意义也不太大。 1、人口属性：用于描述一个人基本特征的信息，主要作用是帮助金融企业知道客户是谁，如何触达用户。姓名，性别，年龄，电话号码，邮箱，家庭住址都属于人口属性信息。 2、信用属性：用于描述用户收入潜力和收入情况，支付能力。帮助企业了解客户资产情况和信用情况，有利于定位目标客户。客户职业、收入、资产、负债、学历、信用评分等都属于信用信息。 3、消费特征：用于描述客户主要消费习惯和消费偏好，用于寻找高频和高价值客户。帮助企业依据客户消费特点推荐相关金融产品和服务，转化率将非常高。为了便于筛选客户，可以参考客户的消费记录将客户直接定性为某些消费特征人群，例如差旅人群，境外游人群，旅游人群，餐饮用户，汽车用户，母婴用户，理财人群等。 4、兴趣爱好：用于描述客户具有哪方面的兴趣爱好，在这些兴趣方面可能消费偏好比较高。帮助企业了解客户兴趣和消费倾向，定向进行活动营销。兴趣爱好的信息可能会和消费特征中部分信息有重复，区别在于数据来源不同。消费特征来源于已有的消费记录，但是购买的物品和服务不一定是自己享用，但是兴趣爱好代表本人的真实兴趣。例如户外运动爱好者，旅游爱好者，电影爱好者，科技发烧友，健身爱好者，奢侈品爱好者等。兴趣爱好的信息可能来源于社交信息和客户位置信息。 5、社交信息：用于描述用户在社交媒体的评论，这些信息往往代表用户内心的想法和需求，具有实时性高，转化率高的特点。例如客户询问上海哪里好玩?澳大利亚墨尔本的交通?房屋贷款哪家优惠多?那个理财产品好?这些社交信息都是代表客户多需求，如果企业可以及时了解到，将会有助于产品推广。这些用户画像信息归类基本覆盖了业务需求和产品开发所需要的信息，需要对这些信息进行进行整理和处理。根据业务场景，将定量的数据转化为定性的数据，并将强相关数据进行整理。 五、金融企业用户画像的基本步骤如下参考金融企业的数据类型和业务需求，可以将金融企业用户画像工作进行细化。基本上从数据集中到数据处理，从强相关数据到定性分类数据，从引入外部数据到依据业务场景进行筛选目标用户。 1)画像相关数据的整理和集中金融企业内部的信息分布在不同的系统中，一般情况下，人口属性信息主要集中在客户关系管理系统，信用信息主要集中在交易系统和产品系统之中，也集中在客户关系管理系统中，消费特征主要集中在渠道和产品系统中。兴趣爱好和社交信息需要从外部引入，例如客户的行为轨迹可以代表其兴趣爱好和品牌爱好，移动设备到位置信息可以提供较为准确的兴趣爱好信息。社交信息，可以借助于金融行业自身的文本挖掘能力进行采集和分析，也是可以借助于厂商的技术能力在社交网站上直接获得。社交信息往往是实时信息，商业价值较高，转化率也较高，是大数据预测方面的主要信息来源。例如用用户在社交网站上提出罗马哪里好玩的问题，就代表用户未来可能有出国旅游的需求;如果客户在对比两款汽车的优良，客户购买汽车的可能性就较大。金融企业可以及时介入，为客户提供金融服务。客户画像数据主要分为五类，人口属性、信用信息、消费特征、兴趣爱好、社交信息。这些数据都分布在不同的信息系统，金融企业都上线了数据仓库(DW)，所有画像相关的强相关信息都可以从数据仓库里面整理和集中，并且依据画像商业需求，利用跑批作业，加工数据，生成用户画像的原始数据。数据仓库成为用户画像数据的主要处理工具，依据业务场景和画像需求将原始数据进行分类、筛选、归纳、加工等，生成用户画像需要的原始数据。用户画像的纬度信息不是越多越好，只需要找到可五大类画像信息强相关信息，同业务场景强相关信息，同产品和目标客户强相关信息即可。根本不存在360度的用户画像信息，也不存在丰富的信息可以完全了解客户，另外数据的实效性也要重点考虑。 2)找到同业务场景强相关数据依据用户画像的原则，所有画像信息应该是5大分类的强相关信息。强相关信息是指同业务场景强相关信息，可以帮助金融行业定位目标客户，了解客户潜在需求，开发需求产品。只有强相关信息才能帮助金融企业有效结合业务需求，创造商业价值。例如姓名、手机号、家庭地址就是能够触达客户的强人口属性信息，收入、学历、职业、资产就是客户信用信息的强相关信息。差旅人群、境外游人群、汽车用户、旅游人群、母婴人群就是消费特征的强相关信息。摄影爱好者、游戏爱好者、健身爱好者、电影人群、户外爱好者就是客户兴趣爱好的强相关信息。社交媒体上发表的旅游需求，旅游攻略，理财咨询，汽车需求，房产需求等信息代表了用户的内心需求，是社交信息场景应用的强相关信息。金融企业内部信息较多，在用户画像阶段不需要对所有信息都采用，只需要采用同业务场景和目标客户强相关的信息即可，这样有助于提高产品转化率，降低ROI，有利于简单找到业务应用场景，在数据变现过程中也容易实现。千万不要将用户画像工作搞的过于复杂，同业务场景关系不大，这样就让很多金融企业特别是领导失去用户画像的兴趣，看不到用户画像的商业，不愿意在大数据领域投资。为企业带来商业价值才是用户画像工作的主要动力和主要目的。 3)对数据进行分类和标签化(定量to定性)金融企业集中了所有信息之后，依据业务需求，对信息进行加工整理，需要对定量的信息进行定性，方便信息分类和筛选。这部分工作建议在数据仓库进行，不建议在大数据管理平台(DMP)里进行加工。定性信息进行定量分类是用户画像的一个重要工作环节，具有较高的业务场景要求，考验用户画像商业需求的转化。其主要目的是帮助企业将复杂数据简单化，将交易数据定性进行归类，并且融入商业分析的要求，对数据进行商业加工。例如可以将客户按照年龄区间分为学生，青年，中青年，中年，中老年，老年等人生阶段。源于各人生阶段的金融服务需求不同，在寻找目标客户时，可以通过人生阶段进行目标客户定位。企业可以利用客户的收入、学历、资产等情况将客户分为低、中、高端客户，并依据其金融服务需求，提供不同的金融服务。可以参考其金融消费记录和资产信息，以及交易产品，购买的产品，将客户消费特征进行定性描述，区分出电商客户，理财客户，保险客户，稳健投资客户，激进投资客户，餐饮客户，旅游客户，高端客户，公务员客户等。利用外部的数据可以将定性客户的兴趣爱好，例如户外爱好者，奢侈品爱好者，科技产品发烧友，摄影爱好者，高端汽车需求者等信息。将定量信息归纳为定性信息，并依据业务需求进行标签化，有助于金融企业找到目标客户，并且了解客户的潜在需求，为金融行业的产品找到目标客户，进行精准营销，降低营销成本，提高产品转化率。另外金融企业还可以依据客户的消费特征、兴趣爱好、社交信息及时为客户推荐产品，设计产品，优化产品流程。提高产品销售的活跃率，帮助金融企业更好地为客户设计产品。 4)依据业务需求引入外部数据利用数据进行画像目的主要时为业务场景提供数据支持，包括寻找到产品的目标客户和触达客户。金融企业自身的数据不足以了解客户的消费特征、兴趣爱好、社交信息。金融企业可以引入外部信息来丰富客户画像信息，例如引入银联和电商的信息来丰富消费特征信息，引入移动大数据的位置信息来丰富客户的兴趣爱好信息，引入外部厂商的数据来丰富社交信息等。外部信息的纬度较多，内容也很丰富，但是如何引入外部信息是一项具有挑战的工作。外部信息在引入时需要考虑几个问题，分别是外部数据的覆盖里，如何和内部数据打通，和内部信息的匹配率，以及信息的相关程度，还有数据的鲜活度，这些都是引入外部信息的主要考虑纬度。外部数据鱼龙混杂，数据的合规性也是金融企业在引入外部数据时的一个重要考虑，敏感的信息例如手机号、家庭住址、身份证号在引入或匹配时都应该注意隐私问题，基本的原则是不进行数据交换，可以进行数据匹配和验证。外部数据不会集中在某一家，需要金融企业花费大量时间进行寻找。外部数据和内部数据的打通是个很复杂的问题，手机号/设备号/身份证号的MD5数值匹配是一种好的方法，不涉及隐私数据的交换，可以进行唯一匹配。依据行业内部的经验，没有一家企业外部数据可以满足企业要求，外部数据的引入需要多方面数据。一般情况下，数据覆盖率达到70%以上，就是一个非常高的覆盖率。覆盖率达到20%以上就可以进行商业应用了。金融行业外部数据源较好合作方有银联、芝麻信用、运营商、中航信、腾云天下、腾讯、微博、前海征信，各大电商平台等。市场上数据提供商已经很多，并且数据质量都不错，需要金融行业一家一家去挖掘，或者委托一个厂商代理引入也可以。独立第三方帮助金融行业引入外部数据可以降低数据交易成本，同时也可以降低数据合规风险，是一个不错得尝试。另外各大城市和区域的大数据交易平台，也是一个较好的外部数据引入方式。 5)按照业务需求进行筛选客户(DMP的作用)用户画像主要目的是让金融企业挖掘已有的数据价值，利用数据画像技术寻找到目标客户和客户到潜在需求，进行产品推销和设计改良产品。用户画像从业务场景出发，实现数据商业变现重要方式。用户画像是数据思维运营过程中到一个重要闭环，帮助金融企业利用数据进行精细化运营和市场营销，以及产品设计。用户画像就是一切以数据商业化运营为中心，以商业场景为中，帮助金融企业深度分析客户，找到目标客户。DMP(大数据管理平台)在整个用户画像过程中起到了一个数据变现的作用。从技术角度来讲，DMP将画像数据进行标签化，利用机器学习算法来找到相似人群，同业务场景深度结合，筛选出具有价值的数据和客户，定位目标客户，触达客户，对营销效果进行记录和反馈。大数据管理平台DMP过去主要应用在广告行业，在金融行业应用不多，未来会成为数据商业应用的主要平台。DMP可以帮助信用卡公司筛选出未来一个月可能进行分期付款的客户，电子产品重度购买客户，筛选出金融理财客户，筛选出高端客户(在本行资产很少，但是在他行资产很多)，筛选出保障险种，寿险，教育险，车险等客户，筛选出稳健投资人，激进投资人，财富管理等方面等客户，并且可以触达这些客户，提高产品转化率，利用数据进行价值变现。DMP还可以了解客户的消费习惯、兴趣爱好、以及近期需求，为客户定制金融产品和服务，进行跨界营销。利用客户的消费偏好，提高产品转化率，提高用户黏度。DMP还作为引入外部数据的平台，将外部具有价值的数据引入到金融企业内部，补充用户画像数据，创建不同业务应用场景和商业需求，特别是移动大数据、电商数据、社交数据的应用，可以帮助金融企业来进行数据价值变现，让用户画像离商业应用更加近一些，体现用户画像的商业价值。用户画像的关键不是360度分析客户，而是为企业带来商业价值，离开了商业价值谈用户画像就是耍流氓。金融企业用户画像项目出发点一定要从业务需求出发，从强相关数据出发，从业务场景应用出发。用户画像的本质就是深度分析客户，掌握具有价值数据，找到目标客户，按照客户需求来定制产品，利用数据实现价值变现。 六、金融行业用户画像实践1)银行用户画像实践介绍银行具有丰富的交易数据、个人属性数据、消费数据、信用数据和客户数据，用户画像的需求较大。但是缺少社交信息和兴趣爱好信息。到银行网点来办业务的人年纪偏大，未来消费者主要在网上进行业务办理。银行接触不到客户，无法了解客户需求，缺少触达客户的手段。分析客户、了解客户、找到目标客户、为客户设计其需要的产品，成了银行进行用户画像的主要目的。银行的主要业务需求集中在消费金融、财富管理、融资服务，用户画像要从这几个角度出发，寻找目标客户。银行的客户数据很丰富，数据类型和总量较多，系统也很多。可以严格遵循用户画像的五大步骤。先利用数据仓库进行数据集中，筛选出强相关信息，对定量信息定性化，生成DMP需要的数据。利用DMP进行基础标签和应用定制，结合业务场景需求，进行目标客户筛选或对用户进行深度分析。同时利用DMP引入外部数据，完善数据场景设计，提高目标客户精准度。找到触达客户的方式，对客户进行营销，并对营销效果进行反馈，衡量数据产品的商业价值。利用反馈数据来修正营销活动和提高ROI。形成市场营销的闭环，实现数据商业价值变现的闭环。另外DMP还可以深度分析客户，依据客户的消费特征、兴趣爱好、社交需求、信用信息来开发设计产品，为金融企业的产品开发提供数据支撑，并为产品销售方式提供场景数据。简单介绍一些DMP可以做到的数据场景变现。A：寻找分期客户利用银联数据+自身数据+信用卡数据，发现信用卡消费超过其月收入的用户，推荐其进行消费分期。B：寻找高端资产客户利用银联数据+移动位置数据(别墅/高档小区)+物业费代扣数据+银行自身数据+汽车型号数据，发现在银行资产较少，在其他行资产较多的用户，为其提供高端资产管理服务C：需找理财客户利用自身数据(交易+工资)+移动端理财客户端/电商活跃数据。发现客户将工资/资产转到外部，但是电商消费不活跃客户，其互联网理财可能性较大，可以为其提供理财服务，将资金留在本行。D：寻找境外游客户利用自身卡消费数据+移动设备位置信息+社交好境外强相关数据(攻略，航线，景点，费用)，寻找境外游客户为其提供金融服务。E：寻找贷款客户：利用自身数据(人口属性+信用信息)+移动设备位置信息+社交购房/消费强相关信息，寻找即将购车/购房的目标客户，为其提供金融服务(抵押贷款/消费贷款)。 2)保险行业用户画像实践保险行业的产品是一个长周期产品，保险客户再次购买保险产品的转化率很高，经营好老客户是保险公司一项重要任务。保险公司内部的交易系统不多，交易方式不是很复杂，数据主要集中在产品系统和交易系统之中，客户关系管理系统中也包含丰富了信息，但是数据集中在很多保险公司还没有完成，数据仓库建设可能需要在用户画像建设前完成。保险公司主要数据有人口属性信息，信用信息，产品销售信息，客户家人信息。缺少兴趣爱好、消费特征、社交信息等信息。保险产品主要有寿险，车险，保障，财产险，意外险，养老险，旅游险。保险行业DMP用户画像的业务场景都是围绕保险产品进行的，简单的应用场景可以是。A：依据自身数据(个人属性)+外部养车App活跃情况，为保险公司找到车险客户B：依据自身数据(个人属性)+移动设备位置信息—户外运动人群，为保险企业找到商旅人群，推销意外险和保障险。C：依据自身数据(家人数据)+人生阶段信息，为用户推荐理财保险，寿险，保障保险，养老险，教育险D：依据自身数据+外部数据，为高端人士提供财产险和寿险 3)证券行业用户画像2015年4月13日，一码通实施之后，证券行业面临了互联网证券平台的强力竞争，依据TalkingData发布的金融App排行榜，移动互联网证券App，排名前5位的证券类App，只有一家传统券商华泰证券。排名第一的互联网券商同化顺覆装机量是排名第一传统券商的6倍，前三名的互联券商总体覆盖用户接近6000万用户。用户总数还在不断增加。传统证券行业现在面临的主要挑战是用户交易账户的争夺，证券行业如何增加新用户?如何留住用户?如何提高证券行业用户的活跃?如何提高单个客户的收入?是证券行业主要的业务需求。证券行业拥有的数据类型有个人属性信息例如用户名称，手机号码，家庭地址，邮件地址等。证券公司还拥有交易用户的资产和交易纪录，同时还拥有用户收益数据，利用这些数据和外部数据，证券公司可以利用数据建立业务场景，筛选目标客户，为用户提供适合的产品，同时提高单个客户收入。证券公司可以利用用户画像数据来进行产品设计，下面举几个例子，看看用户画像和用户分析来帮助证券公司创造商业价值。 七、外部数据介绍金融企业内部数据主要集中在个人属性，信用属性和消费特征上，缺少社交属性和兴趣偏好等信息，这些信息可以通过第三方获得。社交数据就是客户在社交媒体上发表的言论和行为，可以是评论，文章，图片，甚至可以是表情符号，音频和视频。社交数据可以依靠第三方平台，在社交网站上利用爬虫技术进行获得(Spider)。社交数据的打通是一个挑战，如果能够客户的授权最好，金融企业就可以将社交数据纳入到用户画像之中。社交数据具有实时和反映内心需要的特点，富国银行已经将社交数据作为分析客户需求的一个重要数据纬度。例如如果某一个客户在社交媒体上发表了一个问题，罗马有哪些好玩的地方，金融企业就会推测客户可能近期会有出境游的计划，就会向客户推销一些旅游相关产品。社交媒体数据正在成为金融企业积极争取获得的数据，除了利用网络爬虫技术到微博上进行数据采集之外，金融企业自身网站上到文本数据采集和呼叫中心(callcenter)纪录的信息都可以进行文本挖掘。通过客户编号，进行打通，将其补充到客户画像之中。社交数据需要通过数据挖掘将其定义为结构化数据，并且同业务场景、客户需求向结合，清晰进行分类。例如将母婴论坛发言活跃的用户定义为潜在教育需求客户，将学生论坛活跃的客户定义为学区房需要客户，将境外自助游论坛上活跃的客户定义为境外旅游客户，将雪球上活跃的客户定义为理财客户等。金融企业完全可以从社交数据中挖掘出客户近期的消费需求，及时进行市场营销和定制产品。兴趣爱好数据可以借助于移动大数据位置信息获得，客户手机设备的位置轨迹信息可以揭示客户喜欢何种品牌，喜欢吃辣还是吃火锅，客户喜欢旅游还是喜欢宅在家里，客户喜欢看电影还是喜欢运动。客户喜欢中档品牌还是高档品牌，客户喜欢喝茶还是喝咖啡。移动手机上App的安装情况和活动频次一样可以揭示客户的兴趣和爱好。同时移动大数据进行加工之后还可以告诉金融企业，客户近期的需求是买车还是买房。外部数据引入过程中，金融企业面临的巨大挑战是外部数据的覆盖率，如何打通内外部数据，外部数据同内部客户的匹配率，外部数据同业务的相关度，外部数据的活跃程度等。用户画像平台(DMP)可以通过技术手段将外部数据引入到金融企业内部，建立标准的标签体系，提供灵活的用户画像方式，按照业务场景进行筛选客户。 八、移动大数据的商业价值移动互联网时代，移动大数据具有较高的商业价值。如果一个用户不喜欢一个App，其不会装在手机上。客户经常使用的App可以推测用户的兴趣爱好和消费偏好。另外移动设备的位置信息可以帮助金融企业了解客户行为轨迹、兴趣爱好、品牌偏好和消费需求。 1)移动App提供一切服务，App可以反映用户喜好智能手机上安装的App正在代替PC互联网为所有客户提供服务，清晨起床可以看看墨迹天气，了解一下今天的天气情况。出门时可以通过嘀嘀打车来预定出租车，安排出行。或者通过百度地图来了解路况信息，决定进行从哪条路到公司。快到中午时，可以通过饿了吗或者百度外卖预定午餐，如果想出去吃饭可以利用大众点评订餐和买单。中午可以利用携程App预定家庭旅行机票和酒店，还可以将通过App看看理财产品。如果需要看电影，可以通过格瓦拉来预定要电影票，如果需要看医生，可以通过微医网预约医生。晚上可以通过淘宝来购物，通过学习宝来监督子女教育等。可以看出移动App已经可以满足人们大部分生活需要，提供了人们的衣食住行、教育、医疗、旅游、金融等服务。移动App包围了人们的日常生活，成为人们消费的主要场所。智能手机上App使用的频率，可以代表用户的喜好。例如喜欢理财的客户，其智能手机上一定会安装理财App，并经常使用;母婴人群也会安装和母婴相关的App，频繁使用;商旅人群使用商旅App的频率一定会高于其他移动用户。80后、90后的消费行为将会以移动互联网为主，App的安装和活跃数据更加能够反应出年轻人的消费偏好。 2)智能设备的位置信息，商业价值广大智能手机设备的位置信息代表了消费者的位置轨迹，这个轨迹可以推测出消费者的消费偏好和习惯。在美国，移动设备位置信息的商业化较为成熟，GPS数据正在帮助很多企业进行数据变现，提高社会运营效率。在中国，移动大数据的商业应用刚刚开始，在房地产业、零售行业、金融行业、市场分析等领域取得了一些效果。移动大数据中的位置信息代表了用户轨迹，商业应用较早。2014年，美国移动设备位置信息的市场规模接近1000亿美金。但中国移动设备位置信息的商业应用才刚刚开始。目前主要的应用在互联网金融的反欺诈领域。线上的欺诈行为具有较高的隐蔽性，很难识别和侦测。P2P贷款用户很大一部分来源于线上，因此恶意欺诈事件发生在线上的风险远远大于线下。中国的很多数据处于封闭状态，P2P公司在客户真实信息验证方面面临较大的挑战。移动大数据可以验证P2P客户的居住地点，例如某个客户在利用手机申请贷款时，填写自己居住地是上海。但是P2P企业依据其提供的手机设备信息，发现其过去三个月从来没有居住在上海，这个人提交的信息可能是假信息，发生恶意欺诈的风险较高。移动设备的位置信息可以辨识出设备持有人的居住地点，帮助P2P公司验证贷款申请人的居住地。借款用户的工作单位是用户还款能力的强相关信息，具有高薪工作的用户，其贷款信用违约率较低。这些客户成为很多贷款平台积极争取的客户，也是恶意欺诈团伙主要假冒的客户。某个用户在申请贷款时，如果声明自己是工作在上海陆家嘴金融企业的高薪人士，其贷款审批会很快并且额度也会较高。但是P2P公司利用移动大数据，发现这个用户在过去的三个月里面，从来没有出现在陆家嘴，大多数时间在城乡结合处活动，那么这个用户恶意欺诈的可能性就较大。移动大数据可以帮助P2P公司在一定程度上来验证贷款用户真实工作地点，降低犯罪分子利用高薪工作进行恶意欺诈的风险。P2P企业可以利用移动设备的位置信息，了解过去3个月用户的行为轨迹。如果某个用户经常在半夜2点出现在酒吧等危险区域，并且经常有飙车行为，这个客户定义成高风险客户的概率就较高。移动App的使用习惯和某些高风险App也可以帮助P2P企业识别出用户的高风险行为。如果用户经常在半夜2点频繁使用App，其成为高风险客户的概率就较大。移动大数据在预防互联网恶意欺诈和高风险客户识别方面，已经有了成熟的应用场景。前海征信、宜信、聚信立、闪银已经开始利用TalkingData的数据，预防互联网恶意欺诈和识别高风险客户，并取得了较好的效果。移动大数据应用场景正在被逐步挖掘出来，未来移动大数商业应用将更加广阔。用户画像是大数据商业应用的重要领域，其实并没有多么复杂，只要掌握用户画像的原则和方法，以及实施步骤。结合金融企业的业务场景，用户画像可以帮助金融企业创造商业价值，实现大数据直接变现。文章来源]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>用户画像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD转DataFrame的一道面试题]]></title>
    <url>%2FRDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%80%E9%81%93%E9%9D%A2%E8%AF%95%E9%A2%98.html</url>
    <content type="text"><![CDATA[题目现在在我们HDFS文件系统上面 存了一个文件，该文件格式是 .txt文件格式，要求把这个文件格式转换成为parquet文件格式 :解题思路:1）先读取文件生成一个RDD2）把RDD转换成为一个DataFrame，RDD[Person].toDF3) 写数据，指定文件格式就可以了！！代码实现 :12345678910val conf = new SparkConf().setMaster("local").setAppName("DataFrameReflection") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) import sqlContext.implicits._ val peopelRDD: RDD[People] = sc.textFile("hdfs://hadoop01:9000/resources/people.txt") .map(line =&gt; People(line.split(",")(0),line.split(",")(1).trim.toInt)) val df = peopelRDD.toDF() df.write.format("parquet").save("hdfs://hadoop01:9000/test/")]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>面试题</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD转DataFrame的两种方法]]></title>
    <url>%2FRDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95.html</url>
    <content type="text"><![CDATA[介绍一下Spark将RDD转换成DataFrame的两种方式。 通过是使用case class的方式，不过在scala 2.10中最大支持22个字段的case class,这点需要注意 是通过spark内部的StructType方式，将普通的RDD转换成DataFrame 装换成DataFrame后，就可以使用SparkSQL来进行数据筛选过滤等操作 方法一123456789101112131415161718192021import org.apache.spark.rdd.RDDimport org.apache.spark.sql.SQLContextimport org.apache.spark.&#123;SparkConf, SparkContext&#125;//需要提前知道列名及类型case class People(var name: String,var age:Int)object DataFrameReflection &#123; def main(args:Array[String]):Unit = &#123; val conf = new SparkConf().setMaster("local").setAppName("DataFrameReflection") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) import sqlContext.implicits._ val peopelRDD: RDD[People] = sc.textFile("people.txt") .map(line =&gt; People(line.split(",")(0),line.split(",")(1).trim.toInt)) val df = peopelRDD.toDF() df.createOrReplaceTempView("people") sqlContext.sql("select * from people").show() &#125;&#125; 方法二12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import org.apache.spark.rdd.RDDimport org.apache.spark.sql.types.&#123;IntegerType, StringType, StructField, StructType&#125;import org.apache.spark.sql.&#123;DataFrame, Row, SQLContext&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;object DataFrameProgrammatically &#123; def main(args:Array[String]): Unit = &#123; val conf = new SparkConf().setMaster("local").setAppName("DataFrameProgrammatically") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) //读取文件 val rdd: RDD[String] = sc.textFile("people.txt") /** * 得到 rowRDD */ val rowRDD: RDD[Row] = rdd.map(line =&gt; &#123; val fields = line.split(",") Row(fields(0), fields(1).trim.toInt) &#125;) /** * 得到structType */ val structType = StructType( StructField("name",StringType,true) :: StructField("age",IntegerType,true) :: Nil ) /** * rowRDD:RDD[Row] * schema: StructType */ val df: DataFrame = sqlContext.createDataFrame(rowRDD,structType) df.createOrReplaceTempView("people") sqlContext.sql("select * from people").show()//// /**// * 官网schema实现方法// */// val schemaString = "name age"// val fields = schemaString.split(" ")// .map(fieldName =&gt; StructField(// fieldName,StringType,nullable = true// ))// val schema = StructType(fields) &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>RDD</tag>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java单例模式]]></title>
    <url>%2FJava%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[饿汉式1234567891011//饿汉式单例类，在类初始化时，已经自行实例化public class Singleton1 &#123; //私有的默认构造方法 private Singleton1()&#123;&#125; //已经自行实例化 private static final Singleton1 single = new Singleton1() ; //静态工厂方法 private static Singleton1 getInstance()&#123; return single ; &#125;&#125; 懒汉式单例类 1234567891011121314//懒汉式单例类，在第一次调用的时候实例化public class Singleton2 &#123; //私有的默认构造方法 private Singleton2()&#123;&#125; //注意，这里没有final private static Singleton2 single = null ; //静态工厂方法，如果不加synchronized线程不安全 public synchronized static Singleton2 getInstance() &#123; if(single == null )&#123; single = new Singleton2() ; &#125; return single ; &#125;&#125; 登记式单例类 123456789101112131415161718192021222324252627282930313233343536public class RegistSingleton &#123; //用ConcurrentHashMap来维护映射关系，这是线程安全的 public static final Map&lt;String,Object&gt; REGIST=new ConcurrentHashMap&lt;String, Object&gt;(); static &#123; //把RegistSingleton自己也纳入容器管理 RegistSingleton registSingleton=new RegistSingleton(); REGIST.put(registSingleton.getClass().getName(),registSingleton); &#125; private RegistSingleton()&#123;&#125; public static Object getInstance(String className)&#123; //如果传入的类名为空，就返回RegistSingleton实例 if(className==null) className=RegistSingleton.class.getName(); //如果没有登记就用反射new一个 if (!REGIST.containsKey(className))&#123; //没有登记就进入同步块 synchronized (RegistSingleton.class)&#123; //再次检测是否登记 if (!REGIST.containsKey(className))&#123; try &#123; //实例化对象 REGIST.put(className,Class.forName(className).newInstance()); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; //返回单例 return REGIST.get(className); &#125;&#125; 总结java中单例模式是一种常见的设计模式，单例模式分三种：懒汉式单例、饿汉式单例、登记式单例三种。单例模式有一下特点：1、单例类只能有一个实例。2、单例类必须自己自己创建自己的唯一实例。3、单例类必须给所有其他对象提供这一实例。单例模式确保某个类只有一个实例，而且自行实例化并向整个系统提供这个实例。在计算机系统中，线程池、缓存、日志对象、对话框、打印机、显卡的驱动程序对象常被设计成单例。这些应用都或多或少具有资源管理器的功能。每台计算机可以有若干个打印机，但只能有一个Printer Spooler，以避免两个打印作业同时输出到打印机中。每台计算机可以有若干通信端口，系统应当集中管理这些通信端口，以避免一个通信端口同时被两个请求同时调用。总之，选择单例模式就是为了避免不一致状态，避免政出多头。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经验之谈]]></title>
    <url>%2F%E7%BB%8F%E9%AA%8C%E4%B9%8B%E8%B0%88.html</url>
    <content type="text"><![CDATA[1、重构是程序员的主力技能。2、工作日志能提升脑容量。3、先用profiler调查，才有脸谈优化。4、注释贵精不贵多。杜绝大姨妈般的“例注”。漫山遍野的碎碎念注释，实际就是背景噪音。5、普通程序员+google=超级程序员。6、单元测试总是合算的。7、不要先写框架再写实现。最好反过来，从原型中提炼框架。8、代码结构清晰，其它问题都不算事儿。9、好的项目作风硬派，一键测试，一键发布，一键部署；烂的项目生性猥琐，口口相传，不立文字，神神秘秘。10、编码不要畏惧变化，要拥抱变化。11、常充电。程序员只有一种死法：土死的。12、编程之事，隔离是方向，起名是关键，测试是主角，调试是补充，版本控制是后悔药。13、一行代码一个兵。形成建制才能有战斗力。单位规模不宜过大，千人班，万人排易成万人坑。14、重构/优化/修复Bug，同时只能做一件。15、简单模块注意封装，复杂模块注意分层。16、人脑性能有限，整洁胜于杂乱。读不懂的代码，尝试整理下格式；不好用的接口，尝试重新封装下。17、迭代速度决定工作强度。想多快好省，就从简化开发流程，加快迭代速度开始。18、忘掉优化写代码。过早优化等同恶意破坏；忘掉代码做优化。优化要基于性能测试，而不是纠结于字里行间。19、最好的工具是纸笔；其次好的是markdown。20、Leader问任务时间，若答不上来，可能是任务拆分还不够细。21、宁可多算一周，不可少估一天。过于“乐观”容易让boss受惊吓。22、最有用的语言是English。其次的可能是Python。23、百闻不如一见。画出结果，一目了然。调试耗时将大大缩短。24、资源、代码应一道受版本管理。资源匹配错误远比代码匹配错误更难排查。25、不要基于想象开发， 要基于原型开发。原型的价值是快速验证想法，帮大家节省时间。26、序列化首选明文文本 。诸如二进制、混淆、加密、压缩等等有需要时再加。27、编译器永远比你懂微观优化。只能向它不擅长的方向努力。28、不要定过大、过远、过细的计划。即使定了也没有用。29、至少半数时间将花在集成上。时间，时间，时间总是不够。30、与主流意见/方法/风格/习惯相悖时，先检讨自己最可靠。31、出现bug主动查，不管是不是你的。这能让你业务能力猛涨、个人形象飙升；如果你的bug被别人揪出来…..呵呵，那你会很被动～≧﹏≦32、不知怎么选技术书时就挑薄的。起码不会太贵，且你能看完。33、git是最棒的。简单，可靠，免费。34、仅对“可预测的非理性”抛断言。35、Log要写时间与分类。并且要能重定向输出。36、注释是稍差的文档。更好的是清晰的命名。让代码讲自己的故事。37、造轮子是很好的锻炼方法。前提是你见过别的轮子。38、code review最好以小组/结对的形式。对业务有一定了解，建议会更有价值（但不绝对）。而且不会成为负担。管理员个人review则很容易成team的瓶颈。39、提问前先做调研。问不到点上既被鄙视，又浪费自己的时间。原文地址]]></content>
      <categories>
        <category>干货</category>
      </categories>
      <tags>
        <tag>经验之谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM常见面试题]]></title>
    <url>%2FJVM%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98.html</url>
    <content type="text"><![CDATA[简单说说JVM的架构运行时区 堆 栈 方法区 程序计数器 本地方法 描述一下垃圾回收的过程(常见)垃圾回收主要发生在堆中 年轻代 eden区 s1 s2 年老代常见的垃圾回收的算法 标记清除法 复制算法 标记整理算法说说常见的垃圾回收器 串行—Serial 并行—Parallel 并发—CMS写一段代码，让它报堆内存溢出 不断的创建对象 你要去用这些对象写一段代码，让它报栈内存溢出 递归一直调用即可设置堆或栈的参数 -Xms -Xmx -Xss364k]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解java虚拟机精华总结]]></title>
    <url>%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3java%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%B2%BE%E5%8D%8E%E6%80%BB%E7%BB%93%EF%BC%88%E9%9D%A2%E8%AF%95%EF%BC%89.html</url>
    <content type="text"><![CDATA[运行时数据区域Java虚拟机管理的内存包括几个运行时数据内存：方法区、虚拟机栈、本地方法栈、堆、程序计数器，其中方法区和堆是由线程共享的数据区，其他几个是线程隔离的数据区 程序计数器程序计数器是一块较小的内存，他可以看做是当前线程所执行的行号指示器。 字节码解释器工作的时候就是通过改变这个计数器的值来选取下一条需要执行的字节码的指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Native方法，这个计数器则为空。此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemotyError情况的区域 Java虚拟机栈虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧用于储存局部变量表、操作数栈、动态链接、方法出口等信息。每个方法从调用直至完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 栈内存就是虚拟机栈，或者说是虚拟机栈中局部变量表的部分 局部变量表存放了编辑期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（refrence）类型和returnAddress类型（指向了一条字节码指令的地址） 其中64位长度的long和double类型的数据会占用两个局部变量空间，其余的数据类型只占用1个。 Java虚拟机规范对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常。如果虚拟机扩展时无法申请到足够的内存，就会跑出OutOfMemoryError异常 本地方法栈本地方法栈和虚拟机栈发挥的作用是非常类似的，他们的区别是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务本地方法栈区域也会抛出StackOverflowError和OutOfMemoryErroy异常 Java堆 堆是Java虚拟机所管理的内存中最大的一块。 Java堆是被所有线程共享的一块内存区域，在虚拟机启动的时候创建，此内存区域的唯一目的是存放对象实例，几乎所有的对象实例都在这里分配内存。所有的对象实例和数组都在堆上分配。 Java堆是垃圾收集器管理的主要区域。Java堆细分为新生代和老年代。 不管怎样，划分的目的都是为了更好的回收内存，或者更快地分配内存。Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。如果在堆中没有完成实例分配，并且堆也无法在扩展时将会抛出OutOfMemoryError异常 方法区方法区它用于储存已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据，除了Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载。当方法区无法满足内存分配需求时，将抛出OutOfMemoryErroy异常。 运行时常量池它是方法区的一部分。Class文件中除了有关的版本、字段、方法、接口等描述信息外、还有一项信息是常量池，用于存放编辑期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放Java语言并不要求常量一定只有编辑期才能产生，也就是可能将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法当常量池无法再申请到内存时会抛出OutOfMemoryError异常 hotspot虚拟机对象对象的创建 检查虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已经被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程 分配内存接下来将为新生对象分配内存，为对象分配内存空间的任务等同于把一块确定的大小的内存从Java堆中划分出来。假设Java堆中内存是绝对规整的，所有用过的内存放在一遍，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针指向空闲空间那边挪动一段与对象大小相等的距离，这个分配方式叫做“指针碰撞” 如果Java堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式成为“空闲列表”选择那种分配方式由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 Init执行new指令之后会接着执行Init方法，进行初始化，这样一个对象才算产生出来 对象的内存布局在HotSpot虚拟机中，对象在内存中储存的布局可以分为3块区域：对象头、实例数据和对齐填充 对象头包括两部分：a) 储存对象自身的运行时数据，如哈希码、GC分带年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳b) 另一部分是指类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例 对象的访问定位 使用句柄访问Java堆中将会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址优势:reference中存储的是稳点的句柄地址,在对象被移动(垃圾收集时移动对象是非常普遍的行为)时只会改变句柄中的实例数据指针，而reference本身不需要修改 使用直接指针访问Java堆对象的布局就必须考虑如何访问类型数据的相关信息,而refreence中存储的直接就是对象的地址 优势：速度更快，节省了一次指针定位的时间开销，由于对象的访问在Java中非常频繁，因此这类开销积少成多后也是一项非常可观的执行成本 OutOfMemoryError 异常Java堆溢出Java堆用于存储对象实例，只要不断的创建对象，并且保证GCRoots到对象之间有可达路径来避免垃圾回收机制清除这些对象，那么在数量到达最大堆的容量限制后就会产生内存溢出异常，如果是内存泄漏，可进一步通过工具查看泄漏对象到GC Roots的引用链。于是就能找到泄露对象是通过怎样的路径与GC Roots相关联并导致垃圾收集器无法自动回收它们的。掌握了泄漏对象的类型信息及GC Roots引用链的信息，就可以比较准确地定位出泄漏代码的位置如果不存在泄露，换句话说，就是内存中的对象确实都还必须存活着，那就应当检查虚拟机的堆参数（-Xmx与-Xms），与机器物理内存对比看是否还可以调大，从代码上检查是否存在某些对象生命周期过长、持有状态时间过长的情况，尝试减少程序运行期的内存消耗 虚拟机栈和本地方法栈溢出对于HotSpot来说，虽然-Xoss参数（设置本地方法栈大小）存在，但实际上是无效的，栈容量只由-Xss参数设定。关于虚拟机栈和本地方法栈，在Java虚拟机规范中描述了两种异常： 如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出StackOverflowError如果虚拟机在扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError异常在单线程下，无论由于栈帧太大还是虚拟机栈容量太小，当内存无法分配的时候，虚拟机抛出的都是StackOverflowError异常如果是多线程导致的内存溢出，与栈空间是否足够大并不存在任何联系，这个时候每个线程的栈分配的内存越大，反而越容易产生内存溢出异常。解决的时候是在不能减少线程数或更换64为的虚拟机的情况下，就只能通过减少最大堆和减少栈容量来换取更多的线程 方法区和运行时常量池溢出String.intern()是一个Native方法，它的作用是：如果字符串常量池中已经包含一个等于此String对象的字符串，则返回代表池中这个字符串的String对象；否则，将此String对象包含的字符串添加到常量池中，并且返回此String对象的引用 由于常量池分配在永久代中，可以通过-XX:PermSize和-XX:MaxPermSize限制方法区大小，从而间接限制其中常量池的容量。 Intern(): JDK1.6 intern方法会把首次遇到的字符串实例复制到永久代，返回的也是永久代中这个字符串实例的引用，而由StringBuilder创建的字符串实例在Java堆上，所以必然不是一个引用 JDK1.7 intern()方法的实现不会再复制实例，只是在常量池中记录首次出现的实例引用，因此intern()返回的引用和由StringBuilder创建的那个字符串实例是同一个 垃圾收集程序计数器、虚拟机栈、本地方法栈3个区域随线程而生，随线程而灭，在这几个区域内就不需要过多考虑回收的问题，因为方法结束或者线程结束时，内存自然就跟随着回收了 判断对象存活引用计数器法给对象添加一个引用计数器，每当由一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的 可达性分析算法通过一系列的成为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径成为引用链，当一个对象到GCROOTS没有任何引用链相连时，则证明此对象时不可用的 Java语言中GC Roots的对象包括下面几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈JNI（Native方法）引用的对象 引用强引用就是在程序代码之中普遍存在的，类似Object obj = new Object() 这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象软引用用来描述一些还有用但并非必须的元素。对于它在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存才会抛出内存溢出异常 弱引用用来描述非必须对象的，但是它的强度比软引用更弱一些，被引用关联的对象只能生存到下一次垃圾收集发生之前，当垃圾收集器工作时，无论当前内存是否足够都会回收掉只被弱引用关联的对象 虚引用的唯一目的就是能在这个对象被收集器回收时收到一个系统通知 Finalize方法任何一个对象的finalize()方法都只会被系统自动调用一次，如果对象面临下一次回收，它的finalize()方法不会被再次执行，因此第二段代码的自救行动失败了 回收方法区永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类 废弃常量：假如一个字符串abc已经进入了常量池中，如果当前系统没有任何一个String对象abc，也就是没有任何Stirng对象引用常量池的abc常量，也没有其他地方引用的这个字面量，这个时候发生内存回收这个常量就会被清理出常量池 无用的类： 该类所有的实例都已经被回收，就是Java堆中不存在该类的任何实例 加载该类的ClassLoader已经被回收 该类对用的java.lang.Class对象没有在任何地方被引用，无法再任何地方通过反射访问该类的方法 垃圾收集算法 标记—清除算法 算法分为标记和清除两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象、不足:一个是效率问题，标记和清除两个过程的效率都不高；另一个是空间问题，标记清楚之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后再程序运行过程中需要分配较大的对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作 复制算法 他将可用内存按照容量划分为大小相等的两块，每次只使用其中的一块。当这块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可 不足：将内存缩小为了原来的一半 实际中我们并不需要按照1:1比例来划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor 当另一个Survivor空间没有足够空间存放上一次新生代收集下来的存活对象时，这些对象将直接通过分配担保机制进入老年代 标记整理算法 让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存 分代收集算法 只是根据对象存活周期的不同将内存划分为几块。一般是把java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用标记清理或者标记整理算法来进行回收 垃圾收集器 Serial收集器： 这个收集器是一个单线程的收集器，但它的单线程的意义不仅仅说明它会只使用一个COU或一条收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集时，必须暂停其他所有的工作线程，直到它手机结束 ParNew 收集器： Serial收集器的多线程版本，除了使用了多线程进行收集之外，其余行为和Serial收集器一样 并行：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态 并发：指用户线程与垃圾收集线程同时执行（不一定是并行的，可能会交替执行），用户程序在继续执行，而垃圾收集程序运行于另一个CPU上 Parallel Scavenge 收集器是一个新生代收集器，它是使用复制算法的收集器，又是并行的多线程收集器。 吞吐量：就是CPU用于运行用户代码的时间与CPU总消耗时间的比值。即吞吐量=运行用户代码时间/（运行用户代码时间+垃圾收集时间） Serial Old 收集器： 是Serial收集器的老年代版本,是一个单线程收集器，使用标记整理算法 Parallel Old 收集器： Parallel Old是Paraller Seavenge收集器的老年代版本，使用多线程和标记整理算法 CMS收集器： CMS收集器是基于标记清除算法实现的，整个过程分为4个步骤： 1.初始标记2.并发标记3.重新标记4.并发清除 优点：并发收集、低停顿 缺点： CMS收集器对CPU资源非常敏感，CMS默认启动的回收线程数是（CPU数量+3）/4， CMS收集器无法处理浮动垃圾，可能出现Failure失败而导致一次Full G场地产生 CMS是基于标记清除算法实现的 G1收集器： 它是一款面向服务器应用的垃圾收集器 并行与并发：利用多CPU缩短STOP-The-World停顿的时间 分代收集 空间整合：不会产生内存碎片 可预测的停顿 运作方式：初始标记，并发标记，最终标记，筛选回收6.内存分配与回收策略对象优先在Eden分配：大多数情况对象在新生代Eden区分配，当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC 大对象直接进入老年代：所谓大对象就是指需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组。这样做的目的是避免Eden区及两个Servivor之间发生大量的内存复制 长期存活的对象将进入老年代如果对象在Eden区出生并且尽力过一次Minor GC后仍然存活，并且能够被Servivor容纳，将被移动到Servivor空间中，并且把对象年龄设置成为1.对象在Servivor区中每熬过一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度（默认15岁），就将会被晋级到老年代中 动态对象年龄判定为了更好地适应不同程序的内存状况，虚拟机并不是永远地要求对象的年龄必须达到了MaxTenuringThreshold才能晋级到老年代，如果在Servivor空间中相同年龄所有对象的大小总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入到老年代，无须登到MaxTenuringThreshold中要求的年龄 空间分配担保：在发生Minor GC 之前，虚拟机会检查老年代最大可 用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor DC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许那么会继续检查老年代最大可用的连续空间是否大于晋级到老年代对象的平均大小，如果大于，将尝试进行一次Minor GC，尽管这次MinorGC 是有风险的：如果小于，或者HandlePromotionFailure设置不允许冒险，那这时也要改为进行一次Full GC 虚拟机类加载机制虚拟机吧描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制在Java语言里面，类型的加载。连接和初始化过程都是在程序运行期间完成的 类加载的时机类被加载到虚拟机内存中开始，到卸载为止，整个生命周期包括：加载、验证、准备、解析、初始化、使用和卸载7个阶段 加载、验证、准备、初始化和卸载这5个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始，而解析阶段则不一定：它在某些情况下可以再初始化阶段之后再开始，这个是为了支持Java语言运行时绑定（也成为动态绑定或晚期绑定） 虚拟机规范规定有且只有5种情况必须立即对类进行初始化： 遇到new、getstatic、putstatic或invokestatic这4条字节码指令时，如果类没有进行过初始化，则需要触发其初始化。生成这4条指令的最常见的Java代码场景是：使用new关键字实例化对象的时候、读取或设置一个类的静态字段（被final修饰、已在编译期把结果放入常量池的静态字段除外）的时候，以及调用一个类的静态方法的时候 使用java.lang.reflect包的方法对类进行反射调用的时候，如果类没有进行过初始化，则需要先触发其初始化 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化 当虚拟机启动时候，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类 当使用JDK1.7的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化被动引用： 通过子类引用父类的静态字段，不会导致子类初始化 通过数组定义来引用类，不会触发此类的初始化 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化接口的初始化：接口在初始化时，并不要求其父接口全部完成类初始化，只有在正整使用到父接口的时候（如引用接口中定义的常量）才会初始化类加载的过程加载 通过一个类的全限定名类获取定义此类的二进制字节流 将这字节流所代表的静态存储结构转化为方法区运行时数据结构 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口 怎么获取二进制字节流？ 从ZIP包中读取，这很常见，最终成为日后JAR、EAR、WAR格式的基础 从网络中获取，这种场景最典型的应用就是Applet 运行时计算生成，这种常见使用得最多的就是动态代理技术 由其他文件生成，典型场景就是JSP应用 从数据库中读取，这种场景相对少一些（中间件服务器）数组类本身不通过类加载器创建，它是由Java虚拟机直接创建的数组类的创建过程遵循以下规则： 如果数组的组件类型(指的是数组去掉一个维度的类型)是引用类型，那就递归采用上面的加载过程去加载这个组件类型，数组C将在加载该组件类型的类加载器的类名称空间上被标识 如果数组的组件类型不是引用类型(列如int[]组数)，Java虚拟机将会把数组C标识为与引导类加载器关联 数组类的可见性与它的组件类型的可见性一致，如果组件类型不是引用类型，那数组类的可见性将默认为public 验证验证阶段会完成下面4个阶段的检验动作：文件格式验证，元数据验证，字节码验证，符号引用验证 文件格式验证第一阶段要验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理。这一阶段可能包括： 是否以魔数oxCAFEBABE开头 主、次版本号是否在当前虚拟机处理范围之内 常量池的常量中是否有不被支持的常量类型(检查常量tag标志) 指向常量的各种索引值中是否有指向不存在的常量或不符合类型的常量 CONSTANT_Itf8_info 型的常量中是否有不符合UTF8编码的数据 Class文件中各个部分及文件本身是否有被删除的或附加的其他信息这个阶段的验证时基于二进制字节流进行的，只有通过类这个阶段的验证后，字节流才会进入内存的方法区进行存储，所以后面的3个验证阶段全部是基于方法区的存储结构进行的，不会再直接操作字节流元数据验证 这个类是否有父类(除了java.lang.Object之外,所有的类都应当有父类) 这个类的父类是否继承了不允许被继承的类（被final修饰的类） 如果这个类不是抽象类，是否实现类其父类或接口之中要求实现的所有方法 类中的字段、方法是否与父类产生矛盾(列如覆盖类父类的final字段,或者出现不符合规则的方法重载，列如方法参数都一致，但返回值类型却不同等)第二阶段的主要目的是对类元数据信息进行语义校验，保证不存在不符合Java语言规范的元数据信息字节码验证第三阶段是整个验证过程中最复杂的一个阶段，主要目的似乎通过数据流和控制流分析，确定程序语言是合法的、符合逻辑的。在第二阶段对元数据信息中的数据类型做完校验后，这个阶段将对类的方法体进行校验分析，保证被校验类的方法在运行时不会做出危害虚拟机安全的事件。 保证任意时刻操作数栈的数据类型与指令代码序列都能配合工作，列如，列如在操作数栈放置类一个int类型的数据，使用时却按long类型来加载入本地变量表中 保证跳转指令不会跳转到方法体以外的字节码指令上 保证方法体中的类型转换时有效的，列如可以把一个子类对象赋值给父类数据类型，这个是安全的，但是吧父类对象赋值给子类数据类型，甚至把对象赋值给与它毫无继承关系、完全不相干的一个数据类型，则是危险和不合法的符号引用验证发生在虚拟机将符号引用转化为直接引用的时候，这个转化动作将在连接的第三阶段——解析阶段中发生。 符号引用中通过字符串描述的全限定名是否能找到相对应的类 在指定类中是否存在符合方法的字段描述符以及简单名称所描述的方法和字段 符号引用中的类、字段、方法的访问性是否可被当前类访问 对于虚拟机的类加载机制来说，验证阶段是非常重要的，但是不一定必要（因为对程序运行期没有影响）的阶段。如果全部代码都已经被反复使用和验证过，那么在实施阶段就可以考虑使用Xverify：none参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间 准备准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量都在方法区中进行分配。这个时候进行内存分配的仅包括类变量(被static修饰的变量)，而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在Java堆中。其次，这里说的初始值通常下是数据类型的零值。假设public static int value = 123；那变量value在准备阶段过后的初始值为0而不是123，因为这时候尚未开始执行任何Java方法，而把value赋值为123的putstatic指令是程序被编译后，存放于类构造器()方法之中，所以把value赋值为123的动作将在初始化阶段才会执行，但是如果使用final修饰，则在这个阶段其初始值设置为123 解析解析阶段是虚拟机将常量池内符号引用替换为直接引用的过 初始化类的初始化阶段是类加载过程的最后一步，前面的类加载过程中，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才正真开始执行类中定义的Java程序代码(或者说是字节码) 类的加载器双亲委派模型：只存在两种不同的类加载器：启动类加载器（Bootstrap ClassLoader），使用C++实现，是虚拟机自身的一部分。另一种是所有其他的类加载器，使用JAVA实现，独立于JVM，并且全部继承自抽象类java.lang.ClassLoader. 启动类加载器（Bootstrap ClassLoader），负责将存放在\lib目录中的，或者被-Xbootclasspath参数所制定的路径中的，并且是JVM识别的（仅按照文件名识别，如rt.jar，如果名字不符合，即使放在lib目录中也不会被加载），加载到虚拟机内存中，启动类加载器无法被JAVA程序直接引用。 扩展类加载器，由sun.misc.Launcher$ExtClassLoader实现，负责加载\lib\ext目录中的，或者被java.ext.dirs系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader），由sun.misc.Launcher$AppClassLoader来实现。由于这个类加载器是ClassLoader中的getSystemClassLoader()方法的返回值，所以一般称它为系统类加载器。负责加载用户类路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 这张图表示类加载器的双亲委派模型（Parents Delegation model）. 双亲委派模型要求除了顶层的启动加载类外，其余的类加载器都应当有自己的父类加载器。，这里类加载器之间的父子关系一般不会以继承的关系来实现，而是使用组合关系来复用父类加载器的代码。 双亲委派模型的工作过程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都是应该传送到顶层的启动类加载器中，只有当父类加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载。 这样做的好处就是：Java类随着它的类加载器一起具备了一种带有优先级的层次关系。例如类java.lang.Object,它存放在rt.jar中，无论哪一个类加载器要加载这个类，最终都是委派给处于模型最顶端的启动类加载器进行加载，因此Object类在程序的各种类加载器环境中都是同一个类。相反，如果没有使用双亲委派模型，由各个类加载器自行去加载的话，如果用户自己编写了一个称为java.lang.object的类，并放在程序的ClassPath中，那系统中将会出现多个不同的Object类，Java类型体系中最基础的行为也就无法保证，应用程序也将会变得一片混乱就是保证某个范围的类一定是被某个类加载器所加载的，这就保证在程序中同 一个类不会被不同的类加载器加载。这样做的一个主要的考量，就是从安全层 面上，杜绝通过使用和JRE相同的类名冒充现有JRE的类达到替换的攻击方式 Java内存模型与线程内存间的交互操作关于主内存与工作内存之间的具体交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步到主内存之间的实现细节，Java内存模型定义了以下八种操作来完成： lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。 unlock（解锁）：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read（读取）：作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。 write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。 如果要把一个变量从主内存中复制到工作内存，就需要按顺寻地执行read和load操作， 如果把变量从工作内存中同步回主内存中，就要按顺序地执行store和write操作。Java内存 模型只要求上述操作必须按顺序执行，而没有保证必须是连续执行。也就是read和load之间， store和write之间是可以插入其他指令的，如对主内存中的变量a、b进行访问时，可能的顺 序是read a，read b，load b， load a。 Java内存模型还规定了在执行上述八种基本操作时，必须满足如下规则： 不允许read和load、store和write操作之一单独出现 不允许一个线程丢弃它的最近assign的操作，即变量在工作内存中改变了之后必须同步到主内存中。 不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步回主内存中。 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量。即就是对一个变量实施use和store操作之前，必须先执行过了assign和load操作。 一个变量在同一时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。lock和unlock必须成对出现如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值如果一个变量事先没有被lock操作锁定，则不允许对它执行unlock操作；也不允许去unlock一个被其他线程锁定的变量。对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作）。 重排序在执行程序时为了提高性能，编译器和处理器经常会对指令进行重排序。重排序分成三种类型： 编译器优化的重排序。编译器在不改变单线程程序语义放入前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从Java源代码到最终实际执行的指令序列，会经过下面三种重排序： 为了保证内存的可见性，Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。Java内存模型把内存屏障分为LoadLoad、LoadStore、StoreLoad和StoreStore四种： 对于volatile型变量的特殊规则当一个变量定义为volatile之后，它将具备两种特性： 第一：保证此变量对所有线程的可见性，这里的可见性是指当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。普通变量的值在线程间传递需要通过主内存来完成由于valatile只能保证可见性，在不符合一下两条规则的运算场景中，我们仍要通过加锁来保证原子性 运算结果并不依赖变量的当前值，或者能够确保只有单一的线程修改变量的值。 变量不需要与其他的状态变量共同参与不变约束 第二：禁止指令重排序，普通的变量仅仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中执行顺序一致，这个就是所谓的线程内表现为串行的语义 Java内存模型中对volatile变量定义的特殊规则。假定T表示一个线程，V和W分别表示两个volatile变量，那么在进行read、load、use、assign、store、write操作时需要满足如下的规则： 只有当线程T对变量V执行的前一个动作是load的时候，线程T才能对变量V执行use动作；并且，只有当线程T对变量V执行的后一个动作是use的时候，线程T才能对变量V执行load操作。线程T对变量V的use操作可以认为是与线程T对变量V的load和read操作相关联的，必须一起连续出现。这条规则要求在工作内存中，每次使用变量V之前都必须先从主内存刷新最新值，用于保证能看到其它线程对变量V所作的修改后的值。 只有当线程T对变量V执行的前一个动是assign的时候，线程T才能对变量V执行store操作；并且，只有当线程T对变量V执行的后一个动作是store操作的时候，线程T才能对变量V执行assign操作。线程T对变量V的assign操作可以认为是与线程T对变量V的store和write操作相关联的，必须一起连续出现。这一条规则要求在工作内存中，每次修改V后都必须立即同步回主内存中，用于保证其它线程可以看到自己对变量V的修改。 假定操作A是线程T对变量V实施的use或assign动作，假定操作F是操作A相关联的load或store操作，假定操作P是与操作F相应的对变量V的read或write操作；类型地，假定动作B是线程T对变量W实施的use或assign动作，假定操作G是操作B相关联的load或store操作，假定操作Q是与操作G相应的对变量V的read或write操作。如果A先于B，那么P先于Q。这条规则要求valitile修改的变量不会被指令重排序优化，保证代码的执行顺序与程序的顺序相同。对于long和double型变量的特殊规则Java模型要求lock、unlock、read、load、assign、use、store、write这8个操作都具有原子性，但是对于64为的数据类型（long和double），在模型中特别定义了一条相对宽松的规定：允许虚拟机将没有被volatile修饰的64位数据的读写操作分为两次32为的操作来进行，即允许虚拟机实现选择可以不保证64位数据类型的load、store、read和write这4个操作的原子性原子性、可见性和有序性原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。Java内存模型是通过在变量修改后将新值同步会主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方式来实现可见性，valatile特殊规则保障新值可以立即同步到祝内存中。Synchronized是在对一个变量执行unlock之前，必须把变量同步回主内存中（执行store、write操作）。被final修饰的字段在构造器中一旦初始化完成，并且构造器没有吧this的引用传递出去，那在其他线程中就能看见final字段的值 可见性：可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 有序性：即程序执行的顺序按照代码的先后顺序执行。 先行发生原则这些先行发生关系无须任何同步就已经存在，如果不再此列就不能保障顺序性，虚拟机就可以对它们任意地进行重排序 程序次序规则：在一个线程内，按照程序代码顺序，书写在前面的操作先行发生于书写在后面的操作。准确的说，应该是控制顺序而不是程序代码顺序，因为要考虑分支。循环等结构 管程锁定规则：一个unlock操作先行发生于后面对同一个锁的lock操作。这里必须强调的是同一个锁，而后面的是指时间上的先后顺序 Volatile变量规则：对一个volatile变量的写操作先行发生于后面对这个变量的读操作，这里的后面同样是指时间上的先后顺序 线程启动规则：Thread对象的start()方法先行发生于此线程的每一个动作 线程终止规则：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread.joke()方法结束、ThradisAlive()的返回值等手段检测到线程已经终止执行 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断时间的发生，可以通过Thread.interrupted()方法检测到是否有中断发生 对象终结规则：一个对象的初始化完成(构造函数执行结束)先行发生于它的finalize()方法的开始 传递性：如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论Java线程调度 协同式调度：线程的执行时间由线程本身控制 抢占式调度：线程的执行时间由系统来分配 状态转换 新建 运行：可能正在执行。可能正在等待CPU为它分配执行时间 无限期等待：不会被分配CUP执行时间，它们要等待被其他线程显式唤醒 限期等待：不会被分配CUP执行时间，它们无须等待被其他线程显式唤醒，一定时间会由系统自动唤醒 阻塞：阻塞状态在等待这获取到一个排他锁，这个时间将在另一个线程放弃这个锁的时候发生；等待状态就是在等待一段时间，或者唤醒动作的发生 结束：已终止线程的线程状态，线程已经结束执行 线程安全 不可变：不可变的对象一定是线程安全的、无论是对象的方法实现还是方法的调用者，都不需要再采取任何的线程安全保障。例如：把对象中带有状态的变量都声明为final，这样在构造函数结束之后，它就是不可变的。 绝对线程安全 相对线程安全：相对的线程安全就是我们通常意义上所讲的线程安全，它需要保证对这个对象单独的操作是线程安全的，我们在调用的时候不需要做额外的保障措施，但是对于一些特定顺序的连续调用，就可能需要在调用端使用额外的同步手段来保证调用的正确性 线程兼容：对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全使用 线程对立：是指无论调用端是否采取了同步措施，都无法在多线程环境中并发使用的代码线程安全的实现方法 互斥同步：同步是指在多个线程并发访问共享数据时，保证共享数据在同一个时刻只被一个（或者是一些，使用信号量的时候）线程使用。而互斥是实现同步的一种手段，临界区、互斥量和信号量都是主要的互斥实现方式。互斥是因，同步是果：互斥是方法，同步是目的. 在Java中，最基本的互斥同步手段就是synchronized关键字，它经过编译之后，会在同步块的前后分别形成monitorenter和monitorexit这两个字节码指令，这两个字节码都需要一个reference类型的参数来指明要锁定和解锁的对象。如果Java程序中的synchronized明确指定了对象参数，那就是这个对象的reference；如果没有指明，那就根据synchronized修饰的是实例方法还是类方法，去取对应的对象实例或Class对象来作为锁对象。在执行monitorenter指令时，首先要尝试获取对象的锁。如果这个对象没有被锁定，或者当前线程已经拥有了那个对象的锁，把锁的计数器加1，对应的在执行monitorexit指令时会将锁计数器减1，当计数器为0时，锁就被释放。如果获取对象锁失败，哪当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止 Synchronized，ReentrantLock增加了一些高级功能 等待可中断：是指当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情，可中断特性对处理执行时间非常长的同步块很有帮助 公平锁：是指多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁；非公平锁则不能保证这一点，在锁被释放时，任何一个等待锁的线程都有机会获得锁。Synchronized中的锁是非公平的，ReentrantLock默认情况下也是非公平的，但可以通过带布尔值的构造函数要求使用公平锁 锁绑定多个条件是指一个ReentrantLock对象可以同时绑定多个Condition对象，而在synchronized中，锁对象的wait()和notify()或notifyAll()方法可以实现一个隐含的条件，如果要和多余一个的条件关联的时候，就不得不额外地添加一个锁，而ReentrantLock则无须这样做，只需要多次调用newCondition方法即可 非阻塞同步 无同步方案可重入代码：也叫纯代码，可以在代码执行的任何时刻中断它，转而去执行另外一段代码（包括递归调用它本身）而在控制权返回后，原来的程序不会出现任何错误。所有的可重入代码都是线程安全的，但是并非所有的线程安全的代码都是可重入的。 判断一个代码是否具备可重入性：如果一个方法，它的返回结果是可预测的，只要输入了相同的数据，就都能返回相同的结果，那它就满足可重入性的要求，当然也就是线程安全的 线程本地存储：如果一段代码中所需要的数据必须与其他代码共享，那就看看这些共享数据的代码是否能保证在同一个线程中执行？如果能保障，我们就可以把共享数据的可见范围限制在同一个线程之内，这样，无须同步也能保证线程之间不出现数据争用的问题 锁优化适应性自旋、锁消除、锁粗化、轻量级锁和偏向锁 自旋锁与自适应自旋自旋锁：如果物理机器上有一个以上的处理器，能让两个或以上的线程同时并行执行，我们就可以让后面请求锁的那个线程稍等一下，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁。为了让线程等待，我们只需让线程执行一个忙循环（自旋），这项技术就是所谓的自旋锁 自适应自旋转：是由前一次在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后要获取这个锁时将可能省略掉自过程，以避免浪费处理器资源。 锁消除锁消除是指虚拟机即时编辑器在运行时，对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行消除。如果在一段代码中。推上的所有数据都不会逃逸出去从而被其他线程访问到，那就可以把它们当作栈上数据对待，认为它们是线程私有的，同步加锁自然就无须进行 锁粗化如果虚拟机检测到有一串零碎的操作都是对同一对象的加锁，将会把加锁同步的范围扩展（粗化）到整个操作序列的外部 轻量级锁偏向锁它的目的是消除无竞争情况下的同步原语，进一步提高程序的运行性能。如果轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把这个同步都消除掉，CAS操作都不做了如果在接下俩的执行过程中，该锁没有被其他线程获取，则持有偏向锁的线程将永远不需要在进行同步 逃逸分析逃逸分析的基本行为就是分析对象动态作用域：当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他方法中，成为方法逃逸。甚至还可能被外部线程访问到，比如赋值给类变量或可以在其他线程中访问的实例变量，称为线程逃逸 如果一个对象不会逃逸到方法或线程之外，也就是别的方法或线程无法通过任何途径访问到这个对象，则可能为这个变量进行一些高效的优化栈上分配：如果确定一个对象不会逃逸出方法外，那让这个对象在栈上分配内存将会是一个不错的注意，对象所占用的内存空间就可以随栈帧出栈而销毁。如果能使用栈上分配，那大量的对象就随着方法的结束而销毁了，垃圾收集系统的压力将会小很多 同步消除：如果确定一个变量不会逃逸出线程，无法被其他线程访问，那这个变量的读写肯定就不会有竞争，对这个变量实施的同步措施也就可以消除掉 标量替换：标量就是指一个数据无法在分解成更小的数据表示了，int、long等及refrence类型等都不能在进一步分解，它们称为标量。 如果一个数据可以继续分解，就称为聚合量，Java中的对象就是最典型的聚合量 如果一个对象不会被外部访问，并且这个对象可以被拆散的化，那程序正整执行的时候将可能不创建这个对象，而改为直接创建它的若干个被这个方法使用到的成员变量来代替]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark架构及原理]]></title>
    <url>%2FSpark%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%8E%9F%E7%90%86.html</url>
    <content type="text"><![CDATA[Spark架构及原理]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD原理]]></title>
    <url>%2FRDD%E5%8E%9F%E7%90%86.html</url>
    <content type="text"><![CDATA[RDD概念RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。RDD是Spark的最基本抽象,是对分布式内存的抽象使用，实现了以操作本地集合的方式来操作分布式数据集的抽象实现。RDD是Spark最核心的东西，它表示已被分区，不可变的并能够被并行操作的数据集合，不同的数据集格式对应不同的RDD实现。RDD必须是可序列化的。RDD可以cache到内存中，每次对RDD数据集的操作之后的结果，都可以存放到内存中，下一个操作可以直接从内存中输入，省去了MapReduce大量的磁盘IO操作 RDD可以横向多分区，当计算过程中内存不足时，将数据刷到磁盘等外部存储上，从而实现数据在内存和外存的灵活切换。可以说，RDD是有虚拟数据结构组成，并不包含真实数据体。 RDD的内部属性通过RDD的内部属性，用户可以获取相应的元数据信息。通过这些信息可以支持更复杂的算法或优化。 一组分片（Partition），即数据集的基本组成单位对于RDD来说，每个分片都会被一个task计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配的CPU Core的数据。 计算每个分片的函数Spark中RDD的计算是以分片为单位的，通过函数可以对每个数据块进行RDD需要进行的用户自定义函数运算。函数会对迭代器进行复合，不需要保存每次计算的结果。 RDD之间的依赖关系对父RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 一个Partitioner，即RDD的分片函数当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 分区列表，存储存取每个Partition的优先位置（preferred location）对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 可选属性key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。 可选属性每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”) RDD的特点 创建：只能通过转换 ( transformation ，如map/filter/groupBy/join 等，区别于动作 action) 从两种数据源中创建 RDD 1 ）稳定存储中的数据； 2 ）其他 RDD。 只读：状态不可变，不能修改。 分区：支持使 RDD 中的元素根据那个 key 来分区 ( partitioning ) ，保存到多个结点上。还原时只会重新计算丢失分区的数据，而不会影响整个系统。 路径：在 RDD 中叫世族或血统 ( lineage ) ，即 RDD 有充足的信息关于它是如何从其他 RDD 产生而来的。 持久化：支持将会被重用的 RDD 缓存 ( 如 in-memory 或溢出到磁盘 )。 延迟计算：Spark 也会延迟计算 RDD ，使其能够将转换管道化 (pipeline transformation)。 操作：丰富的转换（transformation）和动作 ( action ) ， count/reduce/collect/save 等。 执行了多少次transformation操作，RDD都不会真正执行运算（记录lineage），只有当action操作被执行时，运算才会触发。 RDD的优点 RDD只能从持久存储或通过Transformations操作产生，相比于分布式共享内存(DSM)可以更高效实现容错，对于丢失部分数据分区只需根据它的lineage就可重新计算出来，而不需要做特定的Checkpoint。 RDD的不变性，可以实现类Hadoop MapReduce的推测式执行。 RDD的数据分区特性，可以通过数据的本地性来提高性能，这不Hadoop MapReduce是一样的。 RDD都是可序列化的，在内存不足时可自动降级为磁盘存储，把RDD存储于磁盘上，这时性能会有大的下降但不会差于现在的MapReduce。 批量操作：任务能够根据数据本地性 (data locality) 被分配，从而提高性能。 RDD的存储与分区 用户可以选择不同的存储级别存储RDD以便重用。 当前RDD默认是存储于内存，但当内存不足时，RDD会spill到disk。 RDD在需要进行分区把数据分布于集群中时会根据每条记录Key进行分区（如Hash 分区），以此保证两个数据集在Join时能高效。 RDD根据useDisk、useMemory、useOffHeap、deserialized、replication参数的组合定义了以下存储级别：12345678910111213//存储等级定义： val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(false, false, true, false) RDD的容错机制RDD的容错机制实现分布式数据集容错方法有两种：数据检查点和记录更新 RDD采用记录更新的方式：记录所有更新点的成本很高。所以，RDD只支持粗颗粒变换，即只记录单个块（分区）上执行的单个操作，然后创建某个RDD的变换序列（血统 lineage）存储下来； 变换序列指，每个RDD都包含了它是如何由其他RDD变换过来的以及如何重建某一块数据的信息。因此RDD的容错机制又称“血统”容错。 要实现这种“血统”容错机制，最大的难题就是如何表达父RDD和子RDD之间的依赖关系。实际上依赖关系可以分两种，窄依赖和宽依赖。 窄依赖：子RDD中的每个数据块只依赖于父RDD中对应的有限个固定的数据块 宽依赖：子RDD中的一个数据块可以依赖于父RDD中的所有数据块。例如：map变换，子RDD中的数据块只依赖于父RDD中对应的一个数据块；groupByKey变换，子RDD中的数据块会依赖于多块父RDD中的数据块，因为一个key可能分布于父RDD的任何一个数据块中 将依赖关系分类的两个特性： 窄依赖可以在某个计算节点上直接通过计算父RDD的某块数据计算得到子RDD对应的某块数据；宽依赖则要等到父RDD所有数据都计算完成之后，并且父RDD的计算结果进行hash并传到对应节点上之后才能计算子RDD。 数据丢失时，对于窄依赖只需要重新计算丢失的那一块数据来恢复；对于宽依赖则要将祖先RDD中的所有数据块全部重新计算来恢复。 所以在“血统”链特别是有宽依赖的时候，需要在适当的时机设置数据检查点。也是这两个特性要求对于不同依赖关系要采取不同的任务调度机制和容错恢复机制。 Spark计算工作流 输入：在Spark程序运行中，数据从外部数据空间（例如，HDFS、Scala集合或数据）输入到Spark，数据就进入了Spark运行时数据空间，会转化为Spark中的数据块，通过BlockManager进行管理。 运行：在Spark数据输入形成RDD后，便可以通过变换算子fliter等，对数据操作并将RDD转化为新的RDD，通过行动（Action）算子，触发Spark提交作业。如果数据需要复用，可以通过Cache算子，将数据缓存到内存。 输出：程序运行结束数据会输出Spark运行时空间，存储到分布式存储中（如saveAsTextFile输出到HDFS）或Scala数据或集合中（collect输出到Scala集合，count返回Scala Int型数据）。 Spark的核心数据模型是RDD，但RDD是个抽象类，具体由各子类实现，如MappedRDD、ShuffledRDD等子类。Spark将常用的大数据操作都转化成为RDD的子类。 RDD编程模型textFile算子从HDFS读取日志文件，返回“file”（RDD）；filter算子筛出带“ERROR”的行，赋给 “errors”（新RDD）；cache算子把它缓存下来以备未来使用；count算子返回“errors”的行数。RDD看起来与Scala集合类型 没有太大差别，但它们的数据和运行模型大相迥异。1234val file = sc.textFile("hdfs://...")val errors = file.filter(_.contains("ERROR"))errors.cache()errors.count() 上面代码给出了RDD数据模型，并将上例中用到的四个算子映射到四种算子类型。Spark程序工作在两个空间中：Spark RDD空间和Scala原生数据空间。在原生数据空间里，数据表现为标量（scalar，即Scala基本类型，用橘色小方块表示）、集合类型（蓝色虚线 框）和持久存储（红色圆柱）。 下图描述了Spark运行过程中通过算子对RDD进行转换， 算子是RDD中定义的函数，可以对RDD中的数据进行转换和操作。 输入算子（橘色箭头）将Scala集合类型或存储中的数据吸入RDD空间，转为RDD（蓝色实线框）。输入算子的输入大致有两类：一类针对 Scala集合类型，如parallelize；另一类针对存储数据，如上例中的textFile。输入算子的输出就是Spark空间的RDD。 因为函数语义，RDD经过变换（transformation）算子（蓝色箭头）生成新的RDD。变换算子的输入和输出都是RDD。RDD会被划分 成很多的分区 （partition）分布到集群的多个节点中，图1用蓝色小方块代表分区。注意，分区是个逻辑概念，变换前后的新旧分区在物理上可能是同一块内存或存 储。这是很重要的优化，以防止函数式不变性导致的内存需求无限扩张。有些RDD是计算的中间结果，其分区并不一定有相应的内存或存储与之对应，如果需要 （如以备未来使用），可以调用缓存算子（例子中的cache算子，灰色箭头表示）将分区物化（materialize）存下来（灰色方块）。 一部分变换算子视RDD的元素为简单元素，分为如下几类： 输入输出一对一（element-wise）的算子，且结果RDD的分区结构不变，主要是map、flatMap（map后展平为一维RDD）； 输入输出一对一，但结果RDD的分区结构发生了变化，如union（两个RDD合为一个）、coalesce（分区减少）； 从输入中选择部分元素的算子，如filter、distinct（去除冗余元素）、subtract（本RDD有、它RDD无的元素留下来）和sample（采样）。 另一部分变换算子针对Key-Value集合，又分为： 对单个RDD做element-wise运算，如mapValues（保持源RDD的分区方式，这与map不同）； 对单个RDD重排，如sort、partitionBy（实现一致性的分区划分，这个对数据本地性优化很重要，后面会讲）； 对单个RDD基于key进行重组和reduce，如groupByKey、reduceByKey； 对两个RDD基于key进行join和重组，如join、cogroup。 后三类操作都涉及重排，称为shuffle类操作。 从RDD到RDD的变换算子序列，一直在RDD空间发生。这里很重要的设计是lazy evaluation：计算并不实际发生，只是不断地记录到元数据。元数据的结构是DAG（有向无环图），其中每一个“顶点”是RDD（包括生产该RDD 的算子），从父RDD到子RDD有“边”，表示RDD间的依赖性。Spark给元数据DAG取了个很酷的名字，Lineage（世系）。这个 Lineage也是前面容错设计中所说的日志更新。 Lineage一直增长，直到遇上行动（action）算子（图1中的绿色箭头），这时 就要evaluate了，把刚才累积的所有算子一次性执行。行动算子的输入是RDD（以及该RDD在Lineage上依赖的所有RDD），输出是执行后生 成的原生数据，可能是Scala标量、集合类型的数据或存储。当一个算子的输出是上述类型时，该算子必然是行动算子，其效果则是从RDD空间返回原生数据空间。 RDD的运行逻辑如图所示，在Spark应用中，整个执行流程在逻辑上运算之间会形成有向无环图。Action算子触发之后会将所有累积的算子形成一个有向无环图，然后由调度器调度该图上的任务进行运算。Spark的调度方式与MapReduce有所不同。Spark根据RDD之间不同的依赖关系切分形成不同的阶段（Stage），一个阶段包含一系列函数进行流水线执行。图中的A、B、C、D、E、F、G，分别代表不同的RDD，RDD内的一个方框代表一个数据块。数据从HDFS输入Spark，形成RDD A和RDD C，RDD C上执行map操作，转换为RDD D，RDD B和RDD F进行join操作转换为G，而在B到G的过程中又会进行Shuffle。最后RDD G通过函数saveAsSequenceFile输出保存到HDFS中。 RDD依赖关系RDD依赖关系如下图所示： 窄依赖 (narrowdependencies) 和宽依赖 (widedependencies) 。 窄依赖是指 父 RDD 的每个分区都只被子 RDD 的一个分区所使用，例如map、filter。 宽依赖就是指父 RDD 的分区被多个子 RDD 的分区所依赖，例如groupByKey、reduceByKey等操作。如果父RDD的一个Partition被一个子RDD的Partition所使用就是窄依赖，否则的话就是宽依赖。 这种划分有两个用处。首先，窄依赖支持在一个结点上管道化执行。例如基于一对一的关系，可以在 filter 之后执行 map 。其次，窄依赖支持更高效的故障还原。因为对于窄依赖，只有丢失的父 RDD 的分区需要重新计算。而对于宽依赖，一个结点的故障可能导致来自所有父 RDD 的分区丢失，因此就需要完全重新执行。因此对于宽依赖，Spark 会在持有各个父分区的结点上，将中间数据持久化来简化故障还原，就像 MapReduce 会持久化 map 的输出一样。 特别说明：对于join操作有两种情况，如果join操作的使用每个partition仅仅和已知的Partition进行join，此时的join操作就是窄依赖；其他情况的join操作就是宽依赖；因为是确定的Partition数量的依赖关系，所以就是窄依赖，得出一个推论，窄依赖不仅包含一对一的窄依赖，还包含一对固定个数的窄依赖（也就是说对父RDD的依赖的Partition的数量不会随着RDD数据规模的改变而改变） Stage的划分： Stage划分的依据就是宽依赖，什么时候产生宽依赖呢？例如reduceByKey，groupByKey等Action。 从后往前推理，遇到宽依赖就断开，遇到窄依赖就把当前的RDD加入到Stage中； 每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition数量决定的； 最后一个Stage里面的任务的类型是ResultTask，前面所有其他Stage里面的任务类型都是ShuffleMapTask； 代表当前Stage的算子一定是该Stage的最后一个计算步骤。 补充：Hadoop中的MapReduce操作中的Mapper和Reducer在Spark中基本等量算子是：map、reduceByKey；在一个Stage内部，首先是算子合并，也就是所谓的函数式编程的执行的时候最终进行函数的展开从而把一个Stage内部的多个算子合并成为一个大算子（其内部包含了当前Stage中所有算子对数据的计算逻辑）；其次是由于Transformation操作的Lazy特性！！在具体算子交给集群的Executor计算之前，首先会通过Spark Framework（DAGScheduler）进行算子的优化。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>RDD</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkCore调优]]></title>
    <url>%2FSparkCore%E8%B0%83%E4%BC%98.html</url>
    <content type="text"><![CDATA[开发角度 原则一：避免创建重复的RDD 原则二：尽可能用同一个RDD 原则三：对多次使用的RDD进行持久化如何选择一种最合适的持久化策略 MEMORY_ONLY MEMORY_ONLY_SER MEMORY_AND_DISK_SER 不考虑：DISK_ONLY和_2后缀 原则四：尽量避免使用shuffle类算子 能不用就不用 能不能用非shuffle类的算子去替代非shuffle类的join -》 map操作替代 原则五：使用map-side预聚合的shuffle操作：groupBykey 和 reduceBykey 原则六：使用高性能的算子： 使用reduceBykey//aggregateBykey替代groupBykey 使用mapPartitions替代普通map 使用foreachPartitions替代foreach 使用filter之后进行coalesce操作 使用repartitionAndSortWithhinPartitions替代repartition与sort类操作 原则七：广播大变量 原则八：使用Kryo优化序列化性能 conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”) conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) 优化数据结构 原则十：Data Locality PROCESS_LOCAL data is in the same JVM as the running code. This is the best locality possible NODE_LOCAL data is on the same node. Examples might be in HDFS on the same node, or in another executor on the same node. This is a little slower than PROCESS_LOCAL because the data has to travel between processes NO_PREF data is accessed equally quickly from anywhere and has no locality preference RACK_LOCAL data is on the same rack of servers. Data is on a different server on the same rack so needs to be sent over the network, typically through a single switch ANY data is elsewhere on the network and not in the same rack 默认值-spark.locality.wait-3s spark.locality.wait.process-建议60s park.locality.wait.node-建议30s spark.locality.wait.rack-建议20s 数据倾斜（面试的重点） 美团技术博客数据倾斜：数据倾斜发生时的现象： 绝大多数task执行的都非常快，但个别task执行极慢 原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常数据倾斜发生的最根本原因如何定位导致数据倾斜的代码： shuffle（找代码里面发生shuffle的算子） stage划分 界面观察就可以定位到是哪个算子导致的数据倾斜如何定位到哪个key导致的数据倾斜： 方式一： countBykey 有可能出来结果，但是会遇到数据倾斜 方式二：sample countBykey 方案解决：解决方案一：使用Hive ETL预处理数据 方案实现思路—-Hive实现预处理 方案实现原理—-数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已 方案优点—–实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升 方案缺点—–治标不治本，Hive ETL中还是会发生数据倾斜 解决方案二：过滤少量导致倾斜的key 方案实现原理-将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。 方案优点—-实现简单，而且效果也很好，可以完全规避掉数据倾斜 方案缺点—-key对于我们来说，没有实际意义才行 解决方案三：提高shuffle操作的并行度（没多大用） 方案优点—-实现起来比较简单，可以有效缓解和减轻数据倾斜的影响 方案缺点—-只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 解决方案四：两阶段聚合（局部聚合+全局聚合） 方案实现原理—-将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。 方案优点—-对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 方案缺点—-仅仅适用于聚合类的shuffle操作—-groupBykey，join类的shuffle操作]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive环境搭建]]></title>
    <url>%2FHive%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.html</url>
    <content type="text"><![CDATA[Hive安装内嵌Dervy版本 上传安装包 apache-hive-2.3.2-bin.tar.gz 解压安装包 tar -zxvf apache-hive-2.3.2-bin.tar.gz -C /home/hadoop/apps/ 进入到 bin 目录，运行 hive 脚本：[hadoop@hadoop02 bin]$ ./hive注意： 这时候一般会报错：Terminal initialization failed; falling back to unsupported，是因为 hadoop（/root/apps/hadoop-2.6.5/share/hadoop/yarn/lib）集群的 jline-0.9.94.jar 包版本 过低，替换成 hive/lib 中的 jline-2.12.jar 包即可。记住：所有 hdfs 节点都得替换 hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.4.jar 替换成 jline-2.12.jar 如果报错就按照此方式解决，没有报错就不用管，在使用新的 hadoop-2.7.5 版本中已经不 存在这个问题。所以不用关注。 外置 MySQL 版本 准备好 MySQL（请参考以下文档，或者自行安装 MySQL，或者一个可用的 MySQL） 上传安装包 apache-hive-2.3.2-bin.tar.gz 解压安装包 tar -zxvf apache-hive-2.3.2-bin.tar.gz -c ~/apps/ 修改配置文件 123[hadoop@hadoop02 conf]# touch hive-site.xml [hadoop@hadoop02 conf]# vi hive-site.xml ` 123456789101112131415161718192021222324&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive_metastore_232?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;!-- 如果 mysql 和 hive 在同一个服务器节点，那么请更改 hadoop02 为 localhost --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 可选配置，该配置信息用来指定 Hive 数据仓库的数据存储在 HDFS 上的目录12345&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/myhive/warehouse&lt;/value&gt; &lt;description&gt;hive default warehouse, if nessecory, change it&lt;/description&gt; &lt;/property&gt; 一定要记得加入 MySQL 驱动包（mysql-connector-java-5.1.40-bin.jar） 该 jar 包放置在 hive 的根路径下的 lib 目录 安装完成，配置环境变量 vi ~/.bashrc 添加以下两行内容： export HIVE_HOME=/home/hadoop/apps/apache-hive-2.3.2-bin export PATH=$PATH:$HIVE_HOME/bin 保存退出。 最后不要忘记：[hadoop@hadoop02 bin]$ source ~/.bashrc 验证 Hive 安装，执行命令hive –help 初始化元数据库注意：当使用的 hive 是 2.x 之前的版本，不做初始化也是 OK 的，当 hive 第一次启动的 时候会自动进行初始化，只不过会不会生成足够多的元数据库中的表。在使用过程中会 慢慢生成。但最后进行初始化。如果使用的 2.x 版本的 Hive，那么就必须手动初始化元 数据库。使用命令： 1[hadoop@hadoop02 bin]$ schematool -dbType mysql -initSchema 启动 Hive 客户端 1[hadoop@hadoop02 bin]$ hive --service cli 退出 Hive Linux RPM 方式安装 MySQL（记得使用 root 账户进行操作，若使用普通用户，那么请修改相应文件夹权限） 检查以前是否装过 MySQL 1rpm -qa|grep -i mysql 发现有的话就都卸载 1rpm -e --nodeps ........ 删除老版本 mysql 的开发头文件和库 123456rm -fr /usr/lib/mysql #数据库目录 rm -fr /usr/include/mysql rm -f /etc/my.cnf rm -fr /var/lib/mysql 注意：卸载后/var/lib/mysql 中的数据及/etc/my.cnf 不会删除，确定没用后就手工删除 准备安装包 MySQL-5.6.26-1.linux_glibc2.5.x86_64.rpm-bundle.tar， 上传，解压 命令：tar -zxvf MySQL-5.6.26-1.linux_glibc2.5.x86_64.rpm-bundle.tar 开始安装 安装 server rpm -ivh MySQL-server-5.6.26-1.linux_glibc2.5.x86_64.rpm 安装客户端 rpm -ivh MySQL-client-5.6.26-1.linux_glibc2.5.x86_64.rpm 登陆 MYSQL（登录之前千万记得一定要启动 mysql 服务） 启动命令： [hadoop@hadoop01 ~]$ service mysql start然后登陆，初始密码在 /root/.mysql_secret 这个文件里 修改密码 set PASSWORD=PASSWORD(‘root’); 退出登陆验证，看是否改密码成功 增加远程登陆权限，执行以下两个命令： 12mysql&gt;GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;root&apos; WITH GRANT OPTION; mysql&gt;FLUSH PRIVILEGES; 命令释义：grant 权限 1,权限 2,„权限 n on 数据库名称.表名称 to 用户名@用户地址 identified by ‘密码’; PS：1,权限 2,„权限 n 代表 select，insert，update，delete，create，drop，index，alter，grant， references，reload，shutdown，process，file 等 14 个权限。 当权限 1,权限 2,„权限 n 被 all privileges 或者 all 代替，表示赋予用户全部权限。 当数据库名称.表名称被.代替，表示赋予用户操作服务器上所有数据库所有表的权限。 用户地址可以是 localhost，也可以是 ip 地址、机器名字、域名。也可以用’%’地址连接。 至此 mysql 安装成功 更改数据库的默认编码为 UTF-8]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hive数据存储]]></title>
    <url>%2FHive%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8.html</url>
    <content type="text"><![CDATA[、Hive 的存储结构包括数据库、表、视图、分区和表数据等。数据库，表，分区等等都对 应 HDFS 上的一个目录。表数据对应 HDFS 对应目录下的文件。 Hive 中所有的数据都存储在 HDFS 中，没有专门的数据存储格式，因为 Hive 是读模式 （Schema On Read） ，可支持 TextFile，SequenceFile，RCFile 或者自定义格式等 只需要在创建表的时候告诉 Hive 数据中的列分隔符和行分隔符，Hive 就可以解析数据 Hive 的默认列分隔符：控制符 Ctrl + A，\x01 Hive 的默认行分隔符：换行符 \n 、Hive 中包含以下数据模型： database：在 HDFS 中表现为${hive.metastore.warehouse.dir}目录下一个文件夹 table：在 HDFS 中表现所属 database 目录下一个文件夹 external table：与 table 类似，不过其数据存放位置可以指定任意 HDFS 目录路径 partition：在 HDFS 中表现为 table 目录下的子目录 bucket：在 HDFS 中表现为同一个表目录或者分区目录下根据某个字段的值进行 hash 散 列之后的多个文件 view：与传统数据库类似，只读，基于基本表创建 Hive 的元数据存储在 RDBMS 中，除元数据外的其它所有数据都基于 HDFS 存储。默认情 况下，Hive 元数据保存在内嵌的 Derby 数据库中，只能允许一个会话连接，只适合简单的 测试。实际生产环境中不适用，为了支持多用户会话，则需要一个独立的元数据库，使用 MySQL 作为元数据库，Hive 内部对 MySQL 提供了很好的支持 Hive 中的表分为 内部表、 外部表、 分区表 和 Bucket表内部表和外部表的区别：删除内部表，删除表元数据和数据 删除外部表，删除元数据，不删除数据 内部表和外部表的使用选择：大多数情况，他们的区别不明显，如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表，但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中使用外部表的场景是针对一个数据集有多个不同的 Schema通过外部表和内部表的区别和使用选择的对比可以看出来，hive 其实仅仅只是对存储在 HDFS 上的数据提供了一种新的抽象。而不是管理存储在 HDFS 上的数据。所以不管创建内部 表还是外部表，都可以对 hive 表的数据存储目录中的数据进行增删操作。 分区表和分桶表的区别：Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。同 时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所 以对添加进分区的数据不做模式校验，分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive架构]]></title>
    <url>%2FHive%E6%9E%B6%E6%9E%84.html</url>
    <content type="text"><![CDATA[基本组成用户接口 CLI，Shell 终端命令行（Command Line Interface），采用交互形式使用 Hive 命令行与 Hive 进行交互，最常用（学习，调试，生产） JDBC/ODBC，是 Hive 的基于 JDBC 操作提供的客户端，用户（开发员，运维人员）通过 这连接至 Hive server 服务 Web UI，通过浏览器访问 Hive Thrift ServerThrift 是 Facebook 开发的一个软件框架，可以用来进行可扩展且跨语言的服务的开发， Hive 集成了该服务，能让不同的编程语言调用 Hive 的接口 元数据存储 元数据，通俗的讲，就是存储在 Hive 中的数据的描述信息。 Hive 中的元数据通常包括：表的名字，表的列和分区及其属性，表的属性（内部表和 外部表），表的数据所在目录 Metastore 默认存在自带的 Derby 数据库中。缺点就是不适合多用户操作，并且数据存 储目录不固定。数据库跟着 Hive 走，极度不方便管理 解决方案：通常存我们自己创建的 MySQL 库（本地 或 远程） Hive 和 MySQL 之间通过 MetaStore 服务交互 Driver：编译器（Compiler），优化器（Optimizer），执行器（Executor） Driver 组件完成 HQL 查询语句从词法分析，语法分析，编译，优化，以及生成逻辑执行 计划的生成。生成的逻辑执行计划存储在 HDFS 中，并随后由 MapReduce 调用执行 Hive 的核心是驱动引擎， 驱动引擎由四部分组成： (1) 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST） (2) 编译器：编译器是将语法树编译为逻辑执行计划 (3) 优化器：优化器是对逻辑执行计划进行优化 (4) 执行器：执行器是调用底层的运行框架执行逻辑执行计划 执行流程HiveQL 通过命令行或者客户端提交，经过 Compiler 编译器，运用 MetaStore 中的元数 据进行类型检测和语法分析，生成一个逻辑方案(Logical Plan)，然后通过的优化处理，产生 一个 MapReduce 任务。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive基本概念]]></title>
    <url>%2FHive%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.html</url>
    <content type="text"><![CDATA[Hive的基本概念 开发者: Facebook实现并开源 作用: 基于Hadoop的一个数据仓库工具，可以将结构化的数据映射为一张数据库表，并提供HQL(Hive SQL)查询功能，底层数据是存储在HDFS上。 本质: 将SQL语句转换为MapReduce任务运行，使不熟悉Mapreduce的用户很方便地利用HQL处理和计算HDFS上的结构化的数据，使用于离线的批量数据计算。 Hive 依赖于 HDFS 存储数据，Hive 将 HQL 转换成 MapReduce 执行 所以说 Hive 是基于 Hadoop 的一个数据仓库工具，实质就是一款基于 HDFS 的 MapReduce 计算框架，对存储在 HDFS 中的数据进行分析和管理 hbase 和 hive 的区别 hbase： 数据库 hive : 数据仓库 区别：1、数据库，对于数据会做精细化的管理，具有事务的概念 数据仓库，存储数据的格式就类似打包，没有事务的概念 2、操作方式的区别： 数据库：NoSQL语法 put get scan delele 数据仓库: SQL方言 Hive的SQL === HQL hibernate:HQL 3、用途的区别： 数据库：OLTP 联机事务处理 增删改 数据仓库： OLAP 联机分析处理 查询 hive是数据仓库，它根本就不支持 update和delete 但是支持insert 4、模式上的区别 数据库： 写模式 hbase无严格模式 ： 仅有的模式校验只有 表名和列簇的名称 数据仓库： 读模式 Hive的特点优点 可扩展性,横向扩展，Hive 可以自由的扩展集群的规模，一般情况下不需要重启服务 横向扩展：通过分担压力的方式扩展集群的规模 纵向扩展： 一台服务器cpu i7-6700k 4核心8线程， 8核心16线程，内存64G =&gt; 128G 延展性，Hive 支持自定义函数，用户可以根据自己的需求来实现自己的函数 良好的容错性，可以保障即使有节点出现问题，SQL 语句仍可完成执行 缺点 Hive 不支持记录级别的增删改操作，但是用户可以通过查询生成新表或者将查询结 果导入到文件中（当前选择的 hive-2.3.2 的版本支持记录级别的插入操作） Hive 的查询延时很严重，因为 MapReduce Job 的启动过程消耗很长时间，所以不能 用在交互查询系统中。 Hive 不支持事务（因为不没有增删改，所以主要用来做 OLAP（联机分析处理），而 不是 OLTP（联机事务处理），这就是数据处理的两大级别） Hive 只适合用来做海量离线数 据统计分析，也就是数据仓库]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase底层原理]]></title>
    <url>%2Fhbase%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86.html</url>
    <content type="text"><![CDATA[1、系统架构 client职责 HBase有两张特殊表 .METE.：记录了用户所有表拆分出来的Region映射信息，.META.可以有多个Region -ROOT-:记录了.METE.表的Region信息，-ROOT-只有一个Region，无论如何不会分裂 Client访问用户数据过程： 1、首先访问zookeeper，找到-root-表的region所在的位置 2、然后访问-ROOT-表，接着访问.META.表 3、最后才能找到用户数据的位置去访问中间需要多次网络操作，不过Client端会做cache缓存 ZooKeeper职责 ZooKeeper 为 HBase 提供 Failover 机制，选举 Master，避免单点 Master 单点故障问题 存储所有 Region 的寻址入口，即-ROOT-表的位置信息 （在哪台服务器上） 实时监控 RegionServer 的状态，将 RegionServer 的上线和下线信息实时通知给 Master 存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family Master职责 为RegionServer分配Region 负责RegionServer的负载均衡 发现失效的 RegionServer 并重新分配其上的 Region HDFS 上的垃圾文件（HBase）回收 处理 Schema 更新请求（表的创建，删除，修改，列簇的增加等等） RegionServer职责 RegionServer 维护 Master 分配给它的 Region，处理对这些 Region 的 IO 请求 RegionServer 负责 Split 在运行过程中变得过大的 Region，负责 Compact 操作 （溢出到磁盘的文件有可能会有很多，会进行合并，把rowkey相同的所有keyvalue对象收集到一起，进行合并） 注意：1、可以看到，client 访问 HBase 上数据的过程并不需要 master 参与（寻址访问 zookeeper 和RegioneServer，数据读写访问 RegioneServer）， Master 仅仅维护者 Table 和 Region 的元数据Stay hungry Stay foolish – http://blog.csdn.net/zhongqi2513 信息，负载很低。 2、.META. 存的是所有的 Region 的位置信息，那么 RegioneServer 当中 Region 在进行分裂之后 的新产生的 Region，是由 Master 来决定发到哪个 RegioneServer，这就意味着，只有 Master 知道 new Region 的位置信息，所以，由 Master 来管理.META.这个表当中的数据的 CRUD所以结合以上两点表明，在没有 Region 分裂的情况，Master 宕机一段时间是可以忍受的 2、物理存储整体物理结构 Table 中的所有行都按照 RowKsey 的字典序排列。 Table 在行的方向上分割为多个 HRegion。 HRegion 按大小分割的(默认 10G)，每个表一开始只有一个 HRegion，随着数据不断插入 表，HRegion 不断增大，当增大到一个阀值的时候，HRegion 就会等分会两个新的 HRegion。 当表中的行不断增多，就会有越来越多的 HRegion。 HRegion 是 Hbase 中分布式存储和负载均衡的最小单元。最小单元就表示不同的 HRegion 可以分布在不同的 HRegionserver 上。但一个 HRegion 是不会拆分到多个 server 上的。 HRegion 虽然是负载均衡的最小单元，但并不是物理存储的最小单元。事实上，HRegion 由一个或者多个 Store 组成，每个 Store 保存一个 Column Family。每个 Strore 又由一个 memStore 和 0 至多个 StoreFile 组成 StoreFile和HFile结构 MemStore和StoreFile 一个 Hregion 由多个 Store 组成，每个 Store 包含一个列族的所有数据 Store 包括位于内存的一个 memstore 和位于硬盘的多个 storefile 组成 写操作先写入 memstore，当 memstore 中的数据量达到某个阈值，HRegionServer 启动 flushcache 进程写入 storefile，每次写入形成单独一个 Hfile 当总 storefile 大小超过一定阈值后，会把当前的 region 分割成两个，并由 HMaster 分配给相 应的 region 服务器，实现负载均衡 客户端检索数据时，先在 memstore 找，找不到再找 storefile HLog(WAL) WAL 意为 Write ahead log(http://en.wikipedia.org/wiki/Write-ahead_logging)，类似 mysql 中的 binlog，用来做灾难恢复之用，Hlog 记录数据的所有变更，一旦数据修改，就可以从 log 中 进行恢复。 每个 Region Server 维护一个 Hlog,而不是每个 Region 一个。这样不同 region(来自不同 table) 的日志会混在一起，这样做的目的是不断追加单个文件相对于同时写多个文件而言，可以减 少磁盘寻址次数，因此可以提高对 table 的写性能。带来的麻烦是，如果一台 region server 下线，为了恢复其上的 region，需要将 region server 上的 log 进行拆分，然后分发到其它 region server 上进行恢复。 HLog 文件就是一个普通的 Hadoop Sequence File： 1、HLog Sequence File 的 Key 是 HLogKey 对象，HLogKey 中记录了写入数据的归属信息，除 了 table 和 region 名字外，同时还包括 sequence number 和 timestamp，timestamp 是”写入 时间”，sequence number 的起始值为 0，或者是最近一次存入文件系统中 sequence number。 HLog Sequece File 的 Value 是 HBase 的 KeyValue 对象，即对应 HFile 中的 KeyValue 3、寻址机制既然读写都在 RegionServer 上发生，我们前面有讲到，每个 RegionSever 为一定数量的 Region 服务，那么 Client 要对某一行数据做读写的时候如何能知道具体要去访问哪个 RegionServer 呢？那就是接下来我们要讨论的问题 老的Region寻址方式在 HBase-0.96 版本以前，HBase有两个特殊的表，分别是-ROOT-表和.META.表，其中-ROOT的位置存储在 ZooKeeper 中， -ROOT-本身存储了.META. Table 的 RegionInfo 信息，并且-ROOT不会分裂，只有一个 Region。而 .META.表可以被切分成多个 Region。读取的流程如下图所示：详细步骤： 第 1 步：Client 请求 ZooKeeper 获得-ROOT-所在的 RegionServer 地址 第 2 步：Client 请求-ROOT-所在的 RS 地址，获取.META.表的地址，Client 会将-ROOT-的相关 信息 cache 下来，以便下一次快速访问 第 3 步：Client 请求.META.表的 RegionServer 地址，获取访问数据所在 RegionServer 的地址， Client 会将.META.的相关信息 cache 下来，以便下一次快速访问 第 4 步：Client 请求访问数据所在 RegionServer 的地址，获取对应的数据 从上面的路径我们可以看出，用户需要 3 次请求才能直到用户 Table 真正的位置，这在一定 程序带来了性能的下降。在 0.96 之前使用 3 层设计的主要原因是考虑到元数据可能需要很 大。但是真正集群运行，元数据的大小其实很容易计算出来。在 BigTable 的论文中，每行 METADATA 数据存储大小为 1KB 左右，如果按照一个 Region 为 128M 的计算，3 层设计可以支持的 Region 个数为 2^34 个，采用 2 层设计可以支持 2^17（131072）。那么 2 层设计的情 况下一个集群可以存储 4P 的数据。这仅仅是一个 Region 只有 128M 的情况下。如果是 10G 呢? 因此，通过计算，其实 2 层设计就可以满足集群的需求。因此在 0.96 版本以后就去掉 了-ROOT-表了。 4、读写过程读请求过程1、客户端通过 ZooKeeper 以及-ROOT-表和.META.表找到目标数据所在的 RegionServer(就是 数据所在的 Region 的主机地址) 2、联系 RegionServer 查询目标数据 3、RegionServer 定位到目标数据所在的 Region，发出查询请求 4、Region 先在 Memstore 中查找，命中则返回 5、如果在 Memstore 中找不到，则在 Storefile 中扫描 为了能快速的判断要查询的数据在不在这个 StoreFile 中，应用了 BloomFilter （BloomFilter，布隆过滤器：迅速判断一个元素是不是在一个庞大的集合内，但是他有一个 弱点：它有一定的误判率） （误判率：原本不存在与该集合的元素，布隆过滤器有可能会判断说它存在，但是，如果 布隆过滤器，判断说某一个元素不存在该集合，那么该元素就一定不在该集合内） 写请求过程 Client 先根据 RowKey 找到对应的 Region 所在的 RegionServer Client 向 RegionServer 提交写请求 RegionServer 找到目标 Region Region 检查数据是否与 Schema 一致 如果客户端没有指定版本，则获取当前系统时间作为数据版本 将更新写入 WAL Log 将更新写入 Memstore 判断 Memstore 的是否需要 flush 为 StoreFile 文件。 Hbase 在做数据插入操作时，首先要找到 RowKey 所对应的的 Region，怎么找到的？其实这 个简单，因为.META.表存储了每张表每个 Region 的起始 RowKey 了。 建议：在做海量数据的插入操作，避免出现递增 rowkey 的 put 操作 如果 put 操作的所有 RowKey 都是递增的，那么试想，当插入一部分数据的时候刚好进行分 裂，那么之后的所有数据都开始往分裂后的第二个 Region 插入，就造成了数据热点现象。 细节描述： HBase 使用 MemStore 和 StoreFile 存储对表的更新。 数据在更新时首先写入 HLog(WAL Log)，再写入内存(MemStore)中，MemStore 中的数据是排 序的，当 MemStore 累计到一定阈值时，就会创建一个新的 MemStore，并且将老的 MemStore 添加到 flush 队列，由单独的线程 flush 到磁盘上，成为一个 StoreFile。于此同时，系统会在 ZooKeeper 中记录一个 redo point，表示这个时刻之前的变更已经持久化了。当系统出现意 Stay hungry Stay foolish – http://blog.csdn.net/zhongqi2513外时，可能导致内存(MemStore)中的数据丢失，此时使用 HLog(WAL Log)来恢复 checkpoint 之后的数据。 StoreFile 是只读的，一旦创建后就不可以再修改。因此 HBase 的更新/修改其实是不断追加 的操作。当一个 Store 中的 StoreFile 达到一定的阈值后，就会进行一次合并(minor_compact, major_compact)，将对同一个 key 的修改合并到一起，形成一个大的 StoreFile，当 StoreFile 的大小达到一定阈值后，又会对 StoreFile 进行 split，等分为两个 StoreFile。由于对表的更 新是不断追加的，compact 时，需要访问 Store 中全部的 StoreFile 和 MemStore，将他们按 rowkey 进行合并，由于 StoreFile 和 MemStore 都是经过排序的，并且 StoreFile 带有内存中 索引，合并的过程还是比较快。 major_compact 和 minor_compact 的区别： minor_compact 仅仅合并小文件（HFile） major_compact 合并一个 region 内的所有文件 Client 写入 -&gt; 存入 MemStore，一直到 MemStore 满 -&gt; Flush 成一个 StoreFile，直至增长到 一定阈值 -&gt; 触发 Compact 合并操作 -&gt; 多个 StoreFile 合并成一个 StoreFile，同时进行版本 合并和数据删除 -&gt; 当StoreFiles Compact后，逐步形成越来越大的StoreFile -&gt; 单个StoreFile 大小超过一定阈值后，触发 Split 操作，把当前 Region Split 成 2 个 Region，Region 会下线， 新 Split 出的 2 个孩子 Region 会被 HMaster 分配到相应的 HRegionServer 上，使得原先 1 个 Region 的压力得以分流到 2 个 Region 上由此过程可知，HBase 只是增加数据，有所得更新 和删除操作，都是在 Compact 阶段做的，所以，用户写操作只需要进入到内存即可立即返 回，从而保证 I/O 高性能。 写入数据的过程补充： 工作机制：每个 HRegionServer 中都会有一个 HLog 对象，HLog 是一个实现 Write Ahead Log 的类，每次用户操作写入 Memstore 的同时，也会写一份数据到 HLog 文件，HLog 文件定期 会滚动出新，并删除旧的文件(已持久化到 StoreFile 中的数据)。当 HRegionServer 意外终止 后，HMaster 会通过 ZooKeeper 感知，HMaster 首先处理遗留的 HLog 文件，将不同 Region 的log数据拆分，分别放到相应Region目录下，然后再将失效的Region（带有刚刚拆分的log） 重新分配，领取到这些 Region 的 HRegionServer 在 load Region 的过程中，会发现有历史 HLog 需要处理，因此会 Replay HLog 中的数据到 MemStore 中，然后 flush 到 StoreFiles，完成数据 恢复。 5、RegionServer工作机制 Region 分配 任何时刻，一个 Region 只能分配给一个 RegionServer。master 记录了当前有哪些可用的 RegionServer。以及当前哪些 Region 分配给了哪些 RegionServer，哪些 Region 还没有分配。 当需要分配的新的 Region，并且有一个 RegionServer 上有可用空间时，Master 就给这个 RegionServer 发送一个装载请求，把 Region 分配给这个 RegionServer。RegionServer 得到请 求后，就开始对此 Region 提供服务。 RegionServer 上线Master 使用 zookeeper 来跟踪 RegionServer 状态。当某个 RegionServer 启动时，会首先在 ZooKeeper 上的 server 目录下建立代表自己的 znode。由于 Master 订阅了 server 目录上的变 更消息，当 server 目录下的文件出现新增或删除操作时，Master 可以得到来自 ZooKeeper 的实时通知。因此一旦 RegionServer 上线，Master 能马上得到消息。 RegionServer 下线 当 RegionServer 下线时，它和 zookeeper 的会话断开，ZooKeeper 而自动释放代表这台 server 的文件上的独占锁。Master 就可以确定： 1、RegionServer 和 ZooKeeper 之间的网络断开了。 2、RegionServer 挂了。 无论哪种情况， RegionServer都无法继续为它的Region提供服务了，此时Master会删除server 目录下代表这台 RegionServer 的 znode 数据，并将这台 RegionServer 的 Region 分配给其它还 活着的同志。 6、Master工作机制Master 上线 Master 启动进行以下步骤: 从ZooKeeper上获取唯一一个代表Active Master的锁，用来阻止其它Master成为Master。 扫描 ZooKeeper 上的 server 父节点，获得当前可用的 RegionServer 列表。 和每个 RegionServer 通信，获得当前已分配的 Region 和 RegionServer 的对应关系。 扫描.META. Region 的集合，计算得到当前还未分配的 Region，将他们放入待分配 Region 列表。 ### Master 下线 由于 Master 只维护表和 Region 的元数据，而不参与表数据 IO 的过程，Master 下线仅 导致所有元数据的修改被冻结(无法创建删除表，无法修改表的 schema，无法进行 Region 的负载均衡，无法处理 Region 上下线，无法进行 Region 的合并，唯一例外的是 Region 的 split 可以正常进行，因为只有 RegionServer 参与)，表的数据读写还可以正常进行。因此 Master 下线短时间内对整个 hbase 集群没有影响。 从上线过程可以看到，Master 保存的信息全是可以冗余信息（都可以从系统其它地方 收集到或者计算出来） 因此，一般 HBase 集群中总是有一个 Master 在提供服务，还有一个以上的 Master 在等 待时机抢占它的位置。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount求共同好友代码实现]]></title>
    <url>%2Fwordcount%E6%B1%82%E5%85%B1%E5%90%8C%E5%A5%BD%E5%8F%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html</url>
    <content type="text"><![CDATA[12345678910package com.Practice.SameFriend;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;import org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.Arrays;/** * 求共同好友合并版 * 主要思路： 第一步：求出每一个好友所对应的所有用户 * 第二步：将第一步中所有用户进行排序，两两组合，最后求出两用户间的共同好友 */public class SameFriendMerge1 &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); //第一个job信息 Job job = Job.getInstance(conf); job.setJar(&quot;wordcountJar/wordcount.jar&quot;); job.setMapperClass(SFMerge1Mapper1.class); job.setReducerClass(SFMerge1Reducer1.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); Path inputPath = new Path(&quot;input/sameFriend&quot;); Path outputPath = new Path(&quot;output/sameFriend&quot;); if(fs.isDirectory(outputPath))&#123; fs.delete(outputPath,true); &#125; FileInputFormat.setInputPaths(job,inputPath); FileOutputFormat.setOutputPath(job,outputPath); //第二个job信息 Job job1 = Job.getInstance(conf); job1.setJar(&quot;wordcountJar/wordcount.jar&quot;); job1.setMapperClass(SFMerge1Mapper2.class); job1.setReducerClass(SFMerge1Reducer2.class); job1.setOutputKeyClass(Text.class); job1.setOutputValueClass(Text.class); Path inputPath1 = new Path(&quot;output/sameFriend&quot;); Path outputPath1 = new Path(&quot;output/sameFriend1&quot;); if(fs.isDirectory(outputPath1))&#123; fs.delete(outputPath1,true); &#125; FileInputFormat.setInputPaths(job1,inputPath1); FileOutputFormat.setOutputPath(job1,outputPath1); ControlledJob ctlJob1 = new ControlledJob(job.getConfiguration()); ControlledJob ctlJob2 = new ControlledJob(job.getConfiguration()); ctlJob1.setJob(job); ctlJob2.setJob(job1); ctlJob2.addDependingJob(ctlJob1); JobControl jobControl = new JobControl(&quot;SameFriends&quot;); jobControl.addJob(ctlJob1); jobControl.addJob(ctlJob2); Thread jobThread = new Thread(jobControl); jobThread.start(); // 每隔一段时间来判断一下该jc线程的任务是否执行完成 while (!jobControl.allFinished())&#123; Thread.sleep(500); &#125; jobControl.stop(); &#125; public static class SFMerge1Mapper1 extends Mapper&lt;LongWritable,Text,Text,Text&gt;&#123; private Text outKey = new Text(); private Text outValue = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] split = value.toString().split(&quot;:&quot;); outValue.set(split[0]); String[] friends = split[1].split(&quot;,&quot;); for (String str : friends) &#123; outKey.set(str); context.write(outKey,outValue); &#125; &#125; &#125; /** * 第一次reducer输出结果： A F,I,O,K,G,D,C,H,B B E,J,F,A C B,E,K,A,H,G,F D H,C,G,F,E,A,K,L E A,B,L,G,M,F,D,H F C,M,L,A,D,G G M H O I O,C J O K O,B L D,E M E,F O A,H,I,J,F */ public static class SFMerge1Reducer1 extends Reducer&lt;Text,Text,Text,Text&gt;&#123; private Text outValue = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); for (Text str : values) &#123; if( sb.length()!= 0 )&#123; sb.append(&quot;,&quot;); &#125; sb.append(str); &#125; outValue.set(sb.toString()); context.write(key,outValue); &#125; &#125; public static class SFMerge1Mapper2 extends Mapper&lt;LongWritable,Text,Text,Text&gt;&#123; private Text outValue = new Text(); private Text outKey = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] splits = value.toString().split(&quot;\t&quot;); String[] strings = splits[1].split(&quot;,&quot;); outValue.set(splits[0]); Arrays.sort(strings); for (int i = 0; i &lt; strings.length - 1; i++) &#123; for (int j = i+1; j &lt; strings.length; j++) &#123; outKey.set(strings[i]+&quot;-&quot;+strings[j]); context.write(outKey,outValue); &#125; &#125; &#125; &#125; /** * 第二次reducer输出结果： A-B E,C A-C D,F A-D E,F A-E B,C,D A-F C,E,O,D,B A-G E,F,C,D A-H C,D,E,O A-I O A-J O,B A-K C,D A-L F,D,E A-M F,E B-C A B-D A,E B-E C B-F C,A,E B-G E,C,A B-H E,C,A B-I A B-K A,C B-L E B-M E B-O A,K C-D A,F C-E D C-F A,D C-G A,D,F C-H D,A C-I A C-K A,D C-L D,F C-M F C-O I,A D-E L D-F A,E D-G E,A,F D-H A,E D-I A D-K A D-L E,F D-M F,E D-O A E-F D,M,C,B E-G C,D E-H C,D E-J B E-K C,D E-L D F-G D,C,A,E F-H A,D,O,E,C F-I O,A F-J B,O F-K D,C,A F-L E,D F-M E F-O A G-H D,C,E,A G-I A G-K D,A,C G-L D,F,E G-M E,F G-O A H-I O,A H-J O H-K A,C,D H-L D,E H-M E H-O A I-J O I-K A I-O A K-L D K-O A L-M E,F */ public static class SFMerge1Reducer2 extends Reducer&lt;Text,Text,Text,Text&gt;&#123; private Text outValue = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); for (Text str : values) &#123; if(sb.length()!= 0)&#123; sb.append(&quot;,&quot;); &#125; sb.append(str); outValue.set(sb.toString()); &#125; context.write(key,outValue); &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount学生成绩普通版案例]]></title>
    <url>%2Fwordcount%E5%AD%A6%E7%94%9F%E6%88%90%E7%BB%A9%E6%99%AE%E9%80%9A%E7%89%88%E6%A1%88%E4%BE%8B.html</url>
    <content type="text"><![CDATA[12345678910111213package com.Practice.StudentScores;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193import java.io.IOException;import java.util.ArrayList;import java.util.Collections;import java.util.List;/** * 题目：学生成绩普通版 * * computer,huangxiaoming,85 computer,xuzheng,54 computer,huangbo,86 computer,liutao,85 computer,huanglei,99 computer,liujialing,85 computer,liuyifei,75 computer,huangdatou,48 computer,huangjiaju,88 computer,huangzitao,85 english,zhaobenshan,57 english,liuyifei,85 english,liuyifei,76 english,huangdatou,48 english,zhouqi,85 english,huangbo,85 english,huangxiaoming,96 english,huanglei,85 english,liujialing,75 algorithm,liuyifei,75 algorithm,huanglei,76 algorithm,huangjiaju,85 algorithm,liutao,85 algorithm,huangdou,42 algorithm,huangzitao,81 math,wangbaoqiang,85 math,huanglei,76 math,huangjiaju,85 math,liutao,48 math,xuzheng,54 math,huangxiaoming,85 math,liujialing,85 * * 1、每一个course的最高分，最低分，平均分 返回结果格式： course max=95 min=22 avg=55 例子： computer max=99 min=48 avg=75 解题思路：在map以course作为key值，其余部分作为value，在reduce中设置变量max，min，avg，通过累计求出，并设置格式 2、求该成绩表当中出现了相同分数的分数，还有次数，以及该分数的人 返回结果的格式： 科目 分数 次数 该分数的人 例子： computer 85 3 huangzitao,liujialing,huangxiaoming 解题思路：求某科目中出现系统分数的人数以及分数，map以科目和分数作为key值，进行分组，在reduce中进行计数，当计数结果大于1时，输出分数，人数和人名 */public class StudentScores1 &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); Job job = Job.getInstance(conf); job.setJar(&quot;wordcountJar/wordcount.jar&quot;);// job.setMapperClass(StudentScoresMapper.class);// job.setReducerClass(StudentScoresReducer.class); job.setMapperClass(StudentScoresMapper2.class); job.setReducerClass(StudentScoresReducer2.class);// job.setOutputKeyClass(Text.class);// job.setOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); Path inputPath = new Path(&quot;input/studentScores&quot;); Path outputPath = new Path(&quot;output/studentScores&quot;); if(fs.isDirectory(outputPath))&#123; fs.delete(outputPath,true); &#125; FileInputFormat.setInputPaths(job,inputPath); FileOutputFormat.setOutputPath(job,outputPath); Boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion ? 0 : 1); &#125; /** * 第一题 */ public static class StudentScoresMapper extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt; &#123; private IntWritable outValue = new IntWritable(); private Text outKey = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] splits = value.toString().split(&quot;,&quot;); outKey.set(splits[0]); outValue.set(Integer.parseInt(splits[2])); context.write(outKey,outValue); &#125; &#125; /** * 第一题 */ public static class StudentScoresReducer extends Reducer&lt;Text,IntWritable,Text,Text&gt; &#123; private Text outValue = new Text(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int min = 1000 ; int max = 0 ; int avg ; int sum = 0 ; int count = 0 ; //方法一：最大最小通过逐一比较得到// for (IntWritable val:// values) &#123;// int score = val.get();// if(max &lt; score)&#123;// max = score;// &#125;// if(min &gt; score)&#123;// min = score ;// &#125;// sum += score ;// count++ ;// &#125;// avg = sum /count ;// String outStr = &quot;max=&quot;+max+&quot; min=&quot;+min+&quot; avg=&quot;+avg;// outValue.set(outStr); //方法二：最大最小值通过集合数组得到 List&lt;Integer&gt; scores = new ArrayList&lt;&gt;(); for (IntWritable val : values) &#123; scores.add(val.get()); sum += val.get(); count++; &#125; Collections.sort(scores); min = scores.get(0); max = scores.get(scores.size()-1); avg = sum / count ; String outStr = &quot;max=&quot;+max+&quot; min=&quot;+min+&quot; avg=&quot;+avg; outValue.set(outStr); context.write(key,outValue); &#125; &#125; /** * 第二题 */ public static class StudentScoresMapper2 extends Mapper&lt;LongWritable,Text,Text,Text&gt;&#123; private Text outKey = new Text(); private Text outValue = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] splits = value.toString().split(&quot;,&quot;); outKey.set(splits[0]+&quot;\t&quot;+splits[2]); outValue.set(splits[1]); context.write(outKey,outValue); &#125; &#125; /** * 第二题 */ public static class StudentScoresReducer2 extends Reducer&lt;Text,Text,Text,Text&gt;&#123; private Text outValue = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); int count = 0 ; for (Text text : values) &#123; if(sb.length()!=0)&#123; sb.append(&quot;,&quot;); &#125; sb.append(text); count++; &#125; //如果count大于等于2，说明有分数重合的 if(count &gt;= 2 )&#123; outValue.set(count+&quot; &quot;+sb.toString()); context.write(key,outValue); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount求学生平均成绩案例]]></title>
    <url>%2Fwordcount%E6%B1%82%E5%AD%A6%E7%94%9F%E5%B9%B3%E5%9D%87%E6%88%90%E7%BB%A9%E6%A1%88%E4%BE%8B.html</url>
    <content type="text"><![CDATA[1234567891011121314package com.Practice.AverageScores;import com.Practice.SameFriend.SameFriend;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import java.io.IOException;import java.util.StringTokenizer;/** * 求学生平均成绩 * 计算学生考试平均成绩 源数据： 张三 98 李四 96 王五 95 张三 90 李四 92 王五 99 张三 80 李四 90 王五 94 张三 82 李四 92 */public class AverageScores &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); Job job = Job.getInstance(conf); job.setJar(&quot;wordcountJar/wordcount.jar&quot;); job.setMapperClass(AverageScoresMapper.class); job.setReducerClass(AverageScoresReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); Path inputPath = new Path(&quot;input/AverageScore&quot;); Path outputPath = new Path(&quot;output/AverageScore&quot;); if(fs.isDirectory(outputPath))&#123; fs.delete(outputPath,true); &#125; FileInputFormat.setInputPaths(job,inputPath); FileOutputFormat.setOutputPath(job,outputPath); boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion ? 0 : 1 ); &#125; public static class AverageScoresMapper extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt; &#123; private Text outKey = new Text(); private IntWritable outValue = new IntWritable(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] splits = value.toString().split(&quot;\t&quot;); outKey.set(splits[0]); outValue.set(Integer.parseInt(splits[1])); context.write(outKey,outValue); &#125; &#125; public static class AverageScoresReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123; private IntWritable outValue = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int avg = 0 ; int sum = 0 ; int count = 0 ; for (IntWritable val : values) &#123; int score = val.get(); sum += score ; count ++ ; &#125; avg = sum /count ; outValue.set(avg); context.write(key,outValue); &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount数据去重案例]]></title>
    <url>%2Fwordcount%E6%95%B0%E6%8D%AE%E5%8E%BB%E9%87%8D%E6%A1%88%E4%BE%8B.html</url>
    <content type="text"><![CDATA[12345678910111213package com.Practice.RemoveDupData;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import java.io.IOException;/** * 题目：数据去重 * 解题思路：将每行数据作为key值，value值为空 * 2012-3-1 a 2012-3-2 b 2012-3-3 c 2012-3-4 d 2012-3-5 a 2012-3-6 b 2012-3-7 c 2012-3-3 c 2012-3-1 b 2012-3-2 a 2012-3-3 b 2012-3-4 d 2012-3-5 a 2012-3-6 c 2012-3-7 d 2012-3-3 c */public class RemoveDupData &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); Job job = Job.getInstance(conf); job.setJar(&quot;wordcountJar/wordcount.jar&quot;); job.setMapperClass(RemoveDupDataMapper.class); job.setReducerClass(RemoveDupDataReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); Path inputPath = new Path(&quot;input/RemoveDupData&quot;); Path outputPath = new Path(&quot;output/RemoveDupData&quot;); if(fs.isDirectory(outputPath))&#123; fs.delete(outputPath,true); &#125; FileInputFormat.setInputPaths(job,inputPath); FileOutputFormat.setOutputPath(job,outputPath); boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion ? 0 : 1 ); &#125; public static class RemoveDupDataMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; context.write(value,NullWritable.get()); &#125; &#125; public static class RemoveDupDataReducer extends Reducer&lt;Text,NullWritable,Text,NullWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key,NullWritable.get()); &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount 互为好友对案例]]></title>
    <url>%2Fwordcount%E4%BA%92%E4%B8%BA%E5%A5%BD%E5%8F%8B%E5%AF%B9%E6%A1%88%E4%BE%8B.html</url>
    <content type="text"><![CDATA[1234567891011121314package com.Practice.SameFriend2;import com.Practice.SameFriend.SameFriend;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import java.io.IOException;/** * 求互粉好友对 比如：如果A和B互为好友，那么A-B即为互粉好友对。 * A:B,C,D,F,E,O B:A,C,E,K C:F,A,D,I D:A,E,F,L E:B,C,D,M,L F:A,B,C,D,E,O,M G:A,C,D,E,F H:A,C,D,E,O I:A,O J:B,O K:A,C,D L:D,E,F M:E,F,G O:A,H,I,J,K */public class SameFriend2 &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); Job job = Job.getInstance(conf); job.setJar(&quot;wordcountJar/wordcount.jar&quot;); job.setMapperClass(SameFriend2Mapper.class); job.setReducerClass(SameFriend2Reducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); Path inputPath = new Path(&quot;input/SameFriend2&quot;); Path outputPath = new Path(&quot;output/SameFriend2&quot;); if(fs.isDirectory(outputPath))&#123; fs.delete(outputPath,true); &#125; FileInputFormat.setInputPaths(job,inputPath); FileOutputFormat.setOutputPath(job,outputPath); boolean waitForCompletion = job.waitForCompletion(true); System.exit(waitForCompletion ? 0 : 1 ); &#125; public static class SameFriend2Mapper extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt; &#123; private Text outKey = new Text(); private IntWritable outValue = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] splits = value.toString().split(&quot;:&quot;); String[] strings = splits[1].split(&quot;,&quot;); for (String str: strings) &#123; //关键代码：如果A和B互为好友，则必定存在A-B和B-A，此处的作用就是将A-B和B-A 都编程为A-B，或者都编程为B-A if(splits[0].compareTo(str)&gt;0)&#123; outKey.set(str+&quot;-&quot;+splits[0]); &#125;else&#123; outKey.set(splits[0]+&quot;-&quot;+str); &#125; context.write(outKey,outValue); &#125; &#125; &#125; /** * 如果互为好友对，count必定为2 */ public static class SameFriend2Reducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123; private IntWritable outValue = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0 ; for (IntWritable val : values) &#123; count += val.get(); &#125; //当count为2时，即可求出好友对 if(count == 2)&#123; outValue.set(count); context.write(key,outValue); &#125; &#125; &#125;&#125; ```]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA windows本地运行wordcount程序]]></title>
    <url>%2FIDEAwindows%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8Cwordcount%E7%A8%8B%E5%BA%8F.html</url>
    <content type="text"><![CDATA[第一步创建maven项目第二步创建WordCountDemo类12345678910111213141516package com.wordcountModel;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.StringTokenizer; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128/** * 本地运行wordcount程序 */public class WordCountDemo &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; //指定hdfs的相关参数 Configuration conf = new Configuration() ; FileSystem fs = FileSystem.get(conf); //通过Configuration对象获取job对象，该job对象会组织所有的该mapreduce的所有各种组件 Job job = Job.getInstance(conf); //指定jar包所在路径，本地模式需要这样指定，如果不是本地，则使用setJarByClass指定所在class文件即可 //job.setJarByClass(&quot;wordcountJar/wordcount.jar&quot;) job.setJar(&quot;wordcountJar/wordcount.jar&quot;); //指定mapper类和reducer类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); //Mapper的输入key-value类型，由mapreduce框架决定 //指定maptask的输出类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 假如 mapTask的输出key-value类型，跟reduceTask的输出key-value类型一致，那么，以上两句代码可以不用设置 // reduceTask的输入key-value类型 就是 mapTask的输出key-value类型。所以不需要指定 // 指定reducetask的输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 为job指定输入数据的组件和输出数据的组件，以下两个参数是默认的，所以不指定也是OK的 // job.setInputFormatClass(TextInputFormat.class); // job.setOutputFormatClass(TextOutputFormat.class); // 为该mapreduce程序制定默认的数据分区组件。默认是 HashPartitioner.class // job.setPartitionerClass(HashPartitioner.class); // 指定该mapreduce程序数据的输入和输出路径 Path inputPath = new Path(&quot;input/wordcount/&quot;); Path outputPath = new Path(&quot;output/wordcount/&quot;); // 设置该MapReduce程序的ReduceTask的个数,默认为1 // job.setNumReduceTasks(3); // 该段代码是用来判断输出路径存在不存在，存在就删除，虽然方便操作，但请谨慎 if(fs.isDirectory(outputPath))&#123; fs.delete(outputPath,true); &#125; //设置wordcount程序的输入路径 FileInputFormat.setInputPaths(job,inputPath); //设置wordcount程序的输出路径 FileOutputFormat.setOutputPath(job,outputPath); // job.submit(); // 最后提交任务(verbose布尔值 决定要不要将运行进度信息输出给用户) boolean waitForCompletion = job.waitForCompletion(true); System.exit( waitForCompletion ? 0 : 1); &#125; /** * Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; * * KEYIN 是指框架读取到的数据的key的类型，在默认的InputFormat下，读到的key是一行文本的起始偏移量，所以key的类型是Long * VALUEIN 是指框架读取到的数据的value的类型,在默认的InputFormat下，读到的value是一行文本的内容，所以value的类型是String * KEYOUT 是指用户自定义逻辑方法返回的数据中key的类型，由用户业务逻辑决定，在此wordcount程序中，我们输出的key是单词，所以是String * VALUEOUT 是指用户自定义逻辑方法返回的数据中value的类型，由用户业务逻辑决定,在此wordcount程序中，我们输出的value是单词的数量，所以是Integer * * 但是，String ，Long等jdk中自带的数据类型，在序列化时，效率比较低，hadoop为了提高序列化效率，自定义了一套序列化框架 * 所以，在hadoop的程序中，如果该数据需要进行序列化（写磁盘，或者网络传输），就一定要用实现了hadoop序列化框架的数据类型 * * Long ----&gt; LongWritable * String ----&gt; Text * Integer ----&gt; IntWritable * Null ----&gt; NullWritable */ public static class WordcountMapper extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt; &#123; /** * LongWritable key : 该key就是value该行文本的在文件当中的起始偏移量 * Text value ： 就是MapReduce框架默认的数据读取组件TextInputFormat读取文件当中的一行文本 */ private Text word = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 切分单词 StringTokenizer tokenizer = new StringTokenizer(value.toString()); while (tokenizer.hasMoreTokens())&#123; word.set(tokenizer.nextToken()); // 每个单词计数一次，也就是把单词组织成&lt;hello,1&gt;这样的key-value对往外写出 context.write(word,new IntWritable(1)); &#125; &#125; &#125; /** * 首先，和前面一样，Reducer类也有输入和输出，输入就是Map阶段的处理结果，输出就是Reduce最后的输出 * reducetask在调我们写的reduce方法,reducetask应该收到了前一阶段（map阶段）中所有maptask输出的数据中的一部分 * （数据的key.hashcode%reducetask数==本reductask号），所以reducetaks的输入类型必须和maptask的输出类型一样 * * reducetask将这些收到kv数据拿来处理时，是这样调用我们的reduce方法的： 先将自己收到的所有的kv对按照k分组（根据k是否相同） * 将某一组kv中的第一个kv中的k传给reduce方法的key变量，把这一组kv中所有的v用一个迭代器传给reduce方法的变量values */ public static class WordcountReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123; /** * Text key : mapTask输出的key值 * Iterable&lt;IntWritable&gt; values ： key对应的value的集合（该key只是相同的一个key） * * reduce方法接收key值相同的一组key-value进行汇总计算 */ private IntWritable result = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; //结果汇总 int sum = 0 ; for (IntWritable val: values ) &#123; sum += val.get(); &#125; //汇总的结果往外输出 result.set(sum); context.write(key,result); &#125; &#125;&#125; 注意事项：wordcount程序加载配置文件的顺序为: 1、conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://hadoop01:9000&quot;); 2、通过加载配置文件： conf.addResource(&quot;hdfs_config/core-site.xml&quot;); conf.addResource(&quot;hdfs_config/hdfs-site.xml&quot;); 3、加载本地hadoop的jar包中的配置文件 所以，如果要进行本地运行wordcount程序，则使用第二种，即不需要手动配置，程序会自动加载。如果配置文件夹中已经存在已经配置好的文件，程序会优先加载配置文件夹中的配置文件。 第三步 导入jar包 第四步 运行程序注：部分注意事项写在程序注释中。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell实现九九乘法表]]></title>
    <url>%2Fshell%E5%AE%9E%E7%8E%B0%E4%B9%9D%E4%B9%9D%E4%B9%98%E6%B3%95%E8%A1%A8.html</url>
    <content type="text"><![CDATA[12345678for (( i=1;i&lt;=9;i++ ))do for (( j=1;j&lt;=i;j++ )) do echo -n "$j*$i=$[$j*$i] " done echo -e '\n'done]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置163云源repo]]></title>
    <url>%2F%E9%85%8D%E7%BD%AE163%E4%BA%91%E6%BA%90repo.html</url>
    <content type="text"><![CDATA[进入yum配置文件目录cd /etc/yum.repos.d/ 备份配置文件 mv CentOS-Base.repo CentOS-Base.repo.bak 下载163的配置 wget http://mirrors.163.com/.help/CentOS6- Base-163.repo，下载下来的文件名为 CentOS6- Base-163.repo 改名 mv CentOS6-Base-163.repo CentOS-Base.repo 更新数据库 yum update]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>rpm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring侵入式和非侵入式的区别]]></title>
    <url>%2FSpring%E4%BE%B5%E5%85%A5%E5%BC%8F%E5%92%8C%E9%9D%9E%E4%BE%B5%E5%85%A5%E5%BC%8F%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content type="text"><![CDATA[简单解释：侵入式：使用者编写代码时，需要继承或者实现框架的类或接口，需要依赖框架。非侵入式：使用者编写代码时，无需继承或者实现框架的类或接口，察觉不到框架的存在。 Spring框架是一种非侵入式的轻量级框架###1.非侵入式的技术体现允许在应用系统中自由选择和组装Spring框架的各个功能模块，并且不强制要求应用系统的类必须从Spring框架的系统API的某个类来继承或者实现某个接口。 2.如何实现非侵入式的设计目标的 1）应用反射机制，通过动态调用的方式来提供各方面的功能，建立核心组间BeanFactory 2）配合使用Spring框架中的BeanWrapper和BeanFactory组件类最终达到对象的实例创建和属性注入 3）优点：允许所开发出来的应用系统能够在不用的环境中自由移植，不需要修改应用系统中的核心功能实现的代码]]></content>
      <categories>
        <category>框架</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见SQL语句]]></title>
    <url>%2F%E5%B8%B8%E8%A7%81SQL%E8%AF%AD%E5%8F%A5.html</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147create database employeeselect * from emp;delete from emp where empno=9999;select ename,sal from emp ;select * from emp where sal &gt; 2000;select * from emp where deptno = 20 and sal &gt; 2000 ;select * from emp where deptno = 20 or sal &gt; 2000 ;select * from emp where sal between 1000 and 3000 ;select * from emp where empno = 7788 or empno = 7369 or empno=7521;&lt;!---more---&gt;-- inselect * from emp where empno in(7788,7369,7521);-- DISTINCT 去重select job from emp ;select DISTINCT job from emp ;-- 别名（字段，表）select empno 员工编号,ename 员工姓名 from emp ;select ename,sal,sal*1.05 新工资 from emp;select ename,sal from emp e ;select e.ename,e.sal from emp e ;-- null 的判断select * from emp where comm is not null ;--模糊查询--查询所有S打头的员工信息（模糊查询） % 代表0到多个字符select * from emp where ename like 'S%';--查询所有N结尾的select * from emp where ename like '%N';--查询所有包含S的select * from emp where ename like '%S%';--查询所有第二个字符为L的员工信息select * from emp where ename like '__L%';--排序 (默认升序 order by字段 [asc] | desc)--默认 升序select * from emp order by sal ;--降序select * from emp order by sal desc ;--按照工资的降序排序，如果工资一样的，则按照empno的升序排序(用逗号)select * from emp order by sal desc , empno ASC;--限制结果查询（limit m,n) m代表起始索引，n代表记录的数目,limit 仅适用于MySQLselect * from emp limit 5,5;--查询20号部门工资最高的员工的信息select * from emp where deptno = 20 order by sal desc limit 0,1 ;--计算3的15次方--数学函数select ABS(10); --绝对值select CEIL(-12.3); --向上取整select FLOOR(12.5); --向下取整select ROUND(12.6); --四舍五入select ROUNT(12.49，1); select POW(3,3); --幂运算select RAND(); --随机数[0,1)--字符串函数desc emp ;select LENGTH(ename) from emp ; --获取字符串长度select length('this is a apple');select LOWER(ename) from emp ; --转换为小写select UPPER('this is a page'); -- 转换大小写select substr('aabbcc',1,2); -- 从1开始select LPAD('smith',10,'*'); -- 开始字符串，总长度，padstr填充的字符select RPAD('smith',10,'*'); -- 右填充select TRIM(' smi th'); --去空格-- 日期select NOW();select sysdate();select CURRENT_DATE(); --当前日期select current_time(); --当前时间select YEAR('1998-09-09');select MONTH('1998-09-09');select DAY('1998-09-09');select DATE_ADD('1998-09-09',INTERVAL 2 YEAR);select * from student ;alter table student add birthday date AFTER age;-- 聚合函数--count /sum/avg/max/min-- 员工数（统计记录数）select count(*) from emp ;select count(1) from emp ;-- 统计非空字段数目select count(comm) from emp;-- SUMselect sum(sal) from emp ;-- AVG select avg(sal) from emp ;-- MINselect min(sal) from emp ;-- MAX select max(sal) from emp ;-- 分组函数 GROUP BY deptno-- 每个部门的平均工资-- group by 根据条件字段的值返回相应的记录数；但是在select字句中，只能出现聚合函数或者分组的条件字段。select deptno, avg(sal) from emp GROUP BY deptno ;-- 各个职位员工数？ jobselect job, count(*) from emp group by job;-- 平均工资大于2000的部门的部门编号和平均工资？ where 不能放group by之后，having可以，group by 和having 经常配套使用 -- 1.求出每个部门的平均工资 -- 2.平均工资&gt;2000select deptno,avg(sal) from emp GROUP BY deptno having avg(sal) &gt; 2000;-- where 和 having 的区别-- 查询工资大于1500的每个部门的平均工资select deptno, avg(sal) from emp where sal &gt; 1500 GROUP BY deptno ;-- 查询平均工资大于1500的部门编号和平均工资select deptno,avg(sal) from emp GROUP BY deptno having avg(sal) &gt; 1500 ;-- 加密函数select MD5('root');select SHA('root');select PASSWORD('root');-- 外键约束： foreign keycreate table classroom( cid int PRIMARY key, cname varchar(20));alter table student add CONSTRAINT FK_CID FOREIGN KEY(cid) REFERENCES classroom(cid);select * from emp ;-- 高级查询-- 多表查询-- 查询的结果集分布于多张表-- 查询员工编号为7788的员工姓名和部门名称。-- 内连接 ，注意：1 内连接的结果集与连接顺序无关 2 在多张表中都出现的数据才会出现在内连接的结果集中select e.ename,d.dname from emp e ,dept d where e.empno = 7788 and e.deptno = d.deptno;-- inner join ...on....select ename,dname from emp inner join dept on emp.deptno = dept.deptno ;-- inner join ...using... (局限,当deptno不同时)select ename,dname from emp inner join dept using(deptno)]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合常见面试题]]></title>
    <url>%2FJava%E9%9B%86%E5%90%88%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98.html</url>
    <content type="text"><![CDATA[Java集合框架是最常被问到的Java面试问题，要理解Java技术强大特性就有必要掌握集合框架。这里有一些实用问题，常在核心Java面试中问到。 什么是Java集合API Java集合框架API是用来表示和操作集合的统一框架，它包含接口、实现类、以及帮助程序员完成一些编程的算法。简言之，API在上层完成以下几件事： ● 编程更加省力，提高城程序速度和代码质量 ● 非关联的API提高互操作性 ● 节省学习使用新API成本 ● 节省设计新API的时间 ● 鼓励、促进软件重用 具体来说，有6个集合接口，最基本的是Collection接口，由三个接口Set、List、SortedSet继承，另外两个接口是Map、SortedMap，这两个接口不继承Collection，表示映射而不是真正的集合。 什么是Iterator 一些集合类提供了内容遍历的功能，通过java.util.Iterator接口。这些接口允许遍历对象的集合。依次操作每个元素对象。当使用 Iterators时，在获得Iterator的时候包含一个集合快照。通常在遍历一个Iterator的时候不建议修改集合本省。 Iterator与ListIterator有什么区别？ Iterator：只能正向遍历集合，适用于获取移除元素。ListIerator：继承Iterator，可以双向列表的遍历，同样支持元素的修改。 什么是HaspMap和Map？ Map是接口，Java 集合框架中一部分，用于存储键值对，HashMap是用哈希算法实现Map的类。 HashMap与HashTable有什么区别？对比Hashtable VS HashMap 两者都是用key-value方式获取数据。Hashtable是原始集合类之一（也称作遗留类）。HashMap作为新集合框架的一部分在Java2的1.2版本中加入。它们之间有一下区别： ● HashMap和Hashtable大致是等同的，除了非同步和空值（HashMap允许null值作为key和value，而Hashtable不可以）。 ● HashMap没法保证映射的顺序一直不变，但是作为HashMap的子类LinkedHashMap，如果想要预知的顺序迭代（默认按照插入顺序），你可以很轻易的置换为HashMap，如果使用Hashtable就没那么容易了。 ● HashMap不是同步的，而Hashtable是同步的。 ● 迭代HashMap采用快速失败机制，而Hashtable不是，所以这是设计的考虑点。 在Hashtable上下文中同步是什么意思？ 同步意味着在一个时间点只能有一个线程可以修改哈希表，任何线程在执行hashtable的更新操作前需要获取对象锁，其他线程等待锁的释放。 什么叫做快速失败特性 从高级别层次来说快速失败是一个系统或软件对于其故障做出的响应。一个快速失败系统设计用来即时报告可能会导致失败的任何故障情况，它通常用来停止正常的操作而不是尝试继续做可能有缺陷的工作。当有问题发生时，快速失败系统即时可见地发错错误告警。在Java中，快速失败与iterators有关。如果一个iterator在集合对象上创建了，其它线程欲“结构化”的修改该集合对象，并发修改异常 （ConcurrentModificationException） 抛出。 怎样使Hashmap同步？ HashMap可以通过Map m = Collections.synchronizedMap（hashMap）来达到同步的效果。 什么时候使用Hashtable，什么时候使用HashMap 基本的不同点是Hashtable同步HashMap不是的，所以无论什么时候有多个线程访问相同实例的可能时，就应该使用Hashtable，反之使用HashMap。非线程安全的数据结构能带来更好的性能。 如果在将来有一种可能—你需要按顺序获得键值对的方案时，HashMap是一个很好的选择，因为有HashMap的一个子类 LinkedHashMap。所以如果你想可预测的按顺序迭代（默认按插入的顺序），你可以很方便用LinkedHashMap替换HashMap。反观要是使用的Hashtable就没那么简单了。同时如果有多个线程访问HashMap，Collections.synchronizedMap（）可以代替，总的来说HashMap更灵活。 为什么Vector类认为是废弃的或者是非官方地不推荐使用？或者说为什么我们应该一直使用ArrayList而不是Vector 你应该使用ArrayList而不是Vector是因为默认情况下你是非同步访问的，Vector同步了每个方法，你几乎从不要那样做，通常有想要同步的是整个操作序列。同步单个的操作也不安全（如果你迭代一个Vector，你还是要加锁，以避免其它线程在同一时刻改变集合）.而且效率更慢。当然同样有锁的开销即使你不需要，这是个很糟糕的方法在默认情况下同步访问。你可以一直使用Collections.sychronizedList来装饰一个集合。 事实上Vector结合了“可变数组”的集合和同步每个操作的实现。这是另外一个设计上的缺陷。Vector还有些遗留的方法在枚举和元素获取的方法，这些方法不同于List接口，如果这些方法在代码中程序员更趋向于想用它。尽管枚举速度更快，但是他们不能检查如果集合在迭代的时候修改了，这样将导致问题。尽管以上诸多原因，oracle也从没宣称过要废弃Vector。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合体系结构以及集合和数组的区别]]></title>
    <url>%2FJava%E9%9B%86%E5%90%88%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E4%BB%A5%E5%8F%8A%E9%9B%86%E5%90%88%E5%92%8C%E6%95%B0%E7%BB%84%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content type="text"><![CDATA[数组和集合的定义一、数组 数组是java语言内置的数据类型，他是一个线性的序列，所有可以快速访问其他的元素，数组和其他语言不同，当你创建了一个数组时，他的容量是不变的，而且在生命周期也是不能改变的，还有JAVA数组会做边界检查，如果发现有越界现象，会报RuntimeException异常错误，当然检查边界会以效率为代价。二、集合 JAVA还提供其他集合，list，map，set，他们处理对象的时候就好像这些对象没有自己的类型一样，而是直接归根于Object，这样只需要创建一个集合，把对象放进去，取出时转换成自己的类型就行了。三、数组和集合的区别 一、数组声明了它容纳的元素的类型，而集合不声明。二、数组是静态的，一个数组实例具有固定的大小，一旦创建了就无法改变容量了。而集合是可以动态扩展容量，可以根据需要动态改变大小，集合提供更多的成员方法，能满足更多的需求。三、数组的存放的类型只能是一种（基本类型/引用类型）,集合存放的类型可以不是一种(不加泛型时添加的类型是Object)。四、数组是java语言中内置的数据类型,是线性排列的,执行效率或者类型检查都是最快的。集合体系结构 Collection├List （有序集合，允许相同元素和null）│├LinkedList （非同步，允许相同元素和null，遍历效率低插入和删除效率高）│├ArrayList （非同步，允许相同元素和null，实现了动态大小的数组，遍历效率高，用的多）│└Vector（同步，允许相同元素和null，效率低）│ └Stack（继承自Vector，实现一个后进先出的堆栈）└Set （无序集合，不允许相同元素，最多有一个null元素） |-HashSet(无序集合，不允许相同元素，最多有一个null元素) Map （没有实现collection接口，key不能重复，value可以重复，一个key映射一个value）├Hashtable （实现Map接口，同步，不允许null作为key和value，用自定义的类当作key的话要复写hashCode和eques方法，）├HashMap （实现Map接口，非同步，允许null作为key和value，用的多）└WeakHashMap（实现Map接口） Collection接口 Collection是最基本的集合接口，一个Collection代表一组Object，即Collection的元素（Elements）。一些Collection允许相同的元素而另一些不行。一些能排序而另一些不行。Java SDK不提供直接继承自Collection的类，Java SDK提供的类都是继承自Collection的“子接口”如List和Set。 所有实现Collection接口的类都必须提供两个标准的构造函数：无参数的构造函数用于创建一个空的Collection，有一个Collection参数的构造函数用于创建一个新的Collection，这个新的Collection与传入的Collection有相同的元素。后一个构造函数允许用户复制一个Collection。 如何遍历Collection中的每一个元素？不论Collection的实际类型如何，它都支持一个iterator()的方法，该方法返回一个迭代子，使用该迭代子即可逐一访问Collection中每一个元素。典型的用法如下： Iterator it = collection.iterator(); // 获得一个迭代子 while(it.hasNext()) { Object obj = it.next(); // 得到下一个元素 } 由Collection接口派生的两个接口是List和Set。一、List接口 List是有序的Collection，使用此接口能够精确的控制每个元素插入的位置。用户能够使用索引（元素在List中的位置，类似于数组下标）来访问List中的元素，这类似于Java的数组。和下面要提到的Set不同，List允许有相同的元素。 除了具有Collection接口必备的iterator()方法外，List还提供一个listIterator()方法，返回一个ListIterator接口，和标准的Iterator接口相比，ListIterator多了一些add()之类的方法，允许添加，删除，设定元素，还能向前或向后遍历。 实现List接口的常用类有LinkedList，ArrayList，Vector和Stack。LinkedList类 LinkedList实现了List接口，允许null元素。LinkenList底层采用了双向链表来存储数据，每个节点都存储着上一个节点和下一个节点的地址以及本节点的数据。此外LinkedList提供额外的get，remove，insert方法在LinkedList的首部或尾部。这些操作使LinkedList可被用作堆栈（stack），队列（queue）或双向队列（deque）。 注意：LinkedList没有同步方法。如果多个线程同时访问一个List，则必须自己实现访问同步。一种解决方法是在创建List时构造一个同步的List： List list = Collections.synchronizedList(new LinkedList(…));ArrayList类 ArrayList实现了可变大小的数组。它允许所有元素，包括null。ArrayList底层采用动态数组的存储方式，便利效率非常高，ArrayList是线程不安全的。size，isEmpty，get，set方法运行时间为常数。但是add方法开销为分摊的常数，添加n个元素需要O(n)的时间。其他的方法运行时间为线性。 每个ArrayList实例都有一个容量（Capacity），即用于存储元素的数组的大小。这个容量可随着不断添加新元素而自动增加，但是增长算法并没有定义。当需要插入大量元素时，在插入前可以调用ensureCapacity方法来增加ArrayList的容量以提高插入效率。ArrayList和LindedList的区别： 1.ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。 2.对于随机访问get和set，ArrayList觉得优于LinkedList，因为LinkedList要移动指针。 3.对于新增和删除操作add和remove，LinedList比较占优势，因为ArrayList要移动数据。Vector类 Vector非常类似ArrayList，但是Vector是同步的。由Vector创建的Iterator，虽然和ArrayList创建的Iterator是同一接口，但是，因为Vector是同步的，当一个Iterator被创建而且正在被使用，另一个线程改变了Vector的状态（例如，添加或删除了一些元素），这时调用Iterator的方法时将抛出ConcurrentModificationException，因此必须捕获该异常。Stack 类 Stack继承自Vector，实现一个后进先出的堆栈。Stack提供5个额外的方法使得Vector得以被当作堆栈使用。基本的push和pop方法，还有peek方法得到栈顶的元素，empty方法测试堆栈是否为空，search方法检测一个元素在堆栈中的位置。Stack刚创建后是空栈。二、Set接口Set是一种无序的并且不包含重复的元素的Collection，即任意的两个元素e1和e2都有e1.equals(e2)=false，Set最多有一个null元素。 很明显，Set的构造函数有一个约束条件，传入的Collection参数不能包含重复的元素。HashSet 类是哈希表实现的,HashSet中的数据是无序的，可以放入null，但只能放入一个null，两者中的值都不能重复，因为HashSet的底层实现是HashMap，但是HashSet只使用了HashMap的key来存取数据所以HashSet存的数据不能重复。 HashSet要求放入的对象必须实现HashCode()方法，放入的对象，是以hashcode码作为标识的，而具有相同内容的 String对象，hashcode是一样，所以放入的内容不能重复。但是同一个类的对象可以放入不同的实例 。 三、Map接口Map没有继承Collection接口，Map提供key到value的映射。一个Map中不能包含相同的key，每个key只能映射一个value。Map接口提供3种集合的视图，Map的内容可以被当作一组key集合，一组value集合，或者一组key-value映射。 Hashtable类Hashtable继承Map接口，实现一个key-value映射的哈希表。任何非空（non-null）的对象都可作为key或者value。 添加数据使用put(key, value)，取出数据使用get(key)，这两个基本操作的时间开销为常数。使用Hashtable的简单示例如下，将1，2，3放到Hashtable中，他们的key分别是”one”，”two”，”three”：Hashtable numbers = new Hashtable();numbers.put(“one”, new Integer(1));numbers.put(“two”, new Integer(2));numbers.put(“three”, new Integer(3)); 要取出一个数，比如2，用相应的key：Integer n = (Integer)numbers.get(“two”);System.out.println(“two = ” + n); 由于作为key的对象将通过计算其散列函数来确定与之对应的value的位置，因此任何作为key的对象都必须实现hashCode和equals方法。hashCode和equals方法继承自根类Object，如果你用自定义的类当作key的话，要相当小心，按照散列函数的定义，如果两个对象相同，即obj1.equals(obj2)=true，则它们的hashCode必须相同，但如果两个对象不同，则它们的hashCode不一定不同，如果两个不同对象的hashCode相同，这种现象称为冲突，冲突会导致操作哈希表的时间开销增大，所以尽量定义好的hashCode()方法，能加快哈希表的操作。 如果相同的对象有不同的hashCode，对哈希表的操作会出现意想不到的结果（期待的get方法返回null），要避免这种问题，只需要牢记一条：要同时复写equals方法和hashCode方法，而不要只写其中一个。 Hashtable是同步的。 HashMap类HashMap和Hashtable类似，不同之处在于HashMap是非同步的，并且允许null，即null value和null key，但是将HashMap视为Collection时（values()方法可返回Collection）。WeakHashMap类WeakHashMap是一种改进的HashMap，它对key实行“弱引用”，如果一个key不再被外部所引用，那么该key可以被GC回收。 总结1.如果涉及到堆栈，队列等操作，应该考虑用List，对于需要快速插入，删除元素，应该使用LinkedList，如果需要快速随机访问元素，应该使用ArrayList。2.如果程序在单线程环境中，或者访问仅仅在一个线程中进行，考虑非同步的类，其效率较高，如果多个线程可能同时操作一个类，应该使用同步的类。3.要特别注意对哈希表的操作，作为key的对象要正确复写equals和hashCode方法。4.尽量返回接口而非实际的类型，如返回List而非ArrayList，这样如果以后需要将ArrayList换成LinkedList时，客户端代码不用改变。这就是针对抽象编程。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据抽象]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E6%8A%BD%E8%B1%A1.html</url>
    <content type="text"><![CDATA[概念数据抽象结构：对现实世界的一种抽象从实际的人、物、事和概念中抽取所关心的共同特性，忽略非本质的细节，把这些特性用各种概念精确地加以描述这些概念组成了某种模型。 三种常用抽象分类（Classification)定义某一类概念作为现实世界中一组对象的类型，这些对象具有某些共同的特性和行为，它抽象了对象值和型之间的“is member of”的语义，在E-R模型中，实体型就是这种抽象。 聚集（Aggregation）定义某一类型的组成成分，它抽象了对象内部类型和成分之间“is part of”的语义，在E-R模型中若干属性的聚集组成了实体型，就是这种抽象。 概括（Generalization）定义类型之间的一种子集联系，它抽象了类型之间的“is subset of”的语义，概括有一个很重要的性质：继承性。子类继承超类上定义的所有抽象。注：原E-R模型不具有概括，本书对E-R模型作了扩充，允许定义超类实体型和子类实体型。用双竖边的矩形框表示子类，用直线加小圆圈表示超类-子类的联系 数据抽象的用途对需求分析阶段收集到的数据进行分类、组织（聚集），形成实体实体的属性，标识实体的码确定实体之间的联系类型(1:1，1:n，m:n) 类背后蕴含的基本思想是数据抽象和封装数据抽象是一种依赖于接口和实现分离的编程和设计技术。类的设计者必须关心类是如何实现的，但是使用该类的程序员不必了解这些细节，仅需抽象地考虑该类型能做什么。封装是一项将低层次的元素组合起来形成新的、高层次实体的技术。函数和类都是封装的形式。被封装的元素隐藏了他们的实现细节，其主要优点在于：避免类内部出现无意的、可能破坏对象状态的用户级错误；使得修改类的实现时只要保持接口不变，就无需改变用户级代码。]]></content>
      <categories>
        <category>干货</category>
      </categories>
      <tags>
        <tag>概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基本数据类型取值范围]]></title>
    <url>%2FJava%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%8F%96%E5%80%BC%E8%8C%83%E5%9B%B4.html</url>
    <content type="text"><![CDATA[在JAVA中一共有八种基本数据类型，他们分别是byte、short、int、long、float、double、char、boolean 整型其中byte、short、int、long都是表示整数的，只不过他们的取值范围不一样byte的取值范围为-128~127，占用1个字节（-2的7次方到2的7次方-1）short的取值范围为-32768~32767，占用2个字节（-2的15次方到2的15次方-1）int的取值范围为（-2147483648~2147483647），占用4个字节（-2的31次方到2的31次方-1）long的取值范围为（-9223372036854774808~9223372036854774807），占用8个字节（-2的63次方到2的63次方-1）可以看到byte和short的取值范围比较小，而long的取值范围太大，占用的空间多，基本上int可以满足我们的日常的计算了，而且int也是使用的最多的整型类型了。在通常情况下，如果JAVA中出现了一个整数数字比如35，那么这个数字就是int型的，如果我们希望它是byte型的，可以在数据后加上大写的 B：35B，表示它是byte型的，同样的35S表示short型，35L表示long型的，表示int我们可以什么都不用加，但是如果要表示long型的，就一定要在数据后面加“L”。 浮点型float和double是表示浮点型的数据类型，他们之间的区别在于他们的精确度不同float 3.402823e+38 ~ 1.401298e-45（e+38表示是乘以10的38次方，同样，e-45表示乘以10的负45次方）占用4个字节double 1.797693e+308~ 4.9000000e-324 占用8个字节double型比float型存储范围更大，精度更高，所以通常的浮点型的数据在不声明的情况下都是double型的，如果要表示一个数据是float型的，可以在数据后面加上“F”。浮点型的数据是不能完全精确的，所以有的时候在计算的时候可能会在小数点最后几位出现浮动，这是正常的。 boolean型（布尔型）这个类型只有两个值，true和false（真和非真）boolean t = true；boolean f = false；char型（文本型）用于存放字符的数据类型，占用2个字节，采用unicode编码，它的前128字节编码与ASCII兼容字符的存储范围在\u0000~\uFFFF，在定义字符型的数据时候要注意加’ ‘，比如 ‘1’表示字符’1’而不是数值1，char c = ‘ 1 ‘;我们试着输出c看看，System.out.println(c);结果就是1，而如果我们这样输出呢System.out.println(c+0);结果却变成了49。如果我们这样定义c看看char c = ‘ \u0031 ‘;输出的结果仍然是1，这是因为字符’1’对应着unicode编码就是\u0031char c1 = ‘h’,c2 = ‘e’,c3=’l’,c4=’l’,c5 = ‘o’;System.out.print(c1);System.out.print(c2);System.out.print(c3);System.out.print(c4);Sytem.out.print(c5);]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础编程题]]></title>
    <url>%2FJava%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B%E9%A2%98.html</url>
    <content type="text"><![CDATA[把一个数组arr[n]进行反转12345678910111213141516171819202122232425//方法一，思路：利用for循环，只循环n/2-1次，在同一个数组里进行值的交换 int[] arr = &#123;1,2,3,4,5,6,7,8,9&#125;; for (int i = 0,j = arr.length-1; i &lt; arr.length; i++,j--) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp ; if(i==3) break; &#125; System.out.println(Arrays.toString(arr)); &lt;!---more---&gt;//方法二，思路：重新new一个数组，将原数组逆向赋值给新数组int[] arrTemp = new int[arr.length]; for (int i = 0; i &lt; arr.length; i++) &#123; arrTemp[i] = arr[arr.length-i-1]; &#125; arr = arrTemp ; System.out.println(Arrays.toString(arr));//方法三，思路：通过Collections.reverse(list); ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); for (int i = 0; i &lt; arr.length; i++) &#123; list.add(arr[i]); &#125; Collections.reverse(list); System.out.println( list);]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动装箱的陷阱]]></title>
    <url>%2F%E8%87%AA%E5%8A%A8%E8%A3%85%E7%AE%B1%E7%9A%84%E9%99%B7%E9%98%B1.html</url>
    <content type="text"><![CDATA[12345678910111213Integer a = 1;Integer b = 2;Integer c = 3;Integer d = 3;Integer e = 321;Integer f = 321;Long g = 3L;System.out.println(c == d);System.out.println(e == f);System.out.println(c == (a+b));System.out.println(c.equals(a+b));System.out.println(g == (a+b));System.out.println(g.equals(a+b)); 输出结果：123456truefalsetruetruetruefalse 包装类“==”运算会发生自动拆装箱，在没有算术运算的情况下不会自动拆箱，以及它们的equals()方法不处理数据转型的关系。 自动装箱过程调用了valueOf()的方法，查看valueOf()方法的源码为：1234567891011121314151617181920212223242526272829303132333435363738public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high"); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125; &#125; 通过阅读源代码，可以发现，java内部为了节省内存，IntegerCache类中有一个数组缓存了值从-128到127的Integer对象。当我们调用Integer.valueOf（int i）的时候，如果i的值时结余-128到127之间的，会直接从这个缓存中返回一个对象，否则就new一个新的Integer对象。321&gt;127,所以重新new了一个对象。 典型题目：请提供一个对i和j的声明，将下面的循环转变成为一个无限循环：123while(j &lt;= i &amp;&amp; i &lt;= j &amp;&amp; i != j)&#123; &#125; 答案：12345Integer i = 200 ;Integer j = 200 ; while(j&lt;=i &amp;&amp; i&lt;=j &amp;&amp; i!= j)&#123; &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二进制、十进制快速转换]]></title>
    <url>%2F%E4%BA%8C%E8%BF%9B%E5%88%B6%E3%80%81%E5%8D%81%E8%BF%9B%E5%88%B6%E5%BF%AB%E9%80%9F%E8%BD%AC%E6%8D%A2.html</url>
    <content type="text"><![CDATA[方法一以8位 来演示: 第一种:00000001 100000010 200000100 400001000 800010000 1600100000 3201000000 6410000000 128 第二种：00000001 100000011 300000111 700001111 1500011111 3100111111 6301111111 1273.第三种：10000000 12811000000 19211100000 22411110000 24011111000 24811111100 25211111110 254举个例子： 11101011 可分为： 11100000（上面第三种类型） 224 00001000（上面第一种类型） 8 00000011（上面第二种类型） 3 我们通过记住上面三种类型的转换，再用加 法（加法口算你会吧）立即得到结果：235方法二：熟记以下排列，其实很Easy了，从右往 左，依次是前一个数的2倍：2^8 2^7 2^6 2^5 2^4 2^3 2^2 2^1 2^0256 128 64 32 16 8 4 2 1随便写个数字比如4848 = 32 + 16,所以在32 和 16所在的位置为1，其余为0，转为2进制就是256 128 64 32 16 8 4 2 10 0 0 1 1 0 0 0 0 二进制转十进制就更简单了，比如随便写的一串 01111101先写上 ： 0 1 1 1 1 1 0 1然后填充 128 64 32 16 8 4 2 1 十进制为 64+32+16+8+4+1=125]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>进制转换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DOS下创建文件和文件夹]]></title>
    <url>%2FDOS%E4%B8%8B%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E5%92%8C%E6%96%87%E4%BB%B6%E5%A4%B9.html</url>
    <content type="text"><![CDATA[DOS下创建文件的三种方法第一种： echo 内容 &gt; a.txt 重定向输出，此时创建文本文件a.txt echo 内容 &gt;&gt; a.txt 向a.txt文件中追加信息第二种：copy con a.txt 创建空文本文件a.txt输入完成后，按ctrl+z退出第三种：type nul&gt;filename 可以创建一个名为filename的空文件，在批处理中经常使用copy nul a.txt 可创建一个空文件，如果a.txt已经存在，且有内容，会被清空，在批文件中经常用。 DOS下创建目录的命令 创建：md 目录名 或者 mkdir 目录名 删除：rd 目录名 或者rmdir 目录名 删除非空目录： rd /s /q 目录名 /s 除目录本身外，还将删除指定目录下的所有子目录和文件,用于删除目录树。 /q 安静模式 /s 删除目录树时不要求确认其他一些命令 cmd命令 附件命令： 切换 盘符d: e: f: 查看文件目录清单dirdir /s 查看所有目录和子目录下的文件目录清单dir /p分屏显示 改变当前目录cd 目录cd.. 回退到上一级目录cd/ 回退到根目录 新建文件夹（目录）md 目录名 新建文件copy con 文件名.扩展名内容ctrl + z 显示文件内容type 文件名.扩展名 复制copy 源文件目录 目标文件目录 重命名ren 原文件名 新文件名 移动move 原文件 目标路径 删除目录rd 目录名但是只能删除 空的目录 删除文件del 文件.扩展名del . 清屏cls 退出exit]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>DOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intellij导出JavaDoc编码异常]]></title>
    <url>%2FIntellij%E5%AF%BC%E5%87%BAJavaDoc%E7%BC%96%E7%A0%81%E5%BC%82%E5%B8%B8.html</url>
    <content type="text"><![CDATA[Intellij IDEA 导出JavaDoc时编码异常之解决方案。 javac编译提示错误需要为 class、interface 或 enumHelloWorld.java:1: 需要为 class、interface 或 enum锘缝ublic class HelloWorld{^D:\IDE-workspace\src\com\dudefu\www\Test2.java:1: 错误: 非法字符: ‘\ufeff’?package com.dudefu.www;^D:\IDE-workspace\src\com\dudefu\www\Test2.java:1: 错误: 需要class, interface或enum?package com.dudefu.www; ^3 错误 原因这个错误出现的原因主要是在中文操作系统中，使用一贯的“javac HelloWorld.java”方式编译UTF-8（带BOM）编码的.java源文件，在没有指定编码参数（encoding）的情况下，默认是使用GBK编码。当编译器用GBK编码来编译UTF-8文件时，就会把UTF-8（带BOM）编码文件的文件头的占3个字节的头信息，按照GBK中汉字占两个字节、英文占1个字节的特性解码成了“乱码”的两个汉字。这个源文件应该是用记事本另存存为UTF-8编码造成的。 对于非GBK及其子集编码（GB2312）的正确的源文件，编译方式为“javac -encoding “UTF-8” HelloWord.java”，这样代码错误的指定代码里就不会出现乱码的中文。 但是依然会有错误，提示“HelloWorld.java:1: 非法字符: \65279。这是因为.java对于UTF-8编码，只识别UTF-8（不带BOM）那种。而记事本只支持保存文件为带签名的UTF-8，那有没有办法解决呢？当然是有的，那就是使用EmEditor、EditPlus、UltraEdit或Notepad++之类的工具另存为UTF（不带BOM）（区别于带UTF + BOM）的编码文件。这时候使用“javac -encoding “UTF-8” HelloWorld.java”，就没有上述编码问题了。也许有人会说，“我干脆都用GBK不就行了吗，为什么还要用UTF-8呢？”这是因为UTF-8支持世界多种语言的文字，被世界多数国家接受，是国际通用编码，也是Java推荐使用的编码。Java集成开发环境Eclipse中默认编码就是UTF-8。如果使用GBK，尤其是做网站，在非汉语国家，将无法正常浏览。在信息化时代，国际交往日益频繁；做软件和网站，不能只着眼当前，也要为日后维护做优化、降低维护成本。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Intellij</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java内存区域与内存溢出异常]]></title>
    <url>%2FJava%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E4%B8%8E%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E5%BC%82%E5%B8%B8.html</url>
    <content type="text"><![CDATA[程序运行时，存储数据的五个地方：1）寄存器 这是最快的存储区，位于处理器内部。但是，寄存器数量极其有限，所以寄存器根据需求进行分配。你不能直接控制，也不能在程序中感觉到寄存器存在的任何迹象（C和C++允许向编译器建议寄存器的分配方式）。寄存器是存在在cpu上的。而内存是挂在数据总线的，数据总线就是用来决定传输数据的大小。而就是通过在寄存器上的地址来寻找相应内存。总的来说，寄存器和内存是两个东西，程序是无法来控制寄存器，所以这里了解一下就可以了。主要涉及到运行程序涉及到的就是下面这些栈（stack）、堆（heap）、静态域、常量池。2）堆栈 即“栈”，位于通用RAM（随机访问存储器）中，但通过堆栈指针可以从处理器那里获得直接支持。堆栈指针若向下移动，则分配新的内存；若向上移动，则释放那些内存。这是一种快速有效的分配存储方法，仅次于寄存器。创建程序时，Java系统必须知道存储在堆栈内所有项的确切生命周期，以便上下移动堆栈指针。这一约束限制了程序的灵活性，所以虽然某些Java数据存储于堆栈中—–特别是对象引用，但是Java对象并不存储于其中。栈中主要存放一些基本类型的变量（ int, short, long, byte,float, double, boolean, char ）和对象引用。 对象是不会放置在里面的。3）堆 一种通用的内存池（位于RAM区），用于存放所有的Java对象，以及动态生成的对象（包括数组）和程序运行时生成的一些数据（包括对象的定义和变量的定义）。堆不同于堆栈的好处是：编译器不需要知道存储的数据在堆里存活多长时间。因此，在堆里分配存储有很大的灵活性。new一个对象，会自动在堆里进行存储分配。 当然，为这种灵活性必须付出相应的代价：用堆进行存储分配和清理可能比堆栈进行存储分配需要更多的时间。4）常量存储。常量值通常直接存放在程序代码内部，这样做是安全的，因为它们永远不会被改变。5）非RAM存储 比如流对象和持久化对象。在流对象中，对象转化成字节流，通常被发送给另一台机器。在“持久化对象”中，对象被存放磁盘中。这种存储方式的技巧在于：把对象转化成可以存放在其他媒介上的事物，在需要时，可以恢复成常规的、基于RAM的对象。 其他数据共享 这个数据共享主要也是由于引用的是地址来决定的，举个例子：char str1=”str1”;char str2=”str1”;这时候再次声明Str2，同时指定两个不同的引用而相同的变量;这时候并不需要重新开辟另外一份内存，只需要两者都指向相同的地址就可以了。这样数据共享带来的就是内存上的节省。定义和声明 这里需要对这两个动词进行一些说明。因为在平时过程中，我是对这两个概念比较模糊。一说就是定义声明了一个变量。但是事实上确实不一样的。声明就只是定义这个变量的名字，告诉编译器会有这么一个变量。而定义就不同了，定义就是在声明之后对变量进行初始化、设置一个初始值的过程。如：int i；int i=1；就是这个区别。而在java变量的声明过程中，是不允许没有初始化变量的。 Data segment 这个包括静态域和常量池。 静态域 这个就是咱们存放在对象中的静态变量。 常量池 这个主要是在编译完成后，存放在.class文件中（code segment）。包括一些基本的数据类型和相应的类的接口和声明。换言之就是在编译后，程序中经常使用的不会改变的。例如：基本数据类型（这个是规定，肯定没法改）。接口的命名：这个你肯定不会闲到改改这个来解闷的。 内存分析，java程序执行的过程，一般变量的内存粗存放过程。 实例：下面通过分析一个例子来说明java变量是怎么存放在内存中的Code segment：arraylistlist[]=new arraylist[2]；Arraylist[0]=2;arraylist[1]=3;arraylist[2]=4;因为list[]是一个变量，这是一个声明我们放到栈中。而后面每个数组实例化出来的变量，所以放到堆中。而实实在在存在的变量的值都是常量，所以放在常量池中，也就是上图中的datasegment。 运行时数据区域Java虚拟机在执行Java程序的过程中，会把所管理的内存划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有些区域则依赖用户线程的启动和结束而建立和销毁。下图是Java虚拟机运行时数据区： 程序计数器程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。在Java虚拟机中，多线程是通过线程 轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对多核处理器来说是一个内核）都只会执行一条线程中的命令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类区域为“线程私有”的内存。如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Native方法，这个计数器值则为空（Undefined）。此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 Java虚拟机栈（栈）与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。两种异常，如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），如果扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常。 本地方法栈与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务。本地方法栈也会抛出StackOverflowError和OutOfMemoryError异常。 Java堆对大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 方法区Method Area,Non-Heap(非堆)与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 运行时常量池运行时常量池（RuntiMe ConStant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用的，这部分内容将在类加载后进入方法区的运行时常量池中存放。 直接内存（Direct Memory）NIO类引入了一种基于通道（Channel）与缓冲区（Buffer）的I/O方式，它可以 使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。这样能在一些场景中显著提供性能，因为避免了在Java堆和Native堆中来回复制数据。服务器管理员在配置虚拟机参数时，会根据实际内存设置-Xmx等参数信息，但经常忽略直接内存，使得各个内存区域总和大于物理内存限制（包括物理的和操作系统的限制），从而导致动态扩展时出现OutOfMemoryError异常。 在虚拟机中，对象创建对象的过程？？？？虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程，在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头）。接下来，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息存放在对象的对象头（Object Header）之中。从虚拟机的视角来看，一个新的对象已经差生了，但从Java程序的视角来看，对象创建才刚刚开始—方法还没有执行，所有的字段都还为零。 所以，一般来说，执行new指令之后会接着执行方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试题：百度前200页都在这里了]]></title>
    <url>%2F%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E7%99%BE%E5%BA%A6%E5%89%8D200%E9%A1%B5%E9%83%BD%E5%9C%A8%E8%BF%99%E9%87%8C%E4%BA%86.html</url>
    <content type="text"><![CDATA[基本概念操作系统中 heap 和 stack 的区别什么是基于注解的切面实现什么是 对象/关系 映射集成模块什么是 Java 的反射机制什么是 ACIDBS与CS的联系与区别Cookie 和 Session的区别fail-fast 与 fail-safe 机制有什么区别get 和 post请求的区别Interface 与 abstract 类的区别IOC的优点是什么IO 和 NIO的区别，NIO优点Java 8 / Java 7 为我们提供了什么新功能什么是竞态条件？ 举个例子说明。JRE、JDK、JVM 及 JIT 之间有什么不同MVC的各个部分都有那些技术来实现?如何实现?RPC 通信和 RMI 区别什么是 Web Service（Web服务）JSWDL开发包的介绍。JAXP、JAXM的解释。SOAP、UDDI,WSDL解释。WEB容器主要有哪些功能? 并请列出一些常见的WEB容器名字。一个”.java”源文件中是否可以包含多个类（不是内部类）？有什么限制简单说说你了解的类加载器。是否实现过类加载器解释一下什么叫AOP（面向切面编程）请简述 Servlet 的生命周期及其相关的方法请简述一下 Ajax 的原理及实现步骤简单描述Struts的主要功能什么是 N 层架构什么是CORBA？用途是什么什么是Java虚拟机？为什么Java被称作是“平台无关的编程语言”什么是正则表达式？用途是什么？哪个包使用正则表达式来实现模式匹配什么是懒加载（Lazy Loading）什么是尾递归，为什么需要尾递归什么是控制反转（Inversion of Control）与依赖注入（Dependency Injection）关键字 finalize 什么是finalize()方法finalize()方法什么时候被调用析构函数(finalization)的目的是什么final 和 finalize 的区别finalfinal关键字有哪些用法final 与 static 关键字可以用于哪里？它们的作用是什么final, finally, finalize的区别final、finalize 和 finally 的不同之处？能否在运行时向 static final 类型的赋值使用final关键字修饰一个变量时，是引用不能变，还是引用的对象不能变一个类被声明为final类型，表示了什么意思throws, throw, try, catch, finally分别代表什么意义Java 有几种修饰符？分别用来修饰什么volatile volatile 修饰符的有过什么实践volatile 变量是什么？volatile 变量和 atomic 变量有什么不同volatile 类型变量提供什么保证？能使得一个非原子操作变成原子操作吗能创建 volatile 数组吗？transient变量有什么特点super什么时候使用public static void 写成 static public void会怎样说明一下public static void main(String args[])这段声明里每个关键字的作用请说出作用域public, private, protected, 以及不写时的区别sizeof 是Java 的关键字吗static static class 与 non static class的区别static 关键字是什么意思？Java中是否可以覆盖(override)一个private或者是static的方法静态类型有什么特点main() 方法为什么必须是静态的？能不能声明 main() 方法为非静态是否可以从一个静态（static）方法内部发出对非静态（non-static）方法的调用静态变量在什么时候加载？编译期还是运行期？静态代码块加载的时机呢成员方法是否可以访问静态变量？为什么静态方法不能访问成员变量switch switch 语句中的表达式可以是什么类型数据switch 是否能作用在byte 上，是否能作用在long 上，是否能作用在String上while 循环和 do 循环有什么不同操作符 &amp;操作符和&amp;&amp;操作符有什么区别?a = a + b 与 a += b 的区别？逻辑操作符 (&amp;,|,^)与条件操作符(&amp;&amp;,||)的区别3*0.1 == 0.3 将会返回什么？true 还是 false？float f=3.4; 是否正确？short s1 = 1; s1 = s1 + 1;有什么错?数据结构 基础类型(Primitives) 基础类型(Primitives)与封装类型(Wrappers)的区别在哪里简述九种基本数据类型的大小，以及他们的封装类int 和 Integer 哪个会占用更多的内存？ int 和 Integer 有什么区别？parseInt()函数在什么时候使用到float和double的默认值是多少如何去小数四舍五入保留小数点后两位char 型变量中能不能存贮一个中文汉字，为什么类型转换 怎样将 bytes 转换为 long 类型怎么将 byte 转换为 String如何将数值型字符转换为数字我们能将 int 强制转换为 byte 类型的变量吗？如果该值大于 byte 类型的范围，将会出现什么现象能在不进行强制转换的情况下将一个 double 值赋值给 long 类型的变量吗类型向下转换是什么数组 如何权衡是使用无序的数组还是有序的数组怎么判断数组是 null 还是为空怎么打印数组？ 怎样打印数组中的重复元素Array 和 ArrayList有什么区别？什么时候应该使用Array而不是ArrayList数组和链表数据结构描述，各自的时间复杂度数组有没有length()这个方法? String有没有length()这个方法队列 队列和栈是什么，列出它们的区别BlockingQueue是什么简述 ConcurrentLinkedQueue LinkedBlockingQueue 的用处和不同之处。ArrayList、Vector、LinkedList的存储性能和特性StringStringBuffer ByteBuffer 与 StringBuffer有什么区别HashMap HashMap的工作原理是什么内部的数据结构是什么HashMap 的 table的容量如何确定？loadFactor 是什么？ 该容量如何变化？这种变化会带来什么问题？HashMap 实现的数据结构是什么？如何实现HashMap 和 HashTable、ConcurrentHashMap 的区别HashMap的遍历方式及效率HashMap、LinkedMap、TreeMap的区别如何决定选用HashMap还是TreeMap如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办HashMap 是线程安全的吗？并发下使用的 Map 是什么，它们内部原理分别是什么，比如存储方式、 hashcode、扩容、 默认容量等HashSet HashSet和TreeSet有什么区别HashSet 内部是如何工作的WeakHashMap 是怎么工作的？Set Set 里的元素是不能重复的，那么用什么方法来区分重复与否呢？是用 == 还是 equals()？ 它们有何区别?TreeMap：TreeMap 是采用什么树实现的？TreeMap、HashMap、LindedHashMap的区别。TreeMap和TreeSet在排序时如何比较元素？Collections工具类中的sort()方法如何比较元素？TreeSet：一个已经构建好的 TreeSet，怎么完成倒排序。EnumSet 是什么Hash算法 Hashcode 的作用简述一致性 Hash 算法有没有可能 两个不相等的对象有相同的 hashcode？当两个对象 hashcode 相同怎么办？如何获取值对象为什么在重写 equals 方法的时候需要重写 hashCode 方法？equals与 hashCode 的异同点在哪里a.hashCode() 有什么用？与 a.equals(b) 有什么关系hashCode() 和 equals() 方法的重要性体现在什么地方Object：Object有哪些公用方法？Object类hashcode,equals 设计原则？ sun为什么这么设计？Object类的概述如何在父类中为子类自动完成所有的 hashcode 和 equals 实现？这么做有何优劣。可以在 hashcode() 中使用随机数字吗？LinkedHashMap LinkedHashMap 和 PriorityQueue 的区别是什么List List, Set, Map三个接口，存取元素时各有什么特点List, Set, Map 是否继承自 Collection 接口遍历一个 List 有哪些不同的方式LinkedListLinkedList 是单向链表还是双向链表LinkedList 与 ArrayList 有什么区别描述下 Java 中集合（Collections），接口（Interfaces），实现（Implementations）的概念。LinkedList 与 ArrayList 的区别是什么？插入数据时，ArrayList, LinkedList, Vector谁速度较快？ArrayListArrayList 和 HashMap 的默认大小是多数ArrayList 和 LinkedList 的区别，什么时候用 ArrayList？ArrayList 和 Set 的区别？ArrayList, LinkedList, Vector的区别ArrayList是如何实现的，ArrayList 和 LinkedList 的区别ArrayList如何实现扩容Array 和 ArrayList 有何区别？什么时候更适合用Array说出ArraList,Vector, LinkedList的存储性能和特性Map Map, Set, List, Queue, StackMap 接口提供了哪些不同的集合视图为什么 Map 接口不继承 Collection 接口Collections 介绍Java中的Collection FrameWork。集合类框架的基本接口有哪些Collections类是什么？Collection 和 Collections的区别？Collection、Map的实现集合类框架的最佳实践有哪些为什么 Collection 不从 Cloneable 和 Serializable 接口继承说出几点 Java 中使用 Collections 的最佳实践？Collections 中 遗留类 (HashTable、Vector) 和 现有类的区别什么是 B+树，B-树，列出实际的使用场景。 接口 Comparator 与 Comparable 接口是干什么的？列出它们的区别对象 拷贝(clone) 如何实现对象克隆深拷贝和浅拷贝区别深拷贝和浅拷贝如何实现激活机制写clone()方法时，通常都有一行代码，是什么比较 在比较对象时，”==” 运算符和 equals 运算有何区别如果要重写一个对象的equals方法，还要考虑什么两个对象值相同(x.equals(y) == true)，但却可有不同的hash code，这句话对不对构造器 构造器链是什么创建对象时构造器的调用顺序不可变对象 什么是不可变象（immutable object）为什么 Java 中的 String 是不可变的（Immutable）如何构建不可变的类结构？关键点在哪里能创建一个包含可变对象的不可变对象吗如何对一组对象进行排序 方法 构造器（constructor）是否可被重写（override）方法可以同时即是 static 又是 synchronized 的吗abstract 的 method是否可同时是 static，是否可同时是 native，是否可同时是synchronizedJava支持哪种参数传递类型一个对象被当作参数传递到一个方法，是值传递还是引用传递当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递我们能否重载main()方法如果main方法被声明为private会怎样GC 概念 GC是什么？为什么要有GC什么时候会导致垃圾回收GC是怎么样运行的新老以及永久区是什么GC 有几种方式？怎么配置什么时候一个对象会被GC？ 如何判断一个对象是否存活System.gc() Runtime.gc()会做什么事情？ 能保证 GC 执行吗垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？Minor GC 、Major GC、Young GC 与 Full GC分别在什么时候发生垃圾回收算法的实现原理如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？垃圾回收的最佳做法是什么GC收集器有哪些 垃圾回收器的基本原理是什么？串行(serial)收集器和吞吐量(throughput)收集器的区别是什么Serial 与 Parallel GC之间的不同之处CMS 收集器 与 G1 收集器的特点与区别CMS垃圾回收器的工作过程JVM 中一次完整的 GC 流程是怎样的？ 对象如何晋升到老年代吞吐量优先和响应优先的垃圾收集器选择GC策略 举个实际的场景，选择一个GC策略JVM的永久代中会发生垃圾回收吗收集方法 标记清除、标记整理、复制算法的原理与特点？分别用在什么地方如果让你优化收集方法，有什么思路JVM 参数 说说你知道的几种主要的jvm 参数-XX:+UseCompressedOops 有什么作用类加载器(ClassLoader) Java 类加载器都有哪些JVM如何加载字节码文件内存管理 JVM内存分哪几个区，每个区的作用是什么一个对象从创建到销毁都是怎么在这些部分里存活和转移的解释内存中的栈(stack)、堆(heap)和方法区(method area)的用法JVM中哪个参数是用来控制线程的栈堆栈小简述内存分配与回收策略简述重排序，内存屏障，happen-before，主内存，工作内存Java中存在内存泄漏问题吗？请举例说明简述 Java 中软引用（SoftReferenc）、弱引用（WeakReference）和虚引用内存映射缓存区是什么jstack，jstat，jmap，jconsole怎么用32 位 JVM 和 64 位 JVM 的最大堆内存分别是多数？32 位和 64 位的 JVM，int 类型变量的长度是多数？怎样通过 Java 程序来判断 JVM 是 32 位 还是 64 位JVM自身会维护缓存吗？是不是在堆中进行对象分配，操作系统的堆还是JVM自己管理堆什么情况下会发生栈内存溢出双亲委派模型是什么 多线程 基本概念 什么是线程多线程的优点多线程的几种实现方式用 Runnable 还是 Thread什么是线程安全Vector, SimpleDateFormat 是线程安全类吗什么 Java 原型不是线程安全的哪些集合类是线程安全的多线程中的忙循环是什么如何创建一个线程编写多线程程序有几种实现方式什么是线程局部变量线程和进程有什么区别？进程间如何通讯，线程间如何通讯什么是多线程环境下的伪共享（false sharing）同步和异步有何异同，在什么情况下分别使用他们？举例说明Current ConcurrentHashMap 和 Hashtable的区别ArrayBlockingQueue, CountDownLatch的用法ConcurrentHashMap的并发度是什么CyclicBarrier 和 CountDownLatch有什么不同？各自的内部原理和用法是什么Semaphore的用法Thread 启动一个线程是调用 run() 还是 start() 方法？start() 和 run() 方法有什么区别调用start()方法时会执行run()方法，为什么不能直接调用run()方法sleep() 方法和对象的 wait() 方法都可以让线程暂停执行，它们有什么区别yield方法有什么作用？sleep() 方法和 yield() 方法有什么区别Java 中如何停止一个线程stop() 和 suspend() 方法为何不推荐使用如何在两个线程间共享数据如何强制启动一个线程如何让正在运行的线程暂停一段时间什么是线程组，为什么在Java中不推荐使用你是如何调用 wait（方法的）？使用 if 块还是循环？为什么生命周期 有哪些不同的线程生命周期线程状态，BLOCKED 和 WAITING 有什么区别画一个线程的生命周期状态图ThreadLocal 用途是什么，原理是什么，用的时候要注意什么ThreadPool 线程池是什么？为什么要使用它如何创建一个Java线程池ThreadPool用法与优势提交任务时，线程池队列已满时会发会生什么newCache 和 newFixed 有什么区别？简述原理。构造函数的各个参数的含义是什么，比如 coreSize, maxsize 等线程池的实现策略线程池的关闭方式有几种，各自的区别是什么线程池中submit() 和 execute()方法有什么区别？线程调度 Java中用到的线程调度算法是什么什么是多线程中的上下文切换你对线程优先级的理解是什么什么是线程调度器 (Thread Scheduler) 和时间分片 (Time Slicing)线程同步 请说出你所知的线程同步的方法synchronized 的原理是什么synchronized 和 ReentrantLock 有什么不同什么场景下可以使用 volatile 替换 synchronized有T1，T2，T3三个线程，怎么确保它们按顺序执行？怎样保证T2在T1执行完后执行，T3在T2执行完后执行同步块内的线程抛出异常会发生什么当一个线程进入一个对象的 synchronized 方法A 之后，其它线程是否可进入此对象的 synchronized 方法B使用 synchronized 修饰静态方法和非静态方法有什么区别如何从给定集合那里创建一个 synchronized 的集合锁 Java Concurrency API 中 的 Lock 接口是什么？对比同步它有什么优势Lock 与 Synchronized 的区别？Lock 接口比 synchronized 块的优势是什么ReadWriteLock是什么？锁机制有什么用什么是乐观锁（Optimistic Locking）？如何实现乐观锁？如何避免ABA问题解释以下名词：重排序，自旋锁，偏向锁，轻量级锁，可重入锁，公平锁，非公平锁，乐观锁，悲观锁什么时候应该使用可重入锁简述锁的等级方法锁、对象锁、类锁Java中活锁和死锁有什么区别？什么是死锁(Deadlock)？导致线程死锁的原因？如何确保 N 个线程可以访问 N 个资源同时又不导致死锁死锁与活锁的区别，死锁与饥饿的区别怎么检测一个线程是否拥有锁如何实现分布式锁有哪些无锁数据结构，他们实现的原理是什么读写锁可以用于什么应用场景Executors类是什么？ Executor和Executors的区别什么是Java线程转储(Thread Dump)，如何得到它如何在Java中获取线程堆栈说出 3 条在 Java 中使用线程的最佳实践在线程中你怎么处理不可捕捉异常实际项目中使用多线程举例。你在多线程环境中遇到的常见的问题是什么？你是怎么解决它的请说出与线程同步以及线程调度相关的方法程序中有3个 socket，需要多少个线程来处理假如有一个第三方接口，有很多个线程去调用获取数据，现在规定每秒钟最多有 10 个线程同时调用它，如何做到如何在 Windows 和 Linux 上查找哪个线程使用的 CPU 时间最长如何确保 main() 方法所在的线程是 Java 程序最后结束的线程非常多个线程（可能是不同机器），相互之间需要等待协调才能完成某种工作，问怎么设计这种协调方案你需要实现一个高效的缓存，它允许多个用户读，但只允许一个用户写，以此来保持它的完整性，你会怎样去实现它 异常 基本概念 Error 和 Exception有什么区别UnsupportedOperationException是什么NullPointerException 和 ArrayIndexOutOfBoundException 之间有什么相同之处什么是受检查的异常，什么是运行时异常运行时异常与一般异常有何异同简述一个你最常见到的runtime exception(运行时异常)finally finally关键词在异常处理中如何使用如果执行finally代码块之前方法返回了结果，或者JVM退出了，finally块中的代码还会执行吗try里有return，finally还执行么？那么紧跟在这个try后的finally {}里的code会不会被执行，什么时候被执行，在return前还是后在什么情况下，finally语句不会执行throw 和 throws 有什么区别？OOM你遇到过哪些情况？你是怎么搞定的？SOF你遇到过哪些情况？既然我们可以用RuntimeException来处理错误，那么你认为为什么Java中还存在检查型异常当自己创建异常类的时候应该注意什么导致空指针异常的原因异常处理 handle or declare 原则应该如何理解怎么利用 JUnit 来测试一个方法的异常catch块里别不写代码有什么问题你曾经自定义实现过异常吗？怎么写的什么是 异常链在try块中可以抛出异常吗 JDBC 通过 JDBC 连接数据库有哪几种方式阐述 JDBC 操作数据库的基本步骤JDBC 中如何进行事务处理什么是 JdbcTemplate什么是 DAO 模块使用 JDBC 操作数据库时，如何提升读取数据的性能？如何提升更新数据的性能列出 5 个应该遵循的 JDBC 最佳实践IO FileFile类型中定义了什么方法来创建一级目录File类型中定义了什么方法来判断一个文件是否存在 流为了提高读写性能，可以采用什么流Java中有几种类型的流JDK 为每种类型的流提供了一些抽象类以供继承，分别是哪些类对文本文件操作用什么I/O流对各种基本数据类型和String类型的读写，采用什么流能指定字符编码的 I/O 流类型是什么序列化什么是序列化？如何实现 Java 序列化及注意事项Serializable 与 Externalizable 的区别Socketsocket 选项 TCP NO DELAY 是指什么Socket 工作在 TCP/IP 协议栈是哪一层TCP、UDP 区别及 Java 实现方式说几点 IO 的最佳实践直接缓冲区与非直接缓冲器有什么区别？怎么读写 ByteBuffer？ByteBuffer 中的字节序是什么当用System.in.read(buffer)从键盘输入一行n个字符后，存储在缓冲区buffer中的字节数是多少如何使用扫描器类（Scanner Class）令牌化面向对象编程（OOP） 解释下多态性（polymorphism），封装性（encapsulation），内聚（cohesion）以及耦合（coupling）多态的实现原理封装、继承和多态是什么对象封装的原则是什么?类获得一个类的类对象有哪些方式重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？说出几条 Java 中方法重载的最佳实践抽象类抽象类和接口的区别抽象类中是否可以有静态的main方法抽象类是否可实现(implements)接口抽象类是否可继承具体类(concrete class)匿名类（Anonymous Inner Class）匿名内部类是否可以继承其它类？是否可以实现接口 内部类内部类分为几种内部类可以引用它的包含类（外部类）的成员吗请说一下 Java 中为什么要引入内部类？还有匿名内部类继承继承（Inheritance）与聚合（Aggregation）的区别在哪里继承和组合之间有什么不同为什么类只能单继承，接口可以多继承存在两个类，B 继承 A，C 继承 B，能将 B 转换为 C 么？如 C = (C) B如果类 a 继承类 b，实现接口c，而类 b 和接口 c 中定义了同名变量，请问会出现什么问题接口接口是什么接口是否可继承接口为什么要使用接口而不是直接使用具体类？接口有什么优点泛型 泛型的存在是用来解决什么问题泛型的常用特点List能否转为List工具类 日历Calendar Class的用途如何在Java中获取日历类的实例解释一些日历类中的重要方法GregorianCalendar 类是什么SimpleTimeZone 类是什么Locale类是什么如何格式化日期对象如何添加小时(hour)到一个日期对象(Date Objects)如何将字符串 YYYYMMDD 转换为日期MathMath.round()什么作用？Math.round(11.5) 等于多少？Math.round(-11.5)等于多少？ XMLXML文档定义有几种形式？它们之间有何本质区别？解析XML文档有哪几种方式？DOM 和 SAX 解析器有什么不同？Java解析XML的方式用 jdom 解析 xml 文件时如何解决中文问题？如何解析你在项目中用到了 XML 技术的哪些方面？如何实现动态代理 描述动态代理的几种实现方式，分别说出相应的优缺点设计模式 什么是设计模式（Design Patterns）？你用过哪种设计模式？用在什么场合你知道哪些商业级设计模式？哪些设计模式可以增加系统的可扩展性单例模式除了单例模式，你在生产环境中还用过什么设计模式？写 Singleton 单例模式单例模式的双检锁是什么如何创建线程安全的 Singleton什么是类的单例模式写出三种单例模式实现适配器模式适配器模式是什么？什么时候使用适配器模式和代理模式之前有什么不同适配器模式和装饰器模式有什么区别什么时候使用享元模式什么时候使用组合模式什么时候使用访问者模式什么是模板方法模式请给出1个符合开闭原则的设计模式的例子开放问题 用一句话概括 Web 编程的特点Google是如何在一秒内把搜索结果返回给用户哪种依赖注入方式你建议使用，构造器注入，还是 Setter方法注入树（二叉或其他）形成许多普通数据结构的基础。请描述一些这样的数据结构以及何时可以使用它们某一项功能如何设计线上系统突然变得异常缓慢，你如何查找问题什么样的项目不适合用框架新浪微博是如何实现把微博推给订阅者简要介绍下从浏览器输入 URL 开始到获取到请求界面之后 Java Web 应用中发生了什么请你谈谈SSH整合高并发下，如何做到安全的修改同一行数据12306网站的订票系统如何实现，如何保证不会票不被超卖网站性能优化如何优化的聊了下曾经参与设计的服务器架构请思考一个方案，实现分布式环境下的 countDownLatch请思考一个方案，设计一个可以控制缓存总体大小的自动适应的本地缓存在你的职业生涯中，算得上最困难的技术挑战是什么如何写一篇设计文档，目录是什么大写的O是什么？举几个例子编程中自己都怎么考虑一些设计原则的，比如开闭原则，以及在工作中的应用解释一下网络应用的模式及其特点设计一个在线文档系统，文档可以被编辑，如何防止多人同时对同一份文档进行编辑更新说出数据连接池的工作机制是什么怎么获取一个文件中单词出现的最高频率描述一下你最常用的编程风格如果有机会重新设计你们的产品，你会怎么做如何搭建一个高可用系统如何启动时不需输入用户名与密码如何在基于Java的Web项目中实现文件上传和下载如何实现一个秒杀系统，保证只有几位用户能买到某件商品。如何实现负载均衡，有哪些算法可以实现如何设计一个购物车？想想淘宝的购物车如何实现的如何设计一套高并发支付方案，架构如何设计如何设计建立和保持 100w 的长连接如何避免浏览器缓存。如何防止缓存雪崩如果AB两个系统互相依赖，如何解除依如果有人恶意创建非法连接，怎么解决如果有几十亿的白名单，每天白天需要高并发查询，晚上需要更新一次，如何设计这个功能如果系统要使用超大整数（超过long长度范围），请你设计一个数据结构来存储这种超大型数字以及设计一种算法来实现超大整数加法运算）如果要设计一个图形系统，请你设计基本的图形元件(Point,Line,Rectangle,Triangle)的简单实现如果让你实现一个并发安全的链表，你会怎么做应用服务器与WEB 服务器的区别？应用服务器怎么监控性能，各种方式的区别？你使用过的应用服务器优化技术有哪些大型网站在架构上应当考虑哪些问题有没有处理过线上问题？出现内存泄露，CPU利用率标高，应用无响应时如何处理的最近看什么书，印象最深刻的是什么描述下常用的重构技巧你使用什么版本管理工具？分支（Branch）与标签（Tag）之间的区别在哪里你有了解过存在哪些反模式（Anti-Patterns）吗你用过的网站前端优化的技术有哪些如何分析Thread dump你如何理解AOP中的连接点（Joinpoint）、切点（Pointcut）、增强（Advice）、引介（Introduction）、织入（Weaving）、切面（Aspect）这些概念你是如何处理内存泄露或者栈溢出问题的你们线上应用的 JVM 参数有哪些怎么提升系统的QPS和吞吐量知识面 解释什么是 MESI 协议(缓存一致性)谈谈 reactor 模型Java 9 带来了怎样的新功能Java 与 C++ 对比，C++ 或 Java 中的异常处理机制的简单原理和应用简单讲讲 Tomcat 结构，以及其类加载器流程虚拟内存是什么阐述下 SOLID 原则请简要讲一下你对测试驱动开发（TDD）的认识CDN实现原理Maven 和 ANT 有什么区别UML中有哪些常用的图LinuxLinux 下 IO 模型有几种，各自的含义是什么。Linux 系统下你关注过哪些内核参数，说说你知道的Linux 下用一行命令查看文件的最后五行平时用到哪些 Linux 命令用一行命令输出正在运行的 Java 进程使用什么命令来确定是否有 Tomcat 实例运行在机器上什么是 N+1 难题什么是 paxos 算法什么是 restful，讲讲你理解的 restful什么是 zab 协议什么是领域模型(domain model)？贫血模型(anaemic domain model) 和充血模型(rich domain model)有什么区别什么是领域驱动开发（Domain Driven Development）介绍一下了解的 Java 领域的 Web Service 框架Web Server、Web Container 与 Application Server 的区别是什么微服务（MicroServices）与巨石型应用（Monolithic Applications）之间的区别在哪里描述 Cookie 和 Session 的作用，区别和各自的应用范围，Session工作原理你常用的持续集成（Continuous Integration）、静态代码分析（Static Code Analysis）工具有哪些简述下数据库正则化（Normalizations）KISS,DRY,YAGNI 等原则是什么含义分布式事务的原理，优缺点，如何使用分布式事务？布式集群下如何做到唯一序列号网络HTTPS 的加密方式是什么，讲讲整个加密解密流程HTTPS和HTTP的区别HTTP连接池实现原理HTTP集群方案Nginx、lighttpd、Apache三大主流 Web服务器的区别是否看过框架的一些代码持久层设计要考虑的问题有哪些？你用过的持久层框架有哪些数值提升是什么你能解释一下里氏替换原则吗你是如何测试一个应用的？知道哪些测试框架传输层常见编程协议有哪些？并说出各自的特点编程题 计算加班费 加班10小时以下加班费是时薪的1.5倍。加班10小时或以上，按4元/时算。提示：（一个月工作26天，一天正常工作8小时） 计算1000月薪，加班9小时的加班费计算2500月薪，加班11小时的加班费计算1000月薪，加班15小时的加班费卖东西 一家商场有红苹果和青苹果出售。（红苹果5元/个，青苹果4元/个）。 模拟一个进货。红苹果跟青苹果各进200个。模拟一个出售。红苹果跟青苹果各买出10个。每卖出一个苹果需要进行统计。提示：一个苹果是一个单独的实体。 日期提取 有这样一个时间字符串：2008-8-8 20:08:08 ， 请编写能够匹配它的正则表达式，并编写Java代码将日期后面的时分秒提取出来，即：20:08:08 线程 8设计4个线程，其中两个线程每次对j增加1，另外两个线程对j每次减少1。写出程序。用Java写一个多线程程序，如写四个线程，二个加1，二个对一个变量减一，输出wait-notify 写一段代码来解决生产者-消费者问题数字 判断101-200之间有多少个素数，并输出所有素数用最有效率的方法算出2乘以17等于多少有 1 亿个数字，其中有 2 个是重复的，快速找到它，时间和空间要最优2 亿个随机生成的无序整数,找出中间大小的值10 亿个数字里里面找最小的 10 个1到1亿的自然数，求所有数的拆分后的数字之和，如286 拆分成2、8、6，如1到11拆分后的数字之和 =&gt; 1 + … + 9 + 1 + 0 + 1 + 1一个数如果恰好等于它的因子之和，这个数就称为 “完数 “。例如6=1＋2＋3.编程 找出1000以内的所有完数一个数组中所有的元素都出现了三次，只有一个元素出现了一次找到这个元素一球从100米高度自由落下，每次落地后反跳回原高度的一半；再落下，求它在 第10次落地时，共经过多少米？第10次反弹多高？求100－1000内质数的和求1到100的和的平均数求s=a+a+aaa+aaaa+aa…a的值，其中a是一个数字。例如2+22+222+2222+22222(此时共有5个数相加)，几个数相加有键盘控制。 求出1到100的和算出1到40的质数，放进数组里显示放组里的数找出第[5]个数删除第[9]个数，再显示删除后的第[9]个有 3n+1 个数字，其中 3n 个中是重复的，只有 1 个是不重复的，怎么找出来。有一组数1.1.2.3.5.8.13.21.34。写出程序随便输入一个数就能给出和前一组数字同规律的头5个数计算指定数字的阶乘开发 Fizz Buzz给定一个包含 N 个整数的数组，找出丢失的整数一个排好序的数组，找出两数之和为m的所有组合将一个正整数分解质因数。例如：输入90,打印出90=233*5。打印出所有的 “水仙花数 “，所谓 “水仙花数 “是指一个三位数，其各位数字立方和等于该数本身。例如：153是一个 “水仙花数 “，因为153=1的三次方＋5的三次方＋3的三次方原地交换两个变量的值找出4字节整数的中位数找到整数的平方根实现斐波那契网络 用Java Socket编程，读服务器几个字符，再写入本地显示反射 反射机制提供了什么功能？反射是如何实现的哪里用到反射机制反射中 Class.forName 和 ClassLoader 区别反射创建类实例的三种方式是什么如何通过反射调用对象的方法如何通过反射获取和设置对象私有字段的值反射机制的优缺点数据库 写一段 JDBC 连Oracle的程序,并实现数据查询算法 50个人围坐一圈，当数到三或者三的倍数出圈，问剩下的人是谁，原来的位置是多少实现一个电梯模拟器用写一个冒泡排序写一个折半查找随机产生20个不能重复的字符并排序写一个函数，传入 2 个有序的整数数组，返回一个有序的整数数组写一段代码在遍历 ArrayList 时移除一个元素古典问题：有一对兔子，从出生后第3个月起每个月都生一对兔子，小兔子长到第四个月后每个月又生一对兔子，假如兔子都不死，问每个月的兔子总数为多少约瑟芬环游戏正则 请编写一段匹配IP地址的正则表达式写出一个正则表达式来判断一个字符串是否是一个数字字符串 写一个方法，入一个文件名和一个字符串，统计这个字符串在这个文件中出现的次数。写一个程序找出所有字符串的组合，并检查它们是否是回文串写一个字符串反转函数，输入abcde转换成edcba代码小游戏，倒转句子中的单词将GB2312编码的字符串转换为ISO-8859-1编码的字符串请写一段代码来计算给定文本内字符“A”的个数。分别用迭代和递归两种方式编写一个截取字符串的函数，输入为一个字符串和字节数，输出为按字节截取的字符串。 但是要保证汉字不被截半个，如“我ABC”4，应该截为“我AB”，输入“我ABC汉DEF”，6，应该输出为“我ABC”而不是“我ABC+汉的半个”给定 2 个包含单词列表（每行一个）的文件，编程列出交集打印出一个字符串的所有排列将一个键盘输入的数字转化成中文输出(例如：输入1234567，输出:一百二拾三万四千五百六拾七)在Web应用开发过程中经常遇到输出某种编码的字符，如从 GBK 到 ISO8859-1等，如何输出一个某种编码的字符串日期 计算两个日期之间的差距]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java逃逸分析]]></title>
    <url>%2FJava%E9%80%83%E9%80%B8%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[概念引入 我们都知道，Java 创建的对象都是被分配到堆内存上，但是事实并不是这么绝对，通过对Java对象分配的过程分析，可以知道有两个地方会导致Java中创建出来的对象并一定分别在所认为的堆上。这两个点分别是Java中的逃逸分析和TLAB（Thread Local Allocation Buffer）线程私有的缓存区。 基本概念介绍 逃逸分析，是一种可以有效减少Java程序中同步负载和内存堆分配压力的跨函数全局数据流分析算法。通过逃逸分析，Java Hotspot编译器能够分析出一个新的对象的引用的使用范围从而决定是否要将这个对象分配到堆上。在计算机语言编译器优化原理中，逃逸分析是指分析指针动态范围的方法，它同编译器优化原理的指针分析和外形分析相关联。当变量（或者对象）在方法中分配后，其指针有可能被返回或者被全局引用，这样就会被其他过程或者线程所引用，这种现象称作指针（或者引用）的逃逸(Escape)。通俗点讲，如果一个对象的指针被多个方法或者线程引用时，那么我们就称这个对象的指针发生了逃逸。 Java在Java SE 6u23以及以后的版本中支持并默认开启了逃逸分析的选项。Java的 HotSpot JIT编译器，能够在方法重载或者动态加载代码的时候对代码进行逃逸分析，同时Java对象在堆上分配和内置线程的特点使得逃逸分析成Java的重要功能。 代码示例1234567891011121314151617181920212223242526272829303132333435package me.stormma.gc;/** * &lt;p&gt;Created on 2017/4/21.&lt;/p&gt; * * @author stormma * * @title &lt;p&gt;逃逸分析&lt;/p&gt; */public class EscapeAnalysis &#123; public static B b; /** * &lt;p&gt;全局变量赋值发生指针逃逸&lt;/p&gt; */ public void globalVariablePointerEscape() &#123; b = new B(); &#125; /** * &lt;p&gt;方法返回引用，发生指针逃逸&lt;/p&gt; * @return */ public B methodPointerEscape() &#123; return new B(); &#125; /** * &lt;p&gt;实例引用发生指针逃逸&lt;/p&gt; */ public void instancePassPointerEscape() &#123; methodPointerEscape().printClassName(this); &#125; class B &#123; public void printClassName(EscapeAnalysis clazz) &#123; System.out.println(clazz.getClass().getName()); &#125; &#125;&#125; 逃逸分析研究对于 java 编译器有什么好处呢？我们知道 java 对象总是在堆中被分配的，因此 java 对象的创建和回收对系统的开销是很大的。java 语言被批评的一个地方，也是认为 java 性能慢的一个原因就是 java不支持栈上分配对象。JDK6里的 Swing内存和性能消耗的瓶颈就是由于 GC 来遍历引用树并回收内存的，如果对象的数目比较多，将给 GC 带来较大的压力，也间接得影响了性能。减少临时对象在堆内分配的数量，无疑是最有效的优化方法。java 中应用里普遍存在一种场景，一般是在方法体内，声明了一个局部变量，并且该变量在方法执行生命周期内未发生逃逸，按照 JVM内存分配机制，首先会在堆内存上创建类的实例（对象），然后将此对象的引用压入调用栈，继续执行，这是 JVM优化前的方式。当然，我们可以采用逃逸分析对 JVM 进行优化。即针对栈的重新分配方式，首先我们需要分析并且找到未逃逸的变量，将该变量类的实例化内存直接在栈里分配，无需进入堆，分配完成之后，继续调用栈内执行，最后线程执行结束，栈空间被回收，局部变量对象也被回收，通过这种方式的优化，与优化前的方案主要区别在于对象的存储介质，优化前是在堆中，而优化后的是在栈中，从而减少了堆中临时对象的分配（较耗时），从而优化性能。 使用逃逸分析进行性能优化(-XX:+DoEscapeAnalysis开启逃逸分析)123456public void method() &#123; Test test = new Test(); //处理逻辑 ...... test = null;&#125; 这段代码，之所以可以在栈上进行内存分配，是因为没有发生指针逃逸，即是引用没有暴露出这个方法体。 栈和堆内存分配比较123456789101112131415161718192021package me.stormma.gc;/** * &lt;p&gt;Created on 2017/4/21.&lt;/p&gt; * * @author stormma * @description: &lt;p&gt;内存分配比较&lt;/p&gt; */public class EscapeAnalysisTest &#123; public static void alloc() &#123; byte[] b = new byte[2]; b[0] = 1; &#125; public static void main(String[] args) &#123; long b = System.currentTimeMillis(); for (int i = 0; i &lt; 100000000; i++) &#123; alloc(); &#125; long e = System.currentTimeMillis(); System.out.println(e - b); &#125;&#125; JVM 参数为-server -Xmx10m -Xms10m -XX:-DoEscapeAnalysis -XX:+PrintGC, 运行结果JVM 参数为-server -Xmx10m -Xms10m -XX:+DoEscapeAnalysis -XX:+PrintGC, 运行结果 性能测试12345678910111213141516171819202122232425package me.stormma.gc;/** * &lt;p&gt;Created on 2017/4/21.&lt;/p&gt; * * @author stormma * * @description: &lt;p&gt;利用逃逸分析进行性能优化&lt;/p&gt; */public class EscapeAnalysisTest &#123; private static class Foo &#123; private int x; private static int counter; public Foo() &#123; x = (++counter); &#125; &#125; public static void main(String[] args) &#123; long start = System.nanoTime(); for (int i = 0; i &lt; 1000 * 1000 * 10; ++i) &#123; Foo foo = new Foo(); &#125; long end = System.nanoTime(); System.out.println("Time cost is " + (end - start)); &#125;&#125; 使用逃逸分析优化 JVM输出结果( -server -XX:+DoEscapeAnalysis -XX:+PrintGC)1Time cost is 11012345 未使用逃逸分析优化 JVM 输出结果( -server -Xmx10m -Xms10m -XX:-DoEscapeAnalysis -XX:+PrintGC)12345[GC (Allocation Failure) 33280K-&gt;408K(125952K), 0.0010344 secs][GC (Allocation Failure) 33688K-&gt;424K(125952K), 0.0009799 secs][GC (Allocation Failure) 33704K-&gt;376K(125952K), 0.0007297 secs][GC (Allocation Failure) 33656K-&gt;456K(159232K), 0.0014817 secs]Time cost is 68562263 分析结果，性能优化1/6。原文出处]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据区、代码区、栈区、堆区]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E5%8C%BA%E3%80%81%E4%BB%A3%E7%A0%81%E5%8C%BA%E3%80%81%E6%A0%88%E5%8C%BA%E3%80%81%E5%A0%86%E5%8C%BA.html</url>
    <content type="text"><![CDATA[概念 栈区(stack)：由系统的编译器自动的释放，主要用来存放 方法中的参数，一些临时的局部变量 等，并且方法中的参数一般在操作完后，会由编译器自动的释放掉。 堆区(heap)：由程序员决定，在Java中，如果程序员不释放的话，一般会由垃圾回收机制自动的清理掉。此区域主要存放： 创建的对象、动态申请的临时空间等 。 数据区(data seg)：也称 全局区或者静态区 ，存放 静态变量、全局变量等 都会存放到数据区，此区域上的东西都被全局所共享。比如我们通常采用 类名. 的方式就可以访问到方法，这就是所谓的静态方法，存放到数据区的。 代码区：存放程序编译后可以执行代码的地方。比如执行代码时写的While语句、if语句等，都会存放到此。 内存分析123456789// Person类class Person &#123; int id ; int age = 20 ; Person(int _id, int _age)&#123; id = _id ; age = _age ; &#125;&#125; 接下来对new一个对象进行分析执行语句： Person tom = new Person(1,25) ; 第一步：我们知道每一个类都有一个默认的构造函数，即Person(),因此上述会先调用默认的构造函数 第二步：执行构造函数New Person(1,25)时，我们知道调用的是Person(int _id, int _age)，所以此时栈空间会分配方法的参数的临时变量如下 第三步:执行构造函数 Person(int _id, int _age),id = _id; age = _age;此时栈中的临时变量会改变默认构造函数创建的对象,赋值完后，栈中的临时变量会自动的销毁，然后创建的临时变量Tom会自动的指向创建的对象。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java常用类和方法重点总结]]></title>
    <url>%2FJava%E5%B8%B8%E7%94%A8%E7%B1%BB%E5%92%8C%E6%96%B9%E6%B3%95%E9%87%8D%E7%82%B9%E6%80%BB%E7%BB%93.html</url>
    <content type="text"><![CDATA[简述Java中内存分配的问题 凡是new() 出来的东西，都是堆中进行分配的 局部变量【数据类型+变量名】都是在栈中进行分配的 静态变量【static】和字符串常量【”String”】都是在数据区进行分配的 方法【代码】都是在代码区进行存放的 简述Java中Object类的地位 Java中所有的类【自己定义的类以及Sun公司提供的类】都默认自动继承了Obeject类 Java中所有的类都从Object类中继承了toString()方法、hashCode()方法和equals等方法 简述Object类中toString()方法的注意事项 toString()方法的返回值是一个字符串 toString()方法返回的是类的名字和该对象的哈希码组成的一个字符串，即toString()方法返回的是 该对象的字符串表现形式 在Java中，System.out.println(类对象名)实际输出的是该对象的toString()方法返回的字符串，即括号中的内容等价于类对象名.toString()，toString()方法的好处 在碰到println方法的时候，会被自动调用，不用显示的写出来 ， eg: 12String str = new String();System.out.println(str) &lt;==&gt; System.out.println(str.toString()) ----&gt;toString()自动隐藏 建议所有的子类都重写从Object类中继承过来toString方法，否则toString方法的返回值没有什么实际含义（为什么要重写的原因） 简述Object类中equals()类方法的注意事项 equals方法的返回值为true或false Object类中equals方法只有在 两个对象是同一块内存区域时，即不但内容相同、地址还必须相同时，才返回true，否则即便内容相同、如果地址不同只会返回false 重写Object类中的equals方法目的在于：保证只要两个对象的内容相同，equals方法就返回true（为什么要重写的原因） 简述Object类中hashCode()方法的注意事项 哈希码原本指的是 内存空间地址的十六进制表示形式 hashCode()方法返回的是 **该对象的哈希码，即该对象的真实内存地址的十六进制表示形式，但是重写完hashCode()方法之后，返回的不再是该对象真实内存地址的十六进制表示形式 学习Java中toString方法、equals方法、hashCode方法共同的一个注意事项在Java中，凡是动态分配的内存都是没有名字的，而是将其地址赋给一个引用变量【引用】，用引用变量去代表这个事物，所以引用和动态分配的内存有本质上的区别，但是学习Java中的toString方法、equals方法和hashCode方法时默认引用和其指向的动态分配的内存是一个事物，不区分彼此 从逻辑上阐述为什么要重写equals方法和hashCode方法 （重点） 对于用户来说，逻辑上只要两个对象的内容相同，其地址以及这两个对象就应该相等，而要保证地址相同就应该重写hashCode方法，而要保证对象相同就应该重写equals方法 凡是Java中自带的类都已经重写了equals方法和hashCode方法，重写之后只要两个对象的内容相同，hashCode方法的返回值就相同，保证地址相同，equals方法就返回true，保证两个对象是同一个对象，而Java中凡是用户自己定义的类只能自己区重写这两个方法（ 为什么我们自己定义的类，要自己重写这两个方法，toString()也要重写 ）；【new Integer(1)与new Integer(2)】 简述String类中的equals方法与Object类中的equals方法的不同点 String类中的equals方法是用来判断两个对象的内容是否相同、而Object类中的equals方法是用来判断两个对象是否是同一个对象，所谓同一个对象指的是内存中的同一块存储空间 对于Java中StringBuffer类的由来 String类对象表示不可修改的UniCode编码字符串、即String类对象一旦创建就不可在更改，即只要创建一个字符串，就会重新分配一块内存空间，因此如果经常对字符串的内容进行修改而使用String类的话，就会造成空间以及时间的浪费，因此如果经常对字符串的内容进行修改的话，可以使用StringBuffer类，StringBuffer类可以一直对同一块内存空间进行操作，对一个字符串不断的进行修改，正因为StringBuffer类的这个优点，所以StringBuffer类中存在着大量修改字符串的方法，但是String类中却没有 简述String、StringBuffer类中常用的一个方法 valueOf():将基本类型数据转化为字符串 简述String类与StringBuffer类的关联 先使用StringBuffer类将字符串的内容不断的进行修改、最后将成品放到String类里面去 StringBuffer类中的toString方法可以将String类对象转化为StringBuffer类对象String str1 = str2.toString（）; 在Java中双引号括起来的字符串也可以被当做String类对象 如：“zhang”.length();]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给出一组年龄，用冒泡排序求最大年龄，最小年龄]]></title>
    <url>%2F%E7%BB%99%E5%87%BA%E4%B8%80%E7%BB%84%E5%B9%B4%E9%BE%84%EF%BC%8C%E6%B1%82%E6%9C%80%E5%A4%A7%E5%B9%B4%E9%BE%84%EF%BC%8C%E6%9C%80%E5%B0%8F%E5%B9%B4%E9%BE%84.html</url>
    <content type="text"><![CDATA[法一123456789101112131415161718//求出最大年龄,求出最小年龄,求出平均年龄 int[] ages = &#123;34,11,45,23,16&#125;; boolean flag = true ; for (int i = 4; flag &amp;&amp; i &gt;0 ; i--) &#123; flag = false ; for (int j = 0; j &lt; i; j++) &#123; if(ages[j]&gt;ages[j+1])&#123; int temp = ages[j]; ages[j] = ages[j+1]; ages[j+1] = temp ; flag = true ; &#125; &#125; System.out.println("-------"); &#125; for (int i = 0; i &lt; 5; i++) &#123; System.out.print(ages[i]+ "\t"); &#125; 法二12345678910111213141516171819int[] ages = &#123;1,2,3,4,5&#125;; boolean flag = true ; for (int i = 0; flag &amp;&amp; i &lt; ages.length-1; i++) &#123; flag = false ; for (int j = 0; j &lt; ages.length - i - 1; j++) &#123; if(ages[j] &gt; ages[j+1])&#123; int temp = ages[j]; ages[j] = ages[j+1]; ages[j+1] = temp ; flag = true ; &#125; &#125; System.out.println("-----"); &#125; for (int i = 0; i &lt; ages.length; i++) &#123; System.out.print(ages[i]+"\t"); &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不使用第三个变量交换两变量的值]]></title>
    <url>%2F%E4%B8%8D%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%89%E4%B8%AA%E5%8F%98%E9%87%8F%E4%BA%A4%E6%8D%A2%E4%B8%A4%E5%8F%98%E9%87%8F%E7%9A%84%E5%80%BC.html</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435/** 有两个整数 分别是 a=10 b=8 在不使用第三个变量的情况下 对其值进行交换*/class Work1&#123; public static void main(String[] args)&#123; int a = 10; int b = 8; /* a=a+b; b=a-b; a=a-b; a=a^b; b=a^b; a=a^b; 1010 1000 ---------- 0010 2 0010 1000 ----- 1010 10 0010 1010 ----- 1000 8 */ a=b+(b=a)*0; System.out.println("a="+a+",b="+b); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Java倒序输出12345]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8Java%E5%80%92%E5%BA%8F%E8%BE%93%E5%87%BA12345.html</url>
    <content type="text"><![CDATA[原理：%取最后一位，/取整除掉最后一位逐行输出123456int t = 12345 ;while(t&gt;0)&#123; int num = t%10; t /= 10 ; System.out.print(num); &#125; 一次性输出1234567891011121314int num = 12345; /* num%10 5 num/10%10 4 num/100%10 num/10/10%10 3 num/1000%10 num/10/10/10%10 2 num/10000%10 num/10/10/10/10%10 1 */ int result=0; while(num&gt;0)&#123; result = result*10+num%10; num/=10; &#125; System.out.print(result);]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发和并行的区别]]></title>
    <url>%2F%E5%B9%B6%E5%8F%91%E5%92%8C%E5%B9%B6%E8%A1%8C%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content type="text"><![CDATA[观点一并行（parallelise）同时刻（某点），并发（concurrency）同时间（某段） 观点二深入理解计算机系统CSAPP的回答。并发（Concurrency）是说进程B的开始时间是在进程A的开始时间与结束时间之间，我们就说A和B是并发的。并行（Parallel Execution）是并发的真子集，指同一时间两个进程运行在不同的机器上或者同一个机器不同的核心上。作者：starrynight链接：https://www.zhihu.com/question/33515481/answer/67962756来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 观点三如果某个系统支持两个或者多个动作（Action）同时存在，那么这个系统就是一个并发系统。如果某个系统支持两个或者多个动作同时执行，那么这个系统就是一个并行系统。并发系统与并行系统这两个定义之间的关键差异在于“存在”这个词。在并发程序中可以同时拥有两个或者多个线程。这意味着，如果程序在单核处理器上运行，那么这两个线程将交替地换入或者换出内存。这些线程是同时“存在”的——每个线程都处于执行过程中的某个状态。如果程序能够并行执行，那么就一定是运行在多核处理器上。此时，程序中的每个线程都将分配到一个独立的处理器核上，因此可以同时运行。我相信你已经能够得出结论——“并行”概念是“并发”概念的一个子集。也就是说，你可以编写一个拥有多个线程或者进程的并发程序，但如果没有多核处理器来执行这个程序，那么就不能以并行方式来运行代码。因此，凡是在求解单个问题时涉及多个执行流程的编程模式或者执行行为，都属于并发编程的范畴。摘自：《并发的艺术》 — 〔美〕布雷谢斯在豆瓣阅读书店查看：https://read.douban.com/ebook/10034459/ 作者：BeginMan链接：https://www.zhihu.com/question/33515481/answer/105348019来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 观点四并发：交替或者同时做不同事的能力并行：同时做不同事的能力 行话解释：并发：不同代码块交替或者同时执行的性能并行：不同代码块同时执行的性能 观点五作者：李运华链接：https://www.zhihu.com/question/33515481/answer/121050539来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 详细请参考： Concurrency vs. ParallelismConcurrency并发的反义词是顺序，concurrency vs sequential，例如： 顺序处理：你陪女朋友先看电影（Task1），看完后陪女朋友到花店买了一束花（Task2），然后陪女朋友去西餐厅吃烛光晚餐（Task3），这就是“顺序处理”，因为整个过程中只有你这一个处理器，事情只能一件一件的做（要么是你亲自做，要么你要等别人做）。Task1你要花2小时，Task2等花做好你要花30分钟，Task3等菜做好要30分钟，从你开始看电影到开始吃饭，全程需要3小时（假设走路不算时间）。 并发处理：你陪女朋友先看电影（Task1），同时打电话给花店预定一束花，花店安排人员在20：00送到西餐厅（Task2）；同时你打电话给西餐定预定20：00的浪漫烛光晚餐，西餐厅开始给你准备晚餐（Task3）；等到你电影看完跑到西餐厅，花也送到了，晚餐也准备好了，你跑过去直接献花吃饭然后开房即可，这就是并发处理。Task1还是2小时，但Task2和Task3也在这2小时完成了，从你开始看电影到开始吃饭，全程只需要2小时，3个任务是并发完成的。秘诀就是有3个处理器了：你、花店、餐厅在同一个时间段内都在做各自的任务。Parallelism并行的反义词是串行，Parallelism vs Serial，比如说给你一个100万的整形数组，挑出其中最小的值。 串行处理从数组的第一个开始扫描到最后一个，类似冒泡排序一样 并行处理将数组分为10组，每组10万个整形，同时扫描10组得到10个数值，然后再将这10个数值排列一下。上面这个简单的例子也可以看出，串行改为并行其实并不那么简单，涉及到任务分解（有先后依赖的任务就不能做到并行）、任务运行（可能要考虑互斥、锁、共享等）、结果合并。以Java的并行垃圾回收器Parallel为例，标记阶段、回收阶段各自可以多线程并行，但不能将回收阶段和标记阶段一起并行，因为回收阶段的处理依赖标记阶段的结果。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java实现九九乘法表]]></title>
    <url>%2FJava%E5%AE%9E%E7%8E%B0%E4%B9%9D%E4%B9%9D%E4%B9%98%E6%B3%95%E8%A1%A8.html</url>
    <content type="text"><![CDATA[关键难点：内循环的条件判断123456789public static void main(String[] args) &#123; //九九乘法表 for(int i = 1 ;i&lt;=9;i++)&#123; for(int j = 1;j&lt;=i;j++)&#123; System.out.print(j+"*"+i+"="+j*i+" "); &#125; System.out.println("\n"); &#125; &#125; 输出结果：]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中内存分配策略及堆和栈的比较]]></title>
    <url>%2Fjava%E4%B8%AD%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5%E5%8F%8A%E5%A0%86%E5%92%8C%E6%A0%88%E7%9A%84%E6%AF%94%E8%BE%83.html</url>
    <content type="text"><![CDATA[内存分配策略按照编译原理的观点,程序运行时的内存分配有三种策略,分别是静态的,栈式的,和堆式的.静态存储分配是指在编译时就能确定每个数据目标在运行时刻的存储空间需求,因而在编译时就可以给他们分配固定的内存空间.这种分配策略要求程序代码中不允许有可变数据结构(比如可变数组)的存在,也不允许有嵌套或者递归的结构出现,因为它们都会导致编译程序无法计算准确的存储空间需求.栈式存储分配也可称为动态存储分配,是由一个类似于堆栈的运行栈来实现的.和静态存储分配相反,在栈式存储方案中,程序对数据区的需求在编译时是完全未知的,只有到运行的时候才能够知道,但是规定在运行中进入一个程序模块时,必须知道该程序模块所需的数据区大小才能够为其分配内存.和我们在数据结构所熟知的栈一样,栈式存储分配按照先进后出的原则进行分配。静态存储分配要求在编译时能知道所有变量的存储要求,栈式存储分配要求在过程的入口处必须知道所有的存储要求,而堆式存储分配则专门负责在编译时或运行时模块入口处都无法确定存储要求的数据结构的内存分配,比如可变长度串和对象实例.堆由大片的可利用块或空闲块组成,堆中的内存可以按照任意顺序分配和释放. 堆和栈的比较上面的定义从编译原理的教材中总结而来,除静态存储分配之外,都显得很呆板和难以理解,下面撇开静态存储分配,集中比较堆和栈:从堆和栈的功能和作用来通俗的比较,堆主要用来存放对象的，栈主要是用来执行程序的.而这种不同又主要是由于堆和栈的特点决定的:在编程中，例如C/C++中，所有的方法调用都是通过栈来进行的,所有的局部变量,形式参数都是从栈中分配内存空间的。实际上也不是什么分配,只是从栈顶向上用就行,就好像工厂中的传送带(conveyor belt)一样,Stack Pointer会自动指引你到放东西的位置,你所要做的只是把东西放下来就行.退出函数的时候，修改栈指针就可以把栈中的内容销毁.这样的模式速度最快, 当然要用来运行程序了.需要注意的是,在分配的时候,比如为一个即将要调用的程序模块分配数据区时,应事先知道这个数据区的大小,也就说是虽然分配是在程序运行时进行的,但是分配的大小多少是确定的,不变的,而这个”大小多少”是在编译时确定的,不是在运行时.堆是应用程序在运行的时候请求操作系统分配给自己内存，由于从操作系统管理的内存分配,所以在分配和销毁时都要占用时间，因此用堆的效率非常低.但是堆的优点在于,编译器不必知道要从堆里分配多少存储空间，也不必知道存储的数据要在堆里停留多长的时间,因此,用堆保存数据时会得到更大的灵活性。事实上,面向对象的多态性,堆内存分配是必不可少的,因为多态变量所需的存储空间只有在运行时创建了对象之后才能确定.在C++中，要求创建一个对象时，只需用 new命令编制相关的代码即可。执行这些代码时，会在堆里自动进行数据的保存.当然，为达到这种灵活性，必然会付出一定的代价:在堆里分配存储空间时会花掉更长的时间！这也正是导致我们刚才所说的效率低的原因,看来列宁同志说的好,人的优点往往也是人的缺点,人的缺点往往也是人的优点(晕~). JVM中的堆和栈JVM是基于堆栈的虚拟机.JVM为每个新创建的线程都分配一个堆栈.也就是说,对于一个Java程序来说，它的运行就是通过对堆栈的操作来完成的。堆栈以帧为单位保存线程的状态。JVM对堆栈只进行两种操作:以帧为单位的压栈和出栈操作。我们知道,某个线程正在执行的方法称为此线程的当前方法.我们可能不知道,当前方法使用的帧称为当前帧。当线程激活一个Java方法,JVM就会在线程的 Java堆栈里新压入一个帧。这个帧自然成为了当前帧.在此方法执行期间,这个帧将用来保存参数,局部变量,中间计算过程和其他数据.这个帧在这里和编译原理中的活动纪录的概念是差不多的.从Java的这种分配机制来看,堆栈又可以这样理解:堆栈(Stack)是操作系统在建立某个进程时或者线程(在支持多线程的操作系统中是线程)为这个线程建立的存储区域，该区域具有先进后出的特性。个Java应用都唯一对应一个JVM实例，每一个实例唯一对应一个堆。应用程序在运行中所创建的所有类实例或数组都放在这个堆中,并由应用所有的线程共享.跟C/C++不同，Java中分配堆内存是自动初始化的。Java中所有对象的存储空间都是在堆中分配的，但是这个对象的引用却是在堆栈中分配,也就是说在建立一个对象时从两个地方都分配内存，在堆中分配的内存实际建立这个对象，而在堆栈中分配的内存只是一个指向这个堆对象的指针(引用)而已。JVM运行时，将内存分为堆和栈，堆中存放的是创建的对象，JAVA字符串对象内存实现时，在堆中开辟了一快很小的内存，叫字符串常量池，用来存放特定的字符串对象。关于String对象的创建，两种方式是不同的，第一种不用new的简单语法，即String s1=”JAVA”;创建步骤是先看常量池中有没有与”JAVA”相同的的字符串对象，如果有，将s1指向该对象，若没有，则创建一个新对象，并让s1指向它。第二种是new语法String s2=”JAVA”;这种语法是在堆而不是在常量池中创建对象，并将s2指向它，然后去字符串常量池中看看，是否有与之相同的内容的对象，如果有，则将new出来的字符串对象与字符串常量池中的对象联系起来，如果没有，则在字符串常量池中再创建一个包含该内容的字符串对象，并将堆内存中的对象与字符串常量池中新建出来的对象联系起来。这就是字符串的一次投入，终生回报的内存机制，对字符串的比较带来好处。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java内存机制和内存地址]]></title>
    <url>%2FJava%E5%86%85%E5%AD%98%E6%9C%BA%E5%88%B6%E5%92%8C%E5%86%85%E5%AD%98%E5%9C%B0%E5%9D%80.html</url>
    <content type="text"><![CDATA[问题一：String str1 = “abc”;String str2 = “abc”;System.out.println(str1==str2); //true 问题二：String str1 =new String (“abc”);String str2 =new String (“abc”);System.out.println(str1==str2); // false 问题三：String s1 = “ja”;String s2 = “va”;String s3 = “java”;String s4 = s1 + s2;System.out.println(s3 == s4);//falseSystem.out.println(s3.equals(s4));//true由于以上问题让我含糊不清，于是特地搜集了一些有关java内存分配的资料,以下是网摘： Java 中的堆和栈Java把内存划分成两种：一种是栈内存，一种是堆内存。在函数中定义的一些基本类型的变量和对象的引用变量都在函数的栈内存中分配。当在一段代码块定义一个变量时，Java就在栈中为这个变量分配内存空间，当超过变量的作用域后，Java会自动释放掉为该变量所分配的内存空间，该内存空间可以立即被另作他用。堆内存用来存放由new创建的对象和数组。在堆中分配的内存，由Java虚拟机的自动垃圾回收器来管理。在堆中产生了一个数组或对象后，还可以在栈中定义一个特殊的变量，让栈中这个变量的取值等于数组或对象在堆内存中的首地址，栈中的这个变量就成了数组或对象的引用变量。引用变量就相当于是为数组或对象起的一个名称，以后就可以在程序中使用栈中的引用变量来访问堆中的数组或对象。 具体的说：栈与堆都是Java用来在Ram中存放数据的地方。与C++不同，Java自动管理栈和堆，程序员不能直接地设置栈或堆。Java的堆是一个运行时数据区,类的(对象从中分配空间。这些对象通过new、newarray、anewarray和multianewarray等指令建立，它们不需要程序代码来显式的释放。堆是由垃圾回收来负责的，堆的优势是可以动态地分配内存大小，生存期也不必事先告诉编译器，因为它是在运行时动态分配内存的，Java的垃圾收集器会自动收走这些不再使用的数据。但缺点是，由于要在运行时动态分配内存，存取速度较慢。栈的优势是，存取速度比堆要快，仅次于寄存器，栈数据可以共享。但缺点是，存在栈中的数据大小与生存期必须是确定的，缺乏灵活性。栈中主要存放一些基本类型的变量（,int, short, long, byte, float, double, boolean, char）和对象句柄。栈有一个很重要的特殊性，就是存在栈中的数据可以共享。假设我们同时定义：int a = 3;int b = 3；编译器先处理int a = 3；首先它会在栈中创建一个变量为a的引用，然后查找栈中是否有3这个值，如果没找到，就将3存放进来，然后将a指向3。接着处理int b = 3；在创建完b的引用变量后，因为在栈中已经有3这个值，便将b直接指向3。这样，就出现了a与b同时均指向3的情况。这时，如果再令a=4；那么编译器会重新搜索栈中是否有4值，如果没有，则将4存放进来，并令a指向4；如果已经有了，则直接将a指向这个地址。因此a值的改变不会影响到b的值。要注意这种数据的共享与两个对象的引用同时指向一个对象的这种共享是不同的，因为这种情况a的修改并不会影响到b, 它是由编译器完成的，它有利于节省空间。而一个对象引用变量修改了这个对象的内部状态，会影响到另一个对象引用变量。String是一个特殊的包装类数据。可以用：String str = new String(“abc”);String str = “abc”;两种的形式来创建，第一种是用new()来新建对象的，它会在存放于堆中。每调用一次就会创建一个新的对象。而第二种是先在栈中创建一个对String类的对象引用变量str，然后查找栈中有没有存放”abc”，如果没有，则将”abc”存放进栈，并令str指向”abc”，如果已经有”abc” 则直接令str指向“abc”。 比较类里面的数值是否相等时，用equals()方法；当测试两个包装类的引用是否指向同一个对象时，用==，下面用例子说明上面的理论。 方式一：String str1 = “abc”;String str2 = “abc”;System.out.println(str1==str2); //true可以看出str1和str2是指向同一个对象的。 方式二：String str1 =new String (“abc”);String str2 =new String (“abc”);System.out.println(str1==str2); // false用new的方式是生成不同的对象。每一次生成一个。因此用第一种方式创建多个”abc”字符串,在内存中其实只存在一个对象而已. 这种写法有利与节省内存空间. 同时它可以在一定程度上提高程序的运行速度，因为JVM会自动根据栈中数据的实际情况来决定是否有必要创建新对象。而对于String str = new String(“abc”)；的代码，则一概在堆中创建新对象，而不管其字符串值是否相等，是否有必要创建新对象，从而加重了程序的负担。另一方面, 要注意: 我们在使用诸如String str = “abc”；的格式定义类时，总是想当然地认为，创建了String类的对象str。担心陷阱！对象可能并没有被创建！而可能只是指向一个先前已经创建的对象。只有通过new()方法才能保证每次都创建一个新的对象。由于String类的immutable性质，当String变量需要经常变换其值时，应该考虑使用StringBuffer类，以提高程序效率。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java语言基础]]></title>
    <url>%2FJava%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80.html</url>
    <content type="text"><![CDATA[1、 java程序的组成：关键字，标识符，注释，变量，语句，表达式，数组，方法 2、 关键字：Java语言内部使用了的一些用于特殊用途的词汇，那么在程序中用户不能使用。语言本身保留了一些词汇用于语言的语法等用途。 3、 已用到的关键字：class 声明一个类。public: 表示该类可以被外界调用 如果一个类被声明为public,那么该类所在的文件名要和类名一致。编译后的文件名和类名相同。static:表示是静态的。void：表示没有返回值 4、 关键字都是小写的。5、 标识符：标识符就是用户在程序自定义使用的一些名词。在程序中标识符可以用来表示类名，变量名，方法名，参数名等。在java中标识符的组成：大小写字母，数字，下划线以及$组成。不能以数字开头。标识符通常要有意义。不能随便乱取。建议使用英文表示。类名：以大写字母开头，所有单词首字母大写。采用的Pascal命名规则变量名：第一个单词首字母以小写开头，其后的所有单词首字母以大写开头。采用的驼峰命名规则。 6、 注释：在程序中，方便人们阅读代码而写的一些说明文字。在java中有3类注释：单行注释：用// 开头多行注释：用/开头 /结尾文档注释：用/* 开头 /结尾，并且在其中会用到一些java定义好的注解来声明注意：单行注释和多行注释不会被编译 也不会被jvm解释执行，文档注释会编译，可以通过jvm生成对应的html文档,方便用户查询使用。 7、 总结：第一条：常用的关键字要熟悉，能够记住。第二条：标识符的命名要符合规范。第三条：程序一定要注释。建议先写注释，再写代码。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变量和常量]]></title>
    <url>%2F%E5%8F%98%E9%87%8F%E5%92%8C%E5%B8%B8%E9%87%8F.html</url>
    <content type="text"><![CDATA[1、为什么需要变量？方便对一个数据的修改和使用。如：一个数据在多处使用时，如果要修改，那么多个地方同时都需要修改，这个需要一个变量用于存储数据，在使用时，直接使用变量即可，这样当修改数据时，只需要修改变量本身的值就可以了。 2、什么是变量？在java中，变量是一个存储空间的表示。 3、使用变量的语法：数据类型 变量名 = 值 4、数据类型基本可以分为两类：基本数据类型和引用数据类型基本数据类型：整数、小数、布尔、字符 整数：byte:表示是字节 占用的内存空间是1字节short:表示是短整形 占用的内存空间是2字节int:表示整形 占用的内存空间是4字节long:表示长整形 占用的内存空间是8字节 小数：float：单精度浮点数 占用的内存空间是4字节double:双精度浮点数 占用的内存空间是8字节 布尔：boolean:布尔类型的值 只有true和false 1字节 字符：用于表示单个字符char:2个字节 引用数据类型：class ,interface,数组1234567891011121314public static void main(String[] args)&#123; byte age = 125 ; short t = 23 ; char ch = '中'; boolean flag = true ; float score = 98.5f ; double avgScore = 80.4 ; System.out.println(age); System.out.println(t); System.out.println(ch); System.out.println(flag); System.out.println(score); System.out.println(avgScore);&#125; 整数默认是 int,小数默认是 double单精度后面的 f 表示是单精度浮点，需要明确出来，可以 f 或者 F . 5、5. 整数在java中有4种表现形式：二进制：是由数字0和1组成的。八进制：是由数字0到7组成的。十进制：是由数字0到9组成的。十六进制：是由数字0到9和ABCDEF组成。进制之间转换： 6、 变量名：命名规则采用驼峰命名法。7、 数据类型的转换：自动类型转换：强制类型转换：类型提升：当是byte，short，char 进行运算时会自动提升为int。当有float类型，提升为float类型当有double类型，提升为double类型当有long类型，提升为long类型 8、 字符类型：在java中，char用来表示一个字符，一个字符也就是一个字母,用单引号括起来。’A’；也可以用char来表示一个中文。表示中文时特别需要注意编码问题。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java入门]]></title>
    <url>%2FJava%E5%85%A5%E9%97%A8.html</url>
    <content type="text"><![CDATA[1. Java:是SUN(Starfard University Network)公司在1995年开发了一门完全面向对象的，开源的高级编程语言。2. Java发展历史：1995 java诞生1996 jdk1.0发布1998 jdk1.2 发布 J2SE(1.2,1.3,1.4)2004 jdk1.5 发布 JAVASE52014 jdk1.8 发布 JAVASE8 3. Java的优势：跨平台一次编写 多次运行Java是运行在JVM（Java virtual machine）之上的。为不同平台下开发不同的JVM。所有JVM对java语言本身的规范是一样的。 4. Java的版本：JavaSE：java standard edition java标准版 是Java的基础。JavaME：java micro edition :移动端，小型设备，PDA等JavaEE：java enterprise edition：java的企业级版本，java web等 5. JVM,JRE,JDKJvm java virtual machine java虚拟机 运行java程序Jre java runtime environment java运行时环境Jdk java development kit java 开发工具包JDK—&gt;JRE—-&gt;JVM 6. 下载jdk来安装：官方下载地址指定安装位置后 傻瓜式安装在命令行运行时： 需要配置环境变量。在dos中执行命令，操作系统会根据系统的环境变量Path去寻找对应的可执行程序（.exe,.bat）。 7. Path变量的配置计算机 右键 属性 高级系统设置 环境变量添加一个JAVA_HOME变量 值为jdk的安装目录在Path变量中的前面添加%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin;注意：使用分号对每个路径进行分割，都是英文下的分号 8. 编写第一个HelloWorld应用程序：所有java程序都是以.java结尾的。class HelloWorld{ public static void main(String[] args){ System.out.println(“hello world”); }}保存为HelloWorld.java文件 9. 编写好的应用程序,也就是以.java结尾的文件，称为源文件。编写好的源文件需要通过javac命令进行编译，编译的目的是为了让jvm可以认识并且执行。编译后会生成一个.class文件，该文件称为字节码文件，能够被jvm认识并且执行。运行程序：使用java命令进行运行。运行的是.class文件 10. Java程序的编写流程：编写源文件——–&gt;通过javac命令编译源文件——–&gt;通过java命令执行字节码文件。Javac编译时 需要跟上后缀.java,而java命令后直接跟文件即可 不需要后缀。 11. HelloWorld程序详解：12. classpath环境变量：配置path环境变量的目的是在任意路径下都可以执行java,javac命令配置classpath的目的是为了在任意路径下都可以执行.class文件。所谓的classpath指定的是.class文件所在的位置。classpath不设置的时候：java命令会在当前路径下查找.class，如果找不到 如果设置了classpath，那么会从classpath指定的路径去查找.class如果classpath的值后面不加分号：查找的classpath指定的路径下是否有.class文件，如果加了分号：先查询classpath指定的路径，再查找当前目录下是否有.class文件。注意：通常配置classpath会以.;开头，表示先查询的是当前路径CLASSPATH=.;%JAVA_HOME%\lib\tools.jar;%JAVA_HOME%\lib\dt.jar;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机的基础]]></title>
    <url>%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%9F%BA%E7%A1%80.html</url>
    <content type="text"><![CDATA[1、计算机的组成：硬件和软件2、硬件：物理设备（主板、CPU、显示器、存储设备、外设、内存条）3、没有软件的计算机：裸机4、软件：按照一定顺序和逻辑组成的计算机指令——-程序、软件。软件 = 程序 + 数据结构。 5、软件开发：制作软件的过程。6、人机交互：命令（dos）、图形界面。7、计算机语言：人和计算机交流的一种方式。8、计算机语言的分类：a) 机器语言b) 汇编语言c) 高级语言：c , c++ , java , scala , python越高级的语言越接近人们的习惯。越高级的语言执行效率是越低的。 9、操作系统10、了解一些常见的dos命令：win+r —-&gt;运行a) dir 显示当前目录信息b) cd.. 回到上一级目录c) cd. 到当前目录d) cd/ 到根目录e) D: 到D盘f) cd path — &gt; 到指定的目录，不能跳盘符g) md 目录名 — &gt; 新建一个目录 make directoryh) ren oldname newname —–&gt; 重命名 renamei) rd 目录名 —-&gt;删除目录 remove directoryj) move 文件 目标目录 —–&gt; 移动文件k) notepad 打开记事本l) copy 源文件 目标文件 —–&gt; 复制文件m) del 文件名 —-&gt; 删除文件n) cls 清屏]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL备份]]></title>
    <url>%2FMySQL%E5%A4%87%E4%BB%BD.html</url>
    <content type="text"><![CDATA[定时完成数据库的备份① 手动备份数据库(表的)方法cmd控制台:mysqldump –u root –proot 数据库 [表名1 表名2..] &gt; 文件路径比如: 把temp数据库备份到 d:\temp.bakmysqldump –u root –proot temp &gt; d:\temp.bak如果你希望备份是，数据库的某几张表mysqldump –u root –prot temp dept &gt; d:\temp.dept.bak 如何使用备份文件恢复我们的数据.mysql控制台source d:\temp.dept.bak ② 使用定时器来自定完成把备份数据库的指令，写入到 bat文件, 然后通过任务管理器去定时调用 bat文件.mytask.bat 内容是:C:\myenv\mysql5.5.27\bin\mysqldump -u root -proot temp dept &gt; d:\temp.dept.bak☞ 如果你的mysqldump.exe文件路径有空格，则一定要使用 “” 包括.把mytask.bat 做成一个任务，并定时调用在 2:00 调用一次步骤 任务计划-&gt;增加一个任务，选中你的mytask.bat文件 ，最后配置:测试ok 如何在linux下完成定时任务:linux如何备份. 直接执行PHP脚本, 需要在同一个服务器上执行.crontab -e00 /usr/local/bin/php /home/htdocs/phptimer.php2.通过HTTP请求来触发脚本, PHP文件允许不在同一服务器上crontab -e00 /usr/bin/wget -q -O temp.txt http://www.phptimer.com/phptimer.php上面是通过wget来请求PHP文件, PHP输出会保存在临时文件temp.txt中crontab -e00 /usr/bin/curl -o temp.txt http://www.phptimer.com/phptimer.php上面是通过curl -o来请求PHP文件, PHP输出会保存在临时文件temp.txt中crontab -e00 lynx -dump http://www.phptimer.com/phptimer.php上面是通过Lynx文本浏览器来请求PHP文件 分表技术分表技术有(水平分割和垂直分割)当一张越来越大时候，即使添加索引还慢的话，我们可以使用分表以qq用户表来具体的说明一下分表的操作.思路如图 ：首先我创建三张表 user0 / user1 /user2 , 然后我再创建 uuid表，该表的作用就是提供自增的id,走代码:123456789101112131415161718192021create table user0(id int unsigned primary key ,name varchar(32) not null default '',pwd varchar(32) not null default '')engine=myisam charset utf8;create table user1(id int unsigned primary key ,name varchar(32) not null default '',pwd varchar(32) not null default '')engine=myisam charset utf8;create table user2(id int unsigned primary key ,name varchar(32) not null default '',pwd varchar(32) not null default '')engine=myisam charset utf8;create table uuid(id int unsigned primary key auto_increment)engine=myisam charset utf8; 思考: 如果我们做的是一个平安保险公司的一个订单(8999999999000000条)查询功能更.,如何处理海量表?-&gt;按时间. 分表的标准是依赖业务逻辑(时间/地区/….) 安装字符不同. a-z 我们给用户提供的查询界面一定是有条件，不能让用户进行大范围.(世界)，如果需要的可以根据不同的规则，对应多套分表. 检索时候，带分页条件，减少返回的数据. 项目中，灵活的根据需求来考虑. 垂直分割如果一张表某个字段，信息量大，但是我们很少查询，则可以考虑把这些字段，单独的放入到一张表中，这种方式称为垂直分割.]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intellij IDEA 激活方法]]></title>
    <url>%2FIntellij%20IDEA%20%E6%BF%80%E6%B4%BB%E6%96%B9%E6%B3%95.html</url>
    <content type="text"><![CDATA[填入下面的license server: http://intellij.mandroid.cn/ http://idea.imsxm.com/ http://idea.iteblog.com/key.php]]></content>
      <categories>
        <category>有料</category>
      </categories>
      <tags>
        <tag>编程工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL优化（二）]]></title>
    <url>%2FMySQL%E4%BC%98%E5%8C%96%EF%BC%88%E4%BA%8C%EF%BC%89.html</url>
    <content type="text"><![CDATA[四种索引(主键索引/唯一索引/全文索引/普通索引)1. 添加1.1主键索引添加当一张表，把某个列设为主键的时候，则该列就是主键索引create table aaa (id int unsigned primary key auto_increment , name varchar(32) not null defaul ‘’);这是id 列就是主键索引.如果你创建表时，没有指定主键索引，也可以在创建表后，在添加, 指令:alter table 表名 add primary key (列名);举例:create table bbb (id int , name varchar(32) not null default ‘’);alter table bbb add primary key (id); 1.2普通索引一般来说，普通索引的创建，是先创建表，然后在创建普通索引比如:create table ccc(id int unsigned,name varchar(32))create index 索引名 on 表 (列1,列名2); 1.3创建全文索引全文索引，主要是针对对文件，文本的检索, 比如文章, 全文索引针对MyISAM有用.创建 ：CREATE TABLE articles ( id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY, title VARCHAR(200), body TEXT, FULLTEXT (title,body) )engine=myisam charset utf8; INSERT INTO articles (title,body) VALUES (‘MySQL Tutorial’,’DBMS stands for DataBase …’), (‘How To Use MySQL Well’,’After you went through a …’), (‘Optimizing MySQL’,’In this tutorial we will show …’), (‘1001 MySQL Tricks’,’1. Never run mysqld as root. 2. …’), (‘MySQL vs. YourSQL’,’In the following database comparison …’), (‘MySQL Security’,’When configured properly, MySQL …’); 如何使用全文索引:错误用法:select from articles where body like ‘%mysql%’; 【不会使用到全文索引】证明:explain select from articles where body like ‘%mysql%’ 正确的用法是:select * from articles where match(title,body) against(‘database’); 【可以】 ☞ 说明: 在mysql中fulltext 索引只针对 myisam生效 mysql自己提供的fulltext针对英文生效-&gt;sphinx (coreseek) 技术处理中文 使用方法是 match(字段名..) against(‘关键字’) 全文索引一个 叫 停止词, 因为在一个文本中，创建索引是一个无穷大的数，因此，对一些常用词和字符，就不会创建，这些词，称为停止词. 1.4唯一索引①当表的某列被指定为unique约束时，这列就是一个唯一索引create table ddd(id int primary key auto_increment , name varchar(32) unique);这时, name 列就是一个唯一索引.unique字段可以为NULL,并可以有多NULL, 但是如果是具体内容，则不能重复.主键字段，不能为NULL,也不能重复.②在创建表后，再去创建唯一索引create table eee(id int primary key auto_increment, name varchar(32));create unique index 索引名 on 表名 (列表..); 2. 查询索引desc 表名 【该方法的缺点是： 不能够显示索引名.】show index(es) from 表名show keys from 表名 ### 3. 删除 alter table 表名 drop index 索引名;如果删除主键索引。alter table 表名 drop primary key [这里有一个小问题] 4. 修改先删除，再重新创建.为什么创建索引后，速度就会变快? 索引使用的注意事项索引的代价: 占用磁盘空间 对dml操作有影响，变慢 在哪些列上适合添加索引?总结: 满足以下条件的字段，才应该创建索引.a: 肯定在where条经常使用 b: 该字段的内容不是唯一的几个值(sex) c: 字段内容不是频繁变化. 使用索引的注意事项把dept表中，我增加几个部门:alter table dept add index myind (dname,loc); // dname 左边的列,loc就是右边的列说明，如果我们的表中有复合索引(索引作用在多列上)， 此时我们注意:1， 对于创建的多列索引，只要查询条件使用了最左边的列，索引一般就会被使用。explain select from dept where loc=’aaa’\G就不会使用到索引2，对于使用like的查询，查询如果是 ‘%aaa’ 不会使用到索引 ‘aaa%’ 会使用到索引。比如: explain select from dept where dname like ‘%aaa’\G不能使用索引，即，在like查询时，关键的 ‘关键字’ , 最前面，不能使用 % 或者 这样的字符.， 如果一定要前面有变化的值，则考虑使用 全文索引-&gt;sphinx. 如果条件中有or，即使其中有条件带索引也不会使用。换言之，就是要求使用的所有字段，都必须建立索引, 我们建议大家尽量避免使用or 关键字select * from dept where dname=’xxx’ or loc=’xx’ or deptno=45 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来。否则不使用索引。(添加时,字符串必须’’), 也就是，如果列是字符串类型，就一定要用 ‘’ 把他包括起来. 如果mysql估计使用全表扫描要比使用索引快，则不使用索引。explain 可以帮助我们在不真正执行某个sql语句时，就执行mysql怎样执行，这样利用我们去分析sql指令. 如何查看索引使用的情况:show status like ‘Handler_read%’;大家可以注意：handler_read_key:这个值越高越好，越高表示使用索引查询到的次数。 handler_read_rnd_next:这个值越高，说明查询低效。 sql语句的小技巧 在使用group by 分组查询是，默认分组后，还会排序，可能会降低速度.比如:在group by 后面增加 order by null 就可以防止排序. 有些情况下，可以使用连接来替代子查询。因为使用join，MySQL不需要在内存中创建临时表。select from dept, emp where dept.deptno=emp.deptno; [简单处理方式]select from dept left join emp on dept.deptno=emp.deptno; [左外连接，更ok!] 如何选择mysql的存储引擎在开发中，我们经常使用的存储引擎 myisam / innodb/ memorymyisam 存储: 如果表对事务要求不高，同时是以查询和添加为主的，我们考虑使用myisam存储引擎. ,比如 bbs 中的 发帖表，回复表.INNODB 存储: 对事务要求高，保存的数据都是重要数据，我们建议使用INNODB,比如订单表，账号表. 问 MyISAM 和 INNODB的区别 事务安全 查询和添加速度 支持全文索引 锁机制 外键 MyISAM 不支持外键， INNODB支持外键. (在PHP开发中，通常不设置外键，通常是在程序中保证数据的一致)Memory 存储，比如我们数据变化频繁，不需要入库，同时又频繁的查询和修改，我们考虑使用memory, 速度极快. 如果你的数据库的存储引擎是myisam,请一定记住要定时进行碎片整理举例说明:create table test100(id int unsigned ,name varchar(32))engine=myisam;insert into test100 values(1,’aaaaa’);insert into test100 values(2,’bbbb’);insert into test100 values(3,’ccccc’);我们应该定义对myisam进行整理optimize table test100;mysql_query(“optimize tables $表名”);]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL优化（一）]]></title>
    <url>%2FMySQL%E4%BC%98%E5%8C%96%EF%BC%88%E4%B8%80%EF%BC%89.html</url>
    <content type="text"><![CDATA[MySQL数据库的优化技术对MySQL优化时一个综合性的技术，主要包括1、表的设计合理化（符合 3NF）2、添加适当索引(index)：普通索引、主键索引、唯一索引(unique)、全文索引3、分表技术（水平分割、垂直分割）4、读写分离5、存储过程（模块化编程，可以提供读写速度）6、对MySQL配置优化（配置最大并发数my.ini，调整缓存大小）7、升级MySQL服务器硬件8、定时清楚不需要的数据，定时进行碎片处理（MyISAM） 什么样的表才是符合3NF表的范式，是首先符合1NF，才能满足2NF，进一步满足3NF1NF：即表的列具有原子性，不可再分解，即列的信息，不能分解，只要数据库是关系型数据库（mysql/oracle/db2/infomix/sysbase/sql server），就自动满足1NF 数据库的分类关系型数据库：MySQL/oracle/db2/infomix/sysbase/sql server非关系型数据库：（特点：面向对象或者集合）NoSQL数据库：MongoDB(特点是面向文档)2NF：表中的记录是唯一的，就满足2NF，通常我们设计一个主键来实现3NF：即表中不要冗余数据，就是说，表的信息，如果能够被推导出来，就不应该单独的设计一个字段来存放。 SQL语句本身的优化问题是： 如何从一个大项目中，迅速的定位执行速度慢的语句. (定位慢查询)① 首先我们了解mysql数据库的一些运行状态如何查询(比如想知道当前mysql运行的时间/一共执行了多少次select/update/delete.. / 当前连接)show status常用的:show status like ‘uptime’ ;show stauts like ‘com_select’ show stauts like ‘com_insert’ …类推 update delete☞ show [session|global] status like …. 如果你不写 [session|global] 默认是session 会话，指取出当前窗口的执行，如果你想看所有(从mysql 启动到现在，则应该 global)show status like ‘connections’;//显示慢查询次数show status like ‘slow_queries’; ② 如何去定位慢查询构建一个大表(400 万)-&gt; 存储过程构建默认情况下，mysql认为10秒才是一个慢查询. 修改mysql的慢查询.show variables like ‘long_query_time’ ; //可以显示当前慢查询时间set long_query_time=1 ;//可以修改慢查询时间 构建大表-&gt;大表中记录有要求, 记录是不同才有用，否则测试效果和真实的相差大.创建:1234567891011121314151617181920212223CREATE TABLE dept( /*部门表*/deptno MEDIUMINT UNSIGNED NOT NULL DEFAULT 0, /*编号*/dname VARCHAR(20) NOT NULL DEFAULT "", /*名称*/loc VARCHAR(13) NOT NULL DEFAULT "" /*地点*/) ENGINE=MyISAM DEFAULT CHARSET=utf8 ;CREATE TABLE emp(empno MEDIUMINT UNSIGNED NOT NULL DEFAULT 0, /*编号*/ename VARCHAR(20) NOT NULL DEFAULT "", /*名字*/job VARCHAR(9) NOT NULL DEFAULT "",/*工作*/mgr MEDIUMINT UNSIGNED NOT NULL DEFAULT 0,/*上级编号*/hiredate DATE NOT NULL,/*入职时间*/sal DECIMAL(7,2) NOT NULL,/*薪水*/comm DECIMAL(7,2) NOT NULL,/*红利*/deptno MEDIUMINT UNSIGNED NOT NULL DEFAULT 0 /*部门编号*/)ENGINE=MyISAM DEFAULT CHARSET=utf8 ;CREATE TABLE salgrade(grade MEDIUMINT UNSIGNED NOT NULL DEFAULT 0,losal DECIMAL(17,2) NOT NULL,hisal DECIMAL(17,2) NOT NULL)ENGINE=MyISAM DEFAULT CHARSET=utf8; 测试数据：12345INSERT INTO salgrade VALUES (1,700,1200);INSERT INTO salgrade VALUES (2,1201,1400);INSERT INTO salgrade VALUES (3,1401,2000);INSERT INTO salgrade VALUES (4,2001,3000);INSERT INTO salgrade VALUES (5,3001,9999); 为了存储过程能够正常执行，我们需要把命令执行结束符修改123456789101112131415delimiter $$create function rand_string(n INT) returns varchar(255) #该函数会返回一个字符串begin #chars_str定义一个变量 chars_str,类型是 varchar(100),默认值'abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ'; declare chars_str varchar(100) default 'abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ'; declare return_str varchar(255) default ''; declare i int default 0; while i &lt; n do set return_str =concat(return_str,substring(chars_str,floor(1+rand()*52),1)); set i = i + 1; end while; return return_str; end $$ 创建一个存储过程:123456789101112create procedure insert_emp(in start int(10),in max_num int(10))begindeclare i int default 0; #set autocommit =0 把autocommit设置成0 set autocommit = 0; repeat set i = i + 1; insert into emp values ((start+i) ,rand_string(6),'SALESMAN',0001,curdate(),2000,400,rand_num()); until i = max_num end repeat; commit; end $$ #调用刚刚写好的函数, 1800000条记录,从100001号开始call insert_emp(100001,4000000);③ 这时我们如果出现一条语句执行时间超过1秒中，就会统计到.④ 如果把慢查询的sql记录到我们的一个日志中在默认情况下，我们的mysql不会记录慢查询，需要在启动mysql时候，指定记录慢查询才可以bin\mysqld.exe - -safe-mode - -slow-query-log [mysql5.5 可以在my.ini指定]bin\mysqld.exe –log-slow-queries=d:/abc.log [低版本mysql5.0可以在my.ini指定] 先关闭mysql,再启动, 如果启用了慢查询日志，默认把这个文件放在my.ini 文件中记录的位置 #Path to the database rootdatadir=”C:/Documents and Settings/All Users/Application Data/MySQL/MySQL Server 5.5/Data/“ ⑤ 测试,可以看到在日志中就记录下我们的mysql慢sql语句.优化问题.通过 explain 语句可以分析，mysql如何执行你的sql语句]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何阅读源代码]]></title>
    <url>%2F%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E6%BA%90%E4%BB%A3%E7%A0%81.html</url>
    <content type="text"><![CDATA[阅读别人的代码作为开发人员是一件经常要做的事情。一个是学习新的编程语言的时候通过阅读别人的代码是一个最好的学习方法，另外是积累编程经验。如果你有机会阅读一些操作系统的代码会帮助你理解一些基本的原理。还有就是在你作为一个质量保证人员或一个小领导的时候如果你要做白盒测试的时候没有阅读代码的能力是不能完成相应的任务。最后一个就是如果你中途接手一个项目的时候或给一个项目做售后服务的时候是要有阅读代码的能力的。 收集所有可能收集的材料 阅读代码要做的第一件事情是收集所有和项目相关的资料。比如你要做一个项目的售后服务，那么你首先要搞明白项目做什么用的，那么调研文档、概要设计文档、详细设计文档、测试文档、使用手册都是你要最先搞到手的。如果你是为了学习那么尽量收集和你的学习有关的资料，比如你想学习Linux的文件系统的代码，那最好要找到linux的使用手册、以及文件系统设计的方法、数据结构的说明。(这些资料在书店里都可以找到)。 材料的种类分为几种类型基础资料。 比如你阅读turbo c2的源代码你要有turbo c2的函数手册，使用手册等专业书籍，msc 6.0或者Java 的话不但要有函数手册，还要有类库函数手册。这些资料都是你的基础资料。另外你要有一些关于uml的资料可以作为查询手册也是一个不错的选择 和程序相关的专业资料。 每一个程序都是和相关行业相关的。比如我阅读过一个关于气象分析方面的代码，因为里边用到了一个复杂的数据转换公式，所以不得不把自己的大学时候课本 找出来来复习一下高等数学的内容。如果你想阅读linux的文件管理的代码，那么找一本讲解linux文件系统的书对你的帮助会很大。 相关项目的文档资料 这一部分的资料分为两种，一个相关行业的资料，比如你要阅读一个税务系统的代码那么有一些财务/税务系统的专业资料和国家的相关的法律、法规的资料是 必不可少的。此外就是关于这个项目的需求分析报告、概要设计报告、详细设计报告，使用手册、测试报告等，尽量多收集对你以后的代码阅读是很重要的 知识准备 了解基础知识，不要上来就阅读代码，打好基础可以做到事半功倍的效果 留备份,构造可运行的环境 代码拿到手之后的第一件事情是先做备份，最好是刻在一个光盘上，在代码阅读的时候一点不动代码是很困难的一件事情，特别是你要做一些修改性或增强性维护的时候。而一旦做修改就可能发生问题，到时候要恢复是经常发生的事情，如果你不能很好的使用版本控制软件那么先留一个备份是一个最起码的要求了。 在做完备份之后最好给自己构造一个可运行的环境，当然可能会很麻烦，但可运行代码和不可运行的代码阅读起来难度会差很多的。所以多用一点时间搭建一个环境是很值得的，而且我们阅读代码主要是为了修改其中的问题或做移植操作。不能运行的代码除了可以学到一些技术以外，用处有限。 找开始的地方 做什么事情都要知道从那里开始，读程序也不例外。在C语言里,首先要找到main()函数，然后逐层去阅读，其他的程序无论是vb、delphi都要首先找到程序头，否则你是很难分析清楚程序的层次关系。 分层次阅读 在阅读代码的时候不要一头就扎下去，这样往往容易只见树木不见森林，阅读代码比较好的方法有一点象二叉树的广度优先的遍历。在程序主体一般会比较简 单，调用的函数会比较少，根据函数的名字以及层次关系一般可以确定每一个函数的大致用途，将你的理解作为注解写在这些函数的边上。当然很难一次就将全部注 解都写正确，有时候甚至可能是你猜测的结果，不过没有关系这些注解在阅读过程是不断修正的，直到你全部理解了代码为止。一般来说采用逐层阅读的方法可以是 你系统的理解保持在一个正确的方向上。避免一下子扎入到细节的问题上。在分层次阅读的时候要注意一个问题，就是将系统的函数和开发人员编写代码区分开。在 c, c++，java ,delphi中都有自己的系统函数，不要去阅读这些系统函数，除非你要学习他们的编程方法，否则只会浪费你的时间。将系统函数表示出来，注明它们的作用 即可，区分系统函数和自编函数有几个方法，一个是系统函数的编程风格一般会比较好，而自编的函数的编程风格一般比较会比较差。从变量名、行之间的缩进、注 解等方面一般可以分辨出来，另外一个是象ms c6++会在你编程的时候给你生成一大堆文件出来，其中有很多文件是你用不到了，可以根据文件名来区分一下时候是系统函数，最后如果你实在确定不了，那就 用开发系统的帮助系统去查一下函数名，对一下参数等来确定即可。 写注解 写注解是在阅读代码中最重要的一个步骤，在我们阅读的源代码一般来说是我们不熟悉的系统,阅读别人的代码一般会有几个问题，1搞明白别人的编程思想不 是一件很容易的事情，即使你知道这段程序的思路的时候也是一样。2阅读代码的时候代码量一般会比较大，如果不及时写注解往往会造成读明白了后边忘了前边的 现象。3阅读代码的时候难免会出现理解错误，如果没有及时的写注解很难及时的发现这些错误。4不写注解有时候你发生你很难确定一个函数你时候阅读过，它的功能是什么，经常会发生重复阅读、理解的现象。 好了，说一些写注解的基本方法：1猜测的去写，刚开始阅读一个代码的时候，你很难一下子就确定所有的函数的功能，不妨采用采用猜测的方法去写注解，根 据函数的名字、位置写一个大致的注解，当然一般会有错误，但你的注解实际是不但调整的，直到最后你理解了全部代码。2按功能去写，别把注解写成语法说明 书，千万别看到fopen就写打开文件，看到fread就写读数据，这样的注解一点用处都没有，而应该写在此处开发参数配置文件(**。dat)读出 系统初始化参数。。。。。，这样才是有用的注解。3在写注解的使用另外要注意的一个问题是分清楚系统自动生成的代码和用户自 己开发的代码，一般来说没有必要写系统自动生成的代码。象delphi的代码，我们往往要自己编写一些自己的代码段，还要对一些系统自动生成的代码段进行 修改，这些代码在阅读过程是要写注解的，但有一些没有修改过的自动生成的代码就没有必要写注解了。4在主要代码段要写较为详细的注解。有一些函数或类在程 序中起关键的作用，那么要写比较详细的注解。这样对你理解代码有很大的帮助。5对你理解起来比较困难的地方要写详细的注解，在这些地方往往会有一些编程的技巧。不理解这些编程技巧对你以后的理解或移植会有问题。6写中文注解。如果你的英文足够的好，不用看这条了，但很多的人英文实在不怎么样，那就写中文注解吧，我们写注解是为了加快自己的理解速度。中文在大多数的时候比英文更适应中国人。与其写一些谁也看不懂的英文注解还不如不写。 重复阅读 一次就可以将所有的代码都阅读明白的人是没有的。至少我还没有遇到过。反复的去阅读同一段代码有助于得代码的理解。一般来说，在第一次阅读代码的时候 你可以跳过很多一时不明白的代码段，只写一些简单的注解，在以后的重复阅读过程用，你对代码的理解会比上一次理解的更深刻，这样你可以修改那些注解错误的 地方和上一次没有理解的对方。一般来说，对代码阅读3，4次基本可以理解代码的含义和作用。 运行并修改代码 如果你的代码是可运行的，那么先让它运行起来，用单步跟踪的方法来阅读代码，会提高你的代码速度。代码通过看中间变量了解代码的含义,而且对 以后的修改会提供很大的帮助 用自己的代码代替原有代码，看效果，但在之前要保留源代码 600行的一个函数，阅读起来很困难，编程的人不是一个好的习惯。在阅读这个代码的时候将代码进行修改，变成了14个函数。每一个大约是40-50 行左右.]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简析String、StringBuffer与StringBuilder]]></title>
    <url>%2F%E7%AE%80%E6%9E%90String%E3%80%81StringBuffer%E4%B8%8EStringBuilder.html</url>
    <content type="text"><![CDATA[字符串类StringString是final类。Java程序中的所有字符串字面值(如”abc”)都作为此类的实例实现。字符串是常量，它们的值在创建之后不能更改，如果对已经存在的String对象进行修改，都是重新new一个对象，然后把修改后的值保存进去。字符串缓存区支持可变的字符串。因为String对象是不可变的，所以可以共享。字符串的定义很简单，直接给一个字符串类型的变量赋值即可，例如:1String Str = "abc"; 等价于:12char data[] = &#123;'a','b','c'&#125;String str = new String(data); 线程安全的可变字符串类StringBufferStringBuffer 是一个线程安全的可变字符串类，通过构造方法创建对象。类似于String的字符串，不同的是它通过某些方法调用改变该序列的长度和内容。它可将字符串缓存区安全地用于多个线程，可以在必要时对这些方法进行同步。12StringBuffer(); //构造一个空字符串的字符串缓存区StringBuffer(String str); //构造一个字符串缓存区，并将其内容初始化为指定的字符串内容 StringBuffer上的主要操作是append()和insert()方法。通过查看StringBuffer类的三段源码，我们会发现最后调用了System.arraycopy来进行来修改字符串。1234public synchronized StringBuffer append(String str) &#123; super.append(str); return this; &#125; 12345678public AbstractStringBuilder append(String str) &#123; if (str == null) str = "null"; int len = str.length(); ensureCapacityInternal(count + len); str.getChars(0, len, value, count); count += len; return this; &#125; 123456789101112public void getChars(int srcBegin, int srcEnd, char dst[], int dstBegin) &#123; if (srcBegin &lt; 0) &#123; throw new StringIndexOutOfBoundsException(srcBegin); &#125; if (srcEnd &gt; value.length) &#123; throw new StringIndexOutOfBoundsException(srcEnd); &#125; if (srcBegin &gt; srcEnd) &#123; throw new StringIndexOutOfBoundsException(srcEnd - srcBegin); &#125; System.arraycopy(value, srcBegin, dst, dstBegin, srcEnd - srcBegin); &#125; 可变字符串类StringBuilder此类提供一个与StringBuffer兼容的API，但不保证同步。该类被设计用作StringBuffer的一个简易替换，用在字符串缓冲区被单个线程使用的时候。如果可能，建议优先采用该类，因为在大多数实现中，它比StringBuffer要快。 如何选择 String是字符串常量 StringBuffer是字符串变量(线程安全) StringBuilder是字符串变量(非线程安全) 简单地说，String类型和StringBuffer类型的主要性能区别在于: String是不可改变的对象，每次对String类型进行改变的时候，其实都等同于生成了一个新的String对象，然后将引用指向该对象；而对于StringBuffer类，每次操作都是对StringBuffer对象本身进行更改。所以，如果经常改变内容的字符串最好不要用String，因为每次生成对象都会对系统性能产生影响，特别当内存中无引用对象多了以后，JVM的GC就会开始工作，那速度是一定会相当慢的。这种情况推荐使用StringBuffer，特别是字符串对象经常改变的情况下。但是某些特别情况下，String对象的字符串拼接其实是被JVM解释成了StringBuffer对象的拼接，所以这个时候String对象的速度并不会比StringBuffer对象慢，而特别是以下的字符串对象生成中，String效率是远要比StringBuffer快的:12String str = "This is only a " + "simple" + "test";StringBuffer builder = new StringBuilder("This is only a ").append("simple").append("test"); 你会发现，生成str对象的速度明显快多了。其实这是JVM的一个隐藏的实现机制，实际上:1String str = "This is only a " + "simple" + "test"; 其实就是:1String str = "This is only a simple test"; 但是要注意，如果你的字符串是来自另外的String对象，速度就没那么快了,譬如:1234String str1 = "This is only a ";String str2 = "simple";String str3 = "test";String str4 = str1+str2+str3; 为了测试这3种类型当累加不同次数字符串时的效率，我们编写一个测试类，分别按次数累加字符串:1234567891011121314151617181920212223242526272829303132333435363738394041424344public class TestString &#123; static int count = 100; //循环次数 //测试String public static void testString() &#123; long start = System.nanoTime(); String str = ""; for (int i = 0; i &lt; count; i++) &#123; str += "," + i; &#125; long end = System.nanoTime(); System.out.println("String: " + (end - start)); &#125; //测试StringBuffer public static void testStringBuffer() &#123; long start = System.nanoTime(); StringBuffer str = new StringBuffer(); for (int i = 0; i &lt; count; i++) &#123; str.append(",").append(i); &#125; long end = System.nanoTime(); System.out.println("StringBuffer: " + (end - start)); &#125; //测试StringBuilder public static void testStringBuilder() &#123; long start = System.nanoTime(); StringBuilder str = new StringBuilder(); for (int i = 0; i &lt; count; i++) &#123; str = str.append(",").append(i); &#125; long end = System.nanoTime(); System.out.println("StringBuilder: " + (end - start)); &#125; public static void main(String[] args) &#123; TestString.testString(); TestString.testStringBuffer(); TestString.testStringBuilder(); &#125;&#125; 运行该程序执行的测试时间如表所示: 毫微秒 String StringBuffer StringBuilder 1次 38292 32765 1579 10次 52108 45792 6711 100次 230143 70662 64345 1000次 11072907 308305 181193 1万次 400656874 892543 814777 10万次 溢出 4308762 4372318 100万次 溢出 100687270 49812689 String在10万次循环时就溢出了，而StringBuffer在100万次循环时间为100ms，StringBuilder的时间为49ms。显然选择优先级为: StringBuilder &gt; StringBuffer &gt; String 。因此，对于这3个类的使用，我们需要按照以下情况去选择。 如果偶尔对简单的字符串常量进行拼接，那么可以使用String，它足够简单而且轻量级。 如果需要经常进行字符串的拼接、累加操作，请使用StringBuffer或者StringBuilder。 如果是在单线程的环境中，建议使用StringBuilder,它要比StringBuffer快；如果是在多线程的环境中，建议使用StringBuffer，它是线程安全的。因此，StringBuilder实际上是我们的首选，只有在多线程时才可以考虑使用StringBuffer，只有在字符串的拼接足够简单时才使用String。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java基础</tag>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaEE软件开发体系架构]]></title>
    <url>%2FJavaEE%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84.html</url>
    <content type="text"><![CDATA[两层架构传统的客户服务器系统仅只简单地基于两层体系来构建，即客户端（前台）和企业信息系统（后台），没有任何中间件，业务逻辑层与表示层或数据层混在一起。这种两层架构无论从开发、部署、扩展、维护来说，综其只有一个特点——成本高。 三层架构三层架构自上而下将系统分为表示层、逻辑层、持久层。表示层由处理用户交互的客户端组件及其容器所组成；业务逻辑层由解决业务问题的组件组成；数据层由一个或多个数据库组成，并可包含存储过程。这种三层架构，在处理客户端的请求时，使客户端不用进行复杂的数据库处理；透明地为客户端执行许多工作，如查询数据库、执行业务规则和连接现有的应用程序；并且能够帮助开发人员创建适用于企业的大型分布式应用程序。 MVC在MVC模式中，应用程序被划分为模型层（Model）、视图层（View）、控制层（Controller）三部分。MVC模型就是把一个应用程序的开发按照业务逻辑、数据、视图进行分离分层并组织代码。MVC要求把应用的模型按一定的层次规则抽取出来，将业务逻辑聚集到一个部件里面，在改进和个性化定制界面及用户交互的同时，不需要重新编写业务逻辑。模型层负责封装应用的状态，并实现功能，视图层负责将内容呈现给用户，控制层负责控制视图层发送的请求以及程序的流程。Servlet+JSP+JavaBean（MVC）这种模式比较适合开发复杂的web应用，在这种模式下，Servlet负责处理用户请求，JSP负责数据显示，JavaBean负责封装数据。 基于JavaEE架构模式下的MVC在这种架构模式下，模型层（Model）定义了数据模型和业务逻辑。为了将数据访问与业务逻辑分离，降低代码之间的耦合，提高业务精度，模型层又具体划分为了DAO层和业务层，DAO即Data Access Object，其主要职能是将访问数据库的代码封装起来，让这些代码不会在其它层出现或者暴露出来给其它层；业务层是整个系统最核心也是最具有价值的一层，该层封装应用程序的业务逻辑，处理数据，关注客户需求，在业务处理过程中会访问原始数据或产生新数据，DAO层提供的DAO类能很好地帮助业务层完成数据处理，业务层本身侧重于对客户需求的理解和业务规则的适应，总体说来，DAO层不处理业务逻辑，只为业务层提供辅助，完成获取原始数据或持久层数据等操作。JSP：JSP被用来产生Web的动态内容。这层把应用数据以网页的形式呈现给浏览器，然后数据按照在JSP中开发的预定的方式表示出来，这层也可以称之为布局层。Servlet：JSP建立在Servlet之上，Servlet是J2EE的重要组成部分。Servlet负责处理用户请求，Java Web项目的所有配置都写在了web.xml配置文件里，当项目运行的时候，web.xml会将http请求映射给对应的Servlet类。JavaBean：由一些具有私有属性的Java类组成，对外提供get和set方法。JavaBean负责数据，负责处理视图层和业务逻辑之间的通信。Service：业务处理类，对数据进行一些预处理。DAO：数据访问层，JDBC调用存储过程，从数据库（DataBase）那里获取到数据，再封装到Model实体类中去。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaEE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaWeb开发]]></title>
    <url>%2FJavaWeb%E5%AD%A6%E4%B9%A0.html</url>
    <content type="text"><![CDATA[1 JSP3个编译指令 : page,include,taglib2 JSP动作指令7个：jsp:forward ： 执行页面转向，将请求的处理转发到下一个页面jsp:aram : 用于传递参数，必须与其他支持参数的标签一起使用jsp:include ： 用于动态引入一个JSP页面jsp:plugin : 用于下载JavaBean 或 Applet到客户端执行jsp:useBean : 创建一个JavaBean的实例jsp:setProperty : 设置JavaBean 实例的属性值jsp:getProperty : 输出JavaBean 实例的属性值3 include指令include指令是一个动态include指令，也用于包含某个页面，它不会导入被include页面的编译指令，仅仅将被导入页面的body内容插入本页面forward动作指令和include指令的区别：执行forward时，被forward的页面将完全代替原有页面；而执行include时，被include的页面只是插入原有页面。简而言之：forward拿目标页面代替原有页面，而include则拿目标页面插入原有页面。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JavaWeb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Annotations]]></title>
    <url>%2FAnnotations.html</url>
    <content type="text"><![CDATA[原文：You\’ve probably encountered the need to annotate elements of your Java applications by associating metadata (data that describes other data) with them. java has always provided an ad hoc annotation mechanism via the transient reserved word, which lets you annotate fields that are to be excluded during serialization. But it didn\’t offer a standard way to annotate program elements until Java 5.Java 5\’s general annotation mechanism consists of four components:1.An @interface mechanism for declaring annotation types.2.Meta-annotation types, which you can use to identify the application elements to which an annotation type applies; to identify the lifetime of an annotation (an instance of an annotation type); and more.3.Support for annotation processing via an extension to the Java Reflection API, which you can use to discover a program\’s runtime annotations, and the introduction of a generalized tool for processing annotations.4.Standard annotation types.I\’ll explain how to use these components and point out some of the challenges of annotations in the examples that follow.译文：你可能曾遇到过这种需求：通过关联元数据（描述其他数据的数据）来注解你的java应用程序元素。一直以来，java通过transient关键字来提供一种即时注解机制，它可以让你标注一个成员变量在序列化过程中被排除。但是，直到java 5.0版本才提供一种标准的方式来注解程序。Java 5.0常规注解机制包括四部分：1． 声明注解类型@interface机制2． 元注解类型，你可以用来识别注解类型的程序元素应用以及注解的声明周期（注解类型的一个实例）等；3． 通过java反射API的扩展来支持注解处理，你可以用来发现程序运行时的注解，并引入一个泛型注解处理工具。4． 标准的注解类型。接下来，我将通过下面的例子解释如何运用这些机制，并对一些比较有挑战性的注解部分进行标明。]]></content>
      <categories>
        <category>译文</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IE8开发工具“无法附加进程”]]></title>
    <url>%2FIE8%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E2%80%9C%E6%97%A0%E6%B3%95%E9%99%84%E5%8A%A0%E8%BF%9B%E7%A8%8B%E2%80%9D.html</url>
    <content type="text"><![CDATA[问题描述使用IE8开发工具调试时，遇到“无法附加进程，进程可能附加了另一个调试程序”的解决方案本人在遇到此问题时，百度网上的解决方案一般都是重置IE8设置（工具-&gt;Internet选项-&gt;高级-&gt;重置），卸载重装IE8，或者选择别的浏览器进行调试，但是本人的情况是，项目在其他浏览器均可以正常显示，只有IE8以下版本不可以，后面发现是js兼容性问题。我的想法是，通过IE8自带调试工具调试，看看哪里不兼容。后面折腾了好久发现，360浏览器由于用的是IE内核，其开发工具调用的也是IE的开发工具。如果你的系统安装IE9以上版本，可以选择调用IE9以上版本进行开发调试。]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>IE8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis上手入门实例]]></title>
    <url>%2FMyBatis%E4%B8%8A%E6%89%8B%E5%85%A5%E9%97%A8%E5%AE%9E%E4%BE%8B.html</url>
    <content type="text"><![CDATA[MyBatis下载、配置及测试点击下载MyBatis在线中文文档下载之后打开，如图：第一个使我们需要用到的包，pdf文档是MyBatis英文手册，后面两个分别是javadoc文档和源码。在这里我们还需要导入MySQL数据库驱动jar包。官方驱动jar包下载地址这些准备以后，我们开始配置。 打开MyEclipse，导入jar包。在src下新建一个包，并新建配置文件conf.xml。如图： 新建测试数据库mybatis，并新建两条数据。 在Mybatis中定义Mapper信息有两种方式，一种是利用xml写一个对应的包含Mapper信息的配置文件；另一种就是定义一个Mapper接口，然后定义一些相应的操作方法，再辅以相应的操作注解。conf.xml代码，具体参数的解释请看文档。 1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN""http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;transactionManager type="JDBC" /&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="com.mysql.jdbc.Driver" /&gt; &lt;property name="url" value="jdbc:mysql://localhost:3306/mybatis" /&gt; &lt;property name="username" value="root" /&gt; &lt;property name="password" value="root" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource="com/MyBatis/test1/userMapper.xml"/&gt; &lt;mapper class="com.MyBatis.test1.UserMapper1"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 4.新建一个实体类User。 123456789101112131415161718192021222324252627282930313233343536373839package com.MyBatis.test1;public class User &#123; private int id; private String name; private int age; public User() &#123; super(); &#125; public User(int id, String name, int age) &#123; super(); this.id = id; this.name = name; this.age = age; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return "User [id=" + id + ", name=" + name + ", age=" + age + "]"; &#125;&#125; 5.新建userMapper.xml文件，文件参数解释请看文档，代码如下：1234567891011121314151617181920212223&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="com.MyBatis.test1.userMapper"&gt; &lt;select id="getUser" parameterType="int" resultType="com.MyBatis.test1.User"&gt; select * from users where id=#&#123;id&#125; &lt;/select&gt; &lt;select id = "insertUser" parameterType="com.MyBatis.test1.User"&gt; insert into users(name,age) values(#&#123;name&#125;,#&#123;age&#125;); &lt;/select&gt; &lt;delete id="deleteUser" parameterType="int"&gt; delete from users where id=#&#123;id&#125;; &lt;/delete&gt; &lt;update id="updateUser" parameterType="com.MyBatis.test1.User"&gt; update users set name =#&#123;name&#125;,age=#&#123;age&#125; where id = #&#123;id&#125;; &lt;/update&gt; &lt;select id="selectAllUsers" resultType="com.MyBatis.test1.User"&gt; select * from users; &lt;/select&gt;&lt;/mapper&gt; 6.新建 测试类test，此处采用JUnit4测试，然后逐个方法进行测试。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.MyBatis.test1;import java.io.InputStream;import java.util.List;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Test;public class Test1 &#123; String resource = "conf.xml"; InputStream is = Test1.class.getClassLoader().getResourceAsStream(resource); SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(is); SqlSession session = factory.openSession(true); UserMapper1 mapper = session.getMapper(UserMapper1.class); @Test public void insert() &#123; User user = new User(); user.setName("dudefu"); user.setAge(20); String statementInsert = "com.MyBatis.test1.userMapper.insertUser"; session.insert(statementInsert, user); &#125; public void update() &#123; User user = new User(); user.setName("ddf"); user.setAge(20); user.setId(6); String statement = "com.MyBatis.test1.userMapper.updateUser"; session.update(statement, user); &#125; public void selectAllUsers() &#123; String statementSelectAll = "com.MyBatis.test1.userMapper.selectAllUsers"; List&lt;User&gt; selectAllUser = session.selectList(statementSelectAll); System.out.println(selectAllUser); &#125; public void delete() &#123; String statement = "com.MyBatis.test1.userMapper.deleteUser"; int a = session.delete(statement, 10); System.out.println(a); &#125;&#125; 7.采用注解的方式。首先新建接口类userMapper1，此处注意，如果接口类和userMpper.xml在同一个包内，则接口类的名字不能和userMapper.xml的名字相同，否则会报错java.lang.NoClassDefFoundError: com/MyBatis/test1/userMapper (wrong name: com/MyBatis/test1/UserMapper具体原因，启动程序后，程序首先会加载所有文件，这时userMpper.class和userMpper.xml会发生冲突。所以如果在同一个包内需改名，此处改名为userMapper1.class。userMapper1.class代码如下：12345678910111213141516171819202122232425package com.MyBatis.test1;import java.util.List;import org.apache.ibatis.annotations.Delete;import org.apache.ibatis.annotations.Insert;import org.apache.ibatis.annotations.Select;import org.apache.ibatis.annotations.Update;public interface UserMapper1 &#123; @Insert("insert into users(name,age) values(#&#123;name&#125;,#&#123;age&#125;)") public int insertUser(User user); @Delete("delete from users where id=#&#123;id&#125;") public int deleteUserById(int id); @Update("update users set name =#&#123;name&#125;,age=#&#123;age&#125; where id = #&#123;id&#125;") public int updateUser(User user); @Select("select * from users where id=#&#123;id&#125;") public User getUserById(int id); @Select("select * from users") public List&lt;User&gt; getAllUser();&#125; 8.新建测试类test2。代码如下：1234567891011121314151617181920212223242526272829303132333435363738package com.MyBatis.test2;import java.io.InputStream;import java.util.List;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Test;public class Test2 &#123; String resource = "conf.xml"; InputStream is = Test1.class.getClassLoader().getResourceAsStream(resource); SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(is); SqlSession session = factory.openSession(true); UserMapper1 mapper = session.getMapper(UserMapper1.class); public void testInsert()&#123; int insert = mapper.insertUser(new User(-1,"dudefu",23)); System.out.println(insert); &#125; public void testUpdate()&#123; int update = mapper.updateUser(new User(11,"ddf",34)); System.out.println(update); &#125; public void testdelete()&#123; int delete = mapper.deleteUserById(11); System.out.println(delete); &#125; @Test public void selectAllUser()&#123; List&lt;User&gt; selectAllUsers = mapper.getAllUser(); System.out.println(selectAllUsers); &#125;&#125; 按照以上步骤进行操作，测试应该是不报错的。如果出错了，一般出错在 conf.xml中的路径配置，如下段代码； 1234&lt;mappers&gt; &lt;mapper resource=&quot;com/MyBatis/test1/userMapper.xml&quot;/&gt; &lt;mapper class=&quot;com.MyBatis.test1.UserMapper1&quot;/&gt; &lt;/mappers&gt; 和userMapper.xml中。]]></content>
      <categories>
        <category>框架</category>
      </categories>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云Maven仓库地址]]></title>
    <url>%2F%E9%98%BF%E9%87%8C%E4%BA%91Maven%E4%BB%93%E5%BA%93%E5%9C%B0%E5%9D%80.html</url>
    <content type="text"><![CDATA[阿里云Maven仓库地址http://maven.aliyun.com/nexus/#view-repositories;public~browsestorage在maven的settings.xml 文件里配置mirrors的子节点，添加如下mirror123456&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt;]]></content>
      <categories>
        <category>有料</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deepin下搭建基于github和hexo的个人博客]]></title>
    <url>%2Fdeepin%E4%B8%8B%E6%90%AD%E5%BB%BA%E5%9F%BA%E4%BA%8Egithub%E5%92%8Chexo%E7%9A%84%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2.html</url>
    <content type="text"><![CDATA[系统：Linux Deepin 15.4 x64搭建步骤：1、 安装git1$ sudo apt-get install git 查看git版本1$ git version 2、 安装Node.js及npma. 可以直接命令安装,但是命令安装的不是最新版本。12$ sudo apt-get install nodejs$ sudo apt-get install npm b. 本博客采用第二种方法，首先官网下载最新版，然后解压。将node,npm命令设置全局命令：12$ sudo ln -s /home/dudefu/Documents/node-v8.6.0-linux-x64/bin/node /usr/local/bin/node$ sudo ln -s /home/dudefu/Documents/node-v8.6.0-linux-x64/bin/npm /usr/local/bin/npm 查看版本：12$ node -v$ npm -v 3、 安装hexo1$ npm install -g hexo-cli hexo-cli安装路径/home/dudefu/Documents/node-v8.6.0-linux-x64/lib/node_modules/hexo-cli，此时输入命令hexo会提示“未找到命令”，此时要将hexo-cli/bin/文件夹下的hexo命令设置为全局：12$ sudo ln -s /home/dudefu/Documents/node-v8.6.0-linux-x64/lib/node_modules/hexo-cli/bin/$ hexo /usr/local/bin/hexo 再输入hexo命令可以正常显示。创建一个空文件夹，此处名为hexo：12345$ mkdir hexo$ cd hexo$ hexo init .$ npm install $ hexo server -p 5000 到此，hexo安装完毕。浏览器输入本地，前面配置均正确的情况下，正常显示博客首页。4、 hexo配置主要分为两块站点配置和主题配置。此处先下载NexT主题：1$ git clone https://github.com/iissnan/hexo-theme-next themes/next 接下来的详细配置就不细说了，直接看hexo文档，NexT使用文档，主题配置遇到难点的可以访问github：，所有的问题基本可以解决。 我的博客]]></content>
      <categories>
        <category>有料</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>